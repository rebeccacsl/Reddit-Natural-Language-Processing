title,sub_text,id,author,created_utc,score,upvote_ratio,comments_list
"[Question] Please help me figure out this simple probability calculation, I know it's straightforward but I am blanking so hard.","I cannot figure out how to find this through google, I have tried.

Ok, I have a large population and I expect 5 distinct phenotypes within this population in equal proportion (20% of each phenotype). How many people do I need to include in my sample to be 99+% sure that each phenotype is represented in my sample? The actual problem involves thousands of phenotypes but I know the calculation should have the same basis regardless.

Any guidance would be appreciated. My late father (a statistician) would be so disappointed in me rn.",13wyhnk,earlysong,1685568123.0,8,0.9,"['This is known as the coupon collector problem, the details are best see in: https://en.m.wikipedia.org/wiki/Coupon_collector%27s_problem']"
[Q] In what sense is a variance a mean?,"In what sense is a variance a mean?

&#x200B;

I need help urgently as I do not understand this question.",13wx1s5,SGS0108,1685564873.0,0,0.25,"['Compare the formulas for sample mean and sample variance and see if you can spot similarities.', ""Not sure what exactly you're asking, but the Possion distribution has equal mean and variance. If that's what your asking."", 'How do you build the sample variance?', 'When you say ""I do not understand this question""... you\'re doing coursework, right?\n\nIf not, why is it urgent?', 'The variance is the **mean** squared deviation of the observations from the overall mean. \n\nVar(X) = E[(X - )^2]\n\nSo its a way of saying the average distance from the overall average .']"
[Q] How to calculate the likelihood with parameters that vary in time?,"I'm stuck in this for some time. I have a time series \[; y\_t ;\] that follows a Poisson distribution with \[; \\mu\_t ;\] being drawn on every new step. Doing the likelihood I should have  
\[; L(\\theta) = e\^{\\sum \\mu\_t} \\prod \\mu\_t\^{y\_t} ;\]?  
To me, the real problem arises when I'm doing the posteriori, assuming \[; \\mu\_t ;\] is Normal, I have  
\[; p(\\theta | y\_t) = exp(-\\sum \\mu\_t + 1/2((\\mu\_t - \\mu\_N) / \\sigma)\^2) \\prod \\mu\_t \^{y\_t} ;\]  
and I don't know what to do with this lonely \[; \\mu\_t ;\]. Should I iterate this over time and do the sum again every time? Am I interpreting this all wrong?",13wubs6,RopsXuh,1685558508.0,1,1.0,"[""The poisson has one parameter and you model every observation as from a poisson with its separate parameter. Is there anything in your model that makes the data point t=1 carry information about observation t=2? It doesn't seem so. Then you've got basically a separate model for every data point that you can solve separately\n\nI usually see varying parameters in models that have some non-varying parameters or still force some connection between parameters at different points in time (eg, they do a random walk). Then you have to do the joint problem aggregating the data."", 'General answer to the title question, without reference to the specific details of your model: Parameters that vary are not single parameters, but a collection of parameters of some process including the way in which parameters are relating to each other over time. The parameter process might potentially involve additional parameters. Write down the joint density for all data (with all parameters)', 'I was sampling the parameter in terms of past y. Should I still model it separately?']"
[Q] diffusion and Brownian motion in statistics,"Hello 

I am in a stochastic processes class and have just started learning about these topics as we move to Gaussian processes. From my understanding these ideas come from physics to describe the behavior of particles and I am interested what their applications are in statistics? What type of situations are modeled as diffusion processes? Any links to papers or general topics is appreciated!",13wqo3q,medylan,1685549914.0,5,1.0,"[""Have you seen the Galton board demonstration where a lot of ball bearings fall through some pegs and spread out into an approximately normal distribution? That's a physics demonstration of a fundamental statistical concept: The final horizontal position of any individual is the sum of random perturbations on its path. Each particle goes on a random walk from the starting location to it's final position. Because the forces acting on the particles are random and symmetrical, the average position of the particles are the same as the starting location, but the variability (the spread) is determined by how many layers of pegs there are in the board.\n\nWeiner diffusion (ie Brownian motion) is the continuous time version of a random walk. Basically anything which can be modelled as some *time to event X* process, which also has random noise, can be modelled using diffusion equations.\n\nhttps://en.m.wikipedia.org/wiki/First-hitting-time_model"", 'Stochastic processes are widely used in finance, for example to model stock prices and futures prices, and hence to price options and other financial derivatives. \n\nNot the only application, naturally.', 'Ive heard that Brownian motion can be used to describe the motion of small particulates through a face mask, such as moisture droplets with COVID viral particles in them. I never did a deeper dive into the topic,  it its definitely a currently relevant area of interest.', 'I dont know of applications of diffusion processes in general in statistics.\n\nBut I know the Kolmogorov Smirnov statistic for a hypothesis test on the CDF of a distribution is defined in terms of a brownian motion.', 'Also widely used in cognitive science and neuroscience to describe evidence accumulation.', ""It's the motion of all particles through any medium.""]"
[Education] advice on brushing up over the summer,"I'm a rising sophomore and my freshman year I kind of slacked despite wanting to be a statistics major. The only major courses I took were Calc 1 (A) and Intro to Java (A), (also intro to econ). My school has a ton of required courses so I got some of those out the way. However, this year I am looking to be a bit more serious with progressing with my major and want to take courses in statistical machine learning by senior year.  
  
My statistics schedule is this - Probability Theory, Calculus-statistics, and linear regression analysis.  
  
Probability theory will probably be the hardest course (I took an intro stat in high school so calc stat should be easier), so I am thinking of getting a textbook or doing an online course to brush up on some of the formulas and statistical thinking.  
  
Linear regression analysis is the second course in an applied statistics track, but I already know some R and pandas so I think I should be fine, but was looking for some sort of online course or textbook to get me up to speed in all areas and start working on some of my own projects.  
  
Are there any recommendations for methods to progress my studies over the summer? I make online income \~40hrs/wk but I have a lot of free time this summer to study",13wpyfx,Dry_Shine3545,1685548331.0,1,0.67,[]
[Q]Evaluation of photo quality blind test.,"Dear statistics community,

I recently stumbled upon a community driven smartphone camera blind test. The users were presented several pictures taken with 9 different smartphones and had to choose, which shot they preferred the most. They had to this for 10 daytime scenes and 5 night scenes individually. There was no ranking asked from the user, just the ""favorite"" shot. The order of smartphones was randomized and of course blinded.

chi-sqare test because I'm dealing with frequencies. However, since the separate scenes cannot be treated like levels of the dependent variable I transformed the votes into ranks and then treated these ranks as the levels of the dependent variable (so smartphone A was 4 times ""most favorite"" 5 times ""2nd favorite"" etc. So in the end I have a 9x9 contingency table.

Calculation of chi-square gave me 103.2 (df = 64) and p = 0.0014. So now I know there is an association, meaning that the placement is associated with the type of smartphone. How should I proceed to find out which smartphone is ""the most favorite"". My first idea was to compare every phone against each other and correct the resulting p-values with the Bonferroni procedure (36 comparisons). 

I'm also somewhat concerned that the levels of the dependent variable (rank) are not independent of each other. Because a phone that was regularly scoring rank1 is also more likely to score more often rank2. These levels are not on a nominal but ordinal scale. Is there maybe a better approach to handle this?

Thank you for the input.",13wmzyv,Setsuna04,1685541334.0,1,1.0,[]
[Q] Which test should I use if I suspect someone is lying about dice (in a weird way)?,"Hello! I have the following problem.

Someone claims to be generating random numbers by throwing a fair 20-sided die. However, when I asked them for the outcomes 20 million rolls, they gave me a list of numbers where each number from 1 to 20 showed up exactly 1 million times. What kind of test can I use/experiment can I design to reject the hypothesis that they really are rolling a fair die?

I've tried to find an answer to this by google and reading old posts, but I suppose I don't know how to phrase my question in a way that leads me to an answer. Most of the articles/posts I find are about using the chi squared test to reject the hypothesis that the die is fair, but that isn't really my problem; my problem is that the data are *too* uniform.

I guess this is the sort of pattern that happens when people try to falsify data by picking numbers that ""look"" random, and I'd like to know how to look for it in a more precise way than saying ""this looks fishy"".

Thanks!",13wl1zy,TonicAndDjinn,1685536501.0,30,1.0,"[""There's a couple different ways to get at this... ultimately you are talking about entropy estimation though. \n\nSo a long but easy and effective way can be asking her to generate 20-50 rolls 50-100 times. (Assuming this is for a TTRPG, you can hide this motive in game). A true entropy source will generate close to a perfect mean (mean = median of total options) only in a large data set. But in SMALL data sets the mean will not be close to the perfect mean. \nSo you can take the general descriptive statistics and measure the variance between them.\n\nOn the more complicated but quicker end, you can take your current data set, hop it into a CSV file, and run it through an entropy estimation test, or even look at the min-entropy level (both of these you can look up premade python and r code for). These tests will give you the likelihood (with error estimates) that the results came from a truly random source or if it was more likely generated by an algorithm (which also applies to humans trying to fake random)"", 'I just implemented randomness testing using chi squared test. Donald Knuth goes into some detail about it in his Art of Computer Programming.', 'Ask for a second set of 20 million rolls.\n\nSee if it also shows exactly 1 million of each entry again.\n\nUse /u/-Hazel_s method of computing the exact probability. And then square it.\n\nBlind hunch I have: do any numbers repeat in the sequence? I wonder if this guy is just randomly shuffling the order. Look for sets of 3 of the same number within 20 spaces of each other lol.', 'This is similar to a (possibly made up) story about a professor asking students to flip a coin 100 times and record the results. I think the professor spotted students who made up the flips by looking at the autocorrelation of their series of flips. Real flips should be completely independent while made up flips tended to be negatively autocorrelated. You can probably extend this to your d20 example.\n\nThere are probably also tests around sampling different subsets of the data. For example, pick some event that has a 1% probability when you sample a block of 1000 data points. If the data are iid this should occur in 1% of your blocks. If the data are not iid it will probably occur less than 1% (or never). No idea how you formalize this, would be interested if someone knew more.', 'One possibility that I haven\'t seen discussed is to look for temporal dependencies between subsequent rolls (assuming that you get the read-out of the numbers in the order that they are rolled). \n\nIn that case, you could ask something like ""what is the mutual information between roll i and roll i+1. If the dice is fair, then each roll should be a random draw and so the temporal MI should be ~0 bit (although make sure you correct for the upward bias in MI). On the flip side, if you *do* see some statistically significant temporal dependency, then you can rule out the possibility that each roll is i.i.d. If you thought that you friends was writing down strings of numbers that they thought ""looked"" random, then you might expect some temporal MI, since their choice of what number to write next would be influenced by the past few numbers. \n\nI should note that, while a significant temporal MI would (imo) be enough to rule out a fair die, the *lack* of a significant temporal MI would not be enough to show that the die is fair. \n\nLook up ""active information storage"" for more on this.', 'Theres a few ways to do it, depending on what you suspect to be happening, but with one caveat. We can only really talk about things being unlikely to be random by observing the output, whether were talking about biased or oddly uniform dice rolls. The only way to prove without a doubt that the series didnt come from that die based on its output would be if it had 0s or 21s as results, for example. \n\nIf we assume its suspect by being oddly uniform, one way for it to be this perfect is to be a repeating pattern that has a total length of 20m divided by its prime factorials. I.e. that the pattern repeats over a length 10m or 4m. If you can show a repeating pattern then its highly unlikely to be random. We only have to compare two sets of numbers to rule this out, given how patterns repeat. If this condition doesnt hold, its possible that theres still a more complicated pattern but checking for those would really only make sense to do with a computer. \n\nIf you dont have the series and just the subtotals, we cant really say anything useful. Its technically possible, and is equally likely (or unlikely) as any other set of outcomes. \n\nAlso no computer generated series is truly random (yet) so maybe thats the easiest excuse.', ""That's easy but it's way too long because the exponents are too big. So I'll simplify it a bit. We can use the binomial theorem if you have two outcomes x and not x. In the same fashion you can use a trinomial expansion to characterize something with 3 outcomes. Now, imagine doing 9 rolls on a 3 sided die and the outcome is 3 for 1, 3 for 2 and 3 for 3 in no particular order. How would we calculate the probability of such an outcome? Just like in binomial theorem but generalized to higher terms the probability = (9!/3!3!3!) (1/3)^3 (1/3)^3 (1/3)^3\n 0.085\n\nSo for a 3 sided die the chance of a completely uniform outcome after 9 rolls is just about 8.5%\n\nYou can easily extend this idea to 20 terms but im too lazy for that."", ""You can be quite sure someone *isn't* actually throwing a 20-sided die: (i) a physical die would wear out so much that its properties would be severely altered, well before 20 million throws;  (ii)  at one throw every six seconds (given that it has to be thrown and recorded and the results collected), at 8 hours a day, 5 days a week it would take *16 years* to complete.\n\nIt's not a plausible claim.\n\nBut let's imagine instead that you don't literally mean they're throwing a die and the number is generated by some more rapid and less ephemeral mechanism (i.e. where its properties don't change so quickly), like a pseudo-random number generator.\n\nYour suspicion -- and hence what you want to look for (i.e. the hypothesis you want to test, which is presumably that the data are artificially even) -- was determined from the data you want to run the test on. That form of cherry-picking (notice the weirdest thing about a data set, and test that thing as if it was a hypothesis specified in advance) *really* screws with the behavior of p-values, amounting to a form of p-hacking.\n\nhttps://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data\n\n\nSo if you insist on hypothesis testing *either* \n\n1. you need a separate data set from the one that led to the specific kind of non-randomness you want to test for\n\n   [If you can get them to generate a new set of data (a much smaller one would do - a few hundred should easily suffice if you believe there would be equality a second time), you can test that hypothesis. If the numbers were equal a second time, you can be sure they're not generated independently; the p-value is infinitesimal.]  \n\n    \\- OR -\n\n2. find some way to adjust the p-value for the cherry picking. In some situations this is actually possible, but I don't see a way to do it here; there's no obvious boundary on what things you *might* have noticed were weird.\n\nThat said, in the large data set, what happened would be *such* a surprising thing thing to happen with independent data that you should  quite reasonably decide that something was definitely up. Just don't bother with p-values, because they'll be meaningless on the original data."", '[deleted]', 'I guess what you would want to test is the ""iid\'ness"" of each draw; otherwise, basically, you are within the system so just the fact that the probability of obtaining this outcome is low doesn\'t mean anything. It looks like they are not iid draws as that would most likely lead to some deviation. If you have the indices, you can use a time series sort of approach to test for that.', 'I am assuming what you have is the sample, and you want to check if it was actually generated from the 20 sided dice. For that you could use a randomness test, and that means looking into paterns to determine how ""random"" the sample is. I remember the runs test for checking ordinal data, so that might be enough. You could also sample from a fair 20 sided dice and compare the 2 empirical distributions. If your hunch is right and your friend made the sample manually, then you might catch on that. \n\nHowever, to know if the dice is fair or not you\'d have to get a real iid sample that you can trust and then do the standard chi square test.', 'You can check sample randomness hypothesis.', 'I think common sense in this situation it\'s the best tool you have. Physically it\'s near impossible to build a completely fair dice, casinos tries every dice in expensive machines and the ones that are ""too much"" biased are discharged. In this situation after 20M launches done somehow in a not controlled and carefully built setting you are obtaining the exact pure result you would expect in a mental experiment.', 'Thanks, I\'ll look into the topic. Can you recommend a resource beyond the wikipedia article on ""entropy estimation""? (My mathematical background is pretty solid but I don\'t have much in statistics, fwiw.)', ""Okay, thanks. I'll look into it. I suppose I was getting distracted by too many people writing about the chi squared test to check if the distribution is uniform rather than independent."", ""I think my friend might figure out I'm suspicious and this time give me 999,999 1's and 1,000,001 20's, so I'd like something a bit more robust.\n\nShorter subsequences are also too uniform, but I did find three 9's too close together pretty early in the sequence."", ""It's not equally likely as any other outcome. It's maximally likely when discussing totals"", 'So let me make this a bit more precise. Let\'s suppose I can see the sequence of numbers, not just the frequencies, and that I can ask for more data after I design the experiment.\n\nMy hypothesis is that my friend is generating the numbers in her head, and trying to make them look uniform. Because humans are bad at deciding what looks uniform, she tends to ""over-correct"" when things deviate from the mean; so if 1 has not occurred in the last 30 numbers she is quite a bit more than 5% likely to say it, and if 1 has occurred 5 times in the last 30 numbers she is almost guaranteed not to say it. Or perhaps she really is rolling a die, but if she sees a number which has come up too often lately she ignores it and re-rolls.\n\nThe null hypothesis is that she really is rolling a die to generate the numbers. What test can I use to gain confidence that she really is fixing the numbers?', 'If someone kept a list of the values 1-20, and then used a random number generator to pick from that list, but taking out the picked number until all were exhausted, and then starting over until 20 million outcomes were obtained, they would get OPs result. There would be 20! number of combinations, so there would be no detectable repeating pattern.\n\nI suppose to check for that scenario you could see if you can predict every 20th digit, or multiples of it, like every 40th outcome.', 'I agree that I can compute explicitly the probability of seeing the result I saw and that it would be quite small, but that alone isn\'t enough to reject the hypothesis that it does come from rolling a die. After all, it is still more likely than any other particular outcome.\n\nThere\'s a vague sense in which the result of the experiment ""should"" be at least some distance from the mean; I believe (but haven\'t worked out the numbers carefully) that the expected difference between the observed number of fives and the expected number of fives should be proportional to the square root of the number of trials.', ""> You can be quite sure someone isn't actually throwing a 20-sided die: (i) a physical die would wear out so much that its properties would be severely altered, well before 20 million throws; (ii) at one throw every six seconds (given that it has to be thrown and recorded and the results collected), at 8 hours a day, 5 days a week it would take 16 years to complete.\n> \n> It's not a plausible claim.\n\nMy friend has very high quality dice, and can roll them and record the results very quickly. ;)\n\nBut more seriously, yes, I want to falsify the claim that some process is producing iid samples of a uniform distribution on 20 points.\n\n> you need a separate data set from the one that led to the specific kind of non-randomness you want to test for [If you can get them to generate a new set of data (a much smaller one would do - a few hundred should easily suffice if you believe there would be equality a second time), you can test that hypothesis. If the numbers were equal a second time, you can be sure they're not generated independently; the p-value is infinitesimal.]\n\nFortunately my friend is willing to produce more samples on demand. She *really* likes rolling dice. Or at least, she claims to; I'm not convinced that's what she's doing. But I want to figure out the right way to set up the test before I ask her to."", '> ctrl-f\n\n> ""efrique""\n\n> Yep he already made the same point I was going to make\n\nThis always happens on all the fun threads', ""The probability density of each outcome is binomial with p=1/20. You don't have to simulate it. \n\nAlso, at such large sample sizes, you would probably be asking more about your RNG of choice than about mathematics if you resorted to simulation, which in this case you have no reason to.\n\nThis doesn't answer the OP question though."", '> You could also sample from a fair 20 sided dice and compare the 2 empirical distributions. If your hunch is right and your friend made the sample manually, then you might catch on that.\n\nI mean I tried it and when you sample a d20 twenty million times you don\'t wind up with an exactly even distribution of outputs so it\'s pretty suspicious. But I\'d like something with a bit more formality than ""this is pretty suspicious"".\n\n> However, to know if the dice is fair or not you\'d have to get a real iid sample that you can trust and then do the standard chi square test.\n\nThe problem is with independence, not with bias. The chi squared test just tells me ""wow these things really do look uniform"", far more so than the outcomes of an *actual* iid uniform sample.', 'Sure, so going through my library and cross-referencing with sites like libgen (to make sure they\'re easily accessible), I can recommend :\n\n>Maximum Entropy and Bayesian Spectral Analysis and Estimation Problems (1983) by E.T.Jaynes\n\nAnd\n\n>A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications (2000) Edited by the National Institute of Standards and Technology (Tech Administration of US Department of Commerce) \n\nAlso I just found: \n\n>Random Number Generators, Principals, and Practices (2018) By David Johnston \n\nBased on an article he published I would expect this book to help...\n\n\n\nFor the most part the research in this field is dominated by cryptology since their business is attempting to hide and identify messages in seemingly random ""noise"". So for further research I\'d recommend pointing yourself there. \n\nAs far as accessible codes to run the equations, GitHub should have a number of them floating around.', ""Lol with 20M launches the differences should be in order of hundreds not just 1-2 less or more. Ask for another sequence. If they are again too fair it's obvious that are generated from a shuffled list"", 'Im not sure what differentiation youre making, could you elaborate?\n\nI was seeing an event as a set of 20 numbers that totaled to 20m which should be fine when extended to a sigma algebra.', ""by the sounds of it, this person is providing you the numbers in the order in which they were rolled.  if they were to simply randomize the order at the end, i feel this would change things.  if we are to assume the order to be correct, and that these rolls are made individually back-to-back (with a million rolls, i'd assume use of some coded simulation that provides the dataset instantly), then you could try doing a scatter plot of the 20 million rolls with x,y points (roll #, # that was rolled), like (1, 14), (2, 20), (3, 1), etc. and inspect visually for any patterns or deterrence from what random noise would look like.  \n\ni feel it would be easier to prove tampering after they provide multiple datasets.  if they all have equal roll totals, then that's suspecterer.  if there's 20 million rolls with a d20, then the total *expected* sum of each dataset would be 20mil*10.5=250mil"", ""In statistical thermodynamics there is a concept of microstates vs macrostates. A microstate is the exact outcome, but a macrostate is a set of microstates with a shared value for some characteristic of the microstates. For example, the microstates with an equal sum of squared difference in frequencies from the expected frequencies of each outcome could be a macrostate. I think for you, while all equal outcomes is the most likely microstate, it may not be the most likely macrostate. Figuring out how likely your friend's macrostate is compared to more likely macrostates would be my first approach."", ""Isn't it just the number of uniformly distributed outcomes/ total number of outcomes? I think you might be overthinking this. The problem lies when you compare a collection of outcomes to the chance of getting a particular outcome. Sure it's higher but there are way more non-uniform outcomes than there are uniform ones."", 'Well if the problem is indepence the runs test should work, or any of the randomess measures could work to have something more than ""this seems odd"". \n\nhttps://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test this test is for 2 categories but I believe it can be generalized (haven\'t looked at this in years)\n\nOtherwise there are a bunch of randomess metrics \nhttps://en.wikipedia.org/wiki/Randomness_test', ""Thanks, I'll check them out!"", 'Well, yeah, I agree. But my point is I need to figure out what ""too fair"" means *before* I ask for that second list.', 'I was just correcting this portion. \n\n> If you dont have the series and just the subtotals, we cant really say anything useful. Its technically possible, and is equally likely (or unlikely) as any other set of outcomes.\n\nGetting exactly the mean of the multinomial distribution is not equally likely as any other set of outcomes.\n\nAny exact series is equally likely but this sentence is about if you only have subtotals.\n\nSorry, quoting it in my first comment would have been more clear.', 'Adjacent rolls probably avoid the diagonal more than they should (I don\'t have the data handy so I can\'t check right now). That\'s a good idea for something to look for.\n\nBut is there a more empirical thing I can look for than ""these don\'t land on the diagonal enough""? That\'s still a bit fuzzy for me.', 'In some sense it\'s a macrostate that has made me suspicious: I saw an equal number of each result rather than something ""close to but not exactly"" the mean. But if I phrase the macrostate just as a point in R^20 recording the frequency of each output, her macrostate is technically the most likely.\n\nSo how should I define my macroscopic properties so that the next time I ask for data I can catch her?', ""Absolutely! But among all possible profiles of frequencies of each number -- the statistic I'm looking at -- it's still the most likely outcome if this is coming from a die.\n\nI can't accuse someone of fixing the numbers just because the result they got was unlikely, especially if that result is also the most likely for the particular statistic I'm using. So my question is, what experiment can I design so that, if they are cheating, I can catch them the next time I ask for data?"", ""Thanks, I'll check them out!"", 'Search the technical data sheet of the best 20 sided dices manufacturer in the world. Look at the guaranteed tolerance they have. Calculate the credible interval of your data, I guess the dice they are using is way too fair than the best dice you can buy.', '> then you could try doing a scatter plot of the 20 million rolls with x,y points (roll #, # that was rolled), like (1, 14), (2, 20), (3, 1), etc. and inspect visually for any patterns or deterrence from what random noise would look like. \n\nThe more rigorous way of testing this would be to compute the mutual information between successive rolls (or between sets of successive rolls) and see if it is significantly different from 0 bit (after correcting for the upward bias in discrete MI estimators.\n\nYou could significance test with a permutation null.', ""The most straightforward macrostate is just the distribution of the values' counts, ignoring which value each count is for. There's only one way to have the same number of each outcome, so it's a very unlikely macrostate. There are n choose 2 different ways to be one off even for two values, so it is a more likely macrostate."", ""I don't get it. The result is improbable since there are more non uniform results than there are uniform ones correct? Then what exactly do you mean by it's also the most likely result? Are you confusing mean with probabilities?"", ""> I guess the dice they are using is way too fair than the best dice you can buy.\n\nBut that's not the problem. Even if you performed this experiment theoretically, in a pure mathematical framework, with an honest iid sequence of uniform random variables, it's a fishy result. There's not enough variance around the mean."", 'I mean that it\'s more likely to get 1 million of each number than, say, 1,000,001 20\'s and 999,999 1\'s. So among all profiles of ""counts of each number"" I saw the most likely single outcome.\n\nBut I guess that the point is it\'s not enough to say ""this is fishy because the one specific thing I saw was unlikley"". Like I (claim I) just ran 20,000,000 random numbers in python, and saw the following frequencies:\n\n    1: 1001180\n    2: 1000521\n    3: 1000835\n    4: 999321\n    5: 1000549\n    6: 1000328\n    7: 999404\n    8: 1000370\n    9: 999813\n    10: 1000693\n    11: 999180\n    12: 998473\n    13: 999508\n    14: 1000471\n    15: 1001807\n    16: 1000196\n    17: 997712\n    18: 999265\n    19: 1000368\n    20: 1000006\n\nThose particular frequencies are *even less likely* than getting 1,000,000 of each, but (at least from my point of view) less suspicious.']"
[Q] Self-studying statistics and data analysis. How to theory into praxis?,I'm learning about statistics in my free time. What are the ways I can utilize my theoretical knowledge to gain greater proficiency at applying statistics in praxis?,13wkm2e,Ok-Talk-4303,1685535364.0,1,1.0,"['I dont understand what praxis is?', 'Well the theoretical part is the hard part for most people', 'Literally to do. Start with simple real problems and work up to what you want to be able to do.\n\nLike the old joke that started *Hey, how do I get to Carnegie Hall?*', 'The act of doing something opposed to theorizing about it', 'read ""praxis"" as ""practice"" and you\'ll usually get the right sense of its meaning', 'Thats the first time I have heard this word. I knew it was related to practice from the sentence but I though you made a spelling mistake lol']"
[Q] Generalized Linear Model,How to get p-trend in GLM when using a categorical variable (I am using SPSS).,13wil4j,Verdictologist,1685529596.0,0,0.33,"['What\'s a ""p-trend""? Do you have a link to the definition of this term?']"
"[Q] In a regression model, does the covariance between the regressors and the error term depend on the distribution of the regressor?","Hi,

as you know in order to apply OLS there shouldn't be any correlation/covariance between the error term and the regressors, otherwise you should apply other methods like 2SLS.

The thing that makes me confused is that sometime the regressor are not random variables: the main example of this is if you want to do a regression on a time series using as regressor the time (I know it's not the best method, but methodologically there shouldn't be anything wrong).

In this case, the single values of the regressor are not random variables, or you can imagine them as degenerate random variables, in any case the correlation between them and everything else should be 0 since they are degenerate RVs.

My first idea to approach this problem was to conclude that if you perform a test for correlation between error terms and the regressor and the test outcome is significance, then it means you have generally mispecified the model.

But then I thought that in this time series scenario you could hypothethically assign any distribution you want to the X and find an actual theorethical probabilistic correlation between the X and the error terms.

For example, to stick on with this example, we could assume that the instants t that we'll go to observe are generally normally distributed, and in this case there could be a correlation with the error terms.

But ""pragmatically"" we can also study the time series by ""choosing"" the times t we are going to observe (like choosing the classical first n integer numbers) since all this regression stuff is the study of conditional expected values and variances.

But of course all of this works only if the covariance doesn't depend on the distribution of the regressor, which is my question, excluding limit scenarios where variables are RVs of course.

Hope to have clarified the problem, thanks to everybody.",13wfy1b,Cawuth,1685520262.0,1,1.0,"[""Distribution of the regressor doesn't directly affect this covariance. The covariance is more about the relationship between the error term and the regressors, not the distribution of the regressors themselves. Also, take into account that distribution of the regressors can indirectly influence that relationship. For example, if the distribution of the regressors is such that it leads to a systematic pattern in the error term, then it could violate the assumption of zero covariance. If you can, perform a test for correlation between error terms and the regressor and test outcome is significant, it could indeed suggest that the model is misspecified. Another angle to this, could also be due to other reasons like omitted variable bias or measurement error. Hope this make sense"", 'You cant measure or observe or test the error terms. Not sure how time is not a random variable-of course it is!!!', 'Time could be or not viewed as a random variable. In time series, time is just an index, observations of the ""dependent"" variable (Y) are INDEXED on time. If there is some correlation between observations (Y variable) that depends on time, then you have problems (independence between observations does not hold) and inference assumptions of OLS are violated. You should note that here I am talking about correlation between observations as a function of time, not correlation of time and Y or residuals and time.  \nThe same happens on spatial data. There, geographic coordinates are not viewed as random variables but indexes on which other variables can have correlation based on distance between each other.', "">Yeah appoligies, maybe I have misintepreted the message, but as for time being a random variable, it depends on the context. In many statistical models, time is treated as a fixed variable because it's deterministic and progresses in a predictable manner. However, in some contexts, the specific points in time at which observations are made can be considered random, no?"", 'Can you give an example of one of these models where time is not a RV? Opposite of my experience. Treat it like any other variable.', 'In stochastic process, time is an index, not a RV']"
[Q] Significance of 1 in the Prediction Interval for a New Response,"I have a question regarding the prediction interval formula in linear regression.

I understand that the prediction interval has to take into account the additional variation in the response variable in its calculation compared to the CI for Mean Response.

However, I am confused about why 1 is added to the denominator.

I found proofs online that helped break it down nicely, but I still do not understand why the extra term is 1, other than it accounts for the extra uncertainty in the prediction.

Is there some fundamental mathematical or statistical principle that I am not understanding?

References: 
https://online.stat.psu.edu/stat501/lesson/3/3.3

https://online.stat.psu.edu/stat415/lesson/8/8.2",13wd9jr,Ry_Guy-85,1685510599.0,1,1.0,"[""Keep in mind what's outside the square root; in particular, the estimate of sigma.\n\nIf you square the term for the standard-deviation  of the prediction, and bring the s^2 in you'll get one term for the process noise and one for the uncertainty in the prediction of the mean.\n\nThe first of those corresponds to the 1."", 'The variance of the prediction is the variance of the confidence interval plus an independent term, which Is the variance associated to the error (sigma squared)']"
"Incorporating Quarterly, Monthly, and Daily Data Together [R]","Hello. I'm trying to create a LSTM that that takes in various inputs such as opening price, interest rate, consumer sentiment index, and EPS to predict the closing price of a stock. However, the problem is that some of these variables are recorded on a monthly basis and quarterly basis and I'm sure of how to incorporate all these different kinds of data together. 

  
For some more information regarding my model, the model will take 3 months worth of data to predict 7 days of the closing price of APPL stock. This is for a project. Thank you for all your help.",13watjq,Successful-Fee4220,1685503179.0,1,1.0,"[""I do not believe what you're trying to do is possible with monthly data, but there are probably more creative minds on this sub than mine""]"
MS statistics prior to PhD statistics preparation [Q],"Im going to be attending a funded terminal MS program in statistics in the fall. The department and school are quite small, and doesnt have a PhD program, so the program emphasizes coursework such that it places students into PhD programs. The coursework covers the casella Berger sequence and then linear model theory. So essentially the first year-two years of PhD program. There is also a MS thesis involved. 

My question is, for those of you who did an MS in Statistics elsewhere, reapplied and got into a PhD program in statistics. How much did the coursework in the MS prepare you? Did you feel like you were breezing through the first few courses? Or not really? How much did it help/hurt you when taking the quals?",13w6m89,Direct-Touch469,1685491749.0,22,0.95,"['Every school is different. Solving all the problems in Casella and Berger definitely helps but youll later encounter linear models, estimability, all sorts of linear algebra tricks, different estimation algorithms and deriving the algorithm for your specific setting, matrix differential calculus, measure theory, and last but not least the nitty gritty details of statistical methodology development. A good statistician also seamlessly goes back and forth between mathematical expressions and what their physical implications are, for example, how random effects affect the interpretation of the different modeling components and how they translate in clinical trials terms.\n\nPhD programs try to teach you all this to some extent but different programs will have different foci and strengths. I did very well in masters courses but still spent a lot of time solving stuff in PhD courses. No amount of masters level coursework can prepare you for the entirety of PhD courses. Youre gonna have to face difficulties head-on as you go. But dont worry: its doable! Weve all done it', 'I see. Thats good to know. Seems like it is still an opportunity to learn more. \n\nAlso, does a good performance in an MS make you more competitive for PhD programs? Can it make up for a sub par undergrad? I didnt get into any PhD programs this cycle. Im hoping the MS can boost my chances in a year or so.', 'A good performance in a masters program definitely helps! Getting involved in research helps even more.\n\nHey the thing is, a lot of schools stopped accepting as many students this year as they did prepandemic because of shrinking cash flow. And some students didnt/couldnt join the program during Covid and are now joining, taking up some spots that couldve otherwise gone to newly applying students. Its not entirely your fault you didnt get in this year', 'I see. I guess that makes sense. So right now Im rewriting or editing some of my statements. I realized, for my MS thesis, I wont actually defend it till spring of when I graduate, so I actually wont have it completely done by the time I apply. Can I still talk about it in my statement?']"
[Q] Mac-Compatible Symbol Reference,"I'm currently working on my graduate stats certificate, and taking 2 courses this summer in which the professor prefers typed homework vs. handwritten. For my Mac users, do you have a go-to reference for statistics symbols to insert into Word, Excel, etc.? I'm spending way too much time looking for a sigma hat that's compatible vs. completing my actual assignment.
Thank you from a science grad student that went rogue and added this cert :)",13w4t8i,LehrMoo007,1685487224.0,1,1.0,"['Just use overleaf (i.e. LaTeX)', ""Write it in markdown if you're using RStudio."", ""There's two good suggestions here already. \n\nIn a pinch (though long in the past now) - where there was no better alternative - I've occasionally made a suitable image of a symbol to paste inline. It's not a long term solution, but if you're racing against time and just need one more symbol to look okay, it can get you over the line."", 'Ah I didnt know this existed. I briefly checked it out and looks like this is exactly what Im looking for. Thank you!', 'Thank you! Is there a similar option in SAS that can be used? Something in-house would be super convenient.', 'Ive done the same for previous courses, but it was so tedious. I was going to resort to that but wanted to see if there were any better options. Thankful for this sub!']"
"[q] If the MLE of the Poisson distribution is the sample mean, why do we even need numerical methods to solve it?","If the MLE of the Poisson distribution is the sample mean, why do we even need numerical methods to solve it?  
  
I did out the proof showing MLE is the sample mean. Therefore, if I was modeling height by shoe size, for example, I could just take the average of all shoe sizes to estimate height.  
  
I get this might be more complicated for more variables. If I was modeling height by shoe size and hand size for example. But since each explanatory variable has its own MLE, wouldn't I just take the sample mean for each feature?",13w2etu,Significant-Work-204,1685481444.0,0,0.5,"[""You're confused \n\nYou computed the mle for an intercept-only model. If you want to find the mle of the parameters for a more complicated glm you gave to work the mle for that out.\n\nIt's usually not available in closed form.\n\nYour strategy doesn't even work for ordinary regression in general let alone Poisson regression\n\nThere are some special cases *such as a one-way type model* where the average of the response within groups is relevant"", 'In the simplest case, yes you are right that we can take a sample, find the sample mean then use that to create our Poisson distribution and ask questions like whats the probability X is greater than 5?\n\nHowever, what if your estimate of the mean has some uncertainty in it for some reason? What if our sample size is small so there is significant uncertainty in the sample mean being a good estimate? Perhaps there are unknown factors that we may not be modeling but we still want that uncertainty to flow through to our calculation of the probability of X being greater than 5?\n\nThis is where we would use some numerical methods to incorporate the uncertainty of the mean of the population into our final distribution. So if the mean of the population has a distribution due to our uncertainty then the final distribution has different properties.\n\nThe Wikipedia page on conjugate prior has a good explanation in the practical example section. The MLE is the most likely distribution to have generated our data but other distributions could have generated our data and Bayesian methods would create a posterior distribution over all possible parameters weighted by their probability given our data.', ""As you said, the MLE of the Poisson distribution is the sample mean, and in that case you don't need a numerical method. You need numerical methods when stuff gets way more complicated, like in logistic regression."", 'I thought to find the MLE, you take the log of the PMF and then the derivative, which you then set equal to 0 and solve for lambda.  Doing that simplified to lambda= sample mean?', 'I presume you\'ve seen regression done. Consider that the MLE of the mean of a normal Y is also the sample mean, but in multiple regression the parameter estimates of the coefficients *are not just sample means*. Consider that this implies that your reasoning is flawed.\n\nYou solved the case where your predictor is just ""1"", an intercept-only model, but you wrote about modelling a relationship between two variables (using height to predict shoe size as an example, though of course shoe size is not a count but a binned/interval-censored continuous variable and the Poisson would not be a suitable model for that)\n\nYou have some model for the conditional mean of Y as a function of some x, something like Y|x ~ F(x), and more specifically E[Y|x] as some function of x.  \n\nYou would write the likelihood of the data in terms of the parameters of that function and solve for those. e.g. if you fitted a straight line (though that\'s not the usual default with the Poisson for fairly obvious reasons), you\'d have \n\nE[Y|x] = +x.  \n\nand so the likelihood for Poisson Y\'s in that case would be \n\n exp(-) ^(y)/y!   \n =  exp(-(+x)) (+x)^(y)/y!\n\nfor which you then find the arguments (, ) which maximize that expression to get their MLEs\n\nIn some very simple cases, means of y\'s and x\'s will come into it. e.g. a 0-intercept straight-line model will yield the ratio of the mean y to the mean x for the slope. But in general that\'s not so.\n\nMore usually the link function for the Poisson is not the identity link, because (among other things) that can allow a predicted mean to go negative, which isn\'t a possible situation with the Poisson. Estimation is impacted because the mean function is no longer linear in parameters. However the general conclusion is unaffected.', ""The MLE for a Poisson rate certainly doesn't need numerical methods. But you're talking about some kind of Poisson regression model. In that case, what's being modelled is the *conditional* mean given the value of the predictor. Have you tried working out the MLEs for the coefficients of a model where the mean depends on a continuous predictor? That is, y_i = Poisson(lambda_i), where lambda_i = exp(a + bx_i)? What are the MLEs for a and b?"", 'Yes, for Poisson without any covariate', 'OK I think that makes sense. Lambda is substituted with the linear predictor. Then you take each partial derivative and solve the system of equations. This requires numerical methods.']"
[R] In-Market Commercial testing,"Hello all! I could use some help trying to solve a question from work.

&#x200B;

""Why"" Context: I work in Market Research, exclusively in Brand Health. Our ad research team had 2 Mat Leaves and a resignation all within a month, and I've volunteered myself to help out (just tryna climb the ladder and make an impression). Overall, I've understood the scope of the work handed to me, but one question came out of a recent presentation that I am trying to figure out how best to solve. My brain is in a pretzel amidst the mountain of work I have right now.

&#x200B;

""What"" Context: The team runs pre-testing of ads from Vendor A before they go into the market, and the ads score based on a number of metrics. For this example, let's use ""Enjoyment"" as the metric. This is survey research, and the data is presented as a Percentile based to a norm owned by the vendor. result. Example Below:

Ad 1: 64

Ad 2: 45

Ad 3: 71

Ad 4: 55

Vendor B provides in-market metrics, and the closest comp metric is ""Likability"" represented as Top 2 Box Percentages. I have metrics for the first month, mid-flight, and cumulative time the ad was in the field.

Ad 1: 1st month: 64% mid-flight: 70% Cumulative: 66%

Ad 2: 1st month: 62% mid-flight: 50% Cumulative: 57%

Ad 3: 1st month: 56% mid-flight: 78% Cumulative: 66%

Ad 4: 1st month: 60% mid-flight: 70% Cumulative: 72%

&#x200B;

My task is to see if the metrics from the pre-testing phase are predictive of what we see in-market. So, in this example, is Enjoyment a good predictor of Likeability? Should I create some sort of rank-order, or some kind of index that I can then sort? I don't have any tools outside of excel. All of the data above is made-up and for example purposes; but I have each ad in a row and the pre-test metrics in columns, and the in-market metrics in other columns. Just hoping the wizards of Reddit have some ideas for how I can attack this without boiling the ocean. Any suggestions?",13w1ox6,mutantfrog25,1685479784.0,1,0.67,"['I have 2 masters degrees. One in Marketing Management and another in Statistics and Research Methods so you can imagine my excitement when I saw your question!\n\nNow imagine my disappointment when I could not figure out what the heck you want!', 'Fair enough, haha! Sorry if its confusing. Im tasked with seeing if pre-market metrics (vendor A) are predictive of in-market metrics (vendor B). So, if an ad tests well for enjoyment, how likely is it to test well for likeability? I would want to do whatever analysis for each length of time. Does that make better sense? Just looking for advice or suggestions. My brain is scrambled.', 'You seem to have 2 outcomes you want to measure?']"
[Q] R studio says OR is not significant but CI is above 1.00,"My (wide) odds ratio from a logistic regression is above 1.00 in Odds ratio in lower confidence interval but the p-value is still above >0.05? I am assuming that it is due to measurement using Wald interval from googling but how can I get a confidence interval which reflects the p-value?

I've seem some use confint.default but my understanding is that this method is also wrong. Should I calculate it manually using the 1.96 SD way? or has anyone else had this problem?

This is the R studio console response:
https://imgur.com/a/ysgiey1",13w16zq,FlyLikeMcFly,1685478622.0,2,0.76,"['By default the tests in summary are wald tests and the confidence intervals from confint are profile likelihood based confidence intervals. In order to ensure consistency you can either run likelihood ratio tests, or calculate wald confidence intervals. For the latter coefci from the lmtest should do what you want.', 'Try with the `margins` package', 'I heard chatgpt helps well with R studio problems', '> Should I calculate it manually using the 1.96 SD way?  \n\nIf you want to have a 95% CI that is exactly consistent with the reported p-value then yes. But the 95% CI you\'re getting with confint is generally considered to be better. You could pick a different statistical test to match that CI. It might be a likelihood ratio test, but I\'m not sure. Try fitting another model with just the intercept and comparing them with anova(model7yx, yourothermodel, test = ""LRT"").', 'Reading through the documentation (?summary.glm), we can see that the p-value it prints is ""the two-tailed p-value corresponding to the t or z ratio based on a Student t or Normal reference distribution"", so it\'s presumably some sort of Wald test.\n\nYou can instead use the output to do a likelihood ratio test using the Null and Residual deviance, which gets p-value < 0.05. See [here](https://stats.stackexchange.com/questions/568534/glm-tests-involving-deviance-and-likelihood-ratios).', ""> Should I calculate manually using the 1.96 SD way\n\nThat is exactly what the main output is doing for you. If you divide the coef/se you get 1.8, just like the table says.\n\nI'm honestly not sure why the odds ratio confidence interval excludes one while the log odds ratio confidence interval includes zero but I'm confident that the log odds ratio confidence interval is the one to trust as its closer to the raw estimation. That is, it ain't significant (or to be pedantic, you can't exclude the null of no association between X and log odds of Y at standard confidence level of .05 alpha.)"", 'This is the way.', 'which function should I use in the Margins package?', ""It doesn't explain why the p-value is non-significant and the confidence interval is above 1.00."", 'The margins function.\n\nCheck whether you get different CIs for link and response. The documentation is useful']"
[Q] Adding inflation as a variable to linear model?,"Ive been given an assignment to make a forecast on number of customers for the rest of the year, which includes a guess on how the (falling) inflation will impact number of customers. 

Usually I have been using a basic linear model, based on historical data, to make forecasts. Can you guys help me out for methods, to add inflation as a variable to the historical data? Or give me some advice on other methods I could use?",13vzcjp,johnjohn1913,1685474317.0,7,0.83,"['I\'m no expert on any part of this topic, but I\'ll try triggering Cunningham\'s law for you .\n\nBy ""basic linear model"" do you mean OLS linear regression? If so, bear in mind that a key assumption of the model is IID samples - the key here being independence. Is the number customers today really and truly independent from the number of customers yesterday? If there\'s any known seasonality in your industry, then probably not. And certainly inflation will be serially autocorrelated, violating the assumption. So a linear model may not be the best approach for forecasting time series data. ARIMA and related models might be a better bet. \n\nIf you only care about predictions/forecasting and you have enough data then you might try some type of RNN instead, where the \'R\' stands for recurrent and means that your last prediction will be fed into the next prediction to help account for the non-independence of time series data. Mathematically these are exceedingly similar to ARIMA-type models.\n\nFinally, it\'s worth mentioning that while it\'s reasonable to assume that changes in inflation might play a role, they also may not and in the case if a parametric model you may want to examine how useful the variable is. RNNs and many other non-parametric models will do the variable selection for you so it\'s less of an issue.', 'Thank you very much for your suggestion. I will try looking into it. \n\nAnd yes, I meant a OLS linear regression. I forgot to mention, that Ive added a seasonal Index, to prior forecasts. But its probably not the most optimal for this.']"
[Q] Compositional regression with multiple compositions,"I've been attempting a regression to explain variance in median county income by variance in different county properties that I was able to get from the census bureau.  The predictors that I have form several different compositions.  For example, I have the age of household head data (fraction of household heads below age 25, 25-44, 45-64, 65+) and the education data (fraction of those age 25+ with less than high school, with high school, 4 year degree, graduate degree).  


The texts that I have been working through only really cover regression with one composition as a predictor.  Where could I find more information on performing regression in the case of having multiple compositions as predictors?",13vzc2j,Gregervich,1685474288.0,1,0.67,"['Do you have a variable you are interested in? Or you only doing predictions?', ""At this point I'm just trying to fit a regression model to explain median household income with the data that I have (maybe 4 or 5 compositions)""]"
"[Q] 1% vs 5% drop rate in a game, why does 4% make such a big difference?","Not sure if this is the appropriate place for this question or even if it makes sense what Im asking, but thought I would submit this here.  Im a gamer and sometimes will grind many hours to get a good item with a low drop rate.  If an item has a 1% drop rate vs 5% drop rate, it makes a HUGE difference and usually represents many extra hours spent grinding drop attempts.  However if it was 48% vs 52% the difference is essentially negligible.  Maybe its a dumb question but does anyone want to explain why?",13vyxeg,adjacent_analyzer,1685473386.0,2,0.6,"['better to look at it like this:\n\n48% -> 52%: you basically go from getting the item half the time to...getting the item half the time\n\n1%  -> 5%: you go from getting 1 drop every 100 times to 1 drop every 20 times.', 'Relative versus absolute differences.', '5% is 5 times higher than 1%', ""It's the relative difference.\n\nImagine a job that pays either 4800$ or 5200$ a month. Not that big of a difference, right?\n\nNow Imagine another One that pays either 1000$ or 5000$.\n\nThat's the same relative differences as in your examples, and as you can see it's way more noticeable in the second case."", 'it might help to go back to fractions. 1% means 1/100 tries on average. 5% means 1/20 tries on average. 48/100 vs 52/100 is almost minuscule, you could look at that as 1/2 tries on average.', 'You should be spending 5 times more time when the probability goes down by one fifth', ""To compare drop frequencies, you should be looking at ratios not differences\n\n>  it makes a HUGE difference and usually represents many extra hours spent grinding drop attempts\n\nOf course, 5%drops happen *five times as frequently* as 1% ones would. For example, if you play for say 10 hours on average to get 20 good drops at the 5% rate, on average you'd have to play for 50 hours when it was 1%\n\n>  However if it was 48% vs 52% the difference is essentially negligible\n\n52/48 = 13/12  ... 52 is a little over 8% bigger than 48. Not nothing - you'd start to notice it in the long run - but not a lot."", 'The expected number of enemies you have to kill is reciprocal in the drop rate (see https://en.wikipedia.org/wiki/Geometric\\_distribution). \n\n5 % drop rate means 1/20 chance of success, so that the expected number of trials before you succeed is 1/ (1/20) = 20. \n\n1 % drop rate means 1/100 chance of success, so that the expected number of trials before you succeed is 1/ (1/100) = 100. \n\nPlot the graph of 1/p (where p is the success probability) and look at what happens near 0. The expected number of trials (1/p) explodes as p goes down to 0.', 'I guess thats pretty intuitive, I do feel kinda dumb now. Basically because 5% represents a 5x increase from 1%, but 52% is just a 1.08x increase from 48%.  It just seemed kinda confusing that even though theyre both 4% total increase its so much more impactful when you start from 1%']"
[Q] Books on Statistical Sampling / Designing Sampling Plans?,"I work as a Data Analyst at a Not-for-Profit. As the only one on my team with a Degree in Statistics, I sometimes get asked  to help design sampling plans. I've taken a classes in sampling before, but it mostly just covered the basics of different estimators, SRS, stratified sampling, post-stratification, non-response bias etc. I feel like my knowledge is limited, and I'd like to improve / have a reference book.

Do you have any recommendations for books on Statistical Sampling? I don't mind math-y books. I am also open to papers, online courses, youtube videos etc.",13vx2yu,BloatedGlobe,1685469180.0,10,1.0,"['[Sarndal et al, ""Model assisted survey sampling""](https://link.springer.com/book/9780387406206) is one of the grand-daddy texts on the topic.', 'Experimental design and data analysis for biologists, by Quinn & Keough', 'I remember this from my grad course, very comprehensive', ""I came here to say Sarndal as well. That's what we used in my 400 level sampling techniques class.""]"
[R] Detecting Dataset Drift and Non-IID Sampling: A k-Nearest Neighbors approach that works for Image/Text/Audio/Numeric Data,"Hey Redditors!

Before modeling a dataset, do you remember to check if it seems IID?

Distribution drift and interactions between datapoints (autocorrelation) are common violations of the Independent and Identically Distributed (IID) assumption which make data-driven inference **untrustworthy**.

I present an automated check for such IID violations that you can quickly run on any {numeric, image, text, audio, etc.} dataset! My method helps you understand: *does the order in which my data were collected matter?*  When the answer is yes, you must take special precautions in modeling to ensure proper generalization from data violating the IID property. Almost all of standard Machine Learning and Statistics relies on this fundamental property!

I just published a [paper](https://arxiv.org/abs/2305.15696) detailing this non-IID check and open-sourced its code in the [cleanlab](https://github.com/cleanlab/cleanlab) package  just one line of code will check for this and many other types of issues in your dataset.

Dont let such issues mess up your data analysis, use automated software to detect them before you dive into modeling!",13vu8gr,jonas__m,1685462578.0,27,0.86,"[""If you're interested in reading more, you can also check out the [blog post](https://cleanlab.ai/blog/non-iid-detection/)!"", ""Thanks! I'll check this out""]"
[R] what statistical test should I use?,"
Hi, qualitative researcher here (so sorry in advance for my poor understanding of stats) 

I was wondering if anyone could give me some advice on my quantitative analysis. Im looking at crime outcomes (solved and unsolved) and trying to identify any trends if that makes sense. Im essentially trying to figure out which crimes are solved more than others and if there are any interesting differences for example if crime with male victims are solved more than those with female victims or if crimes involving weapons are solved more than those without. Any advice would be greatly appreciated as SPSS has broken my brain",13vttdz,Foolofatook995,1685461602.0,2,1.0,"['As your outcome is binary (0 = unsolved, 1 = solved), logistic regression would be a natural starting point.', 'You could also start just by using cross tabulation (""crosstabs"").  This will give you the proportion of solved and unsolved for, say, weapon and no weapon.', '*Im essentially trying to figure out which crimes are solved more than others*\n\nSome exploratory data analysis would be a good start. Have some ideas or hypothesis in mind, and plot your outcome variable against your chosen input variables.\n\n*if crime with male victims are solved more than those with female victims, if crimes involving weapons are solved more than those without.*\n\nWithout knowing more background, it sounds like T-test of proportions to me (outcome variable = proportion of crimes resolved, comparing between two groups of observations). Definitely check against the assumptions of T-test.', 'Explain the dependent and independent variables. What exactly are you thinking?']"
[Education] Is it possible to succeed in a biostats PhD program without a formal mathematics background?,"I hold bachelors' and a master's degrees in what would be considered ""math-adjacent fields"" - economics, data science, and bioinformatics. So while I feel okay in terms of my applied quantitative abilities, I can't say that I have all that much in the way of theoretical or proof-based training - besides a bit of probability theory. 

I took some graduate level biostatistics classes during my masters and really enjoyed them, and in the process narrowed my research interests and considered that I might want to pursue a PhD in biostats. In preparation, I've worked as a research analyst since graduation to bolster my publications, and also am enrolled to take some of the more foundational math classes (Calc III, real analysis) in coming semesters before applications are due. 

However, after researching some of the programs and faculty I would be interested in (as well as students in these programs), I have become really, **really** discouraged.

It seems most everyone in biostats departments have undergrad degrees in mathematics or statistics, or even some graduate level math training before going on to pursue their PhD. Likewise, most of the dissertations coming out of these programs are almost incomprehensible to me with how technical they are. Its not that I don't enjoy *trying* to understand them, but it is incredibly disheartening to me that I genuinely can't understand most of what is being written beyond the abstract and conclusion. 

To be clear, it is not lost on me that biostatistics is, of course, going to be math heavy and also involve a great extent of methodology and theoretical research. I have researched faculty that I would be interested in working with and a lot of their *current* work is certainly within the realm of what I would like to do. 

But with all of this being said, are my concerns overinflated here? By that I mean, is the level of quantitative rigor that is expected both in the application process and the dissertation process true for all biostatistics departments? Is most of the knowledge that is required to succeed in a PhD program coming from an individuals undergraduate background, or is it gained during the program? 

If someone could provide some insight here, I would greatly appreciate it. I am on the verge of just accepting that I have set myself up poorly to pursue a PhD in biostats, and should look into other domains entirely.",13vr5at,coldlikeastone,1685455236.0,17,0.87,"[""A lot of math in biostats is incomprehensible to many researchers in biostats and dissertations can be the worst because they're an extremely deep dive. A lot of math in biostats is also for show--important to getting jobs and promotions but poorly communicated. So your concerns are likely overblown."", 'No, the knowledge you gained in undergrad does not determine your success in a PhD program. It can help make your learning curve less steep, but by the end of the first year or two most any advantages from prior experience will have washed out. \n\nEvaluating and critiquing research (as youre doing when looking at dissertations) will be a core skill you develop in a PhD program. You arent expected to know this now. Stop reading dissertations! Theyre awful haha but seriously, focus on articles by faculty youre interested in working with and dont sweat not knowing things. \n\nIf youre passionate about this, if its something you really want to do, focus on making that happen, and getting your application plan in place. Try to meet with grad students and faculty to ask questions about what to expect**, build relationships now that will be useful when applying, and start working on developing a supportive group of academic people who are in your corner and that you can reach out to when youre in a program. \n\n**ideally these are people you have met through prior study or can connect with using your alumni status (eg reaching out to grad students and faculty at the institutions you received your undergrad and masters from), the RA work youve been doing since, etc.', ""I strongly suggest getting the prerequisites before starting a PhD.\n\nLinear algebra, differential, integral and multivariate calculus. Also, probability ad the undergraduate level. And some programming experience in R/MATLAB/Python, whatever. That's all."", ""Yes, it is totally possible. Biostats PhD programs span a wide gamut from theoretical to more applied and everything in between. No matter where you wind up, though, you should find yourself with a lot of leeway in the kind of work you ultimately want to do, assuming you have a PI willing to support you. And seconding the advice in this thread to not read dissertations. As a counterpoint to your experience drowning in math-heavy dissertations, my friend just graduated from a very well-regarded biostats PhD program and their ~150-page dissertation had something like 3 or 4 equations in it. \n\nAlso, take a look at the required coursework for each program. That will give you a good idea of what they emphasize. If you need to take measure theory to graduate, and you feel meh about the idea of studying measure theory, that's a good clue that maybe it's not a good fit for you."", ""If it's any comfort, once you're at PhD level many people have hard times really following each others' research. I suspect you will be fine."", 'Different programs have different expectations and requirements.  You would not be sucessful at places like Washington, Hopkins or UNC, but there are a lot of other programs that do not require/have has much emphasis on theoretical aspects of the field.  Look at some of the recent dissertations produced by students at places like Drexel, Boston University or University of Florida.', ""If they have degrees in data science and economics, I'd be surprised if they haven't had those courses already.""]"
[Q] Are EdX courses worth it if you don't pay for the materials?,"I have my masters in data science. My problem with my degree is that while I did some about statistics, I'm not a proficient as I'd like to be. I've considered a second masters in applied mathematics/statistics, but prefer to not take on more student loan debt.  
I found some courses that i'm interested in taking  (Linear Algebra, statistics, etc.) on EdX (by Georgia Tech), but problem is a lot of course material is only available if you pay. has anyone taken these courses? If so, is it still worth taking if you don't pay for the materials? Or are there other courses/resources on the internet that you've found helpful? Or should I suck it up and get a second masters.  
Thanks.",13vq1ab,InjuryNeat7483,1685452514.0,3,1.0,"['Dont get a second masters thats ridiculous. \n\nWherever you get hired, youre not getting hired to do statistics. Thats what my stats professor told me and its true, I could definitely do without knowing Gauss Markov theorem. Most of my work is automating reports and data processes using some tricky business logic. Truth is only a couple companies want some goated data scientist that knows everything about statistics and programming. \n\nYou just need to master the basics. That happens with a good manager that will fill in the gaps in your education that we all have/had. I will teach you about Gauss markov, loss functions, and design of experiments when we cross that bridge. Nobody knows everything and were all lifelong learners here. The funniest part is that professional development and becoming a leader is arguably more important in industry (if youre as ambition as you seem). \n\nGo through wackerly mathematical statistics if you want to learn more on your own. There has to be some free structured stuff out there. If not you can lookup homework/exams for each chapter and probably rip it from a school.']"
[Q] Can I use kappa to compare accuracy for a scoring system?,"Say there is a scoring system from 0-75, and I want to compare how good at chart reviewers are at coming up with the same score, can I do that using Kappa, or is there a better way to do it?",13vpxb5,FalseListen,1685452234.0,15,1.0,"['I second this. I would use ICC here, as typically any ordinal variables with more than 10 categories are typically considered continuous\n\nthis article can help you pick which ICC type based on your study design https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/', 'also considering whether Spearman correlation would make sense', 'Kappa is intended for and works best for ordinal valued variables with a small number of levels (say 5 to 10). It also demands that not all of those cells be sparse. \n\nHowever, the ICC can be used since you can regard your variable as continuous. The type you need to use depends specifically on your experimental setup.']"
[Q] Question about chances of a dice game.,"Hello,

So at our student house we have a game where we simultaniously throw 216 dices. There is one rule, each time you throw a 6 you have to re-throw that specific dice. The other dices that are not 6 are out of the game.

You can earn 'points' once you have thrown the number 6 three times with the same dice. When you have thrown three times a 6 with the same dice, you of course have to rethrow the dice again, because of the one rule that you have to rethrow every 6. Now if you again throw a 6 with this dice, you get another point. The game stops when there are no sixes left.

Score example: You have thrown 3 times a 6 and then a 5 with one dice (1 point), with another dices you have thrown 4 times a 6 and then a 2 (2 points) and with another dice you have thrown 3 times a 6 with and then a 1 (1 point). You have now got a total of 4 points, 1 + 2 + 1 = 4.

I would like to calculate the chances that you get 1, 2, 3, 4, 5 etc... points, taking into account that you can get for example '2' points in different ways. e.g. with throwing 4 times 6 with one dice and with throwing 3 times 6 with two different dices. I just cant wrap my head arround this calculation and thought I might try my luck here.

Kind regards,

Wouter",13vntx2,Sharestar,1685446525.0,1,0.67,"['Ill be honest, the english is kind of hard to understand.\n\nOdds of getting 3 6s in a row is 1/6 * 1/6 * 1/6. The last number of rolls is irrelevant since getting a 6 is a reroll so the only condition is 3 6s in a row. Expected value (what you would expect to actually get) is a uniform distribution of 1-5, so 3. (1+2+3+4+5)*(1/n),n=5.\n\n216 dice * 1/6^3 *3 = 3', 'If I understand correctly, you are rolling a d6 and points equals the length of a streak of rolling a 6 on the d6. The modal number of points is thus 0.\n\nIf so, the probability function for points is the geometric distribution. The arithmetic is pretty simple. See the Wikipedia article. \n\nhttps://en.m.wikipedia.org/wiki/Geometric_distribution', 'Maybe you gotta calculate the combinations for the points yourself which is kinda tedious but i guess if we consider both subsequent rolls of the same dice to be independent &  rolls of different dice to be independent then let X\\_1 .... X\\_216 be the number of 6\'s rolled from that dice,\n\nfor example calculating total points = 1 \n\nsum of these joint probability mass function\n\nP(X\\_1 = 3, X\\_2 < 3, ...., X\\_216 < 0)  +  P(X\\_1 < 3, X\\_2 = 3,....,X \\_216 < 3) ...... (216 times, since the die that score the ""1"" point can be any of the 216 dices)\n\nthen by independence\n\n= P(X\\_1 = 3) \\* P(X\\_2 < 3) \\* ... \\* ( X\\_216 < 3) \\* 216\n\nP(X\\_1 = 3) = 1/6 \\*\\* 3\n\nP(X\\_2 < 3) = P(X\\_2 = 0) + P(X\\_2 = 1) ... P(X\\_2 = 2)  ------- for this you gotta use the geometric distribution since the ""success"" (failure to roll a 6) must be the last roll \n\nthen u can just multiply them together.', 'Everyone is giving you serious answers, but I would just like to say - what a wild game. 216 dice?? That must take forever to sort through']"
[Q] can I use anova to see the interaction effect of x and y on Z. I’m measuring all 3 using questionnaires/ scales,"I am using questionnaires / likert scales to measure the 3 variables. Can I use anova to measure interaction effect? 

can I use regression to analyse the interaction effect?",13vii59,sapienhomo1,1685428029.0,5,1.0,"['Do you have more than 5 categories in the variables? If so, run a regression with an interaction term (x*z) for the relevant interaction.']"
[Discussion] Have you heard of the Twitter user @cremieuxrecueil?,"This person claims to be a stats scholar, they ridicule social scientists based on decades old research - and also post fake research from known white supremacists as truth. The problem is that they are followed by many software gurus and VCs - many of whom have reposted the nonsense of this cremieux recueil - I would appreciate the help of my statistics bros to help speak truth to the nonsense this person is putting out into the world.",13vf89k,garden_province,1685417363.0,0,0.2,"[""Never heard of them, don't care to know about them. \n\nWhy are you giving them even more attention by bringing up their drama here?"", ""Haven't heard of them. I looked and I don't agree with your impression. I'm not an expert on IQ research, so I can't speak specifically on that issue, but otherwise this person appears to be doing legitimate statistics.\n\nIn general, if you judge someone by the conclusions of their arguments, that is, statistically speaking, all wrong. What makes someone a good statistician is the rigorous methods they use to reach their conclusions.\n\nI didn't see examples of this person ridiculing social scientists, but it is definitely true that scientists of just about every flavor make mistakes with statistics. It's not unusual at all for a statistician to criticize scientists.\n\nHere's what I recommend-- pick out whatever claim you think is suspect, then make a post asking specifically about that. Hopefully someone here will know the data well enough to give you a good answer.""]"
[Q] How to calculate inter-rater reliability for one item with multiple raters,"Dear all,

  
I have just spent hours upon hours trying to figure this out, and I have gotten nowhere. My question revolves around calculating the inter-rater reliability within my dataset. I have run an experiment which has five experimental conditions. Participants were randomly shown a situation and were consequently asked to answer some questions about this (these questions were the same regardless of the experimental condition).

  
Now, I wanted to calculate the inter-rater reliability within each condition, in other words, whether the participants in each condition were in agreement or whether there was a lot of variation. I have a dataset uploaded in Python with one column 'Condition' which can take 5 values and then three items 'WTI\_1', 'WTI\_2', 'WTI\_3'. I want to calculate for each item, in each condition the inter-rater reliability.

  
I have tried for hours to find a way to accomplish this, preferably in Python where I have my data. Since it entails more than 2 'raters' I have tried ICC (which apparently is sometimes used for inter-rater reliability), Fleiss' kappa, Krippendorff's alpha and some other things. I have not been able to get anything to work. 

  
Does anyone have an idea how I could go about this, since I do not have time to spend another 5 hours to look into this?",13va1nl,MainKoen,1685403059.0,1,1.0,[]
[E] Rigorous Variable/Feature Selection Resources and Techniques,"Hello all,  
  
A bit of background; Postgrad in Statistics, but looking for some additional oopmh to increase the recall/specificity of some models I'm working on through the conjunction of better feature selection/searching for complex interactions/etc.  I've recently moved into the very high dimensional domain  
  
I'm looking for some good resources for feature selection for larger dimensional data (Especially in the context of limited computational resources. Any selection techniques  that are robust to model type/is done prior to any fitting would be amazing).

As computation is a huge concern for me, I want to be able to consider interactions and downstream contrasts. I have been trying RELIEF based techniques, primarily as of now, but I'd like something with some robust stats behind it (I have a largish sample, and was thinking about doing something like 'cross validating' chi squared results over some number of k folds with my categorical target, ranking variables by score , then applying something like B-H correction to filter out low-relevance predictors, or bootstrapping some of these statistics, but I do worry about generalization error a bit here). I have also been using some regularization techniques, but with so many noisy variables; some performance is being degraded at the additional cost of more computation that I'd rather use for searching for complex interactions.  
  
Unfortunately, finding these can be hard in google, as many medium/towards data science/etc instruct you how to create models with leakage and overoptimistic fits as they do not incorporate feature selection within the cv process (I.E, select 10 most correlated features with target, split and then train over k=4 folds)  
  
I have been making my way through Variable Selection for Pattern Recognition, and I find myself reading articles and papers on the discussed techniques, as the material is very, very topical within the  book (it really doesn't provide a ton of explanation in a lot of places)  
  
Thanks!",13v6i3k,relevantmeemayhere,1685394205.0,4,0.84,"[""Try Frank Harrell's 'Regression Modeling Strategies'.  I just took his class.\n\nThe book is at:  [https://hbiostat.org/doc/rms/](https://hbiostat.org/doc/rms/)\n\nNote - Frank Harrell is much more pessimistic than others you will see.  A lot of that is from long experience of watching models fail to replicate on the next data set in ongoing studies."", 'My thesis is somewhat related to feature selection - but mainly applied to RCT. There have been recent developments in elastic net regularization of high dimensional data, where reasonably modest machines were capable of model selection at p=1000. I would look into it.', 'To add to this, here are slides + a talk that goes over a simulation about feature selection [https://www.fharrell.com/talk/stratos19/](https://www.fharrell.com/talk/stratos19/) . The TLDR is that feature selection is very unreliable.', 'Thank you!  I will do this l. Ive used elastic net and lasso in the past; but beyond fitting I havent used them for feature selection per se. Ive used discriminate analysis in the past for mixed results. \n\nDo you have a nice paper or text to start?  If not totally cool; looking to narrow down those google queries', "">Thank you!  I will do this l. Ive used elastic net and lasso in the past; but beyond fitting I havent used them for feature selection per se. Ive used discriminate analysis in the past for mixed results.  \n>  \n>Do you have a nice paper or text to start?  If not totally cool; looking to narrow down those google queries\n\nIf you don't have much experience with Lasso style techniques (and Lasso and all the derivatives sound like a reasonable choice in your situation):- Statistical Learning with Sparsity: [https://hastie.su.domains/StatLearnSparsity/](https://hastie.su.domains/StatLearnSparsity/) (for free)- Bhlman/Van de Geer: Statistics for High-Dimensional Data: far more math-y than the 1st book- Giraud: Introduction to High-Dimensional Statistics, somehow available on this university site: [https://www.imo.universite-paris-saclay.fr/\\~christophe.giraud/Orsay/Bookv3.pdf](https://www.imo.universite-paris-saclay.fr/~christophe.giraud/Orsay/Bookv3.pdf)"", 'I swear I had just read the paper last night, but alas I can no longer find it. I recommend starting with this: https://doi.org/10.1111/j.1467-9868.2005.00503.x and going from there. Best of luck!', ""I've used quite a bit of LASSO, but in terms of finding best subsets I have not done so in an iterative fashion.  I'm also trying to avoid creating over-optimistic models; so I really want to get into some of the baser feature selection methods that are agnostic to model procedure and have good generalization properties over a set of models.  Especially because a lot of the data I'm looking at is quite imbalanced, so I need some flexible learners\n\nI really don't want to use embedded methods as a crutch and treat this as a combinatorics problem, and I'd like to begin additional feature engineering (as well as collecting more longitudinal predictors) in the future.  Interaction effects are one thing Im struggling to account for, because this space is a little on the larger side. \n\nThank you for the links!  I will check them out"", 'Do you think sparse statistical learning / feature selection / high dimensional statistics and the topics relevant to the statistical learning with sparsity book is still going to be an active area of research in statistics departments? Is it right now a hot topic / active area? How much will deep learning and CS departments make this area of statistics seem outdated? Asking because I have a high interest in researching those fields in a PhD program in 2 years, but Im worried the whole field would have shifted by then.', 'Thank you']"
[Q] Is there a method for adding random effects to an interval censored time to event model?,"Im doing some research with individual and neighborhood level time to event data and would like to include random intercepts in my models to account for similarities between individuals who live in the same neighborhoods but Im having trouble finding a way to do this with interval censored data. 

I have found some info on frailty models and that seems close to what Id like to do but most of what I have seen is for right censored data and Im using interval censored data. So far Im having trouble finding out if this can be done or if there is a way to do it in Stata/SAS. Is this even a thing?",13v3cwt,cmon_sun,1685386812.0,4,1.0,"[""Haven't read this paper in a while but it might be helpful? \nhttps://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.4459?saml_referrer\nDOI: 10.1002/sim.4459 \nEstimating survival of dental fillings on the basis of interval-censored data and multi-state models, Pierre Joly, 2012"", 'My approach to problems like this is to write down the proposed model mathematically first, in extreme detail. I find hierarchical form to be the easiest way to break it down piece by piece. Once I have the maths then I turn it into a [Stan](https://mc-stan.org/) model. Last step is to use the Stan output to answer the research questions.']"
"[C] Working as biostatistician in hospitals vs universities vs pharma, which is the best?","I have worked in a hospital. My experience is not great so far, no career development for statiscians and there are real challenges about data collection process for retrospective studies but leadership (who are mostly with nursing or MD background) can't come up with data solutions, and they don't know how to support the only 4 statisticians in the team. 

Since this was my first job ever, and have been working there for 2 years, I decided to move on. I applied for universities hoping that the leadership would be academic people who appreciate data science and technology, and career development. 

I don't know much about pharmaceuticals but I guess the experience would be similar to the hospitals. 

But for the sake of anyone like me, who wants to work in biostatistics. What are the best sectors to work as early career statistician? Universities, hospitals/healthcare institutions or pharma/biotech companies?",13uzpo2,Rosehus12,1685378118.0,32,0.97,"['> I applied for universities hoping that the leadership would be academic people who appreciate data science and technology, and career development. \n\nSpeaking as an academic, I\'m afraid this one isn\'t going to pan out. University ""leadership"" these days is basically mindless bureaucrats whose entire goal in life is to make Higher Education increasingly resemble a hellscape of MBAs. The question all anyone seems to care about is: ""what is the minimum possible amount of money we can pay the people who teach the undergrads before they stop teaching and the undergrads go away (and so stop paying tuition)."" \n\nIf there ever was a day in which University Leadership reflected the best, brightest, and most forward-thinking members of the faculty, that day is long behind us.\n\nAt my Uni, you could plausibly fire the top 5 most highly paid administrators and replace them with ChatGPTs trained on business jargon and buzzwords. No one would really notice.', ""I have a title of data analyst (but basically a biostatistician) for a small research team at a university and their faculty of public health sciences. I quite enjoy it, have a great work-life balance, and work on some pretty interesting papers. The biggest cons for me are (1) lower pay compared to private industry, and (2) I am the main data/stats person on the team and I often feel like a lone wolf. There are other academics available to you for help on bigger picture issues though, and it's nice to have contacts with other research organizations, academics at other universities, and government, etc.\n\nIf you are looking at universities, I think the experience could really vary on the research team.  What kind of career development are you looking for? My principal investigator really wants me to do a PhD part-time while working on this team, so that is one way to advance your career at a university. I don't think I'm up for that though lol."", 'Extremely published medical academic here. Worked at a research 1 academic school. Our biostatisticians were treated like a a valued collaborator and resource. They consulted, but also ran numbers and collaborated in design. All were credited in publications. If you want to be appreciated and work around peers, Id recommend academia.', 'Pharma is very different to what your experience in hospital sounds like. Highly statistical environment with plenty colleagues (my mid-size pharma that you probably havent heard of has ~40 or so statisticians), and enough technical challenges to keep me happy and grow. \n\nIf you havent worked in clinical trials before, its a big learning curve terminology/admin wise, and depending on where you land, a large part of the work week would deal with data and logistics aspects, but I can even find those parts fun.  The technical bits comes in in study design, analysis plan writing, and for various ad-hoc analysis that pop up around the place.', ""I work in healthcare with well compensated workmates and  if you are looking for intellectually stimulating and breakthrough stuff....this isn't the place. Between the red tape, approvals, and political games the best thing would be to do what is expected of you and enjoy your work life balance. Coming from someone working in a top 10 hospital system in the US... If you haven't, make sure you know SQL and at least python or r."", ""I've got a fair amount of experience (industry, health system and currently in an academic statistics group).\n\nFor each one, what's crucial is the group and the funding.  If you are in a group which is basically devoted to a single project, you will probably lose your job when that project is over.  \n\nIn academia, you really want hard money, rather than grant money ('soft money'), unless you are in a group which works on multiple projects, acting much like a consulting firm.\n\nI am in that latter situation, and it's great!  I've worked on a greater variety of work since whenever."", 'I worked in academic, the private, then back to academic. If youre looking for pro dev opportunities Ive found academic to be the best possible place for it. You still need to have a good supervisor willing to invest in you and like any other job there are toxic teams but in general the ability to do interesting research work and have some protected learning opportunities with solid work life balance are miles above in academic. When I spent time in private I got a very nice pay bump but the flip side of that was feeling like I was an assembly line worker, comparatively limited opportunities for interesting work, and a really strong billable hours/profit driven mentally where ROI led all decisions. The more methods focused work still exists in pharma/CRO but in general youre much more likely to get it with a decade of academic experience in your pocket than trying to climb the ladder as an early career MSc biostat. If you legitimately love school Id consider going for a PhD since you will be more open to independent research options with a doctorate in all of the above areas.', '>\tbut leadership (who are mostly with nursing or MD background) cant come up with data solutions, and they dont know how to support the only 4 statisticians in the team.\n\nTo clarify, most hospital leadership are not MDs. Most research leaders however will be MD if not MD/PhD. This is because only they have clinical access to patients. \n\nThe role of the MD is not to provide statistical support or design - nor can they easily implement data solutions. Their role is to query a healthcare system to learn to provide better patient care, while first and foremost providing care. If MDs truly ran the hospitals they would be a very different place. \n\nHealthcare is built against data mining, every aspect of healthcare data is deeply walled and hard to access for patient privacy. The MDs doing research have no ability to change this reality. It is up to the research team to find a strong use case to do research and mine data - and simply associating variables to outcomes and modelling will lead to spurious relationships and horribly confounded outcomes. \n\nSpeaking as an MD, who previously held statistics roles in clinical research and who is pursuing further graduate research in clinical medicine. I also perform and consult on clinical research for a few different groups.', ""That's right, university leaderships are chasing after money too. It is sad that these educational institutions are turning this way. But I meant the direct leaders/managers. I would be reporting to a professor in biostatistics they will be extremely busy which is fine but whenever I find them, they can really help with some advice and I won't feel alone in this difficult profession, it is easy to feel imposter if I'm continously consulting non stats people."", 'Fucking A', 'Well, I guess I\'ll be a lone wolf as well because statisticians tend to be considered ""the experts in statistics within a group of clueless life science or medical researchers"". The money is clearly better in industry but this is not my concern for now. Also, they use SAS which i don\'t prefer. \n\nThe type of career development I\'m looking for is at least being able to enroll in a class in the fall or spring and learn new skills. It took me 2.5 years to finish my masters in biostatistics although it should only be 2 years, not because I was terrible in my classes but because I was enjoying school and I took extra electives. There are still some statistical method I want to learn deeply that I didn\'t take it grad school. Also, I won\'t say no to a phd part time but that will take forever, I would go for it if I plan to stay at this job long term or simply do a masters degree. Also, universities is easier to get involved with intellectual people, I don\'t know if I\'m just fantasizing but it seems more enjoyable.', ""Can I ask how much you're being paid for your research team?"", 'Sounds fantastic!', ""I learned some SQL online and wanted to gain hands on skills at work but the politics at our department are resistant (few coworkers are either lack the motivation to learn or worried they will become sql monkeys and the analysts won't help anymore with data extraction) also the management is kind of worried about us not being trained to handle sensetive warehouse data if we pull it ourselves. Currently, all projects are on hold because there is no solution. \nThere is great work life balance but the stress from clients to get things done and that we are unable to get anything done because of lack of data is not worth it."", ""I'd say it depends on the type of structure you work with and the kind of study you do. Yes there's a ton of red tape however in pharma, statisticians and project managers are generally different positions from my experience. The project manager deals with the red tape then the statistician comes in and does the actual analysis or adds the actual expertise. \n\nI agree that healthcare isn't the most intellectually stimulating for a stat though as reviewers/evaluators are rarely (if ever) statisticians themselves and will criticize anything but the most robust and old-school methods.\n\nAgreed with R, SQL and Python, I would add SAS too. It may seem old school but a lot of places use existing macros coded with SAS and haven't transitioned yet."", ""Yes most of the jobs I applied in academia they act like a consulting firm to help phd students or medical interns. Currently in the hospital, it is more focused on graduate medical education they're all beginners in research which takes lots of effort from my side to teach them and supervise them. Teaching is fine but I hate dealing with all the stuff in between (following up with their studies if they finished their homework or not before they graduate) makes me feel that I deal with high schoolers, especially that research is required for their program and I must push them. I would rather be out of it and just help with the studies."", 'Big downside for phd is the opportunity cost and being on low salary after getting used to be paid decently for 2 years. Best choice is to work in academia and use their career development plans.', ""In public health I think SAS is still mainly used, but I think there are definitely research groups out there that have been getting into R, Python, and more data science tools. Asking about the tools a research team uses should definitely be something you ask in interviews.\n\nThat's awesome about wanting to take courses! There's free tuition here at my university for staff members up to two courses, which I've been doing once in a while as a non-degree grad student. I'm certain other places are willing to subsidize the costs of taking courses too though."", '>\tWell, I guess Ill be a lone wolf as well because statisticians tend to be considered the experts in statistics within a group of clueless life science or medical researchers. The money is clearly better in industry but this is not my concern for now. Also, they use SAS which i dont prefer.\n\nAgain as I wrote in my other comment. MDs and clinical personnel are not full time researchers and are not trained traditionally to be fluent statisticians - hence your job. \n\nIf I put you in a clinic and said go treat my patients you would not have a good time either. Very few MDs are fluent in modern statistics as they are fluent in an entire other body of literature. \n\nYour role on this team is clearly misguided - explaining your anger. I would strongly clarify what your role is here and the role of the other team members. Choosing to critisize other team members for attempting to do your job while you sit back and complain you could do it better isnt helpful. Help the clinicians understand the statistical needs and be a team member.', ""I've been working there for about 2 years now, making ~73k CAD. Its ok given that I don't live in a high cost of living area like Toronto, but it hurts knowing I could be making way more in the private industry."", '>The project manager deals with the red tape then the statistician comes in and does the actual analysis or adds the actual expertise. \n\n\nOP should recognize that some (stupid shitfuck idiot) companies will force dealing with that red tape, especially technical stuff getting in the way of analysis, onto their statisticians. PM may only be responsible for deadlines.', 'Sometimes. If you can get some of the larger doctoral awards you can end up with a higher income than a lot of masters stats are taking home after tax upon graduating. Youll also have the opportunity to move higher and faster through a number of career paths than with a terminal masters. It really depends on where you want to be/do long term.', ""I am guessing most universities waive tuitions for staff at least one or two courses a year. I think I have more time for learning because I don't have kids or family and it is good to invest in my career. I usually ask about the software they use during the first interview to be in the same page. I'm considering asking about the career development part as well"", ""Well I didn't deny the role of MDs that they don't know much about statistics and they're not full time researchers (this is the biggest con because they are extremely busy and don't give enough effort to research), I clearly stated that statisticians role specifically in these types of jobs is to act as an expert to help clinicians, this could be a pro or con for some statisticians, and statisticians are professionals who can choose which clients they want to serve based on choosing the sector, working in a hospital=working with lots of MDs more than other sectors. If you're MD and took it personally, that's your problem. I'm in statistics sub asking advice from  experienced statisticians. Your comments is not useful to me in this case but thanks anyways."", 'Good to know. Pharma can afford to hire PM and more it is really stupid to make statisticians deal with all this stuff, especially that pharma has more FDA regulatory work that biostatisticians always involved in. In hospitals and university we deal with IRB only', "">Pharma can afford to hire PM and more it is really stupid to make statisticians deal with all this stuff\n\nSome do and it's an indicator of how much bean counters control everything in that specific site.""]"
[Q] Does anyone know any good resources with extensive practice questions for learning causal inference?,"Ive been going through Brady Neals YouTube course and book, and What If by Hernan and Robins, but these dont contain exercises to cement concepts. Are there any resources with particularly helpful practice questions (Im looking mostly for applied rather than theoretical)?

Id also be grateful if you could recommend other ways to cement concepts.

Thanks in advance!",13uwesv,Birdibrain,1685370446.0,13,0.89,"['Pearls Causal Inferences in Statistics: a Primer is great. But its almost 100% theoretical. The good thing is that its full of questions, though.', 'Maybe the Statistical Rethinking course and book?  \n\n\n[https://youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus](https://youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus)\n\n&#x200B;\n\nThe book contains many exercises.', ""The Effect: An Introduction to Research Design and Casaulity by Nick Huntington-Klein is a great introduction, perhaps the most applied out of the excellent resources that have already been mentioned. It has some code examples and It is available for free at https://theeffectbook.net. Its geared more towards social scientists than Hernan and Robin's What If and is a little more approchable than Casual Inference the Mixtape, but covers similiar content. I would keep in mind that at least in political science and economics there has been massive changes to how Difference and Differences (DID) is estimated with time varying treatment effects in the last couple of years. Similarily, Regression Discontinuity (RD) has been shown to have some serious issues with statistical power for units at the cutoff. Propensity score matching has also gone out of favor as a matching technique as it doesn't always produce matches where covariates overlap. For this reason, I recommend newer books with code that can be updated online to reflect these changes, such as Casual Inference the Mixtape and The Effect. If you really want practice examples, you could google academic papers with a given design within the last couple of years and replicate their results after grabbing their code from the Harvard Dataverse (which is what we did in my Casual Inference class in grad school)."", 'i was looking for something like this long back and went thru many courses. but then i did not practice and lost touch.', ""Cunningham's 'Causal Inference: the Mixtape' has excellent examples  of code to play around with and should be available in a good format online. Maybe not exactly what you are looking for, but a big recommendation in terms of understanding the nature of the arguments."", ""Gelman, Hill, and Vehtari's book, Regression and Other Stories, has a really good couple of chapters on causal inference and quasi-experimental designs. There are applied exercises in R at the end of each chapter.\n\nThe pdf is available here: [https://users.aalto.fi/\\~ave/ROS.pdf](https://users.aalto.fi/~ave/ROS.pdf)"", 'Morgan and Winship (2014) is fantastic. You can also do a lot worse than looking at some courses in causal inference taught by top social science departments. I found really good reading lists by just coming through what grad students at Harvard, Yale, NYU, and CalTech (amongst many others) were being taught. Obviously you lose a bit because you dont benefit from any actual teaching, but the readings are normally sufficient.', 'Second this.', 'Nick Huntington-Klein also has a YouTube channel with numerous videos dedicated to the material covered in his book. Theyre pretty great. \n\nThe author of Causal Inference the Mixtape (Scott Cunningham) also runs causal inference workshops that are pretty affordable (compared to other comparable methods workshops) ', 'Its so easy to lose touch with the advanced stuff. Idk how people remember everything']"
[Q] How to find end state probabilities of a state transition probability matrix?,"Give I have a state transition matrix where we move from state to state by the roll of a die, with 13 in-game states plus states 14, 15 are the two end game states (14 one outcome, 15 another), how do I compute the total probability that say 14 will happen?",13unx01,acidsyzygy,1685345116.0,1,1.0,"[""Choose a zero vector of size 15, and set the first element (or whichever is your initial state) to 1. Call it p, this is your probabilities. Then compute A^m * p, where A is your transition matrix and m is the number of throws. As you increase m, you should expect probabilities for states 14 and 15 to converge to their respective values.\n\nNote. It is better to compute the powers as p=Ap in a for loop. Avoid forming A^m in general. This is how Google's page rank was implemented by the way."", 'Thanks so much for taking the time to answer.\n\nSo I take the state probability matrix and multiply it by a vector {1,0, 0...n=15} ?\n\nHow does one determine the number of throws? Sorry bit new to the topic.\n\nYou can also see the matrix in this post here [https://www.reddit.com/r/AskStatistics/comments/13unyf8/comment/jm1to6x/?context=3](https://www.reddit.com/r/AskStatistics/comments/13unyf8/comment/jm1to6x/?context=3)', ""> So I take the state probability matrix and multiply it by a vector {1,0, 0...n=15} ?\n\nYes, correct. And whatever vector you get, multiply by a matrix again.\n\n> How does one determine the number of throws?\n\nYou stop when the first 13 entries of a vector become zero (or almost zero, say, 1e-3). Ideally we look at m=infinity, but I think 10-15 iterations will suffice.\n\n> You can also see the matrix in this post here\n\nAs it was noted in replies, you must insert 1's in positions (14,14) and (15,15). This means if you reach states 14 and 15, you remain there with probability 1."", 'Amazing thanks mate', 'Is there a name for this method of multiplying by this vector? Just so I can read up on it...', 'Power method\n\nUsed for computing the stationary distribution of discrete Markov chains']"
Any timeline on ISL Python Edition release date? [Q],[ISL](https://www.statlearning.com/),13ulv6d,drBonkers,1685338037.0,9,0.92,"['https://twitter.com/daniela_witten/status/1653785348202438657?s=20', 'Thanks!', "">Thanks!\n\nYou're welcome!""]"
[Q] Getting results from regression simulation,"Hi,

i run a regression 10000 times as one of my variables is obtained by a random event. I now have all the  results with coefficients, t- and p-value, standart errors and r2 for each regression run. I'd like to summarize the results of these 10000 regressions.

I know i can just take the mean of the coefficents but i also want to know wether it is statistically significant and i'd also like to display R2 in some way. Does someone know how?

Or it is statistcally significant when the mean coefficient is not close to zero? If so, how significant?

As you can tell my knowledge about statistics isn't that great so i'd be very thankful for your insights. Any literatur recommandations that does something similar to see how they display their results would also be appreciated!",13ulbk0,Basilis988,1685336274.0,2,1.0,"['p-value is a measure of significance', 'Plot the distribution of the statistics youre interested in. A simply kernel density would be a good place to start.', 'Yes but thats for the individual regression and not the mean coefficient over the 10000 regressions.', 'You are trying to aggregate the series of regressions?', 'yes exactly!', 'To what end?', 'I know i can get the coefficients by just taking the mean, but i would also like to get how statistically significant they are. Can i only do this by saying (e.g. it was statistically signifcant 80% of the time)?', 'Coefficients and significance are not the same thing.  \n\nYour p value is your significance test.  Typically 5%']"
[Q] Is there a less computationally intensive way to output a graph involving a complicated set of probabilities?,"Hey all. Longtime lurker, first time poster on this sub. I have a strictly-for-fun React project I've been working on since 2020. I had an idea for a new feature to add to it that's particularly exciting to me, partly because it's out of the scope of my mathematics/statistics background. I'm posting here in hopes that you brilliant folks might have some insight as to how I could begin to understand the mathematical basis of implementing it. I'm quite strong with React and Javascript in general, so once I understand the actual math I think this new feature will go quickly. I just need to grok it first.

I don't want to go into too much detail because I don't believe it's relevant, but long story short, my React app is a helper app for a mobile game I love. It doesn't interface with or automate any tasks in the game directly, but it does a variety of things to expedite certain in-game tasks one might have, and it has been a lot of fun to build, a lot of fun to share, and surprisingly effective at helping me with in-game decisions. The feature I want to implement can be summarized thusly:

\- Imagine that you have entered a competition where the winner is dictated by a particular score.

\- A player gains points by using one of several different kinds of currency.

\- Spending each type of currency will always return points. The number of points it will return is variable, and based on a known percentage breakdown. For example:

&#x200B;

|Currency|chance to get 5pts|chance to get 20pts|chance to get 100pts|
|:-|:-|:-|:-|
|Pennies|80%|20%|0%|
|Nickels|70%|25%|5%|
|Dimes|50%|30%|20%|
|Quarters|40%|35%|25%|

\- You can see other player's scores.

\- These events happen quite frequently, and it's not terribly difficult to get more pennies, nickels, dimes, or quarters. **Therefore, it would be incredibly useful to know the likelihood of beating the highest score if you spent all the pennies, nickels, dimes, and quarters you had available.** That way, if there was very little chance of beating that high score, you could just save your currency for the next event, where you'd have a better chance of winning after saving more.

So here's my idea:

1. The user inputs the number of pennies, nickels, dimes, and quarters they have in reserve.
2. The app outputs a graph. The x-axis is potential scores. The y-axis is the probability of getting them. At very low numbers of pennies, nickels, dimes, and quarters, you'd expect this to just be a few points (for example, the graph of one penny is just the points \[5, 80\] and \[20, 20\]. The graph of two pennies is \[10, 64\], \[25, 32\], and \[40, 4\]), but as the numbers creep up it would be more useful to simply have a curve.

I know I could just brute-force this graph by taking the user input of the number of pennies, nickels, dimes, and quarters they have, running an arbitrary number of simulations with them, and outputting a bar graph with all the results stacked up next to each other, but that seems terribly computationally intensive for a small app that is (so far) very lightweight.

My question: Is there a set of equations I should be looking into for a way to input these probabilities into some sort of function which would output a less computationally intense version of what I need?

Please let me know if I need to clarify anything. Thank you very much for your time.",13ujn3n,MarvinLazer,1685331085.0,3,1.0,"[""It depends on how accurate you want to get and how many coins you usually spend. If you have a small number of coins to spend and want a very accurate graph then you need to simulate that small number of coins a large number of times and draw a detailed plot of the outcomes. \n\nBut if the number of coins is large and the graph doesn't need to be very accurate then you can use a normal approximation to speed things up tremendously: \n\n1.  Calculate the expected value and variance for each coin. \n2.  Multiply the expected values and variances by the numbers of coins.\n3.  Add up the expected values and add up the variances.\n4.  Draw a normal distribution density with the resulting final expected value and final variance."", ""I don't believe there is a shortcut to compute this. Each currency itself is a multinomial distribution, but I am not sure how you would combine them to answer the question you want.\n\nHow many coins are you usually dealing with? The actual number of combinations is probably a lot less than you think. If you have 10 pennies, there are only 11 unique outcomes you need to look at. For 10 nickels there are only 66 unique outcomes. Each outcome is not equally as likely, but you can pre-compute their likelihood once and save that data somewhere. So even if someone had 10 of each coin, that is just over 3.1 million combinations you need to check.\n\nTo answer you question of beating a max score, you don't need a graph. You could just give  someone the probability of that event. Otherwise you would display a histogram (some plotting packages might be able to display a smoothed version for you)."", 'Can you exchange a nickel for 5 Pennies?  A penny has an expected values of 8 points. .80 * 5 + .20 * 20.   A nickel has an expected value of 13.5 points.  5 Pennies would give you on average 40 points which is much better than the 13.5 points youd expect on average from a nickel.  You should probably work out the math so spending a higher amount yields you a higher expected value than the sum of the equivalent lower amounts .', 'As another example a quarter had an expected value of 34 points .  Its the sum of (.40 x 5)+(.35x20)+(.25x100). If you run a sim as a for loop for 10000 times and do a draw based on those probabilities, sum up the points and divide by 10000 which is the number of sims you will see the mean of your simulation should be very close to 34.   If you do the same for 25 Pennies you should see an average of approx.  (25*8)=200. So in your game youd always prefer 25 Pennies over a quarter.', 'For example, the pennies in your table have an expected value of 5x0.8+20x0.2=8 and a variance of (5x5x0.8+20x20x0.2)-(8x8)=36. So the sum of 100 independent pennies has an expected value of 800 and variance of 3600. The larger the number of coins the closer the spread will come to the shape of a normal distribution (but never reach it).', 'This is the way. As long as each coin is independent (which it sounds like they are), you can easily add each normal approximation together to get the final approximation of turning in all of your coins. If X \\~ Normal(a, b) and Y \\~ Normal(c, d), then X + Y \\~ Normal(a+b, c+d). You can do that for all 4 coins.', ""Hi! Thanks a lot for responding! To answer your question, we're generally looking at numbers of all coins (total, not for each coin) in the hundreds to low thousands.\n\nCan you expand on the point you make in your last paragraph?"", ""No, you can't exchange a nickel for 5 pennies. I probably shouldn't have used RW currency as it might confuse the point, but the fact that there isn't a straight exchange rate between the different types of currency is part of the system.\n\nPerhaps I should amend pennies, nickels, dimes, and quarters to be something more ambiguous like green, blue, orange, and purple pellets."", 'This is EXACTLY the kind of info I need. Thank you!', ""Not sure what you want to expand on exactly. Your OP stated one goal was to know the probability that after turning in all of your coins you would have more than the current maximum score. I was just saying you don't need to show that in a graph as this isn't the easiest thing to graph out.\n\nIf you have hundreds or low thousands this calculation isn't so simple. For pennies if you have N coins there are N+1 unique combinations of outcomes. For nickels, dimes, quarters if you have N coins there are ( 3 + sum\\_{i=1}\\^{N} i+2 ) number of unique combinations of outcomes. For example with 10 nickels, that is 3 + 3+4+5+...+11 = 66. To get to the 3.1 million number you multiply all the possible unique combos together, so for pennies, nickels, dimes, quarters it is 11\\*66\\*66\\*66 \\~ 3.1e6.\n\nSo you can repeat that with the values you expect to see and check if the calculation is feasible. You are probably stuck simulating random draws and just accepting there is some error in your estimate.""]"
[Q] Have we decided on a way to discuss percentages of percentages?,"This comes up occasionally and can be wildly misleading.

The governors approval rating went down 10%

This would be true if it went from 44% to 40 or if it went from 50% to 40%.  

In politics, I think everyone agrees we are talking about the latter - 10 percentage points.  

But I have seen it used in other fields where it is ambiguous.  I wish I could think of Ana example at the moment, but I cant at the moment.  I had been meaning to ask a friend who studied this stuff, but there hasnt been an opportunity, so Im asking yall.

Does anyone know if theres an agreed up method?  Thanks!",13ufh3q,Colin-Grussing,1685319118.0,39,0.9,"['In AP Statistics we say percentage *point* increase/decrease vs percent increase/decrease. \n\n50% to 40% is a 10 percentage point decrease or a 20% decrease. \n\n40% to 50% is a 10 percentage point increase or a 25% increase.', 'Basis points?', 'IIRC, technically The governors approval rating went down 10% means that it went down from 44% to 40%.\\\nTo mean it went from 50% to 40%, you should say ""The governors approval rating went down 10 percentage points"".', '> The governors approval rating went down 10%\n\n> This would be true if it went from 44% to 40\n\nI\'ll be the pedant to say that 44 going down 10% is actually 39.6.\n\nBut for the actual question, agreed with the existing answer that ""percentage points"" is a good way to distinguish. Otherwise, use the terms ""absolute"" or ""relative"" to ensure clarity.', 'I tell students in math stats that anyone using percentages is confusing themselves and everybody else.  Just say no!', 'Percentage points vs percentage.', 'Even if we math people evangelize a non-ambiguous way of communicating this information, the people who write for magazines and newspapers will choose to be blissfully ignorant of it.  So technically, no; there is no agreed upon method.', 'Include whether the % decrease was relative or absolute. E.g.\n1. The governors approval went down from 44% to 40% (approval went down 10% relative). \n2. The governors approval went down from 50% to 40% (approval went down 10% absolute).', 'Depends on the story you want to tell.', ""I work in Statistics and credit model validation, and the number of times I've had to correct myself from flying into a raging fit because some folks don't seem to understand the right way to report. If a model performance has decreased by 5% from an earlier discrimination power of 60%, I'd think it's down to 57% but nope, turns out it's 55%. I've corrected it numerous times to say xx% points - somehow i hacent been able to get it to stick.."", 'In swedish the distinction between the two literally translate to ""percentage units""', 'I just state the starting and ending point: Approval rating dropped from 44% to 40%, a change of 4 percentage points. Many people are bad at math, and many people are bad at unambiguous communication, and neither of those is going to change any time soon.', ""This! There's no ambiguity in the terms themselves, they're very clear! It's just that statistical literacy isn't as high as we would like it to be and some people love to exploit that for their [political/marketing] agenda"", ""I've also heard absolute percent and relative percent increase"", 'Approval rating went down 100bps from 2% to 1%', 'That works too, if everyone is on the same page.  I think if someone said the latter, everyone would understand.  But, if they said 10%, some people would think 44 to 34.', 'Awesome, thats simple and clear.  Thanks!', 'People who use percent when they really mean percentage points are just wrong.', 'You\'re being downvoted, but you\'re right. Choosing to embrace confusing language because ""people should just know better"" is choosing to be a bad communicator. As much as I hate to hand it to the finance industry, they\'ve had this figured out for a long time. In finance, changes in values on the percentage-scale are basis point changes, proportional changes scaled to 100 are percent changes, e.g., a change from 8% to 4% is a 50 percent reduction and a 400bp reduction.', 'Some people may think that, but they would be wrong']"
[Question] Difference between Asymptotic and Non-Asymptotic approach to mathematical statistics?,Can anyone explain it in ELI5 terms?,13u8vwh,Icy-Chard9519,1685302240.0,3,1.0,"['These aren\'t really ""approaches to mathematical statistics"". Mathematical statistics is just the study of the mathematical theory of statistics in general. Certain *procedures* may rely on asymptotic approximations. In that case the general idea is that in practice we may not know exactly the distribution of some quantity, and so we can\'t say exactly what properties it might have (e.g. the probability that it lies within a certain range). In that case, we might rely on the fact that, as the sample size goes to infinity, the distribution becomes closer to something known, in which case we can argue that *for large samples* this asymptotic distribution might be a good approximation.', ""Asymptotics inherits its usual meanings from mathematics, but specifically in statistics asymptotics is most typically used to refer to large *sample* approximations, often distributional approximations or things that would relate to those distributions, like moments or characteristic functions,  for example.\n\nEvery time you use the central limit theorem or Slutsky's theorem (or both together as is often required but not stated) to argue that something should be approximately normal in large samples, you're either relying on or perhaps even directly invoking asymptotic arguments. Similarly when using large sample approximations that yield the Kolmogorov distribution in a KolmogorovSmirnov test, or the generalized extreme value distribution to approximate the distribution of sample maxima in large samples. \n\nAll the derivations of those limiting distributions that the respective distributions converge to as sample size increases in the limit use asymptotic arguments.\n\n(An derivation or approximation that didn't rely on such an asymptotic argument wouldn't be asymptotic.)"", 'With asymptotic results, you are taking a limit as the sample size tends to infinity.\n\nWith a nonasymptotic result, n can be arbitrary, but the result is still valid. Think of Markovs inequality,  for example.']"
[Q] Is it possible to do multivariable analysis here? I don't have access to participant level data,"I currently have the following table (link is to a screenshot):  
[snipboard.io/XfhQGA.jpg](https://snipboard.io/XfhQGA.jpg)

  
I know the values of all the X's., but do not have access to individual level data. Is there a test I can do to achieve what I am looking for? Thanks in advance. Happy to provide more information if needed. Assume that all X values can be given in a percentage or a raw number.",13u7zwo,Jusstonemore,1685299943.0,1,1.0,"['What are you looking for?', 'You should be able to combine the race tables into one big race by risk factor table (if you do it right), because they\'re mutually exclusive categories, so getting White, Black, Asian, Hispanic (and possibly ""Other"" from whatever is left over since those aren\'t exhaustive categories) should be possible.\n\nHowever, you can\'t combine age and race by risk factor, you don\'t have that information there.', 'Account for effect modification. Ie with all the factors in one model, which one comes out to be significant odds ratio', 'Not looking to combine, looking to do multi variable model. Account for effect modification', 'Whats your DV? Unit of analysis?', 'So what I want to do is see if a greater proportion of people with the risk factor exist with certain demographic groups. Then for the groups that are correlated with the risk factor, I want to do multivariable analysis to account for effect modification.']"
[Q] Is it possible to have a significant (<0.05) p-value for Pearson's Correlation test but non-significant (>0.05) for regression analysis?,"My friends and I are doing an analysis using SPSS for a research paper, with one dependent variable and two predictor variables (X and Y). We performed Pearson's Correlation test and saw that both predictor variables are significant when measured against the dependent variable with a P-value of <0.001.

However, when performing the regression analysis, the P-value is different. For predictor X, the p-value stays at <0.001 but for predictor Y, the p-value jumped up to 0.168. We checked normality and multicollinearity but it seems our analysis fits all the assumptions.

Currently, we are not sure how to proceed or what the reason could be for this. Any help will be appreciated!

UPDATE: Hey everyone, thanks for the help! We added more data for our dependent variable (so it has more than 20 distinct values) and that seemed to solved the problem. Thank you for all your suggestions though, highly appreciated!",13tzkpy,Meagster97,1685277809.0,24,0.94,"['> UPDATE: Hey everyone, thanks for the help! We added more data for our dependent variable (so it has more than 20 distinct values) and that seemed to solved the problem. \n\nRelevant xkcd: https://xkcd.com/763/', 'Recall that the coefficients in a regression analysis are telling you the average change of Y given a unit change in X1 holding all other variables (and therefore their relationship with Y) constant. That means if you suck out the variance that is explained by X2, what remains may be less clearly correlated with X1 then youd calculate in a Pearson correlation', 'Are they your p-values for the individual variables in the multiple regression model?\n\nCan you say what these variables are?', 'Do your predictors correlate with each other?', 'Yeah OP did not have a problem that needed to be solved!!!', 'At this point I just want to pass this course so if it works it works, I guess? Hahaha in all seriousness though this problem and comic made me realise I need to brush up on my statistics', ""But I thought that if the variance is low enough, this shouldn't be a problem as it solves the multicollinearity issue that could cause the p-value to be insignificant? Or am I understanding this all wrong?"", ""Yes I think so.\n\nSo we have dependent variable: View on Social Benefits. Our two independent variables are: Trust in others and Trust in government. We used SPSS and found that for Pearson's correlation between Social Benefits and Trust in others the p-value is <0.001, and the Pearson's correlation between Social Benefits and Trust in government the p-value is also <0.001.\n\nFor the regression model, the p-value of Social benefits against trust in others stayed at <0.001 but the p-value of social benefits against trust in government the p-value became 0.168."", 'I recall multiple regression in spss has a measure of collinearity.', 'If you would have a worse grade simply because one of the variables were insignificant, then the assignment is very poorly designed.', ""I taught social science statistics for 3 years. Every assignment. I had to remind all the students. A non significant result does not mean you discovered something of insignificance, and we are not going to fail you for that. Your grade will drop if you can't draw an illogical conclusion from it however."", 'Yes this is wrong. Including other variables in a regression models controls for their effect on the DV.', 'I know you have sorted this but here is where you may be having issues.\n\nThe variable that did not achieve significance with might not be linear. Check your scatter plots, maybe you need to use a different equation to fit the model.\n\nThe other issue is (maybe someone with more advanced stats can confirm) that trust in government might be on the mediating path between trust in others and social benefits so controlling for this might not be a good idea.\n\nThere could be a direct causal path between trust in others and social benefits but not your other independent variable.\n\nMaybe someone can chime in on this matter , someone with more experience with DAGs or causal models.', 'Whats the issue here?', 'I would not phrase it exactly that way. If the independent variables are orthogonal, then the regression equation decomposes to pairwise correlations. Its just that you cannot be simultaneously highly correlated to multiple variables and still be positive definite.', 'Which can be easily shown using the [FWL theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem). The annihilator matrix for X_1, M_1, will get premultiplied to X_2, but since they are orthogonal, M_1X_2=I.', 'Only perfect correlation ( rank < n ) renders a matrix indefinite']"
[Q] Interpreting a clinical trial that didn't test a statistical hypothesis,"Have linked trial at end.

I'm interested in a trial published last week (ELAN) that compares early vs. late initiation of anticoagulation (blood-thinning medicines) after an ischaemic stroke, in those found to have new atrial fibrillation (an irregular heart rhythm that's known to cause strokes which is treated with anticoagulation). The aim was to compare incidence mainly of recurrent strokes, intracranial haemorrhages and death.

As you can tell, I'm not much of a statistician, so I have some (probably very basic) questions regarding the statistical desgn of this paper.

Unlike most other randomized controlled trials, this one didn't have a null hypothesis set up for superiority or non-inferiority, as the trial author said they didn't have data to determine what effect size to aim for. Their end result was that the incidence of the above composite events ""ranged from 2.8 percentage points lower to 0.5 percentage points higher"" with early vs. late anticoagulation initiation.

At 90 days, the composite incidence was 3.7% in the early-treatment group and 5.6% in the later-treatment treatment group (odds ratio 0.65; 95% CI, 0.42 to 0.99). The study also looked at outcomes at 30 days which is where the overall incidence comes from in previous paragraph.

Overall does the lack of hypothesis testing mean that we can't be certain about the data presented in this trial and that it is only meant to be highly probable?

Lastly, the authors say that this trial was made because although there are some small RCTs elsewhere about this topic, the data used for those studies were either poor quality or didn't have enough participants. Does that mean they chose to forego using data from those trials to make a hypothesis for this one?

Thanks for any help!

https://www.nejm.org/doi/full/10.1056/NEJMoa2303048",13txojt,MergeMF,1685271720.0,5,0.86,"['You should probably link the study. From your description it is still unclear if it is interventional or observational. \n\nIn general it might be useful to think about the comparison as ""time (prompt vs delayed)"" since all patients are initiating the anticoagulation.', ""Thanks for the answers guys! \n\nhttps://www.nejm.org/doi/full/10.1056/NEJMoa2303048 \n\nHere's the trial. Needs a NEJM account to view but doesn't need a subscription."", 'If a hypothesis is not stated at the outset, then offering a conclusion about significance after looking at the data is called data dredging or p-value hacking and is considered junk science.', 'If the study is a properly designed randomized controlled trial, then the odds ratio and CI can be a conclusive effect measure. I am not particularly familiar with anticoagulants studies, but I do have a few doubts just from reading this post.\n\n1. I have yet to see any randomized controlled trial can get away with initial activation approval without providing any sort of power analysis. As you know RCT are expensive and nobody wants to fund a study that might be underpowered or overpowered.\n\n2. I dont know if the at 90 days is from the time of randomization or from time of treatment start. They address different questions.\n\n3. The null hypothesis normally is from standard of care, an experienced physician would have a good estimate of the SOC rate to be used as null\n\n4. Most importantly, are there any other comparisons for this dataset that are sharing the type I error? Is the result cherry-picked to only report significant p value (in this case, 95%CI excluding 1)?', ""It's an interventional study, and I've linked in another comment. Thanks for your help."", ""https://ibb.co/0syvQp8\n\nI've attached a screenshot of the statistics protocol of the paper, and have also added link to paper abstract in original post if you'd like to see it.\n\n1. Sounds like they determined the sample size of around 2000 patients recruited based on the estimation of the confidence interval width around estimate of treatment effect.\n\n2. Time from randomization within 48 hours of stroke symptoms. \n\n3. The investigators' rationale for no null hypothesis or non inferiority testing is that there isn't a universally accepted of a non-inferiority margin for this context (any degree of intracranial bleeding, recurrent ischaemic stroke etc. that is acceptable to the clinician and/or patient)\n\n4. Not sure sorry lol""]"
[Q] How to determine the learning effect at the individual level?,"Does anyone know of any good approaches for determining when someone has started to show a difference in performance between conditions in response times? E.g., if we ask participants to do a task with two conditions, that involve some learning, a difference is expected to occur once enough learning has accumulated. Is there a good way of assessing when this difference is different from zero at the individual level? I think I know how I could do this using bayesian inference, but not frequentist. This task has 1000 trials per individual, with a proportion of 90-10 per condition.",13to9dl,majorcatlover,1685238205.0,2,0.75,"['I can help you in that question', ""You would need to either randomize across order of conditions that are presented or present all possible orders an equal number of times and then balance the number of subjects in each; then you can pull out the learning effect. I don't see any reason to expect it to be zero, (even if you fail to reject, that's almost certainly a type II error), but you can account for it and make sure it doesn't confound what you do want to measure."", 'I\'m not sure if this would fit what you are doing, but since you want to estimate a change in behaviour for each individual and you have 1000 samples from each, you could try change-point analysis to determine when the distribution from where you are sampling changes. This would be done for each individual. Then, with this you could have a ""time until event of interest"" variable that can be used to conduct survival analysis and determine the effect of the condition on achieving that learning level. \n\nOn a side note, someone else already commented how you need to randomize the condition you give among participants to account for any confounding, otherwise your 2 groups might not be comparable to begin with.', ""I can't quite figure out what your data is, intervention or outcome, but from what i can gather, you might want to look at something like a staggered difference in differences design."", 'Really? cool. Could you point me to a paper or something?', 'I want to determine WHEN it happens that each participant has learnt, but the learning effect is computed as the difference between conditions. Is there any statistical method that deals with this type of situation? For e.g., I want to know if participant A started showing learning after X number of trials or in the third block (each block contains 200 trials) whilst Participant B only showed robust learning after Y number of trials.', ""The data is already collected, I just need to analyse it. Usually this type of analysis is done at the group-level, seeing if participants have shown evidence of learning. But not at the individual level nor to determine when it occurs. Some people do use some random cutoffs like if participants show more than a 25 millisecond difference between conditions they have learnt. I don't like these arbitrary cutoffs though."", 'No, not an intervention. Just seeing whether the participants show the effect of interest and when that happens. Something like the stroop task if you have ever heard of it.', 'I can do the whole research project for you. A plagiarism free content. I can also do the analysis for you', 'I am unsure of what you want. What is the research question? How long does each individual take to learn given a covariate? How much does a specific condition change the time it takes to learn with respect to no condition at all? Determining what it means ""to learn""?\n\nAlso, I do not get how the experiment was made, did you give a specific condition to some patients and no condition to some? Did you give all of them the condition from the beginning or did you give it to all of them but after a certain amount of attempts?\n\nWithout having these clear it\'s hard to know what can be done or should be done', 'I am asking for help with figuring the best method, not to have someone do it for me.', ""No, all participants do a task with two different conditions though they are unaware of this (e.g., in the stroop task, the stimuli colour is either congruent or incongruent with the word's meaning). I want to see when the difference between response times per condition is great enough to allow us to say that the participants are learning. Typically, at the group level, a difference between conditions that is sig higher than zero is taking as showing learning has occured. But how can I do it at the individual level to determine WHEN the learning has occured instead of looking at all trials at the end of the experiment. I will still have all trials, I just need to know when the learning occured.""]"
"[Q] What is meant by ""minimum cell size""?","Hello, this is my first time posting here but I wasn't sure where else to go for help. I apologize if this is not the appropriate place to ask or I've done something wrong.  
I'm working with a dataset and the guide for the set says the minimum cell size is 100. I've never been taught about this concept, and I've never been good at statistics so I have no idea what this means. I was wondering if someone could explain it and explain what it means in terms of when I do my analysis.  
One of the issues I'm having is the number of observations I've tabulated in SAS does not seem to match what I thought there would be, but now looking at the data on its original platform, the number is marked as ""Cell size"" and not ""n"".",13tnv94,Miserable-Tear-905,1685236960.0,1,1.0,"['Are you fitting some kind of ANOVA? In that case, a ""cell"" refers to a particular combination of factor levels (e.g. if you\'re examining the effects of sex and treatment group on some outcome variable, a cell might be ""males in the treatment group"" or ""females in the control group"").', 'It\'s not 100% certain without more context but they likely intend you to understand that  there\'s at least 100 observations for each combination of categorical IVs (predictors).\n\n> but now looking at the data on its original platform, the number is marked as ""Cell size"" and not ""n"".\n\nAgain, it\'s not a certainty, since your question lacks context, but yes, it\'s likely that ""cell size"" means ""the count in that combination of categorical IVs""', 'I think you need someone to look at your data guide and data. That sounds to me like something about a memory/storage issue for the data itself. When you scan through a column of the data, do the values seem reasonable?', 'Thank you for the response. This makes sense in context for some variables (even though they have not made clear what particular data ""cell size"" applies to), so my next question is what exactly does the ""minimum"" mean? The guide mentions setting a ""minimum cell size for the denominator"" and being aware of that cell size. I have no idea what this means.', 'Appreciated, sorry for the lack of context but quite literally all the guide says is pretty much what I offered lol.', ""Yes, the data is properly formatted and matching the codebook. It doesn't seem to be memory/storage because they talk about numerators and denominators for the data, which would make sense for the frequency variables the organization generated."", 'What guide? What are you doing?', 'I think you may have groups where it cant estimate the parameters due to insufficient number of observations. How many parameters does your model have', '[https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2019/2019\\_YRBS\\_SADC\\_Documentation.pdf](https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2019/2019_YRBS_SADC_Documentation.pdf)\n\n""In general, CDC analyses of YRBS data use 100 as the minimum cell  \ndenominator although other analyses have used lower cutoffs (CDC uses 30  \nas the minimum cell size when analyzing sexual minority data, for instance).  \nDifferent cutoffs can be used but it is important to set a level for any analyses  \nand to be aware of cell size when analyzing the data.  \nNote: This refers to the denominator, not numerator. It is possible to have  \nnumerators of 0; these are reported as 0% with no confidence intervals.""']"
[Q] Maximum likelihood estimator in survival analysis with multiple failure timed,"I have a problem in survival analysis where the time of failure is only known to be in some finite set of time points. When I maximise the likelihood only some time points are assigned a non-zero hazard and I don't understand why!

Example: there are two observations. Observation 1 failed at either t1 or t2. Observation 2 failed at either t2 or t3. (Where t1<t2<t3). Using hn at the hazard at time tn the likelihood looks like this:

(h1+ (1-h1)h2) * ((1-h1)h2 + (1-h1)(1-h2)h3)

Maximising this gives h1= h3 = 0 and h2 = 1.

I find this really odd and counter intuitive! Why wouldn't there be a non-zero hazard at the other times when an event could happen then? 

I've tried this for similar setups and it seems that only if a failure time is known uniquely or is contained in both observations does it have a non-zero maximum likelihood estimate. 

Any thoughts? Does this hold in general?",13tix1g,Elephin0,1685223392.0,1,1.0,"['1) you seem to be saying your data is interval-censored \n\n2) if you have multiple failures, you are dealing with a frailty model \n\nBut your question is very clear so please clarify', ""Ah yes, it looks like interval censoring is what's going on here: there's only one failure but I'm unsure of when exactly it is. But maybe it's more of an optimisation question - I mainly don't understand why it maximises in such an unexpected way..."", 'It is common to write the likelihood function using the kronecker delta.  I think thats what you are seeing, if you are reading a paper.', ""This isn't from a paper, just something I'm messing around with. I'm just doing this non-parametically so I haven't made any assumptions about the hazard itself. I was trying to see how this compared to a Kaplan-Meier estimate""]"
High dimensional statistics in biostats PhD programs vs stats PhD programs [Q],"
High dimensional statistics seems to be a big field in stats. The problems of feature selection and various methods for learning when p>>n is quite interesting. Ive seen high dimensional statistics be a point of research within stats and biostats departments. But this makes me wonder, what the difference is when a PhD student is studying high dimensional statistics between both departments.

If one wants to study high dimensional statistics and develop methods, is this a better idea to do it in a PhD biostats program? Or is it better to study it in a PhD stats program? Can a PhD stats student work on high dimensional stats problems applied to biostats? Such as genomics? Would the coursework in a biostats dept or a stats dept prepare a student more for one whose interested in coming up with methods for p>>n problems?

I see working groups in both departments, so just wondering where a student should go if they wish to study this topic.",13tdqjm,AdFew4357,1685209889.0,18,0.91,"['I think one of the primary considerations is whether or not you _want_ the added considerations of what aspects bio- brings to the required coursework. \n\nMost programs I have heard of will require a) that you complete something akin to some coursework like intro to epidemiological methods, and b) that you have some cognate subject or area of concentration that is both statistical and biology / medicine / health related. \n\nMost programs will have roughly the format of getting your required coursework out of the way in the first couple years and then youll be more free to focus on your doctoral research at which point its up to you how you want to spend your focus as long as youre meeting the expectations of your department and advisors. \n\nI do think that you could work with faculty in biostats or stats departments focused on high dimensional statistics  the question is whether you would like the added constraint of demonstrating your researchs applications relevant to human health. \n\nKeep in mind if you end up having teaching requirements, everything I said about the health statistics related coursework youll be expected to take goes for the teaching requirements too. Do you want to be teaching statistical methods where the students are in the mindset of asking: \n\n* how does this method apply to epidemiological studies?  Clinical trials?  Survival analysis?  Genomics, epigenomics, metabolomics, nutrition, environmental health, cancer, etc? \n\n* how does this intersect with medical ethics?  is bias in the algorithm going to harm efforts to advance health equity? \n\n* if we use these advanced methods how can we communicate the results to doctors, health professionals, and patients who arent versed in statistics at all? \n\nIf thinking about those kinds of applications makes you want to puke then you probably want to think about stats programs where youll be much more focused on the mathematical and statistical theory of everything youre working on.  On the other hand, if those things evoke a feeling like of course we have to think about that! why would we work on these methods without addressing that? Then a Biostats program is likely a good fit for you. \n\nIf youre in the middle, you know you can apply to both sets of programs, and then try to assess based on interviews and correspondence with the faculty which program will fit you best.  Its worth saying not all stats programs are the same, nor are all biostats programs the same, so there may be program level variation that is worth taking into account.', 'If you are interested in biostats, do a phd in biostats. If you want to work on statistics with p>>n no matter the application do a phd in stats. Plus bare in mind that before starting the PhD you are most likely ignorant about a lot of possible research directions that you would enjoy (maybe you will find stuff even more interesting and relevant to you than high dim stats). In that case (which is pretty likely tbh) it is up to you to weight if you want options available within a stats or biostats department. \nBut in any case, apply to all options and see if you get offers. I would advise on choosing based on the department and mostly on the quality/areas of possible supervisors.', 'Gotcha. Wow thats interesting, does this switching of research interests happen often for PhD students.', 'It happens all the time. I went in 99% sure I would do some sort of research involving Bayesian methods (which I do still love), but my dissertation research has turned out to be completely frequentist with interesting applications in areas I never even thought about before.', 'Just think back to your start of undergrad about what you knew and thought you wanted to pursue.', 'I see. May I ask what made you switch over from learning the Bayesian methods to your frequentist Area?', 'I still am trained in Bayesian methods (Ive done some research and took a year sequence of Bayes classes in my program), but my research has turned out to be frequentist mostly because of a project that was brought to me by my advisor and I found to be interesting. In my research area (which involves real-time detection of abnormalities in complex systems), Bayesian methods are pretty uncommon and generally impractical, and non-Bayesian approaches are naturally better suited.', 'I see. Thats cool. So did you go into the decision theoretic properties of Bayesian statistics? Or a general overview (conjugate, heirarchical models)? And is a grad level bayes class measure theoretic?', 'We went into some decision theory and covered a fairly broad range of Bayesian theory and methods. My program is more applied, so it was not measure theoretic, although I imagine many programs have a more rigorous (measure theoretic) approach', 'I see thats cool. So I had a question for you.\n\nDo you think sparse statistical learning / feature selection / high dimensional statistics and the topics relevant to the statistical learning with sparsity book is still going to be an active area of research in statistics departments? Is it right now a hot topic / active area? How much will deep learning and CS departments make this area of statistics seem outdated? Asking because I have a high interest in researching those fields in a PhD program in 2 years, but Im worried the whole field would have shifted by then.', 'Anything high dimensional is still definitely a hot topic these days, especially in certain biostatistical applications.']"
[Education] seeking guidance to leverage statistics to build quantitative reasoning abilities,"Hello everyone!

Firstly, please let me know if this post does not belong here and I will remove it (I have read the rules and think the post fits here).

Some context: I am a product manager and got recently introduced statistics at my job. While I knew that subject existed, only when I started implementing at my day job, I started exploring the concepts.

I started playing around with numbers and concepts to derive insights and make decisions at work. Absolutely loving it and also realised that quantitative reasoning ability is an important life skill.

Efforts so far: I went through [statistic crash course by John Green on YouTube](https://www.youtube.com/watch?v=sxQaBpKfDRk), read some essays on the internet, and also started discussing with my team at work to understand more.

My ask: I want to 1) learn the concepts really well so that my foundation is strong, 2) explore some real life practice problems to build the muscle, and 3) start implementing at work and life at large.

Need community guidance to achieve the next milestone.

Thank you!",13t6eqz,_Gandalf-The-Gay,1685191206.0,9,0.85,"['Check out the ""Learning Statistics With R"" series of textbooks. There are a bunch of books based on the original that cover a wide variety of practical and theoretical (how and why) concepts using a variety of packages (R, Python, Jamovi, etc.). I think the books and software are all open-source.\n\nhttps://learningstatisticswithr.com/', "">  learn the concepts really well so that my foundation is strong\n\nIf you can find a copy to look at, I'd suggest taking a look through *Statistics* by Freedman, *et al* (to decide whether it's for you). It's not full of fancy colours and such but it's really solid on basic ideas and concepts -- which many intro-level books are *not*. Beware, there's a jungle of misinformation out there.\n\nOf course if you want to understand statistics well, some mathematics (calculus and some basics of vectors and matrices) would be a big asset, because then you can learn some of the basic theory rather than just read about the 'standard' techniques and accept whatever someone tells you about them. Learning some of the basic theory will make a big difference to how strong your foundations are."", 'Can you guide me on how to get started with basic theories?', 'If you have the mathematical background, a standard undergraduate text on mathematical statistics would be useful. That will teach you some material on probability (including an introduction to distributions), and then should cover material on inference (estimation, tests, intervals etc).\n\nSome possibilities would be DeGroot and Schervish (*Probability and Statistics*), or Wackerly, Mendenhall and Scheaffer, (*Mathematical Statistics with Applications*) or Rice (*Mathematical Statistics and Data Analysis*), or a dozen others.\n\nWhile these books almost all have a section on probability at the start, you might like to look at a course on probability first; like this one: https://projects.iq.harvard.edu/stat110/home . There\'s a free pdf text there, as well as a bunch of other resources including video lectures and exercises and such.\n\n(It\'s possible to get started on probability with less mathematics - some subjects will have titles like ""Applied probability"", and that\'s handy for the early concepts, but it will soon get to the point where you need a bit more foundation.)\n\nYou might like to consider a formal course on statistics; there\'s a number of online ones -- many are pretty inexpensive.\n\nThe point of covering a subject on mathematical statistics is you can then cover some of the theory of major topics like regression, ANOVA, generalized linear models and so on, rather than just rote-learning stuff about them.', ""Got it.\n\nI don't have a mathematical background. And will check out your suggestions. Thank you!"", ""Either way, I'd suggest Freedman as one useful resource.\n\nEven though it's a pretty simple book, it's one that I've seen many experts recommend - it's not just me. (Freedman was a very accomplished statistician and his expertise allows him to construct explanations and analogies that do a good job of conveying the main point.)""]"
[R]I'm looking for statistics about how many people/companies that are working remotely after the pandemic.,"It would be great if the numbers are from 2022.
Does anyone know where I can find it? I have tried eurostat, but with no luck.",13szowk,hvahvahva,1685167807.0,0,0.14,"['This is a sub where people can seek advice on how to perform statistical methods not ask where to find certain data or analyses that were performed.', 'Fair!']"
[D] Short Time Series Forecasting,"Hi, I would like to know what sort of approaches are used/ is best for forecasting short time series.

Im currently looking at a market supply and price forecasting situation. Where I need to forecast the market supply and price on a daily basis. It seems like taking the historical average is a good approach. But I would like to know what else is out there.

Thank you.",13sz55s,allicrawley,1685165915.0,2,0.75,"['Forecasting principles and practice by hyndman will give you a fairly solid roadmap.', ""How short is short?\n\nExponential smoothing, double exponential smoothing ... among many other things. Kind of depends on the situation.\n\nAre you forecasting both together? I'd assume they don't behave independently.\n\nIf there's weekly cycles, there's potential modifications for that."", 'Kalman filter', 'I would start with a naive forecast (ie the most recent observation is the prediction for the next) as a benchmark. And then try to improve, which might actually be tough.', 'Thanks. Ive started reading that book ', 'I second this!  It starts out simple, and goes step by step.', 'Thank you for your response.\n\nTechnically speaking, Ill have the data since the start of the year. Which might not be short. However, the predictions will be used by an optimiser as an objective function / constraint. Therefore, I need to keep the forecasting technique extremely simple. Furthermore, there are weekly cycles to deal with.\n\nYoure right theyre forecasted separately.\n\nIll look up how I can apply the techniques youve mentioned. Thanks', 'Will look it up. Thanks']"
[Q] is it a good idea to start a job that is not relevant to the degree in statistics if I want a career in data analysis?,"I recently got a job offer for a chemistry assistant job, which is weird to me because the job requires a chemistry/science degree and i would be working in a lab. The job description loosely mentions data analysis. Basically, its very disconnected from the job I actually want, which is data analytics/data science. 

I am a recent graduate, so this would be my first job in the real world. My only concern is that it wouldnt help me at all in getting to my end goal of data analytics, and Im concerned that I will lose a lot of the skills I gained during my studies by the time I leave. Also I just started applying, so Im not even sure if Ill have a hard time finding what Im looking for. 

What were your entry level positions? Is it standard to get a job not relevant to your degree and then transitioning later? Thank you all so much for the advice!",13sslke,lewdjojo,1685145687.0,3,0.81,"['Sounds like the data analysis connection is a bit loose tbh', ""Can you get some clarity on how much analysis you'll be doing? Because if there is a lot of statistical analysis is could be related. What exactly would you be doing according to the job description?"", 'It doesnt seem like much. Here is the job description: \n\nDaily sample preparation and data analysis to ensure laboratory runs smoothly and meets required turnaround times.\n\nMethod optimization as required.\n\nEnsures compliance with internal Standard Operating Procedures (SOPs) and quality systems.\n\nEnsures test equipment is clean, maintained and in good working order.\n\nMaintains a safe, clean and sanitary work area.\n\nMaintains an accurate notebook and other required documentation protocols.\n\nFollows all safety procedures and documents.\nEnsures high quality standards are met through performing calibrations, quality checks and proficiency programs.\n\nMaintains inventory tracking as required.\n\nBeing aware of laboratory hazards and safety regulations, including but not limited to wearing appropriate PPE\n\nAdditional duties as assigned.\n\nIt seems like the most Ill be doing is cleaning up, preparing equipment, and the occasional write up or light data analysis. No mention of statistical software or anything. I have yet to go to a full job interview, but when I do Ill ask about it. Hopefully they might be able to assign me more relevant work, but assuming they dont, Im not sure if I should take it.', 'Yeah that sounds like barely an analysis job. Oof I see your dilemma.', 'Yeah and i dont really know what to say either. I did apply, but it was more of a might as well type of thing. I never thought Id get any attention since I didnt have any of the requirements listed. \n\nIf I do attend an interview can I use some judo mind trick to try and get something more fitting what I want? I need a sentence I can say that sounds polite and understandable. Its not the job I was looking for but its not something I want to pass up either.', ""I'm confused did you get an offer or did you get an offer to apply? If you haven't interviewed yet you could ask them during the interview about what kind of statistical packages they use, what kind of analysis techniques do they use for the data they collect in this lab. That'll give you some clarity about how much analysis is actually done in that research."", 'I applied and I got a call saying they really want me. They even told me the salary for the position which wasnt on the job posting, and said they would forward my application to the executive to skip the HR Process. And considering only 15 people applied for this position I do think its in the bag if I want it. Im pretty good at interviews so If I want it Ill definitely get it. No formal interview yet. \n\nIm going to ask about that, thanks for the tips.']"
[Q] Are entry-level data scientist positions tough with two masters degrees (first in Data Science and second in Statistics) with no practical experience?,"I am a foreign student in the United States of America.  
I completed my MS in Data Science and will pursue an MS in Statistical Science.

  
There were a few reasons for pursuing a second MS degree:

1. The Master of Science in Data Science program did not provide a comprehensive understanding of the statistical component within the curriculum. Emphasis was placed on data processing techniques and utilizing Python for model fitting and training, with some exposure to data visualizations to a limited extent.
2. The current state of the job market is highly challenging. Despite my best efforts, I encountered significant difficulties in securing employment as work visa sponsorships are tough to come by. The student visa restrictions add to this predicament, which necessitates obtaining a job within a specified timeframe. Failure to do so might compel me to explore employment opportunities in a different country. Regrettably, the prospects for Data Science positions in my home country are rather bleak.
3. I have a full-ride scholarship including the living expenses for the MS in Statistical Sciences.  


My MS in Data Science in 2023 and plan on completing an MS in Statistics in 2025. I have no practical work experience.

  
I do understand if I create a stellar portfolio of projects and get in summer internship and a Co-op, I would be in much better shape than I am today, and hence I plan to work on those aspects as well.

  
In 2025, will I be overqualified for the entry-level Data Scientist positions?

  
For recruiters, would you see my two master's degrees without any practical work experience as an anchor to my future career in Data Science?",13ssbn1,Informal-Fly5759,1685144960.0,8,0.7,"[""You're going to do fine in 2025. Nobody can say what the market will be like in a few years but you'll have valuable, hireable skills regardless.\n\nMake sure you don't undervalue connections with other professionals, communication and people skills. Getting interviews is one thing (which connections you make today will help with), but doing well on the interviews you get requires soft skills. It also makes a huge difference in your career as well."", ""Start applying, get used to murder at technical interviews. You won't be over qualified. Trust me."", 'As a former recruiter, whats your visa status? One of the biggest hurdles as a recruiter was dealing with different visas and whether or not the company could work with an individuals visa situation. Transferring a visa is one thing, but sponsoring one is a whole other deal.', ""Don't do a second master's, it sounds like a complete waste of money and time.  Most of what you are missing you can easily review and learn on your own regarding the statistical theory. Which I would strongly advise. A second degree won't necessarily be seen as more experience either, eventually you will need real work experience. Start making side projects and posting to GitHub instead. \n\nMost DS jobs will focus on just a few methods (cluster analysis, regressions, ab testing etc.) that work for their industry, it's the coding, modeling, data pipelines that they care about which you got from your DS masters. \n\nI have a PhD in ecology (focused on stats and machine learning). I work as a software developer /  DS. Although it's anecdotal, the majority of the statistical knowledge that is required is usually capped out at a 2nd or 3rd year understanding (standard deviations, sample design, regression tests, distributions etc). It's the programming and problem solving that they care about. For those times when more complicated methods are required, there are Senior DS who will take these projects and you can learn from them."", 'If you can do write python scripts/ SQL/ build work flows. Look at any healthcare system that is attached to a school on the east coast. The will under pay you at 80k, but after 2-3 years experience you can get better paying jobs elsewhere. Gl', 'Where are you doing your MS? Im looking for funded masters programs because things arent looking hot with a BS right now', 'Interested too']"
[Q] What kind of jobs should I be looking for with an undergrad in stats,"I havent gotten a job yet and Ive applied for like 40 jobs so I must be doing something wrong

I have a BS in statistics and spent a year in a statistical genetics lab on campus where I ran analysis on genotype data and data cleaning/manipulation, and Ive presented many publications to the other lab members. Im applying for jobs like entry-level data analyst positions for healthcare or clinical research or marketing companies. Jobs that require undergrad-level skills using R and python. I got an interview for a research analyst role in clinical research and I never heard back. Ive probably applied for at least 20 jobs that I never heard back from. 

Is the job market that bad or am I applying for the wrong jobs? What s",13spl0p,rabidsaskwatch,1685137923.0,26,0.87,"['Data Analyst positions would be great. If you have good projects in R & Python, Data Scientist positions could be feasible.', 'Another place you might consider is research for the government or legislature.', 'Im in the same boat as you! Job hunt can be tough but we shouldnt get discouraged by rejection. Im applying to entry level data analyst jobs or jobs in data science and ml that I find interesting. Ive even taken a shot at some financial analyst positions because i really only like the companies bahahaha any advice would be helpful from anyone', 'Analyst jobs but listen carefully. They range from quasi Data Scientist to Excel monkey', ""Sounds fine to me. It is very hard these days to get a job. I'm in the same position as you with an MS and I didn't get my first interview until nearly job 50. I'm still looking after more than 200 so it's basically just a numbers game. I'll admit I probably could have had *a* job, but there were a few roles that I just wasn't interested in anymore after interviewing. \n\nIt's nearly 50-50 for me as to whether a job will give a rejection or just simply never reply to your application, so that's normal too. I've noticed that it's gotten a lot harder in the past month or two to find any roles to even apply to, which is also a bit depressing."", 'You could also look into business analyst/intelligence type roles', 'As others say you may try to apply to analysis position. It can be product analysis (quantitatively based) etc. I think your knowledge is suitable for many positions while mastering either r or python (or even java!) is just st a matter of time.\nGood luck', ""Lots of data engineers start off as analysts so that's a possibility down the line too."", 'Sql and excel are probably more useful for entry level data analyst roles. Data scientist roles would be hard to get without masters.', 'What would the title be for those positions?', 'No shame of starting as Excel monkey, most advanced jobs require masters. With some creativity he can convert a boring Excel job to an exciting Python job by automating some of the tasks and maybe work on side projects on your free time.', 'Usually something like Research Analyst.']"
[Q] How to interpret Weibull distribution?,"How should my interpretation of a linear model change when its residuals fit a 3-parameter Weibull (with Threshold) distribution, versus when small p-values reject that hypothesis? What's the practical takeaway, or my next steps to investigate?

For context, I build energy consumption models for whole-building operations, generally with two-year baselines.

If there are any other specifics I should need to provide, please let me know. Thank you.",13smv84,Wesley_Lexus,1685131009.0,2,1.0,"[""It strikes me as weird for energy consumption data to be Weibull distributed. Afaik you get Weibull data mostly from non-memoryless survival times and from block maxima. Is your data censored or representative of maxima over fixed time intervals? If not, how did you arrive at the use of the Weibull distribution?\n\nEdit: if you're using a GLM with Weibull residuals, you interpret it like any GLM model. I don't think the residuals being Weibull changes much."", 'I\'m as bewildered as you. From what I know about how Weibull distributions are applied, I can\'t make it make sense for our applications.\n\nIt\'s part of the analysis script we run after each model is specified. When a statistic is out of spec, we\'re to make a note of it, what it means for the model, and why we believe the model should still be used.\n\nWhen I\'ve asked about it, my manager says ""it\'s a test to the model\'s longevity,"" which makes no sense to me and sounds like something they heard from someone who heard it from someone else who heard it from someone else.', ""I may have misunderstood something here. I'm going to lay out a couple possibilities here \n\n1) (I had assumed this one was the case) You have a linear regression model with residuals that have been found empirically to be Weibull distributed by some distribution fitting excercise, and you therefore are using a generalized linear model with Weibull residuals. The p values of interested in relate to the beta coefficients of some predictor or predictors of some response variable (energy consumption?)\n\n\n2) You are using a glm with the residual distribution specified as Weibull. The p values of interest relate to the beta coefficients of some predictor of predictors. But there's no good reason to be using Weibull beyond a nonsensical assertion made by your manager (your wildly incompetent manager if true).\n\n\n3) I've completely misunderstood what's going on here. Maybe the p values relate to a kolmogorov smornoff test on your residuals, or you're not using a linear model, or maybe I'm misunderstanding the nature of your data and the response variable is a measure of model performance and you're doing a regression to see of the model has continued to perform adequately over time or something.\n\nCan you please tell me which case is true, and if it's 3 describe your modeling problem and methods in clear terms?"", ""I dont think it can be a glm that theyre discussing (though maybe it ought to have been).\n\n With glms you specify the conditional distribution of the response, rather than a residual or error distribution, and that conditional distribution is exponential family (which this shifted and  truncated Weibull isnt).\n\nYou can fit regression-and-glm-like models with a conditional  Weibull response easily enough, but not directly in a glm (unless it was specifically exponential). I've used (parametric) survival models (log link) to take care of the fitting, which worked very well. You can also do it more directly just by maximizing likelihood but I prefer to use already solid code and readily available  output set up for inference already.\n\nIts hard to be sure since the OP didn't explain their analysis steps cIearly but it sounds to me like the issue is just that OLS yields skewed residuals - which might or might not be all that relevant, depending on the aim and what else was wrong with the model.\n\nI'd worry first about the \ndependence over time, issues related to stationarity etc. This may come up at several points, but I'd ponder it from the start.\n\nThen I'd worry about the conditional mean / link function. With regression problems, that's the biggest one. Get it wring and everything else is moot.\n\nThen I'd consider the conditional variance / variance function.\n\nThe shape of the conditional distributions comes after those in importance and might not matter very much unless prediction intervals are required.\n\nI imagine that glms could be a a good place to start with this, if the time series aspects dont prevent that.""]"
[Q] Best nomenclature for series of PDFs?,"So I am creating a stochastic model of a digraph where each edge (defined by i,n for reasons not worth getting into) has a PDF of the RV x associated with it.  I've referred to the variations of x based on the edge it's associated to by x_in (not sure that's a good idea)

I have a function that needs to sum integrals of these pdfs but I don't love how it looks.

Can't figure out if the pdfs should be called f(x_in) or f(x)_in or f_in(x), or maybe something else entirely.  But I need to show that we are iterating through the PDFs by i and n.  All of the pdfs are unique, so I'm like 90% sure that f(x_in) is wrong, which is why I'm not sure about even referring to the RV as x_in.

I am not an academic and searching iterative functions ended me up at the wonkiest math wiki I've seen, so apologies if this is a dumb question.

(Ed: underscore is subscript, thought reddit followed that)",13sm8xv,Hellkyte,1685129438.0,1,1.0,"['Depending on the exact integral, your integrals are presumably equivalent to some probability calculation or some expectation of the random variables associated with each of the various pdfs. These would be commonly written with capital letter names of the random variables: X_n. In that case you could write the sequence out in standard probability or expectation notation: e.g. E(X_n) or P(X_n in (some set))', '`f_{in}(x)` is the only sensible choice. `f(x_{in})` would imply that all random variables have the same density `f`, while `f(x)_{in}` doesnt mean anything.\n\nYou dont need to differentiate the arguments with subscripts, you are using x as an auxiliary variable for integration, so its scope is limited to a single integral and there is no cause for confusion.', 'That makes sense thanks!']"
"[Q] where i can find the ""hot topics in statistics""","Hi, im a former economist and i just started my masters in statistics. I need to write a dissertation in a year and a half, but i dont know the fiel enough to feel in love with a research agenda.

What Twitter pages do you follow? What are the main journals and congress? I need to get inspired, thank you!",13sd04y,ppnasser,1685106984.0,21,0.87,"['Id reccomend signing up for daily email notifications from the statistics arXiv in whatever sub-categories youre loosely interested in ( https://arxiv.org/archive/stat ). The vast majority of new stat papers will be posted on arXiv to get peer feedback/claim intellectual property before going through the formal peer review process with a journal. Its the closest thing to an active pulse on whats going on in the stats research community.\n\nRead the paper titles you get sent by email every day to get a sense of what topics come up a lot. Read the abstracts of one or two papers that stick out to you every day, and skim full papers whose abstracts you find especially interesting. If you find a paper on a topic that really intrigues you, then look in the introduction for reference to a review paper on the topic (trust me, theres almost always one there) and check it out to get a broad overview of that area of research. From there you can go down the rabbit hole as far as you want.\n\nIt wont happen immediately, but over time youll start to develop a very good understanding of the current trends in stats research. You might even start to notice some gaps in the research that you can help fill with your thesis work :) \n\nGood luck!', 'Look into what faculty and students in top departments are publishing.', '> where i can find the ""hot topics in statistics""\n\nTake a look at the major journals related to your specific sub-interests in statistics, read the recent stat uploads to arxiv.org \n\nIf you want general journals, try JASA and JRSS-B\n\nIf your university has a stats dept, the library should have subscriptions to both (among many others).\n\n> What Twitter pages do you follow? \n\nLOL.', 'Are you assigned a faculty mentor for this program?', 'Which are the top departaments?', 'Man Twitter is very interessing, lots of serious researchers post their work in a nutshell there', 'Just a temporary one, i must decide my research field first', ""You'll probably want to look at department (not school) rankings for that. Places like Duke, University of Washington, Cal Berkley, etc. usually pop up in the top 25. Most departments have faculty and research area pages that you can used as a starting point."", 'Stanford is a good start?', ""What do you want to do after you get your master's degree? Do you want to continue working in economics or finance? Statistics is a tool that can be applied so broadly, you should first decide what career path you wish to continue down, then choose a research topic that provides expertise in precisely the job you ultimately want."", 'Yeah! A lot of really cool Bayesian work comes out of Duke too.']"
[Q] The interpretation of the same independent variable in two separate moderated multiple regressions,"In the case of the interpretation of the results in two moderated multiple regressions with one predictor that is identical in each, does the direct effect of that one IV's contribution to the model not control for the presence of the other IV's/moderators? 

I am asking this, as in a study I am conducting I have this scenario where the two coefficients for the direct effects on an identical IV (on the same sample) is producing coefficients differing in both significance and direction.

Logically, the other IVs would be interacting with this effect in some way for it to be different in the model, but I would like some insight into how exactly this is working.",13sbg6g,klexwbaim,1685102974.0,1,1.0,"['I dont quite follow what youre asking. Are you asking how a single variables coefficient can change markedly when you include different controls?', 'Yes, in the two different models, is the direct effect for the same IV controlling for the effects of the other variables present in the model, hence why the value changes so dramatically?', 'Yes correct. As a simple example, suppose you regressed food security on an indicator for whether a household received food stamps (a government program that provides money for food). Youd see that theres a negative coefficient on the food stamps, meaning that it looks like this government program makes people *less* food secure! That seems strange. \n\nBut we are of course missing a key variable here: income. Households in the program have lower income than households not in the program, so the coefficient on food stamps also has this difference in it. Its not really a fair comparison of households with and without the program since these households are very different. Once you control for income, all of a sudden the coefficient is positive! Food stamps increases food security. \n\nThis same idea extends to having a bunch of controls. If you control for different things, you can have very different coefficients on the same variable.', 'Aha, yes I see now. I believe I was confusing myself by discounting the fact that each of the differing factors in the multiple regression act as controls and change it each time. Thanks.']"
[Q] when to remove outliers from a set.,"Please remove if not allowed. I have read the rules and think I am ok. 

My parents have a sheep stud and have participated in a sire evaluation. The study have been conducted as follows. 

* Ewes randomly selected from the same mob
* Inseminated with the different rams semen (provided by different farms) and recorded.
* 760 lambs born to the mob. 42 where from my parents rams. 
* (this is the step they are questioning) they test the different stats of all 760 lambs and remove the outlires. 
* return an adjusted mean of the remaining set. 

On one of the tests almost all of their lambs fall in to the outliers on the better end of the scale. This means that only their lambs that would be considered sub standard in their breeding program are included in the sample set. 

Their question is would it be more appropriate split the set by farms and remove outliers for each farm before recombining taking the mean? 

Obviously this will make their farm look better.",13saeir,Tonka46,1685100030.0,6,0.88,"[""Honestly unless the outliers were data entry errors, I tend to not like omitting them. They're valid datapoints that describe what happened. I would just use different measures of central tendency to describe the set. Why not just use median & IQR rather than an adjusted mean at all?"", 'If almost all your data points are outliers. Then they are not outliers', ""Why are they outliers? Is there something wrong with the measurement? It's not a good idea to just chuck data because it's a bit different."", 'So, being a statistics grad from Texas A&M gig-em! I was taught not to remove outliers unless you have legitimate reasons for thinking the data does not ""deserve"" to be in the dataset,  (i.e. If it was known that the observation was not measured correctly, then don\'t use it (that\'s a reason), if you know that the particular observation is an anomaly (in the sense that when the observation was recorded, something funky was going on) (that\'s a reason) ). To answer their question in terms of a spectrum, if the goal is to make their farm look better, go ahead and just remove all the ""bad"" data and report the ""best"" data, and don\'t say what you did to the data. If the goal is to be honest about the quality of sheeps or whatever, then just report the entire dataset and report things such as mean and variance and how you got those quantities (mean and variance that is).', ""Thanks all I feel more sain seeing these comments. I have asked them to clarify what the test was for to make sure they weren't trying to find the typical sheep. I will leave it here until I have more information."", 'What I have been taught is to first identify if they have a large influence on the model. If they do, then you basically have 2 models to report: 1 with outliers and 1 without, for clarity and consistency in reporting. But if they dont have a large influence, you do nothing with them and continue the analysis.', ""So here's my understanding. Because rams from different farms are randomly allocated to ewes of the same mob, there is no systematic reason that a farm's average results are driven by anything but their rams.\n\nBut randomly, it can of course happen (one farm's rams might get paired with a larger share of really good ewes). A better analysis would control for observed ewe quality, if that is possible. But if not, fine. It means more variance when measuring the quality of a farm's rams, but not necessarily a systematic bias.\n\nNow, regarding outliers. Why on earth are high quality lambs removed from the analysis? What if you have a farm of super rams? Generally speaking, outliers shouldn't be removed if they are legitimate data points. It's better to instead analyse the sensitive of a result to outliers (eg redoing the analysis, removing biggest outlier one at a time), and to also look at measures not sensitive to outliers (eg, the median). \n\nBut I'm really curious what the goal of the analysis/cited reason for not taking some lambs into account is."", 'This is exactly what random effects are for. Use a mixed model approach with a random intercept for farms.', 'I have asked them to check the test is trying to identify the ""average sheep"" that was the case their sheep would skew the data. Still waiting to hear back.', 'i agree. those sheep happened. their data are essential to describe the experience', 'Recently had a situation where people were trying to remove ""outliers"" to make the data follow normal distribution.  Data was from inter-arrival times and was exponentially distributed.', ""To clarify removing the outliers makes my parent's farm look worse no matter how you do it. The analysis is being done by an independent group and they are trying understand why any are being removed."", 'The average sheep or the typical sheep? \n\nEither way, outlier detection should be based on expert knowledge established prior to the experiment, not the observed data.\n\nA different strategy might be to allocate monetary value to all sheep and then calculate an average on that scale.']"
[Q] Is there a point to Variance aside from using it to find Standard Deviation?,"Super newbie here obviously. 
When we were going over Variance and Standard Deviation (SD) I asked what the difference between the two were. I get that SD is the average distance of each data from the mean value. When I asked then what is Variance I was just told that it is SD squared. 
 Are there uses for Variance? If not, why is it a stand alone value. Why not just have the SD formula?

I hope Im making sense. I can try to clarify if needed.",13saaqr,Theregoesmypride,1685099717.0,0,0.5,"['In mathematical statistics and probability theory,  it is mainly the variance of a random variable of estimator we care about as efficiency and precision are mainly stated in terms of the variance. The SD, and by extension the SE (standard errors) , are just  square roots of the variance . This is mainly done  to convert the variance back to the original unit of measurement so that we can construct intervals for the variable of interest or compute p-values.', ""Mean and variance are the first two moments of the probability distribution, a representation of the population (data generating process, phenomena at hand) from which you are drawing your samples. These are central moments. Generally, you either have a mental model of the probability distribution (you infer from your theoretical knowledge about the application) or you fit the distribution by using your data. Mean and variance are the first two moments of central tendency, and their definitions arise from probability theory. Mean and variance are defined for most distributions, though there are some that don't have any (Cauchy for example).\n\nIf you are working with the normal distribution or many other common distributions, you can indeed use the **sample moments** (e.g. sample mean, sample variance) to accurately estimate the **population moments**. If the function of the sample values you use to estimate has an expected value that is equal to the population moments, they are called **unbiased.**\n\nNow, for some probability distributions, this may not be true. Then you cannot, without bias, estimate the mean & variance of your probability distribution using the naive sample standard deviation.\n\nWhat I mean is, variance is a defining characteristic of a probability distribution, not the standard deviation. Standard deviation is just another function that estimates variance, though it can indeed be defined as a moment, but it is variance that we are mostly working on, not SD.\n\nCasella & Berger is a classical textbook that explains these topics deeply at an introductory level."", 'Some things are proportional to variance.', 'Amazing information! Thanks everyone!', 'I think the issue is that your definition of the sd is not correct: the variance is the average squared distance from the mean, and the sd is the squareroot of the variance. The sd is basically defined as a function of the variance (not really vice versa)', 'Yes, its a key component of theoretical statistics. Its the second moment. The covariance is hugely important as well.', 'Ditto, I appreciate these explanations', 'To add to this:\n\nAverage distance from the mean would use absolute value, |Xi - mu|. Absolute value is a major pita whereas squares are easy and have good mathematical properties.\n\nStandard deviation puts the statistic back into the original units, so has a more intuitive appeal for description.']"
[Q] Tukey HSD results not including all 16 groups (statsmodels pairwise_tukeyhsd),"I run this tukey hsd test:

    tukey = pairwise_tukeyhsd(endog=posVariantsDfArray,
                          groups=posVariantsDf.columns.values,
                          alpha=0.05)

variantsDf = my data, see: [https://i.imgur.com/jAZaP6E.png](https://i.imgur.com/jAZaP6E.png)

I expected to see all 16 groups compared to each other in the results (see [https://i.imgur.com/afD3AWJ.png](https://i.imgur.com/afD3AWJ.png)), but it only picked 4. Why is that? How can I compare all 16 groups?

Goal: I'd like to see which variants means are significantly different from each other.

Thanks!",13s8l1b,FabianDR,1685094425.0,1,1.0,"['Try tuckey.results_table and see if that doesnt include all pairs.  If not what does posVariantsArray look like?', "">tuckey.results\\_table\n\nTukeyHSDResults doesn't have a results\\_table attribute.\n\nYou can see what posVariantsArray looks like in the screenshot that I included in my original post above.""]"
[Q] Regression model with interacting variables,"Hi everyone,  
  
I'm doing a university project which involves creating a regression model to explain something about the world.  
  
My chosen topic is to look at what factors are most important to winning a game of football (proper football for the record, not the American sort). The rationale for the study is so that coaches/managers can consider how much of their budget they would want to spend on the top attacking or defending players.  
  
I've completed the basic multiple regression model, with the win ratio on the Y axis and then goals/game, goals conceded/game and possession as the X variables.  
  
In the regression model, increasing goals/match by 1 increases wins/match by 0.211, while for every conceded/match you reduce wins/match by -0.118, so clearly coaches should focus more on scoring goals than conceding them.  
  
The data I've gathered comes from four different football leagues, so I also want to consider whether these variables change depending on the league. In some leagues, depending on the style of football, preventing goals might actually be more important, or at least relatively important, than scoring them.  
  
Can anyone suggest the best way to test whether there is differences between the leagues, short of running regression models across the leagues individually? I believe there should be a way to do this using dummy and interacting variables, but my brain isn't quite figuring it out.  
  
TYIA",13s5m5p,PhoenixNZ,1685083538.0,9,0.92,"['If ""g"" is the number of leagues, create g-1 dummy code variables, with each coded ""1"" for one league and zero for all the rest. Choose one league to serve as the reference group, which is coded zero for all dummy code variables (each variable will compare one league to the reference group, so ideally the reference group will serve as a useful comparison in some way). Create cross-product terms for each predictor variable by multiplying the predictor variable by each dummy code variable. The set of cross-product terms will represent the interaction between a predictor variable and league (with each cross product term comparing the effect for the group coded ""1"" relative to the reference group. If you center the predictor variables by subtracting their mean before calculating the cross-product terms, you can more easily interpret the lower-order terms in the analysis that includes the interaction effects (the lower-order terms will represent simple slopes for one variable when the other is zero, so centering a variable makes zero more meaningful by making it equal to the variable\'s mean).', 'You cant make the leap that coaches should focus more on scoring goals than conceding them solely from that model. Youre not showing something causal.', 'Hello, I can do the statistics project involving regression model....I am statistician by profession', 'Do you have a key variable of interest that you would like to see is different between leagues?', ' also, controlling for league may help account for nonindependence. If residuals are correlated within leagues, your original analysis could have biased standard errors. Controlling for differences between leagues using dummy codes accounts for mean differences between leagues. If your dataset includes more than about 10 leagues, a multilevel model may be a more sensible approach to account for nonindependence and allow you to model how effects vary randomly between leagues using random effects.', 'Depending on how many predictors there are, interacting each one with dummies for each league is a really bad way to overfit the model and eat up power.', 'League dummies wont take care of no independence between units within a league.', 'I agree that adding lots of dummy code variables and interactions could be problematic, but that depends on the context and we do not have enough information to know if it would be sensible or not in this case. The poster said there are 4 leagues and they are asking how to test if there are differences between the leagues. If those leagues were selected systematically and the poster wants to characterize differences between these specific 4 leagues, and if they have a sufficiently large sample size within leagues, then I would stand by my suggestion. If the dataset had many more leagues that were randomly sampled (or if characterizing specific leagues is not of interest), then a multilevel model or random effects model may be more reasonable (again, assuming they have a sufficiently large sample size).', 'It would help account for mean-level differences between leagues. The poster said there are 4 leagues. With only 4 leagues, I think a fixed effects approach to handling nonindependence with dummy code variables and potentially interaction terms may be a reasonable option. With more leagues, a multilevel model or random effects model may be more reasonable.']"
[Q] Question about manual plotting of ROC curve,"I have a binary classifier, that is not an ML method, and am trying to manually plot the ROC curve. We are classifying patients into cancer and non-cancer, where based on biomarker values, we assign them a risk score in the range 0-8.

  
If they are >= 5, they are predicted as a positive cancer case and if they are below, they are predicted as a control patient. I also have the ground-truth values for each patient. I want to calculate the TPR and FPR at each threshold of 0,1,2, etc.

  
I'm following along this tutorial, https://www.statology.org/roc-curve-excel/ where they use a method that at each threshold, they calculate TPR & FPR using

FPR: =1 - (Cumulative Pass / Total Cumulative Pass)

TPR: =1 - (Cumulative Fail / Total Cumulative Fail)

  
I'm confused because this seems to deviate from the standard definitions of:

TPR = TP / (TP + FN)

FPR = FP / (FP + TN)

  
I was hoping to use the cumulative method, because at certain thresholds I'll have TP and FP be zero, which gives me a 0 TPR and 0 FPR. But I am concerned the tutorial is not statistically sound.  
Thanks in advance!",13s3jr3,Inside-Instance6014,1685076698.0,4,0.84,"['FPR = 1 - true negative rate (AKA specificity)\nTPR = 1 - False negative rate (AKA recall)\n\nThis is what seems to be being used here, there are so many of these metrics all intertwined that its easy to get lost in the conversions. Id say what you are trying to achieve here is very similar to credit risk, i.e. binning people into certain risk groups. One way to think about the cumulative method I find is to put cumulative goods on the x axis, cumulative bads on the y and then plot points for each bucket. Might be more obvious to you how this forms a ROC curve.']"
"[Question] Should i use Stdev.P or Stdev.S for my data, and can SEM be calculated from both stdev.P and stdev.S?","To keep it simple, I have an experiment in which cells were injected with a certain treatment, and all these cells were measured. There exist no other cells treated with this treatment, so they must comprise the entire population, as there is no sampling and as a result sampling error, correct?  
  
In this case i should use stdev.P?  
  
""The standard error of the mean (SEM) measures how much discrepancy is likely in a sample's mean compared with the population mean.""  
  
But if is is the entire population, and I am using stdev.P, is SEM still meaningful? Is it calculated the same way for both? (SD/sqrt(n))?",13rrjgz,SerbianSock,1685044518.0,0,0.5,"['You still have a sample of all possible treated cells. Repeat the experiment and the results will be different. So conceptually this is still a sample, not a census. Use sample standard deviation.', '> There exist no other cells treated with this treatment, so they must comprise the entire population\n\nNo, that\'s not the population in this case. If someone were to very carefully replicate your experiment, they would still get a different set of values to you. You weren\'t interested in a conclusion about the values those *specific* cells, you might have chosen different cells to run the experiment on. Typically the population that would be relevant would be the set of values you\'d have had if you\'d used *all the cells that might have been selected* -- specifically the collection of cells about which you wish your conclusions to apply.\n\nOften that population is notional; I expect, for example, that you would like your conclusions to apply to cells that don\'t even exist today, but might be around in a month, or a year. \n\nUnless you\'re happy for people to say ""well, that experiment contains *no information of any value*, because the conclusions *only apply to that small set of cells that were treated*, but I want to know what would happen with *other* cells of the same kind.""', ""It's good if you got some help. You can tag Incase you have another I will help you"", ""Ok fair point. But then nothing can ever be population can it? All humans or sharks or whatever on earth is the ideal definition of a population i would think. Ok but more humans will be born in the future and you'd want your conclusions to apply to humans who dont exist today as well. \n\nWhat I'm lost about is when is stdev.P EVER used?"", ""Remember that when doing research the population that you study is something you define and you must be really specific about what are its elements and delimit it in space and time.\n\nMaybe you decide that the population for your study is the fruits on a specific tree near your house, then you can use stdev.p and it will be correct but your conclusions will only apply to the fruits of that specific tree. If you wanted to make claims about all the trees in the world as of today, you'll definitely need to redefine your population and extract a sample."", ""It's important to think about the way statistics is generally formalized- we care, for example, about the entire population of X, so we start with statistics about X, and then recognize our limitations by sampling from a distribution D from X- and formulate from there.\nThere is no reason to assume that you will never have the full population. Obviously in practice, we almost always start from a sample, but Stdev of the population is sometimes required, so why would you not include the option? Especially when it could be used.\nI'll give an easy example: at my old university, it was fairly isolated and vegetarian was artificial. They only cared about the trees within the campus property area. There basically were no trees outside the area.\nAfter meticulous tagging, every tree within the property was tagged and recorded. They used population level analysis for their year to year analysis on the campus tree and vegetation, because that was the entire scope, and they absolutely had nothing to say about trees in general."", ""> But then nothing can ever be population can it?\n\nThis is essentially correct. \n\nIf I want to write a report on the end of year results for Year 12 at Random High School in 2022, then those are the only results that will ever exist for that class and if I want to include a measure of variability, like SD, to summarise the within-year spread, I can do that. But if I do do that, it's probably because I want to compare the class of 2022 with <something>, formally or informally. In which case, Year 12s in 2022 were still a random sample of all possible Year 12s.\n\nThere are times when we have genuine census (ie whole population) data and only really care about describing what it looks like. But if you're calculating SD for any reason other than description, you're probably conceptualising your census as a sample of some kind, whether or not the data for other possible samples have been, or even could be, collected."", ""The other thing to keep in mind is statisticians typically will use n-1 to have an unbiased estimator, because it's more conservative. With a decent sample size the difference is miniscule. You don't want to push out results with calculations that are biased in favor of making your data look more reliable than they are."", 'That was a good answer, thanks!']"
[Q] Need Help,"What are the easiest statistical methods to measure effectiveness. Its for a paper titled Effectiveness of Marketing Strategy on purchase intention of students for online shopping. 

Im pretty bad at stats and Ill appreciate any help I can get.",13rp1or,Karakay27,1685038623.0,0,0.5,['Youd have to define what you mean by effectiveness first.']
[Q] Stats GRE preparation,"Can anyone give me advice on how to prepare? Is there a book or something I should look at, or do I just go over everything I learned in undergrad? Im not sure where to start.",13rnzit,rabidsaskwatch,1685036102.0,2,1.0,"['Is there a subject GRE exam just for statistics?', 'Definitely dont need to study much (if anything) from undergrad. There are tons of different test prep books or free test prep videos you can use. The math only really covers through college algebra from what I remember. If youre from the US, think of it as basically a slightly higher level SAT or ACT\n\nA lot of schools waive the GRE these days, so make sure you check that requirement at the programs youre applying to in case it lets you save the $200 or whatever for the exam fee.', 'I assumed thats how it works. Is the GRE more like the ACT??', ""There is GRE general, which involves reading comprehension, writing skills, and quantitative reasoning.\n\nThere are also subject GRE, specific for subjects like physics, chemistry, mathematics, etc. I don't think there's one for statistics.""]"
[Question] What statistical methods are appropriate for analyzing ML results? Are ML results normally distributed?,"Say when we want to compare the results of 3 different treatments of input and their effect on the same Machine Learning model. We got the evaluation/testing accuracy per epoch per treatment, like epoch 1 has Accuracy of 0.83 for treatment A, 0.76 for treatment B, 0.33 for treatment C, etc.   
Due to the limitation in expense and time, we only have about 5 runs per treatment (say each run has 10 epochs), so 5 x 10 = 50 data points per treatment.   
The question is: what are the correct methods to use on these ML results? Can we assume the data follows normal distribution in order to use something like ANOVA? If not, then can we use something like Kruskal-Wallis?",13rnvdz,A_Light_Spark,1685035829.0,0,0.25,"['Youd have to inspect the data to see what the distribution looks like', '> What statistical methods are appropriate for analyzing ML results? \n\nThe question is impossibly general. Different approaches might be reasonable in different cases. \n\n> Are ML results normally distributed?\n\nThat\'s a bit like asking ""are fish 35 cm long?""   \n... a few are very close to 35cm long, but for most purposes that won\'t matter.\n\n> epoch 1 has Accuracy of 0.83 for treatment A\n\nThat particular fish (""Accuracy of A"") can\'t actually be 35cm long. That\'s probably not of major consequence, but likely there\'s better choices of a model than ""this fish is exactly 35 cm long"" ... I mean than ""the population of values of which we have a sample are normally distributed"". \n\nIt may well be that equality null hypothesis tests are not the ideal choice of approach whatever distributional model you have, but you haven\'t explained what you were originally trying to find out *before* you tried to turn it into a stats problem.\n\nI presume by *Accuracy* that you\'re talking about classification accuracy. Is that correct? If so, why was your title  question not more specific? Not all ML results are classification accuracies.\n\nIf it is classification accuracy, it will be a count divided by another count, in which case variance will be different at very high accuracy than it will be at accuracies nearer to 0.5. (the number of correct-classifications will presumably be either something like binomial or -more likely- a mixture of binomials with varying p\'s). There\'s not enough detail about the situation for *us* to choose a model for you.\n\nYou\'re also not clear about what exactly you wan to compare with what. Are you comparing within epochs? Presumably there\'s cross-epoch differences. \n\n> Can we assume the data follows normal distribution in order to use something like ANOVA? If not, then can we use something like Kruskal-Wallis?\n\nWhat led you to consider ANOVA, in the first place? Was the original question of interest about *average* accuracy? If so, Kruskal-Wallis would not answer that question. Use an analysis that responds to your question of interest; there\'s many more options than ordinary ANOVA.', 'No, considering the accuracy as function of epoch is completely irrelevant. I assume what you want to do is compare different models performance for the same problem? Its honestly a complicated topic, but something simple and decent to do is to use k-fold data splitting (into training and validation), train model A and model B in each data split and do a paired t-test on these performances on the validation folds.\nEdit I would use a metric that doesnt depend on the threshold which is a choice, better use auroc', 'Clear as mud but this might be a helpful place to start: [Meta-Analysis of Diagnostic Accuracy with mada](https://cran.r-project.org/web/packages/mada/vignettes/mada.pdf)\n\nYou may or may not want to use that package but the references should be helpful.', ""Do I check each model's result or all the results together?"", ""The question is general because I'm trying to gain both a general understanding to statistical methods for education purpose, and also try to solve a practical problem.  \n\nTo answer your questions:  \n>you haven't explained what you were originally trying to find out *before* you tried to turn it into a stats problem.  \n\nI did. I want to verify the results generated across multiple runs are meaningful, not due to random chance. And that each model's results are significantly different from the others.    \n\n>I presume by *Accuracy* that you're talking about classification accuracy. Is that correct?  \n\nYes, we also have other metrics like Precision and Recall. But it doesn't matter. They are numeric float on prediction percentage.  \n\n>You're also not clear about what exactly you wan to compare with what. Are you comparing within epochs? Presumably there's cross-epoch differences.  \n\nWe have the epoch data so obviously we can compare each epoch, but nothing is stopping us from aggregating the data to find mean or max or whatever. And yes, trying to account or test for cross epoch difference would be great too. Essentially, *I'm trying to learn what I can or cannot do with the data*.  \n\n\n>What led you to consider ANOVA, in the first place? Was the original question of interest about *average* accuracy? If so, Kruskal-Wallis would not answer that question. Use an analysis that responds to your question of interest; there's many more options than ordinary ANOVA.  \n\nANOVA was just an example. Honestly I don't know what we can and cannot do, and everyone and mother uses ANOVA so I just mentioned it. Maybe we can't use it since the results isn't normally distributed. We could find the average accuracy, or we don't have to. The point is, we have these metrics on each epoch, and what can we do to show that the data is not random beside k-fold cross validation?"", ""Thanks for exactly answering the question!  \n\nI have considered using k-fold, but the library I'm using doesn't natively support k-fold. Also that increases processing time by a lot. \nWhat I did was truing to manually emulate k-fold by randomizing the train/test split and then repeat each model run. So instead of, say, 20 hrs per model with k-fold, now it takes 15 mins each but then I have to run them 10 times.   \n\nThat's where I'm at. I have the results each run of the model given of the same sample but randomly split differently, where the results are epochs of metrics (accuracy, F1, etc).  \n\nI guess I'm asking if I actually can do manual k-fold or not, and if not, what other evaluation methods I can use to test if each model is significantly different."", 'What are you trying to do? Its not clear.', ""> I did. I want to verify the results generated across multiple runs are meaningful, not due to random chance. And that each model's results are significantly different from the others. \n\nNo. That's *after* you turned it into a stats problem. Except its not correctly stated as a stats problem, which makes things difficult. \n\nHence the need to go back to the underlying questions of interest, so *I* can try to help translate the questions of interest into stats problems.\n\n>  I'm trying to learn what I can or cannot do with the data. \n\nCan: All manner of things, depending on your needs.\n\nCannot: well, it's not like the stats SWAT team will come knock your door down. You should worry more about the properties of what you choose to do (consequences of your choices). \n\n>  I don't know what we can and cannot do,\n\nThe scope of what *can* be done is very large. The choice will be driven by what you need to know and the circumstances you're in, and what you're prepared to assume.\n\nI don't have a handle on these so far, which would lead to facile, stock answers, which sounds less than helpful."", 'How big is your data set? Sound like long times... and i dont see why kfold should be slower than repeatedly random splitting..', 'I want to run statistical test on the results of different machine learning models, to check if the difference in results are meaningful/valid.  \nEach model has its own set of results, i.e. list of accuracy.', 'So you talk big but can contribute nothing. Got it.', ""The dataset itself has been shrunk to just 1MB. But the original file is 200MB. With even just the 1MB file, training takes about 1min per epoch. And since we are running our models hosted on a server, we wanted to minimize continuous training time as the internet can disconnect at some point and we'd lose all data of that session and need to restart again."", 'Thats too vague for us to help.', 'What do you need to know? List your requirements and I can answer them.']"
[Question] Is a Multiple Hypothesis Test correction needed when running an A/A test?,"I am running a test to understand the churn rate on an application page. To confirm the experiment was set up correctly, I initially ran an A/A test before any changes were made for the three subsets of our users before starting the A/B test.

  
Do I need to include the A/A test hypothesis tests in the multiple hypothesis test correction?",13rmxox,sham-ham,1685033644.0,2,1.0,"[""I wouldn't think so since you weren't really testing anything."", 'No. The A/A test is to answer the question ""Are there bugs in my experiment setup?"". Once that is answered you run your A/B test assuming there are no bugs.']"
[Q] What methods can I use to show the statistical significance of differences in my data?,"For my thesis I ran a study, where I presented the participants a handful of attributes of online shops. On a scale (-3, -2, -1, 0, 1, 2, 3) they had to choose how much they would trust an unknown online shop given different attributes.

Now I have a table like this: [https://imgur.com/d6a78733-bced-44c7-8fb8-4803d423afcf](https://imgur.com/d6a78733-bced-44c7-8fb8-4803d423afcf)

&#x200B;

What I did so far:

\- Using python (pandas, matplotlib) creating a dataframe based on all the data and calculating the means and standard deviation for all attributes

Problem: The means lie closely to each other, which is problematic for my further process. I need some way to tell if differences between the means are statistically relevant.

My first approach would be to display the means and deviation on a graph using matplotlib (though I haven't figured out yet how to get the deviation in there as well).

Do you have any suggestions what methods I could choose to encounter this?

Cheers",13rj698,FabianDR,1685024629.0,2,1.0,"[""It sounds like you're approach this all wrong.\n\n\\>The means lie closely to each other, which is problematic for my further process. I need some way to tell if differences between the means are statistically relevant.\n\nThere is no scenario where the values determine if there is statistical relevancy. The values are what they are. It seems you want those values to show something different, so you get the results you want.\n\nYou don't 'show' statistical significance. Significance is not a goal but the result of a test."", 'You can use tukey hsd to see differences among the groups.\nGood luck with your analysis.', '>display the means and deviation on a graph using matplotlib (though I haven\'t figured out yet how to get the deviation in there as well).\n\nImport matplotlib.pyplot as plt\n\nplt.errorbar(xdata,ydata,yerrs=errordata,fmt=""o"",fillstyle=""none"",ecolor=""r"",elinewidth=0.2,capsize=2,capthick=1)\n\nplt.show()\n\nSomething like that. xdata and ydata and errordata are arrays containing your values.\n\nAlso consider showing not the standard deviations, but the standard error of the mean, which is likely more relevant for what you want to convey.', ""Of course. Still, I'd like to properly show that. Tukey's range test should do it (thanks u/whythigh)."", ""Thank you so much! The standard error is mostly about 1, so according to that the difference between most of them isn't enough to be useful for me. But it is pretty much what I was looking for, so I can at least mention the test."", 'cheers, very helpful!', 'What exactly would you like to show?', ""Hmmm have you considered transfrom your data to fit your needs ? I don't possess a full picture but I guess it's manageable"", ""Also, it's not a problem that the means lie close to each other, if the errors are just small enough. In physics we somtiles compare values that are only like 0.000001% different.\n\nIf the means lie outside the errors (not the deviations!!!) of each other, then it points towards them being statistically significantly different."", 'Ohh never mind :) If your means lie close to each other something like contrasts or tukey hsd may help', ""Yeah I don't see/know how transforming would help."", 'Help what?']"
"[Question] ""Average"" distance between binary vectors","I want to determine the average distance between multiple binary vectors.

1. I use (for now) the Sokal-Michener distance to calculate the distance between the vectors, pair wise (Requirement for the calculation I recon)
2. Then I take the sum of all distances between pairs
3. Then I either calculate the mean or median

* My supervisor has concerns that the result might be skewed, when there is a cluster with say 4 out 5 points closely, but one outlier.
   * I tried to account for that with the use of the median (before I had the mean)
* However he said ""try something else, I'm sure there is something different"", but after some poking around I did not find something to calculate the distance between multiple points or vectors. (Even leaving the binary part out of it for the sake of research) 
* Has anyone pointers for me or an idea where to look? Or can justify my median approach?",13rgow4,tanking-cookie,1685018338.0,1,1.0,"[""Can you weight the ones which are linked together multiply? IE you get an extra point for the lengths of chains? k nearest neighbors maybe?\n\nYou've probably already googled, but here are some links which at least discuss the thing.\n\nhttps://stats.stackexchange.com/questions/274731/how-to-statistically-validate-that-one-clustering-technique-is-more-effective-th\n\nhttps://www.coursehero.com/file/p2k4009/Sokal-and-Michener-1958-the-distance-between-two-clusters-is-defined-as-the/\n\nAssociations among similarity and distance measures for binary data in ... https://mz.mf.uni-lj.si/article/download/204/293/334\n\nhttps://documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_cluster_details01.htm"", 'Start with using the Hamming distance', 'Thank you for your help, gonna check the links tomorrow.\n\nI also had a meeting with him today and we talked about it and he had the idea I calculate the sum of the distances of the nearest neighbour.\n\nKinda like your idea. Will have to look into it', 'Why you opt for the hamming?\n\nThe Sokal Michener seemed good for my purposes as it takes same zeros also in consideration. But I already had a ""test"" where i threw every distance under the sun on the problem and it mainly just gives different values if the distance is fitting.\n\nThe squared euclidean also didn\'t look to bad as it gives bigger numbers, but the patterns are all the same. Except the 1 part on zero-one scale distances, this might be interesting if I can differentiate the higher areas better\n\n&#x200B;\n\n    Soakl Michener\n    [[0. 0.42105263 0.43283582 0.5106383 0.84615385]\n    [0.42105263 0. 0.40909091 0.53146853 1. ]\n    [0.43283582 0.40909091 0. 0.5 0.85245902]\n    [0.5106383 0.53146853 0.5 0. 0.77906977]\n    [0.84615385 1. 0.85245902 0.77906977 0. ]]\n    haming\n    [[0. 0.26666667 0.27619048 0.34285714 0.73333333]\n    [0.26666667 0. 0.25714286 0.36190476 1. ]\n    [0.27619048 0.25714286 0. 0.33333333 0.74285714]\n    [0.34285714 0.36190476 0.33333333 0. 0.63809524]\n    [0.73333333 1. 0.74285714 0.63809524 0. ]]\n    seuclidean\n    [[ 0. 10.86853256 10.42832681 11.61895004 19.46150046]\n    [10.86853256 0. 10.69462482 12.47497495 22.29069313]\n    [10.42832681 10.69462482 0. 11.45643924 19.55760722]\n    [11.61895004 12.47497495 11.45643924 0. 18.4729532 ]\n    [19.46150046 22.29069313 19.55760722 18.4729532 0. ]]']"
How much optimization do stats MS/PhD students need to know? [Q],"Is a book like this, by boyd:

https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf

Overkill for stats PhD students in terms of optimization? How much of this would a stat PhD student use?",13rfk1q,AdFew4357,1685015152.0,37,1.0,"['It is a great book for anyone who uses optimization in their research. With this kind of math, you never know when you will need it and if you study it in advance, it also helps you question your problems better.', ""Even though it is one of my research areas, it is not something most statisticians, even PhD statisticians need to know.  There are a few very simple things: difference between global and local optima, consequences of convexity for global optima and uniqueness of optima, various characterizations of convexity, the Dennis-Mor theorem, Kuhn-Tucker conditions.   But they don't need to know most of that book or any optimization book unless they want to use it in research.  I know most of [Rockafellar (1970)](https://www.amazon.com/Analysis-Princeton-Landmarks-Mathematics-Physics/dp/0691015864/) and [Rockafellar and Wets (1998)](https://www.amazon.com/Variational-Analysis-Grundlehren-mathematischen-Wissenschaften/dp/3642083048/) but I wouldn't say every statistician should know that."", ""It's super helpful to know some optimization; I'd recommend the Ken Lange textbook though, more focused on statistical applications."", 'It actually looks aimed at statisticians.', 'Not overkill at all. In fact, Boyd and Vandenberghe is one of the best initial texts to learn convex optimization. I would recommend Statistical Inference via Convex Optimization as well by Juditsky and Nemirovski. Many statistical questions can be decomposed into these neat optimization problems. Id highly recommend reading and working through Boyds book.', ""Ideally all of it but there's enough variance among jobs that less is usually fine... sorry this is just an optimization joke. The other answers here are quite good."", ""For MS in my experience atleast it is overkill, but if you are interested then why not. Mostly it is useful to just know some basic stuff like Gradient descent, SGD, simulated annealing etc. Most of the applied optimization is just a computation time problem. If you are interested in ML stuff then Adam/SGD is interesting. Most of what i've done at my masters is just optimizing a likelihood function with optim() or nlm() in R and finding some Hessians."", ""I'm pretty sure this is the exact textbook I started working from when I had to implement some optimization at work. I think it's a pretty great text. My background is pure math but I work for a national statistical organization and optimization is SO important in all our practical work. Used for stratification, allocation, sample coordination, record linkage, E&I, calibration, disclosure control... Honestly everywhere. And the implementation isn't always easy, so having an expert in optimization - who also understands the statistical theory - is massive.\n\nDo you need it for a grad degree? Probably not. But if it interests you, you can definitely tailor your research or thesis or whatever in that direction, and find tons of practical applications. I work regularly with statisticians with advanced degrees, and while we can figure out a lot of the optimization stuff, we were all super pumped when someone with a degree specifically in optimization joined our group, because we knew it would make a big difference. (And it has.)\n\nTldr: you don't need it, but if you learn it, you'll be in high demand in practical application environments."", 'I just skimmed the table of contents and its good background info but many times techniques like gradient descent or Newtons method which relies on the hessian matrix just arent practical, will get you stuck at a local min/max or are too computationally expensive.  Stochastic algorithms are really key to a lot of things as well as non-gradient optimization methods.', 'Thats good to know. Could you elaborate a bit on the last part? What do you mean by it helps you question your problems better?', 'Statisticians (that do research) routinely need to think on how to implement estimation procedures, which are always optimization problems.', 'Thats a good point. But to piggy back off of that, Would you say statisticians need a working knowledge of anything past convex optimization? Would delving into the world of non convex problems be something that would then dip into the world of computer science? Of course convex optimization has its own set of tools, but with certain methods in statistics (LASSO) I feel as if as long as the problem is convex your good to go, which, of course isnt always the case. But would I be wrong in saying that statisticians mostly focus on convex procedures?', 'Okay great! Thanks! I have been researching about shrinkage estimators and sparse linear models and saw lots of optimization. realized it would be good to get up to speed on some theory via a book.', 'In my MS we learned to implement those that you mention from scratch in Python.  That was a lot of fun and there are so many non-gradient optimization methods out there that dont have to calculate gradients or an expensive hessian matrix.', 'Basically, assumptions you make about your environment have consequences for the type of optimization problem you get in the end. Once you can tell what assumption will lead to a ""solvable"" problem as opposed to something where there is no hope of a nice closed-form solution, you become a better researcher. Sometimes, you don\'t even need to change your assumptions, you use the right duality argument, and all of a sudden, your seemingly intractable problem becomes a convex optimization problem.\n\nFor instance, it is usually easier to deal with a complicated objective and linear/simple constraints than the other way around. However, the natural way to pose many problems has simple objectives and complicated/non-linear constraints. Restating the problem by changing the variables can be very useful in those cases.\n\nBasically, just get a good understanding of what kinds of optimization problems are easy or can be made easy, what kinds can be solved but not easily, and when to start questioning your (life) choices. :)', 'So you would say the entire book is def good to know?', 'Not always, not even for frequentist and never for Bayes.  And not always convex, not even mostly.', 'Yes.  Convex optimization, outside of exponential families, is not what you have in statistics.  So ""mostly focus on convex"" is definitely the [Wrong Thing](http://catb.org/jargon/html/W/Wrong-Thing.html), especially since Rockafellar and Wets (citation above) generalize all of that to the nonconvex case (and even vastly improve theory under convexity).  You don\'t really understand convexity until you read Rockafellar and Wets.', 'Wow, thats fascinating. So In a way, it feels like the way people approach problems in statistics comes from optimization? Ie. Think about it as an optimization problem; and then we have our estimator. Of course, easier said than done, but the workflow seems to be a bit more clear now.', '80% of the book, for sure.', 'So would you say the rockafellar book may be a better statistician focused start than Boyd?', ""I can't speak about how statisticians approach this directly because I didn't really get a degree in statistics. Each field is a little different.\n\nEngineers often force their problems to be nice, even though they are actually nasty but they can get away with it for many practical applications (pretending a set is convex, a function is differentiable, or using Taylor expansions even when it's not strictly necessary; these are usually fine unless you make errors in order of magnitude).\n\nMy research is in game theory and we are rigorous closed-form solutions people so we are often very careful about our assumptions because otherwise we can't get an equilibrium/solution etc. From my conversations with econometrician colleagues, I would say they fall somewhere in between and my guess is statisticians are also mostly in that region."", 'Whats the 20% thats not useful?', 'It is not ""statistician focused"" but is the fundamental book on convexity.  All of Barndorff-Nielsen (1978) and my work built on that is built on Rockafellar.  I also have work built on Rockefellar and Wets.  It to is the best book on the subject.', 'Gotcha. Since your research is in game theory, could you tell me a bit how statistical decision theory (decision theory) differs from game theory? Or how close they are?', 'I see. So what does pure optimization research really consist of?', 'The main distinction is whether we are looking at an individual decision or a system of interdependent decisions. If it is an individual decision-maker, then we call it decision theory. If there are multiple decision-makers whose choices affect the outcomes for others, then it is game theory. So in a way, game theory includes decision theory if we allow the trivial case of a ""single-player game"" (which is more of a decision problem rather than a game, of course). Game theory makes use of all kinds of tools developed in decision theory and applies them to game-theoretic problems but sometimes there are feedbacks that go the other way around as well.\n\nThere are also things like games we play with our future selves. (Things like constraining your options so that your future self has to get his shit together, which is a commitment strategy.) You can think of these as decision theory making use of game theory in some sense.', 'Especially useful for statisticians is when do global or local optimizers of sequences of functions converge to the corresponding optimizer of the limit (function) of the sequence?  What are weakest conditions?  You might think this is useless without probability.  But Prohorov and Skorohod can put it all the probability needed.  See [Geyer (1994)](https://doi.org/10.1214/aos/1176325768).', 'I see. So measure theoretic probability and real analysis seems to be a huge then. Glad I got a copy of Williams.', 'Of course!  Why do you think statistics PhD programs put so much emphasis on that?', 'I thought it was to weed people out haha. I mean Ive heard that some peoples research doesnt require measure theory, or they used very little of it. In my case I will need it.']"
[Q] What do the odd diagonal lines in my ZPRED*ZRESID plot in SPSS mean?,"I am currently working on my Bachelor's thesis and to test my hypotheses I am doing a regression analysis (I am unsure as to which one yet). To check the assumptions for a linear regression I plotted my standardised residuals against my standardised predicted values for the independent variable in a scatterplot. However, the values are not equally distributed above and below zero and there is no equal coverage of values along all ranges. But even more strangely, I see a lot of odd diagonal lines in my plot. What do these lines indicate? I have never seen them before.

I am using IBM SPSS 28 to run my analyses.",13rdyks,myargumentstinks,1685010318.0,1,1.0,['post the graph']
[Q] How to denote the correlation (r or ρ)?,"Hello, I just have a quick question about notation in a scientific paper. I am supposed to mention how the effect size measure I used can be assessed in terms of magnitude. I would like to use the Pearson   
correlation coefficient and refer to Cohen (1988). If I now give the rating of  = .1 (small) / = .3 (moderate) / = .5 (large), do I have to denote the correlation with an r or with  (rho)?

I'm not sure what's right since I am talking about the coefficient in general, independent of a specific calculation.

If this is not the right subreddit, please point me to where I might find the answer. Thanks! :)",13rcj3q,simplySchorsch,1685005475.0,1,0.6,"['Convention is that rho is use for the (unknown) population parameter and r is used for a sample. Either way, clearly define notation in your paper.']"
[Question] Help to find a proper test to perform,"Help on which test to perform?

Hey yall I have this statistics project where Im trying to compare the predicted price of crude oil and the actual price from 200-2008. Im stuck on which test to actually perform. I have a full data set and was thinking about comparing the regression lines of predicted and actual prices but still not sure which test would accomplish this. Im stuck in excel for this for whatever thats worth. Any help would be appreciated.",13r25pj,spaceface545,1684972721.0,1,1.0,"['Assuming youre talking about predicting future prices based on past prices, you should use a time series model (such as ARIMA), not a conventional linear regression model.\n\nTime series model predictive performance is typically evaluated by computing mean-square-prediction error (MSPE) with an expanding or rolling training window.\n\nI dont use excel, so I cant help you there.']"
[E] Value of Terminal Masters,Is there any value for a terminal masters in stats (particularly an online one) following a bs in stats? It seems like the two have very significant overlap and I see that many of my UG courses would be repeated in one of these programs. Do these programs have any value following bs in stats?,13qw6e8,seriesspirit,1684958822.0,2,1.0,"['You\'ll have to tell us a bit more about what this ""terminal masters program"" includes: to most of us (at least in the USA), a masters in statistics is by definition not terminal, because PhDs are frequently awarded in the field. \n\nRegular master\'s degrees are worth a fair bit in terms of what types of jobs they get you past the resume screener for (there are not many jobs doing real statistics, as opposed to data entry/basic number crunching, that a BS in statistics will get you.) Yours might also be.', 'Ive seen professional masters in statistics programs before, assuming thats what youre referring too. They are designed to be less mathematically rigorous and focused more on how to use different existing tools to analyze a few different types of data. So, fairly similar to a BS in stats.\n\nA professional masters programs will probably make you more confident in your ability to analyze some different types of data than just having a BS in stats, but I dont think youll come out with a very deep understanding of how the methods work. A professional masters will probably prepare you more for a job as a data analyst than just a bachelors degree would, but a traditional masters degree will prepare you better for a wider variety of careers both in terms of understanding how things work and how to implement them.\n\nYou didnt ask about this, but I figured Id mention this for anyone interested: If you want to eventually get a PhD, do NOT get a terminal masters. Get a traditional masters with a thesis/research project option. Or go straight to a PhD (very common).', ""The terminal masters I've seen are noted as professional and do not include research. Also their curriculum does not get into too much depth compared to non terminal. It seems to usually be 30 credits of core and some electives and the only prereqs are usually just calc 3 and lin alg. Coursework seems to introduce stats at a graduate level without depth and overlaps a lot with my current ug."", 'Out of curiosity because I\'m unfamiliar but why is a terminal masters so ""bad"" for before a phd. I understand why a nonterminal one would be better but that doesn\'t make a terminal one detrimental right? I\'m currently at a cross roads where I\'m not sure how much education I want after a masters and I want to work while getting a masters. This leaves me with mostly terminal programs I wouldn\'t want this to affect my later if a phd interests me.', 'I know what youre talking about and did it - DM me', 'A professional masters in stats wont give strong evidence of research potential or the ability to perform well in rigorous coursework. So, youll be paying potentially a lot of money for something that wont strengthen your PhD application nearly as much as a traditional masters.\n\nIm not saying a professional masters will hurt your application, but it probably wont be a strong indicator of academic rigor and research potential, which are two of the main things that admissions committees look for. If you have the funds and really want to do the professional program, go for it.\n\nIn general, I think professional masters programs may be a good option for someone who wants to do certain data analyst type roles in industry, but not necessarily for someone interested in a PhD or further research.']"
[Q] Interpreting Multiple Regression Coefficients After Square Transforming All Variables,"Hello,  
I'm knocking the dust off on my stats. I'm currently dealing with an entire data set that is all on the same scales. All the variables have negative skewness ranging from mild (-\~.4) to extreme (<-1.X).  
  
A square transformation to every variable REALLY cuts this down. I know that interpreting multiple regression output after non-linear transformations on the dependent is muddy. I'm wondering if interpreting coefficients for a multiple regression with all variables having the same non-linear transformation is better, and, if so, how I might do that.  
  
Thank you for any input.",13qtxkp,TilYouMakeIt,1684953854.0,1,1.0,"['You can absolutely interpret transformed variables. You just have to look at predicted effects instead of ""a one unit increase in x is associated with a beta increase in y"". The basic idea is that you\n\n* choose a low value of one of your x variables, as well as a high value. This might be the bottom 25th percentile vs the 75th percentile.\n* predict the y value for the low x value and the high x value, holding all other variables at their mean (or median, or anything else really)\n* see if the difference between the predicted y values is meaningful to your context. \n\n[Here\'s](https://www.reddit.com/r/statistics/comments/12yq9x7/q_question_about_holding_variables_constant/?utm_source=share&utm_medium=web2x&context=3) a reddit thread where I commented more on holding all other variables constant.\n\n[Here\'s](https://stats.oarc.ucla.edu/stata/dae/logistic-regression/) a UCLA page on using predicted probabilities to interpret a logistic regression; it relies on the same principles; predicted probabilities are just a particular kind of predicted value. And the version [in R](https://stats.oarc.ucla.edu/r/dae/logit-regression/). The UCLA site is generally fantastic.']"
[Q] is there any literature on using predicted scores in a subsequent model?,"Lets say I use a multiple regression model to predict individual scores and then use those predicted scores as an independent variable in a subsequent model. 

Would I be concealing variability in that second model since the predicted scores are point estimates and are not including the variability about those estimates?",13qt5ot,thebigmotorunit,1684952096.0,1,1.0,"['Its called stacking and theres some literature out there, see for instance Wolpert (1994) and (1996) for some of the earliest work. Theres also blending which is conceptually similar. Both of these techniques are mostly focused on prediction and not statistical inference, that is more data science than statistics.', '>Would I be concealing variability in that second model since the predicted scores are point estimates and are not including the variability about those estimates?\n\nYes. But you could do a bootstrap where you include both models to capture this variability', 'I appreciate you pointing me in this direction!']"
[Q] >99th percentile vs 99th percentile,"My son had is 1 year checkup, and they measured various things and the paperwork has percentiles for those. Some statistics, like head circumference say things like 97th percentile, which i understand, but his height and weight both say >99th percentile, rather than just 99th percentile. Does that just mean 99th percentile or does it mean something else? Is our health provider just weird? (Kaiser in California) I wasn't at the appointment so I didn't get to ask, and googling hasn't brought me anything. Thanks for any help.",13qrtuo,awfl_wafl,1684949162.0,1,1.0,"['It means greater than the 99th percentile, or greater than 99% of other 1 year olds.', '> Is our health provider just weird?\n\nNo, this is pretty conventional for medical stuff\n\n> Does that just mean 99th percentile or does it mean something else? \n\nIt means ""above the 99th percentile"" without specifying whether it\'s 99.1 or 99.7 or 99.99.\n\nSo somewhere within the biggest 1% of the population at that age for both height and weight.', 'Thanks.  That makes sense. He has had ones before that just say 99%, but this time they added the >']"
[Education] [PSA] [Rant] Don't you dare write or post about Gamma distributions without saying what parameterization you are using.,"I mean, really. I've spent the last several days working a model involving old-school ARD priors for factor weights, using a Gamma prior, and related topics.

And ALMOST NONE of the 100+ web pages and PDFs I've been reading EVER take the simple step of explicitly saying what parameterization for Gamma they are referring to in their paper/post. Is it shape? Is it rate? Who knows? 

No, I don't know what's common in your discipline. And I suspect  you don't, either.

No, I can't know for sure just because you use a ""beta"" instead of a ""theta"". Sure, the wikipedia notation is more popular than it used to be, but not everyone uses those consistently.

So if you are one of those people that write about the Gamma distribution without explicitly saying whether you are using shape, rate (or some other!!) parameterization, YOU ARE A BAD PERSON. May all your models fail to converge. May all your reviewers be ""Reviewer #3"". May your IRB committee require you to get informed consent in triplicate not just from subjects, but from subject's parents and grandparents and roomates' cousins' uncles.

My next PSA will be called: ""If you use priors in a paper with empirical results but never tell us what numbers you used for your top-level priors, YOU ARE A BAD PERSON. Even if you are a famous stats god who helped develop a whole field.""",13qrll4,malenkydroog,1684948625.0,143,0.97,"[""> use priors in a paper with empirical results but never tell us what numbers you used for your top-level priors\n\nHere in the wild west of data science they just say stuff like the 'tool uses Bayesian statistics to power its algorithms' or something most of the time leaving me to sit there like ok? and?"", 'You tell em! Why cant they just write the damn density with the symbols they use?', 'undergrad stats major here! my bayesian stats prof went on a 10 minute rant about specifying gamma parameterizations this past semester so dw us future generations will make sure to specify', 'I agree, I see this ambiguity cause a lot of problems. Its not hard to add a couple of words and symbols to be explicit. \n\nSome exposition may help people find their way. The  main parameterizations I\'ve seen across multiple areas are\n\nshape-rate, shape-scale, shape-mean, dispersion-mean. \n\nIve only occasionally seen any other, though they can definitely happen, like in some formulations of parametric survival models (e.g. seeing log-scale instead of scale does come up)\n\nFortunately using alpha () for the shape (if it\'s a shape-parameterization) is nearly universal, which helps, and using the symbol mu () for the mean is pretty standard in parameterizations involving the mean. \n\nYou do often see beta () and theta () for the rate and scale but books, papers etc can differ on which is which; this is the biggest bugbear I think.\n\nThe first parameterization makes some  sense for models for times between or to events (time to the third event from now) because of the connection to event rates. \n\nThe second can also be handy for times but more often comes up with models for physical quantities, length, mass, etc, and amount of money, stuff like that, where \'rate\' isn\'t really relevant. \n\nIn short, the application can sometimes help sort it out.\n\nThe third and fourth  parameterization come up in generalized linear models, since part of your model (the linear predictor & link) yields a model for . Sometimes it\'s written in a ,  form with conditional variance of Y equal to  ^. ^(2), which makes  = 1/ alpha, which we might call a dispersion parameter.  (A variety of symbols might be used instead of )\n\nThings to watch for if the preceding ideas didn\'t get you there: if the mean is given directly as one symbol, you have a parameterization with mean as one parameter. Otherwise it will nearly always be two parameters. If the mean is written as a product, it\'s almost certainly shape-scale. If it\'s written as a ratio, almost certainly shape-rate (numerator/denominator respectively), though if the symbols seem unusual, further checking may be needed.\n\nThen look for variance.  \nV/M = scale = 1/rate,   \nM^(2)/V = shape \n\nIf you see reference to skewness, that will be a function of shape (or possibly some dispersion inversely related to shape) alone. If skewness decreases as the parameter increases, it\'s almost always just shape. Specifically, skew = 1/ means that  is shape.\n\nThese kinds of things often help work it out pretty quick. Rarely are additional facts any further  help, but it happens\n\n>  I suspect you don\'t, either.\n\nOh, I assure you, I do know what the common parameterizations are in my usual areas I work in. All four that I mentioned come up but the first two are easily the most common. It\'s typically \npretty clear in context, fortunately. \n\nBut I regularly have to help other people sort it out. It does come up a lot when calling different functions in software that use different conventions. If I see a question like ""Why does my gamma fit look terrible?"", my first response is usually ""check the parameterizations in everything you\'re using""', 'I agree it\'s best to be clear and explicit. For physical applications, in practice the ""rate"" or ""scale"" parameter has units so the convention can be inferred by dimensional analysis. If they also don\'t report units when they should then re-engage ranting (and indeed, tack on another rant).', 'Thank you for expressing this pet-peeve of mine. I feel seen', 'paging R base vs R.h', 'Wait until you hear about the *generalized* gamma...', 'I thought I was missing something when I had to ask the question: what parametisation are they bloody using?', ""reading this thread, remembering i don't really know stats do i..."", 'You just write the full PDF of every distribution you use, no? Seems completely normal and standard?', ""It'd be one thing if they could simply write Gamma(shape, rate) or something like that next to the distribution reference. But maybe that takes away the mystery? Everyone likes a good mystery!\n\nPersonally, I'm going to start writing all my papers and models using the flugelhorn Gamma parameterization, a rather obscure version of Gamma promulgated by Tibetan econometrician monks in the late 70s. Of course, I'm still going to \\*write\\* it as Gamma(alpha, beta). But I'm sure people can figure it out."", 'Just kernel of the gamma dist theyre using would suffice but Ill accept the form of the mean: alpha*beta vs alpha/beta. Just make sure I can infer ', ""Ha ha, your prof is doing god's work. :D"", 'I appreciate you taking the time to write out some of the use cases for the different versions. :)', 'Also Id like to suggest another pet peeve: 1 chain is not sufficient for accurate and precise Bayesian estimation', ""What's R.h?"", 'Damn bro, who hurt u\n\nJokes aside, i fully agree with the sentiment of your post. There should be no ambiguity in academic writing. Another major annoyance is the use of undefined acronyms, no matter how popular or conventional they may be or have been.', 'Are you the prof him/herself ', 'Ah, I remember those arguments from years ago (""should you run one long chain or several shorter ones?""). \n\nA bit moot nowadays; with current multicore systems, if you have the time to run one chain, you have the time to run *n* chains. And with hyperthreading, your system is still usable while doing it!\n\nBut I\'ll admit the main reason I run multiple chains is because it\'s always been the best informal convergence indicator, imho -- starting off 4+ chains all noticeably overdispersed, and watching them all converge to the same values quickly (and stay there) gives you a nice sense of security.', 'the headers to R that you can call from C.\neg R::dgamma \n\nif I recall, the two dgammas (and rgamma) have different parametrizations.', 'Unlikely, unless you are a time traveler visiting us from the early 2000s... :D', 'Thats my point too! Having independent chains coverage on the same posterior is chefs kiss. Also poor mixing of chains is a great way to reveal poorly parameterized models', 'Good lord, I have seen the darkness...', 'Within R there\'s no confusion - `stats::rgamma` and `stats::dgamma` use the same default parameterization (shape-rate, while offering shape-scale as an easily specified option), see the R help.\n\nIf the call is ""dgamma(x,a,b)"" without ""naming"" any of the arguments, then a is shape and b is rate.', 'yes then try calling that function from C importing R.h', ""I can see this in Rmath.h (which isn't super helpful):\n\n    /* Gamma Distribution */\n    double  dgamma(double, double, double, int);\n    double  pgamma(double, double, double, int, int);\n    double  qgamma(double, double, double, int, int);\n    double  rgamma(double, double);\n\nI'm having to look in the sources on github (https://raw.githubusercontent.com/SurajGupta/r-source) to get (each of these lines is from a different file):\n\n    double dgamma(double x, double shape, double scale, int give_log)\n    double pgamma(double x, double alph, double scale, int lower_tail, int log_p)\n    double qgamma(double p, double alpha, double scale, int lower_tail, int log_p)\n    double rgamma(double a, double scale)\n\nI can see why it would be confusing!"", 'yea docs are terrible and super annoying\n\nand if you try to ask a question, say on stackoverflow, then you may get microaggressions because rtfm']"
[Q] How to handle known dependence of explanatory variables,"I have a set of data of county median income. I want to explain the variation in this data by variation in county characteristics. The county characteristics that I have are all fractions, such as ""fraction of degree holders who received their first degree in engineering.""  
  
These characteristics can be split into different groups where every characteristic within each group is known to be dependent on each other member characteristic of the group, as the sum of each characteristic in a certain group is 1.0 for each county.  
  
For example, one group that I know are all dependent is: pop fraction with less than high school education, fraction with highschool education, fraction with some college, with bacchelors, with graduate degree.  
  
How do I handle this? Alternately what search terms should I use to learn more about analysis techniques for this situation, or what books (or chapters from books) would be able to teach me more about this situation?",13qr9aa,Gregervich,1684947851.0,1,1.0,"['If using a linear model you can just drop 1 of the variables. E.g. if 5 variables always sum to 1, drop one and do regression with remaining ones. This is common when e.g. changing a categorical variable to one-hot encoded and dropping 1 of them.\nAlternatively you could use e.g. a lasso regularized model.', 'You have *compositional* data. You want to search for models with compositional predictors.', 'Thank you so much, this is exactly what I was looking for']"
"[Education] Masters in EU, Canada and Oceania that place well into top US PhD programs?","As per the question I want to know if there are good programs in the EU and Oceania that prepare students well and get placed well into top PhD programs. I am aware of top departments like ETH Zurich, Oxford, UCL. Maybe even Paris Saclay, Sorbonne, Unimelb as well? Mcgill seems to fund all of their masters students but I have no data regarding how many students get placed in good PhDs. I want to know if they are better for industry or furthur education. Funded programs would be better but I will try my hand in anything.",13qpsv7,Hozierisking,1684944490.0,2,1.0,"['Are you asking about Canada as well? UToronto and UBC are the most well known schools, all of them tend to be more academically focused in Canada so are best for further education, but theyre all internationally recognized enough to be able to get into industry roles\n\nEdit - i had McGill listed but found that they dont have MSc for math/stats', ""hello! thank you for your reply. ive heard great things about ubc and UToronto but I'm worried about their PhD placements in the US since I couldnt find any official data.\n\nRegarding Mcgill, when I checked their website a few days ago, MSc thesis and MA thesis option showed up for me :O""]"
"[Software] I made a free and simple web app to generate dummy data (no ads, no tracking, no signup, no BS)","If you want to check it out, it's at [www.DataSmith.click](https://www.DataSmith.click)",13qnl2m,askantik,1684939196.0,10,1.0,"['Neat!', ""I have meaning to do something like this, but I don't know the web dev part. Maybe, we could also specify data types for each column, also the distribution."", 'Awesome. \nAn option to include numerical rather than string data would be great.']"
[Education] [Research] PCA (principal component analysis) evaluation,"Hi,

I have an issue understanding and learning how to use PCA to evaluate my data, interpret it and evaluate the results. Hopefully you know where I can find more Information about it or maybe you got some information yourself. I'd be extremely glad to hear from you, cause I need it for my Bachelor Thesis and it's also very intrigueing too, but I can't quite wrap my head around it. 

Thank, you in advance!",13qih23,EpiCWindFaLL,1684926635.0,7,0.82,"['Statquest pca playlist:\nhttps://youtube.com/playlist?list=PLLViszK5qws1SUpVeEiGxHpjnJC-_ZzMy', ""Why do you think you need PCA? That's a good place to start. \n\nWhile PCA has a myriad of uses, it's mostly a transformation technique to reduce dimensionality by creating vectors from your variables to represent the greatest variance. Plenty of places to go from there but there are a whole other range of statistical methods to consider."", 'In short, the idea is to reduce the amount of features while still maintaining as much information as possible evaluating among other metrics the variance of the vector created by each feature. A concept I personally struggled with when I first started experimenting with it is that it can ONLY be used on numeric continuous variables, not even numeric indicator variables (1, 0): https://builtin.com/data-science/step-step-explanation-principal-component-analysis', ""We're going to need a bit more detail to assist. What exactly is tricky for you (e.g., scree plot, eigenvalues, etc.)?"", ""I need it cause I want to research, wether aboveground and belowgeround traits of different plants correlate or not.\nFrom what it looked like PCA is able to tell me this, but I'm not sure how I can detect correlation/extract it from the plot I yield."", ""First of all I am struggeling with what the goal is, especially what the plots I'm getting display and how I can read them. Then I can worry about how I get there"", ""PCA won't do this for you. Do you trying to determine if the traits of these plants are similar to each other or if they correlate to a particular variable? I'd start with clustering for the first and regression models for the second. Given your experience with PCA, maybe start with a tree model for regression vs worrying to much up from if you have a linear situation or not.\n\nWhat tool are you using?""]"
[R] Looking for an expert in (descriptive) Bayesian statistics for an phd project on folk psychology and typological thinking,"Hello everyone,

As a PhD candidate working in experimental philosophy, I am in the process of exploring a unique idea in the realm of folk psychology. This concept, while still somewhat nebulous, is starting to take shape and I believe that with the right expertise, it could develop into an exciting study.

At this stage, I am eager to tap into the wisdom and experience of this community to assess the feasibility of this idea. Constructive criticism and feedback are not only welcomed, but highly appreciated.

My ultimate goal is to find a statistician who shares a keen interest in this topic and possesses the skills necessary to help refine and guide this study. For those who express interest, I can provide a research outline to give a clearer picture of the project.

Should we find our interests and ideas aligning, I look forward to entering into a paid collaboration, where your role would encompass:
Assisting in the preliminary setup of an experimental design tailored to the research question(s).

Conducting thorough statistical analyses on future collected data.
Providing insightful feedback and suggesting modifications as the project evolves.

Im looking for someone with a deep understanding of (descriptive) Bayesian statistics, experience in experimental design and data analysis, and strong communication skills.

If this prospect interests you or you know someone who might be a good fit, please message me directly or leave a comment below.

Thank you!",13qi2er,GillesMalapert,1684925350.0,1,0.56,"['Why Bayesian?', '> find a statistician who shares a keen interest in this topic\n\nI\'ve never even *heard* of this topic before just now. I\'m a statistician, not a psychologist (*dammit, Jim*...). I don\'t know what ""folk psychology"" and ""typological thinking"" are.', 'Send me a pm.', ""Sounds interesting, my advice is to cast the net wide as you search for collaborators. Bear in mind that r/statistics is a very small, quiet backwater in the world of statistics; you would do well to look for a bigger audience -- I get the impression that stats.stackexchange.com is quite a lot more numerous. Maybe you are already searching more widely, if so, that's great, carry on.""]"
"[S] R-Studio - First time reading R output, need help to read data","https://imgur.com/a/HAK4v0V
^
Title, what does the different numbers mean?

I color-coded them, so its easier to explain. I have been to statistics lectures for 6 months, so i have some knowledge, but not when reading outputs in R.",13qbeyg,ShreddedLifter,1684903721.0,0,0.17,"['Orange colour: it\'s about the coefficient of the regression, with their corresponding p value (last column)\n\nRed colour: * is used to indicate the p value hit which level of significance threshold \n\nThe other boxes are some about statistics concepts, that is not unique to ""R output"" (for example degree of freedom and R squared). If you don\'t understand them, it\'s better to learn the concepts before reading R output.', 'Have you ever read a regression table from any other program? Or are you new to them altogether?', 'People may be able to help more if you post your code and dataset for people to see.\n\n1Q and 3Q refer to [quartiles](https://en.wikipedia.org/wiki/Quartile). The other things you squared are labelled. If you forgot the statistical concept, you can look them up. For example, the red square refers to how `lm()` labels statistical significance.', ""purple are the quartiles (look it up if you don't know what that is)  \n\norange are stuff about the coefficients of your regression. Estimate of their value, their standard deviation, and then t-value and p-values to assess whether they are significantly different from 0. At the end of their line you see a number of stars, here 3.  \n\nred: it's a legend telling you what the stars mean. For example 2 stars mean that the pvalue is < 0.001.\n\nFor the rest at the bottom, they tell you what this is."", 'Try https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/linreg.html', '[This is what you need my friend](https://quantifyinghealth.com/wp-content/uploads/2022/10/output-interpretation-of-a-linear-regression-in-R-01.png)', '> Red colour: * is used to indicate the p value hit which level of significance threshold\n> \n> \n\nDoes  \n0.01 \'*\'  \n  \nMeans that if there\'s * above its P-value is UNDER 0.01? or exact?  \n  \n--------------------------  \nQuick question about the 5 residuals (min, 1Q, Median, 3Q and max).  \n  \nI know what these terms mean in general, but unsure in this situation.  \n  \nSo in this picture the intercept is ""Birth weight"" and ""sigg"" means that the mother smoked at least 1 cigarette during her pregnancy.  \n  \n**Min**: - 2743g = So the baby with the lowest weight was 2743g at birth?  \n**1Q**: -333g from the ""expected weight/average weight""?  \n**Median**: 8 gram what... ?', 'First time reading data from software other than word or 2x2 table.', 'We don\'t have the code for this, the teacher showed it to us.  \n  \nI thought the red one tells us that the **P-value** or ""**Pr(>|t|)**"" is lower than lets say **0.05** AKA **\'.\'**  \n  \nThats wrong?', ""I know Q1 is 25% side and Q2 is 75% side, but the values are confusing.  \n  \nSo the value (Intercept) is birth weight.  \nDoes the picture read:  \n**Expected weigh**: 3395.476g  \n**Lowest weight baby**: 2743.4g  \n**Heaviest baby**: 4285.2g  \n  \n**3Q**: 375g MORE than 3395.476g  \n  \nJust letting you know for no reason: I really love statistics, and I want to keep learning this more during my studies, even though it's a hard subject. I'm not that good at memorizing (and focusing), so it hurts my grades a lot."", ""Can you tell a bit more about the meaning of t value here? I understand everything else but I didn't manage to figure that out.\n\nThanks in advance!"", 'This is a great source! I\'m ""just"" doing a bachelor\'s degree, so I feel like 99% of students will never go ""this far"" to study these subjects since it might not be relevant to ANSWER the exam. Since exam Is all about memorization. Understanding doesn\'t seem helpful in most subjects, but it can help a little in Statistics!', 'The quartiles are given for the residuals. When you fit your model, you are looking for y = **a**.x + **b** + **r**.  \n**b** is your intercept, **a** is your slope coefficient, **r** is your residual that you minimized (think of it as the ""error term"", or the part of the information that the model couldn\'t incorporate into the linear model). In other terms: y - y_hat = r, with y_hat being the values of y that you can recover thanks to your linear model.  \n\nThe quartile here tell you about the distribution of your residuals. In this case, 50% of your residuals are comprised between -333g and +375g (since Q1 cuts the residuals in two groups: 25% have values < Q1 and 75% > Q1. Median does the same at 50%, Q3 does the same at 75%...).', 'Thanks! \n\nDo you know anything about multiple regression? It would affect the sort of explanation required', 'I think itd be more helpful to ask the teacher.', ""The t-value is a value computed from the data that is used to make a t-test. It's a test to check whether the value of the estimate is significantly different from 0. If you search t-test you should have everything.  \n\nIn reading the output here, the column just after (P(x) < x or something like that) correspond to the pvalue associated with the ttest, so you don't really need to process the t-value unless you want to do something else. By memory, the tvalue is (x - mu(H0)) / sd_hat(x), that is the estimate minus the value it should take under the null hypothesis of your ttest, divided by the empirical standard deviation of your data. This tvalue follows a Student distribution and this is how the pvalue is then computed."", 'I do not, we only have simple regression in the current course. But next semester we will learn about multiple I assume.', 'Thank you so much!!']"
[Software] Question about constructing the design matrix in R,"I am trying to construct the design matrix to fit a logistic regression model with lasso penalty-glmnet. I want to include the main effects & 2nd order interaction terms. I have few variables which are factors. When I create the design matrix it seems that the reference category for the factor variable is included as a column in the design matrix.

The following is the code on the mtcars dataset for illustration only

data(mtcars)

\#### select specific columns: mpg,cyl,am(binary response) ####

data\_fit\_model <- mtcars\[,c(1,2,9)\]

\##### convert number of cylinders to a factor ######

data\_fit\_model$cyl <- factor(data\_fit\_model$cyl,levels=c(""4"",""6"",""8""))

\#### specify the formula for main effects & 2nd order interaction without intercept #####

model\_formula <- as.formula(am\~.+.\^2-1)

\#### build the design matrix #####

design\_mat <- model.matrix(model\_formula,data=data\_fit\_model)

However if I specify the following

model\_formula <- as.formula(am\~.+.\^2)

for the model formula then the column for reference category is not included in the design matrix. Can anyone tell me how to write the model formula correctly so that there is no intercept term & the reference category for factor variables is not included as a column?",13q9izi,hasibul21,1684898237.0,2,0.75,"["">  Can anyone tell me how to write the model formula correctly so that there is no intercept term & the reference category for factor variables is not included as a column?\n\nYou can't have both. In a straight one-way model, *either* you omit the reference category and include the intercept or you include the reference category and omit the intercept. I strongly suggest the former, because the latter doesn't generalize to more variables."", 'What you said is correct. The intercept term would be the logodds for the reference category for number of cylinder & mpg 0 in the example above. While having both intercept & reference category would make the columns of the design matrix linearly dependent.']"
[Q] Am I wasting my time on indeed?," I just graduated with my bachelors in stats, and Iv been applying a lot on indeed this past week. Only issue is that these positions usually have 100-500 applicants. Am I wasting my time applying for these jobs? How did you get your first position outside of college? Where do data analysts usually start?

Thank you, sorry if this post is a little irrelevant to the subreddit. I wanted to hear from people in my field of interest.",13q2on4,lewdjojo,1684880218.0,11,0.92,"[""I recommend to keep applying on both Indeed and LinkedIn. A lot of those resumes never reach human eyes, because they are filtered by keywords before they reach the hiring team. When applying, read the job description a couple times, pull some keywords from it and add them to your resume. \n\nPersistence and patience go a long way. Don't be afraid to pour a lot of time into your resume/cover letters. It's the first thing people see. You can search through this forum and find some resume tips. \n\nDon't let the mass of applicants discourage you. Highlight your skills. No need to be humble when applying! \n\nI ended up going into the field of GIS, so my experience is a little different. But once I polished my resume, I finally secured a great job."", 'There was a comment on the data science subreddit a little while ago. And while I dont have the link, heres the general gist. Spoken from their perspective. \n\nWe put out an advert for a data analyst at our company and within 24 hours we had 500 applications so we took it down. \n\nHalf of those applicants didnt have the right to work. Down to 250\n\nAnother 80% didnt meet the necessary qualifications. \nDown to 50\n\nThen all but five had poorly written resumes, no projects, no initiative etc. \n\nOf the 5 interviewed. 3 were vastly overqualified and the other two were considered for the job. \n\nEnd comment. \n\nSo I guess what Im trying to say is that you shouldnt be concerned about the big numbers. If you have your right to work, meet the requirements and a polished resume. I say just apply, eventually youll get lucky and land something. \n\nAnd heck sometimes even if you dont meet the requirements Id throw in a application just for kicks.', 'I got my first job through indeed, as a data analyst', ""Your first job after college is quite likely to be either a research-technician type job at the college you just graduated from, or something outside your primary field of study but in your hometown.\n\nJobs posted on Indeed, and jobs that can be done remotely, routinely attract hundreds of applicants. Local jobs advertised only on a community bulletin board or a college's internal vacancy list might not attract five applicants."", 'No, definitely not. A LOT of people employ ""spray and pray"" when hunting for jobs, including woefully unqualified people and those living half a world away.', ""re: resume: look up how the filtering systems for resumes work and structure your resume so they understand it. Last time I did a job search a few years ago the systems were terrible and would only properly ingest a resume with one line per info item format and one paragraph per previous job. Having the resume actually auto-fill their cursed forms was very helpful, although still hit-or-miss. Hang in there, other than personal connections it's very much a numbers game."", 'First job was entry level, but pivoted into a data analyst. \nThen, networking and applying to 300 roles for 1-3 interviews was the next path for DS. \n\nLinkedIn and Indeed were primary but most leads came from recruiters reaching out directly.', 'Yep :(\n\nReach out to alumni from your school, ask to learn more about their work in a phone/zoom call. Keep things casual and just try to learn/show interest/make a connection. They might not be hiring, but may know someone who is. You may benefit from their advice and insight.', 'Yes, definitely try Linkedin as job opportunities are better and you could reach out to more people through there', 'Does your college use handshake? I also just graduated, but was able to find my job earlier through handshake and had some other offers from career fair meets', 'I see. For one of the positions I found the same job on both indeed and LinkedIn. Should I apply to both or just LinkedIn? The LinkedIn position had like 43 applicants which is pretty good for me but Im still not sure.', 'Thank you for the motivation! Just got laid off after my first year and I was feeling hopeless looking at the numbers today.', 'Take LinkedIns application numbers with a grain of salt. LinkedIn doesnt have access to the actual application process on a companys website and tracks every click of the apply now button as an application. \n\nTheres also a big difference between qualified applicants and unqualified applicants throwing a prayer at a company.', 'No worries. Just passing along a message.']"
[Research] Adjusting Statistical Methodologies for Pandemic-Influenced Data,"Are there any good recent papers that examined how we as statisticians should adjust our methods for pandemic-influenced data in longitudinal studies? There are tons of public health before/during/after studies, but I am looking specifically for published papers aimed at statisticians.",13pr8pk,carabidus,1684854550.0,3,1.0,"['How would we adjust our models? Not sure I understand.', 'Yes, exactly. While we may already know how to adjust our models, I\'m looking for ""best practices"" type papers for this topic. I\'m compiling a list of refs for a paper I\'m writing with some colleagues.', 'Can you give an example?', ""Study with a high probability of impact: Intervention to Improve Hand Hygiene\n\n- We collected baseline data before introducing the intervention\n- We introduced the intervention (i.e., enhanced access to hand-washing facilities) to the population\n- COVID-19 raised the perception of risk\n- Result: The pandemic, rather than the treatment intervention, largely influenced adherence to hand-washing. Therefore, we should control for the impact of the pandemic in the analysis, even if the study wasn't designed as a pre-/post-pandemic investigation.\n\nThere were a multitude of public health studies planned well before the pandemic. The researchers had firmly established the hypotheses, treatment, and data collection protocols. Then, unexpectedly, a global public health crisis arrives. These ongoing longitudinal studies didn't consider the pandemic in their design. How do we approach these studies *statistically* that the pandemic now confounds - with risk perception behaviors as the root cause of these confounds?"", ""I don't see how you can. This is why you do randomized controlled trials. Even if there wasn't a pandemic I don't think the results would have a great deal of external validity - stuff is always happening."", 'The pandemic doesnt confound-its not jointly determining the treatment assignment and the outcome.', ""While the pandemic does not affect treatment assignment, the pandemic affected people's perception of risk of acquiring the virus. Ergo, the incidence of handwashing increased at these handwashing stations, but not due entirely to the treatment (increased access to handwashing facilities). Rather, the incidence of handwashing is now confounded with people's heightened adherence to avoiding the virus, which included behaviors like washing hands more often in public. It's not straightforward how to tease apart the treatment effect versus the pandemic effect in studies like this. Hence, the need exists for some guidelines, especially for junior statisticians."", ""> Ergo, the incidence of handwashing increased at these handwashing stations, but not due entirely to the treatment (increased access to handwashing facilities). \n\nThis is a 100% fine and I fail to see how its an issue? If it affected both the treatment and control groups equally\n\n> Rather, the incidence of handwashing is now confounded with people's heightened adherence to avoiding the virus, which included behaviors like washing hands more often in public. \n\nHow is this confounding?\n\n> It's not straightforward how to tease apart the treatment effect versus the pandemic effect in studies like this. Hence, the need exists for some guidelines, especially for junior statisticians.\n\nWhy?""]"
[Q] How would you go about creating a multiple regression to predict outcomes of 2 player games such as badminton or tennis?,"What type of dependent variable would be suitable? And would you consider the two opponents of a match as two separate points, or would they be the same? If you use two opponents as different points, would that affect the model in anyway since they technically are coming from the same match?",13pqj3v,Suitable_Direction18,1684852942.0,2,0.75,"['https://en.m.wikipedia.org/wiki/BradleyTerry_model\n\nhttps://cran.r-project.org/web/packages/footBayes/vignettes/footBayes_a_rapid_guide.html', ""Make a research question first. We don't even know what you are interested in knowing. How can you build a model without knowing what the point of it is?"", 'Have thought of this before. \nDepends completely on the game. \nTake prev matches as data and maybe rank players depending on their overall winnings? \n\n\nSo if player one is 6 and player 2 is 4 then player one will win is the prediction. \n\n\nOr else we can run a logistic regression with some variables ig.\nMaybe like some players do better on clay courts so variables like that. \nNow I kinda want to work on this', 'Lets say I want to use a multiple regression model to examine how different variables such as ranking, home court, and height will affect the outcome of a game. Since both opponents have different have different input variables, but are playing in the same match, should you consider both players as different data sets? The Match outcome is dependent on both players, so how would you go about adjusting for that?']"
[Q] can somebody explain Correspondence analysis for dummies?,"I am writing a paper for my archaeology studies on the usage of CA in archaeology. My goal is to explain it more detailed than archaeologists did before (put the contingency table into software and push button) but simpler than dedicated literature. I understand the contingency table and calculating the probability of independence, but I am stuck in understanding how to put the chi-squared equation into it to get an inertia and translate it to vectors.

Tl;dr: can someone explain CA to a stupid archaeologist like he's 5?",13pqijt,PrivatePepe,1684852905.0,6,0.88,"['Imagine you have a big toy box filled with different types of toys. You want to see if there\'s any connection between the toys that kids like to play with. To do this, you start by organizing the toys into rows and columns in a table, like a game board.\n\nNow, you ask a bunch of kids to come and play with the toys. As they play, you observe which toys they choose and mark it down on the game board. Each cell in the table represents a combination of two toys, and you count how many times those toys were chosen together.\n\nOnce you have this table, you want to find patterns and understand which toys are more closely related to each other. Here\'s where Correspondence Analysis comes in.\n\nFirst, you calculate the expected probability of two toys being chosen together if they were picked independently (meaning one toy\'s choice doesn\'t influence the other). This is like assuming the kids are randomly picking toys.\n\nNext, you compare the actual counts of toy combinations to these expected probabilities. You subtract the expected probabilities from the actual counts and square the result. Then you add up all these squared differences.\n\nNow, you have a number that represents how much the actual toy choices deviate from what you would expect if the toys were picked randomly. We call this number inertia. The bigger the inertia, the more related the toys are.\n\nTo understand this better, think of inertia as a measure of how surprised you are by the toy choices. If the kids consistently pick certain toys together more often than expected, you\'d be more surprised, and the inertia would be higher.\n\nTo visualize this, you can imagine the table of toys as a big map. The inertia tells you which parts of the map are ""hotspots"" where certain toys are popular together. You can draw arrows from the middle of the map to these hotspots to represent the relationships between toys. The length and direction of the arrows show the strength and direction of the relationship.\n\nSo, Correspondence Analysis helps you discover hidden connections between toys and see which toys tend to go together. It\'s like making a map of toy preferences based on how kids play and showing the important relationships between toys using arrows.', ""Thank you so much, it's really a great example. I still feel like running into a wall. I can follow the logic to creating a contingency table of the observed numbers and creating one of expected numbers, then I found out how to calculate chi-squared values but after that it seems like magic and you get coordinates with certain distances to the mean profile in a point cloud and somehow you get Eigenvalue and row and column scores.\nI guess I'm way in over my head with this...""]"
[E] Statistics Plan for Public Policy — Need Advice for Streamlining,"Cherissime amis. I am trying to see if I am going about my career/education plan the wrong way.

**Current status**: CSULB Statistics student since fall 23.  
**Current plan**: Complete Statistics degree. Double-major in Economics. During or after, intern/part-time[1] with public sector for work experience (possibly for 2 years if class schedule works out). Path splits here: enter public policy/administration program, then work OR work, then come back for public policy/administration program. Same end result since the MPA program at CSULB prefers for people to have work experience anyway. MPP at UCI is also an option.  
**Current education**: Will learn R, SAS, machine learning, survey sampling, and *probably* SQL at my university for stats. Python is introduced but not done in detail (PANDAS not covered). Will cover basic forecasting, urban/public economics, and first-semester graduate econometrics for economics. No plan for real/numerical analysis right now. Have taken a class on public policy. Planning for undergraduate thesis to involve public policy as part of my CSULB program.  
**Potential education options**: I'm open to taking community college classes for cheap, but I'm uncertain if these are respected enough/would add any value. That said, there's a few basic accounting classes and [data science classes](https://catalog.cccd.edu/coastline/pathways/technology/computer-information-systems/data-science-certificate-achievement/#requirementstext) that I could take, but the latter seems to overlap decently with my bachelor's. One of the directors for the program really [upsells it in a video the CC made](https://www.youtube.com/watch?v=l6RKrexzjh4), so I'm feeling a little unsure.  
**The pipe dream**: Take MPA courses in undergrad. After undergrad, go straight into a PhD program in public policy.  
**Absolute ideal sector/career**: Use statistics and economics knowledge to investigate economic effectiveness of educational policies (i.e. educational economics). Preferably operating in public sector. Open to being a professor/lecturer part-time/full-time.  
**Minimum**: Any public sector job involving statistics/data.  
**[1] List of internships/opportunities I found/know of:** Some volunteer-internship opportunities with local government agencies. PPIC summer internships. Federal government student trainee options. JusticeCorps. Maybe something involving education.  

In short, my concerns are as follows:  
1. Will it be difficult for me to find employment within the public sector? I'm not *really* opposed to private as a stopgap/networking between my education to the public sector, but if my job just involves making a funny finance value go up, I don't think I'll be satisfied. Consequently, I'd like to do something decent/more memorable. ~~Also job security.~~  
2. Is my desire to pivot from statistics to public policy viable, or would I need more work experience/classes beyond what I've listed (undergraduate thesis)?  
3. Is there anything else I'm missing in my plans that would make me a better fit for entering into/maintaining an effective career in the public sector/as a statistician?  
4. Are there any other career endpoints/educational options that would make more sense than an MPA/MPP/PhD?  
5. God dang it do I need to take real analysis  
6. Do I *really* need a PhD?  
7. Considering my desired field involves researching education but my undergrad doesn't directly have it, is it worth paying for extra classes in it, or is it probably just a work experience game?

Many thanks for your responses. I wasn't sure if this was better here or in /r/askstatistics.",13pj9fx,Sekka3,1684833528.0,1,0.99,"['What is your goal, i.e. what do you want to do? If you want to be a serious researcher, stats with some good math knowlegde, ie calculus and linear algebra, is a huge advantage in the social sciences and public policy. 5.  Speaking as someone who had only one calculus class in undergrad and is now in a poli sci phd program, I would certainly take more math if I had to go back and do my undergrad over again. Time spent relearning math during a phd is time that could be spent on papers and acquiring substantive knowledge. It will also help you greatly in the phd application process as better programs will be more confident in your ability to understand what is going on under the hood. Will real analysis help you? Maybe, maybe not. If you want to get a PhD public policy, it probably cant hurt you, although I would differ to someone else on this. 4./6. If you really want to do research, i.e. designing experiments, conducting studies, and understanding the interaction between education and economics, a masters is the minimum in most cases, and a PhD is probably required for most research oriented positions. 7. I would concentrate on getting work experience, which will definitely help you with MPP/MPA applications. You can always upskill when you get to the program and it seems like you have a solid base, unless getting an entry level position requires skills you dont have or cant learn on your own. 2. A switch to public/education policy is completely viable with your background, and your background will actually give you an advantage over someone who has never taken stats, econ, or calculus before.', ""Thank you for the reply.\n\n> What is your goal, i.e. what do you want to do?  \n\nI have an *area* I want (educational economics), but past that, I'm not deadset on a specific career. Insofar, policy analyst or researcher in that area would work just fine. I'm not too picky purely because I can't guarantee anything.  \n\n\n>  Speaking as someone who had only one calculus class in undergrad and is now in a poli sci phd program, I would certainly take more math if I had to go back and do my undergrad over again. Time spent relearning math during a phd is time that could be spent on papers and acquiring substantive knowledge.  \n\nInteresting. I was worried that my lack of formal political science knowledge would be an issue and I wouldn't even get my application looked at, but it sounds like your specific program emphasizes mathematical/quantitative techniques?\n\n\n> It will also help you greatly in the phd application process as better programs will be more confident in your ability to understand what is going on under the hood.  \n\nIs my lack of *formal* political science background not as much of an issue, or is it just a matter of making my niche clear/getting work experience/finding the right professors who are similar to what I want to do?  \n\n\n> A switch to public/education policy is completely viable with your background, and your background will actually give you an advantage over someone who has never taken stats, econ, or calculus before.  \n\nMy concern was that similar individuals might have experience/more knowledge in political science/education that'll keep me locked out of the field, but at least hearing it's *possible* is the main assurance I needed."", 'It seems like you are worried that your lack of substantive knowledge of education policy/economics might hold your application back. I think this depends in part on what type of PhD you want to get and what area in. A PhD in Public Policy or Economics with a focus on education policy for your dissertation topic may only allow you to take 1 or 2 courses on education policy, while the rest will be broadly focused on the policy process and policy analysis(public policy phd) or on economic theory (econ phd), with both requiring alot of technical training in econometrics.  Given that it seems like you want to focus on the intersection of economics and education an econ or public policy phd makes the most sense. This is different from a PhD in education or education policy from an education school that is going to be more focused on traditional education topics, like does x program to raise math scores for underperforming kids actually work? If you wanted to do that type of work then I would say that your lack of Ed knowledge might be more of an issue. Would substantive knowledge of Ed policy help you? Absolutely, but you can build that up through masters courses or reading on your own. For example, my own research tends to focus on the politics of policing. I have never taken a class related to policing in either my PhD or Masters and have no formal training in the area other than what I have read on my own or learned from professors (political scientists dont study policing really). Acquiring substantive knowledge is relatively easy, just read more or do a directed reading. It is much harder to gain technical or math skills, which is what most phd programs want to see. I would take some classes now or in your masters on Ed Policy or start to read joirnal articles on topics in that area that interest you. Have an idea of what you want to study for your phd, but realize that it might change.', ""Thanks again. I don't really have any other professor/mentor who's in this area (undergrad in stats doesn't exactly attract an educational policy analyst...), so...\n\n> It seems like you are worried that your lack of substantive knowledge of education policy/economics might hold your application back.   \n\nCorrect. I'm not afraid of learning new material in my PhD/master's, I'm just worried they wouldn't even take it seriously if it's not close enough to the field. Herein, it's a switch from STEM to Social Sciences, but I conjectured that this decision would look intentional enough.\n\n> while the rest will be broadly focused on the policy process and policy analysis(public policy phd) or on economic theory (econ phd), with both requiring alot of technical training in econometrics  \n\nFrom what I heard from other students at my uni trying for PhDs/master's in econ, real analysis will ~apparently~ be necessary/very good for that level of econ, which was why I stressed it so much in my posts here. I'm not very into mathematical proofs, but in the name of my goals, it's an option.\n\n> This is different from a PhD in education or education policy from an education school that is going to be more focused on traditional education topics, like does x program to raise math scores for underperforming kids actually work?  \n\nWhile I would *like* a general understanding on these things so I can sort of talk about the topic with people, I'm not interested in outright designing/criticizing those traditional education topics. I'm more interested in policy side, as of now.  \n\n> Would substantive knowledge of Ed policy help you? Absolutely, but you can build that up through masters courses or reading on your own... Acquiring substantive knowledge is relatively easy, just read more or do a directed reading. \n\nFor my public policy class, I did do a short analysis on ESEA 1965-1975, so I at least have the toolset to approach this subject according to *some* of the standards of my uni's MPA. Sometimes it gets bad enough I'll talk about ESEA 1965-1975 for 5-30 minutes...\n\n> It is much harder to gain technical or math skills, which is what most phd programs want to see.  \n\nInteresting. One of my concerns was that my application wouldn't appear to be literate in political science/theory/methods, but that sounds rectifiable with internships/my undergrad thesis, anyway. Hoping that tripling down on technical skills from statistics + field knowledge of economics + work experience from public policy will be enough.\n\n> I would take some classes now or in your masters on Ed Policy or start to read joirnal articles on topics in that area that interest you. \n\nI was actually thinking of taking educational research/statistics offered at my uni. That said, most of the hot stuff is in grad-level (e.g. a course in educational program evaluation, educational administration), but I'll probably fish around and see what I can squeeze in not in a hurry. As for reading, I really ought to make time for that...\n\n> Have an idea of what you want to study for your phd, but realize that it might change.  \n\nI feel pretty set on either k-12 education or community college. Realizing it might change might be the bigger difficulty.""]"
[E] Recommendations for introductory statistics course or self-study textbook,,13pa991,Schub21,1684806050.0,2,1.0,['Stat2: Modeling with regression and ANOVA.\n\nIt has the advantage of being introductory (i.e. not calc based like some of the other mentions) as well as practical. You can also find R code for all examples as well as exercises online.']
[Career] I got a 100K job. Does a masters in stats make sense?,"Currently I work in the financial industry with 3 years of experience under my belt. The job its boring, not mentally challenging and the skills earned in the job are not really that transferable. I have a degree in economics with an emphasis in econometrics (I took some math and stats courses to prepare for a Masters in economics). I was wondering if a masters in statistics its the right move in my career given that Im already making good money. I enjoy the mathematical rigor required in stats and coding machine learning models.

I know the decision is personal, but I wanted to know what factors would you guys look at, or if its just a dumb decision to leave a good paying job.

PD: I dont mean to sound entitled or ungrateful.",13pa6km,AssignmentHelpful,1684805842.0,35,0.81,"[""I'd look into part time MS programs. You can work and get your MS at the same time, which is what I did. Downside is that it could take up to 5 years depending on your pace, so it's not for everyone. \n\nI don't think the MS actually taught me skills that would get me hired, but it definitely helped introduce me to a lot of theory that makes learning about machine learning concepts a bit easier to grasp. Also I think it helps you get more interviews compared to not having a master's"", ""1. As others have said, don't leave your day job.  If you do you will have to get hired back in.\n2. You may find that your current employer doesn't notice your new degree; you were Fred before, and are still Fred.  You might have to change employers to benefit.\n3. The most important - **talk with people**, both within and outside your current employer.  See who values what, where and when."", ""I didn't regret pursuing stats as a MS even though I never used it - also working in finance and it didn't really make sense to change careers. I don't know how much programs charge these days but my 2 year program didn't break the bank, I spread it out over 4 years, and I loved the material. Maybe what I'm saying is what do you have to lose?"", ""Some perspective from someone with a similar career path. An ms in stats won't really teach you more than what you can find online. It's not enough to push you into a quant role and if you are already making 100k+ it's more likely to open up a lateral role. I'm at six figures after starting back office in finance, moving to fo and then switching industries to healthcare. Take a course and learn python and maybe SQL/other language. Automate your work and think about what you would rather be doing with all of your free time. I'm working on my free school money to get a master's in HR just to give me some flexibility."", ""Another thing  - a master's in statistics covers a lot.  There will be programs which emphasize machine learning, mathematic statistics,and and analytics progams which will be hybrids.  Cast your net widely. I got an MS In Industrial Engineering at the U of Michgan.  Their program was \\*extremely\\* flexibl.  You could get (effectivel) an MS in statistics, operations research, human factors, decision theory, of financial engineering."", '""Does a masters in stats make sense""\n\nIf your primary goal with the degree is to increase your earning potential: probably not at this point.\n\nIf your primary goal with the degree is to deepen your understanding of statistics as a branch of mathematics: yes. \n\nThat\'s basically what it boils down to, imo.', ""I would not leave your job. Consider doing a part time program and potentially casually making a portfolio on the side while you're in school. If you already know Machine Learning it shouldn't really take too much time and you have 2+ years to do it."", 'If you like studying math/stats/coding then I would suggest just studying math/stats/coding outside of a university context. I think if you put in 1-2 hours a night; 5ish nights a week you can make great progress. But that is only sensible if you will enjoy doing it... otherwise it will be a huge slog (though an MS would be too).\n\nYou could either a) find material that is interesting to you and learn it; or b) identify the books used in the curriculum of an MS program that looks good and then follow those.\n\nI would suggest some combination of (a) and (b) above (combined with a fair amount of perusing quant finance papers, if that is where your interest lies). Though I have found that anything that is both interesting to me and somewhat challenging has been really valuable (similar ideas crop up everywhere! It just takes time for everything to connect)\n\nI think one big advantage of self study is that you can identify when you are saturating on material, and then either a) work on a related project/problem/calculation to help solidify, or swap to other material (eg. from theory to applied) and later cycle back. It is generally not helpful for me to spend more than \\~10-20 hours on a topic at a time (though I will revisit a top 3+ times)', 'Find the cheapest shortest online MS program you can. It doesn\'t really matter where it\'s from since you have work experience, but may help get you in the door for an interview to roles you\'re interested in that look for the MS/PhD letters. No reason to spend so much money/time on that when you can do it for cheaper/faster. Some programs won\'t have as ""good"" of classes, but there are better free materials online that you can supplement with anyways (they just don\'t give you the MS title).\n\n""Data science"" programs are also quite prevalent now and may give you a mix of math/stats + coding knowledge. I expect it will be easier to find a program in that area than stats.', 'Hi I like a data. Given ur experience, ur interests, and the type of code you want to do. The job you should be gunning for is quantitive analyst. It\'s highly likely you won\'t take a pay cut. And you probably won\'t have to take another data or dev job to land ur first stats/ml gig. ( and may not require further education) \n\nIf ur looking to leave finance and become a broader data scientist or machine learning engineer who spends most of their time on statistics and ML model development, it\'s gonna be hard to get ur foot in the door with just a masters, your gonna have to have some really stellar projects.  This  second route will be more enjoyable if you have a broader love of data work/ and software development.\n\nI left a career I was burnt out on to go back to school and work in data 18 months ago. And I\'m a year into my first data job and love every second of it. I listened to a podcast called "" build a career in data science"" and that really inspired me to try it.', 'Find the cheapest shortest online MS program you can. It doesn\'t really matter where it\'s from since you have work experience, but may help get you in the door for an interview to roles you\'re interested in that look for the MS/PhD letters. No reason to spend so much money/time on that when you can do it for cheaper/faster. Some programs won\'t have as ""good"" of classes, but there are better free materials online that you can supplement with anyways (they just don\'t give you the MS title).\n\n""Data science"" programs are also quite prevalent now and may give you a mix of math/stats + coding knowledge. I expect it will be easier to find a program in that area than stats.', ""You don't mind me asking, what role do you have in the finance industry? \n\nI've been applying for jobs for almost a year since I graduated in Applied Economics. I have an interview this week with a mutual fund to be a Financial Advisor.\n\nI've been studying similar things you have ironically, and held off enrolling in a masters of data science program because it costs a lot of money and I can't afford it right now. \n\nI was just writing because I was curious if you went down the role of being a financial advisor. If so, how long did it take you to build up a comfortable income?"", 'You seem like a perfect candidate for an Actuarial Program.  \nThey certainly get paid a lot and you already have a great background.', 'Check out OKState - they have an online masters - you can do it part time (2 classes/sem is doable) - and when youre done you have that magic stem masters in related field on future job postings.', 'I second this!\n\nI already have an MSc in Stats (UK) and work as a senior marketing analyst. I am doing a second MSc in Marketing Management but part time while I work.\n\nI have to say that its a real grind but doing a masters thats applied to your daily work is a real bonus! You learn so much more this way because your ideas from learning bleed instantly into your work. This counteracts one of the problems with doing a MSc program and THEN working, its the lack of real world, industry specific, applied problem solving knowledge. I realise this is not quite the same with op since they already work.', 'I appreciate the response. I agree that part-time MS would be ideal, but the rigidity of my job hours may make it challenging to complete. My main goal with getting an MS is too get the tools (math and stats) to better understand ML and the math behind quant research. I understand that an MS in stats wont be enough for a quant role given that the competition usually has PhDs, so thats one of the reasons for my doubts/hesitation.\n\nAre there many jobs outside the finance industry with 6-figure jobs and a high ceiling??', 'Noted, I think this is the simplest and most effective way to think about it', 'This really isnt true. \n\nM.s. in stats is still a really, really good choice. And a lot of SHIt is done wrong with basic statistics. \n\nProbably ninety percent of towards data science/medium/ no mastery authors cant implement filtering properly. A ms in statistics will probably do that early on. Theres so much bad practice out on the internet. Can you pick up a casella-Berger text and self teach?  Sure. But its easier to learn in a program.\n\nThe fact of the matter is that most data scientists dont know what theyre doing, but are pretty confident. And because theres a huge cultural feedback loop as far as industry interest and Google returning pages that gets clicks; chances are youre gonna go down a problematic route if you rely on these avenues to learn basic things.', 'Ty for the reply, I have taken courses in python and SQL and have deployed some ML projects. But I believe there is a difference between coding some ML models using python libraries and really understanding what happens under the hood. Thats why I believe a MS in stats would be beneficial, you get a better understanding of math and you get a line on your CV that kind of backs it up. My indecision regarding the MS in stats comes from the fact that I dont know if an MS is enough to get a 6figure job in the same industry or in any adjacent industry.', 'As much as I like stats and math, I have to worry about job prospects and paths. What Im not sure about is how strong is a MS in stats for quant roles in finance and how abundant are high paying jobs in other industries', 'So, you think the difference in lifetime earnings between the two options is not substancial enough to justify the additional costs related with an MS in stats?\n\nMy fear is getting the MS and getting a worse job, I have the feeling the job market will to continue to tighten over the next years.', 'I dont know, at least in the financial industry it matters where you get your advanced degree. I like the idea but Im doubtful that the time/money savings is worth it.', 'Yeah there are. I can\'t speak for the market right now, but even a data scientist can definitely earn 6 figures.  https://www.levels.fyi/t/data-scientist?compare=Microsoft%2CGoogle%2CFacebook\n\nYou\'ll have to be comfortable getting a lot of compensation as part of equity though, but even at banks data scientists can get 6 figures as base. I know people who are senior level data scientists with only bachelor\'s, so a master\'s or a PhD is not a necessity. \n\nThat said, if you want to become a ""research"" data scientist or MLE, you\'ll probably need a master\'s at the very least.', ""I think if you're about the money the other roles I'd look at are Applied Scientist titles at large tech companies although these are going the way of the quant researcher in being increasingly PhD-centric."", ""Sure, but does it make you money? This is my point. It's great to have but for real, hard, stats? They get a PhD in stats to do it. \n\nThe MS is good sure, but I'm talking about career position op is currently in.  Heres the thing: the question is, will it make you more money? Not as an analyst. Will it help you get a different job? Likely but lateral or lower level."", ""Depending on what you want to do, unless you go into some data heavy/mature industry you wont really be doing anything in-depth. I've seen this first hand, you can get away with regressions and clustering models...most of which wont see the light of day. If you are moving over, the ms might give you enough to be a manager that knows enough to understand and guide others. You can lateral over, maybe but you wont have the years of experience to do what most companies would like a senior analyst or associate doing."", ""That's where talking with a lot of people is important."", 'Stats is one of the best post grad fiends to get into. All the fields above use it.', ""If you're already pulling in six figures, an MS isn't going to bump you much higher, imo. Generally, the masters is what helps people go from the entry-level jobs to the six figure ones. \n\nAs for the future of the market...I have no idea. It certainly *seems* like we're moving towards some kind of recession which will probably put the kibosh on a lot of these high-paying jobs, but also, people have been saying that since before the pandemic, so who knows at this point."", ""Generally speaking, getting a master's degree increases your lifetime earning potential. I don't know how applicable that is when you are already making six figures with your bachelor's."", 'Yes, but only in 2 cases imo:\n\n* You\'re getting a new job without any experience. This doesn\'t apply to you, and by the time you graduate your years of work experience will be what people look at the most. The MS degree will only matter for role requirements a company has as a filter like ""only MS/PhD"" applicants.\n* You\'re going to a very top tier program. If you get accepted to this type of program, it can help you get in the door to top tech/quant firms. Stats might not even be the best route for finance if that\'s what you want, but maybe an MBA. Top tier programs are going to cost you $$$, but can be worth it if it grind through school and your job afterwards.\n\nI would focus on learning things outside the job that are useful while keeping the job money rolling in. Your years of experience will mater the most before you know it, so that\'s why I would recommend trying to get the MS in the cheapest / easiest way. I did the full time work + half time MS program for ~2 years and it\'s not super fun to be honest. A lot of things I learned better on the job than in school anyways, so I don\'t think you need to put yourself in a grind for it all.', 'Thanks, will look into it.', 'Yeah, it does. The market is saturated.  An ms is pretty much the minimum for a lot of jobs in statistics or data science. Esp where its an actual job in that domain and not a glorified analyst gig which limits vertical moves.', 'Im with you in this point. I truly believe that data science and stats requieres a math rigour and understanding thats really hard to obtain from self study. People that really know their stuff are not abundant, and they are not easily replaceable. I appreciate all of your perspectives']"
[Q] What type of sampling should I say I’m using for my study?,"Im conducting statistical social science research (its my first time doing independent research) and Im a bit confused on whether Im using self selection sampling, convenience sampling , purposive sampling, or something else? 

My data was collected via an anonymous survey posted on various peoples social media platforms, with most responses coming from an influencer based in the country whose population I am interested in (Pakistan). 

My survey is in English (not the countrys primarily spoken language) and my research is about gender-based discrimination in the financially advantaged segment of Pakistans female population (a very different group from the millions of people who live in rural areas and villages). 

About ~ 30 responses came from me sending out the survey to people I know. Additionally, only people who have lived 5 or more years in Pakistan were allowed to participate. 

Id appreciate any advice on how to phrase the sampling method I used, ideally in as few words as possible because i have minimal space on my poster",13pa4sg,zugu101,1684805711.0,0,0.5,"['I got you\nrechardpeter09@gmail.com for any assistance you may need in data analysis', 'This sounds like a volunteer response sample to me.', 'I believe this is a rather clear case of convenience sampling. :)']"
[Q] How do I determine statistical significance when working with twitter data?,"If i'm running text analytics (specifically sentiment analysis in R) on a hashtag from twitter for a computational linguistics class. How many tweets do I need to download for statistical significance? One book said 1800, one said 3000, and one said 15000. What do y'all think? I'm also going to need to defend why I chose that amount so is there some mathematical way of determining what's significant? What do y'all think?",13pa3ff,KandaceKooch,1684805605.0,1,0.6,"['You should perform a power analysis.', 'I think you need to have a better understanding of statistics before articulating a question like this, there are plenty of resources. OpenIntro is a good place to start. Properly understand what statistical significance is and isnt is imperative.', 'Its not clear what test you are doing in order to achieve statistical significance. You would need to explain more about your test and experimental design.', ""It depends on the question you want to address and the model you want to use to answer it. It's usually always better to have more data though, so keep that in mind"", 'I would say as soon as you download any tweets you can be quite certain that you have downloaded more than 0 tweets.\n\nBut maybe you have a different question you want to test?\n\nWould not be the worst idea to say which one.', 'Also we should never ever try and achieve statistical significance, its not a goal.', 'Something something overpowered samples something something  \n\n\nBeta issues aside, I actually also agree that having more data is better.', 'Good point!', '>Also we should never ever try and achieve statistical significance, its not a goal.\n\nEarnest question / looking to learn: should ""achieve statistical significance"" never be ""**a** goal"" or ""**the** goal?"" Intuitively, shouldn\'t the design of an experiment include an assessment of its ability to detect an effect confidently? Is that different from achieving statistical significance?\n\nThanks.', '""Look at my super significant p-values!""', 'I have worked with Twitter data before for a very large brand and significance was so misleading.. because of the sheer volume of the numbers I was looking at (post engagements over post impressions to get an engagement rate. With impressions being around 500k-1 mil), basically everything would come out significant compared to benchmark. But in actuality the true difference would only be 0.05 pts off.']"
[Q] How should I perform clustering on angular data?,"I'm currently performing an analysis on users' event timestamps.  Each user has at least one timestamp of interest.  I am specifically interested in answering the following question (use case paraphrased):  **What groupings are there in terms of hour and day-of-the-week in which users prefer to visit a website?**.  For example, one potential finding could be ""there's a group of users who prefers to visit around 5-6PM on weekdays, another group of users who visits in daytime hours throughout the weekend, and a third group who prefers to visit between 8-10AM on weekdays."" However, I can't just treat hours and days of the weeks as linear features because they're cyclical, as Hour 0 is closer to Hour 23 than it is to Hour 4 and Sunday (0) is closer to Saturday (7) than it is to Tuesday (2).

After a lot of research I discovered [directional statistics](https://en.wikipedia.org/wiki/Directional_statistics).  It seems like the most sensible way to represent this data for clustering is to transform hour to points on the unit circle via e.g. 22.3 -> (sin(22.3/24 * 2pi), cos(22.3/24 * 2pi)) and similar for day of week, but with a denominator of 7 instead of 24 (see [StackOverflow](https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes/6335#6335), which gives a transformation that treats the vertical line at y=0 as the reference direction).  This ensures that Hour 0 is closer to Hour 23 than it is to Hour 2 when taking Euclidean distances.  As a result, each timestamp is transformed to a coordinate pair on two different unit circles - one unit circle for hours and another for days-of-week.

I also started skimming through Murda and Jupp (2000) to better understand my options.  It seems like I could also just treat the hours and days-of-week as angles from a reference point (Hour 0 for hours; Sunday=0 for day of week) and somehow work with those.  However, it's not obvious how to do the clustering if I work with the angles directly.  Additionally, there are complications because we have _two_ circular variables that may or may not be independent, and I'm not sure whether it's more sensible to treat the problem as clustering torus data or spherical data.  (Note that I did consider taking one transformation with a separate pair for each hour/dayOfWeek combination, but realized that the distances wouldn't have the properties I wanted.)

Keeping the context of the problem in mind:

* What is the most sensible approach to cluster hours and days-of-the-week to identify groupings of activity?  Euclidean distance on two sets of unit circle coordinates?  Some other approach on a torus or unit sphere?

* How should I deal with the fact that each user has multiple timestamps?  When I initially treated these features as linear, I transformed my data such that one row == one user and made compositional features of the form ""percent of visits in Hour 0"", ""percent of visits in Hour 1"", etc. and similar for day of week such that sum(hour features) == 1 and sum(day_of_week features) == 1.  However, it's not obvious how to do something similar with continuous angular data.  I thought about using a Gaussian mixture model on the unit circle coordinates with partial pooling on userId, but I don't know how to do that in an unsupervised way in R.  (I tried the flexmix package for that.)

* This isn't as important as the first two questions, but it's still somewhat important.  I'm interested in clustering _local_ hour, rather than UTC hour as the data is currently represented.  However, no one logged the time zones!  I know that time zone is determined at the user-level, rather than at the timestamp level, and that all users are within the US.  Is there an approach to clustering that will treat hours and days-of-week in an isometric way?  That is, treat bumps at Hours 20 and 21 for one user the same as for a different user with the same-size bumps at Hours 5 and 6.

Thank you!",13p3uis,coffeecoffeecoffeee,1684790634.0,17,1.0,"['There is also k-means clustering in the presence of periodic boundary conditions. [This package](https://github.com/kpodlaski/periodic-kmeans) and the accompanying paper give an implementation with several examples including the NYC taxi dataset.', ""When it comes to clustering, think about distances.\n\n> there's a group of users who prefers to visit around 5-6PM on weekdays, another group of users who visits in daytime hours throughout the weekend, and a third group who prefers to visit between 8-10AM on weekdays\n\nThis sounds like you consider the distance between, say, 5pm Monday and 5:03 Tuesday to be further appart than the distance between 4pm Friday and 7pm Friday? If so it looks like you're not after 2D coordinates over a torus nor sphere, but 1D coordinates on a circle in terms of *hour-of-the-week*. Otherwise you would need some way to choose the radii of the torus, that is, how far appart is Monday and Tuesday compared to 5pm and 6pm.\n\nTo compute the distance between two hour-of-the-week locations, you could transform with sin and cos and use euclidean distance, but the distance would be the length of the chord of the circle. Another measure you could choose is the distance around the circumference, a.k.a the arc length. The later seems more natural, but clustering is more of an art than a science, and sometimes the less natural approach can work better."", ""Arc length is better, simpler and also linear. Just divide the unit circle (2) to 24 hrs and u get /12 per hour. Meaning u can transform clock hours (T) to radians using (T)/12. Calculating the arc legth is just the absolute difference between those angles. Now, obviously if u do this to hour 23 and hour 1 you'll get a longer distance but u can set a conditional such that if the arc legth (s) is greater than  then the new arc length (d) = 2 - s.\n\nEdit: Taking euclidean distance is not linear. Take a regular clock for example, hr 12 to hr 6 should be twice of what hr 12 to hr 3 is. But in Euclidean distance hr 12 to hr 6 is 2 and hr 12 to hr 3 is sqrt(2)  which made farther points closer than they should have been."", ""K-means is one of the most widely-used algorithms to cluster data. However, it has several limitations: a) it requires the use of L2 distance for efficient clustering, which also b) restricts the data you're clustering to be vectors, and c) doesn't require the means to be datapoints in the dataset.\n\nUnlike in k-means, the k-medoids problem requires cluster centers to be actual datapoints, which permits greater interpretability of your cluster centers. k-medoids also works better with arbitrary distance metrics, so your clustering can be more robust to outliers if you're using metrics like L1. Despite these advantages, most people don't use k-medoids because prior algorithms were too slow.\n\nYou can check our BanditPAM v4.0 (includes R, Python and C++ versions). [paper](https://arxiv.org/abs/2006.06856)\n\nIt's written in C++ for speed, but callable from Python and R. It also supports parallelization and intelligent caching at no extra complexity to end users. Its interface also matches the sklearn.cluster.KMeans interface, so minimal changes are necessary to existing code. [repo](https://github.com/motiwari/BanditPAM)"", ""This seems like a clunky way to handle cyclical variables. Can't you just take the sine of the day like sin([day of week / 7] x pi) and have the hours of the day be like sin([hour of day / 24] x pi) and then just use normal clustering techniques?"", 'Thanks!  This looks potentially like what I need.  Ill take a look and try that tomorrow.', ""> This sounds like you consider the distance between, say, 5pm Monday and 5:03 Tuesday to be further appart than the distance between 4pm Friday and 7pm Friday?\n\nNo, because I'm treating hour and day-of-week separately.  So, if a user visits on many days around 5PM and 6PM, then I want those distances to be carried over regardless of the day of the week.  Additionally, if a user visits often on Mondays and Tuesdays, I want Monday and Tuesday to be the same distance regardless of which hours on those days the user visits.\n\nThis is largely for ease of interpretation, as a circle with 216 positions is going to be very difficult to present compared to two circles with 24 and 7 positions, respectively.\n\nOf course, I might also be thinking about this totally wrong, and if that's the case I'm fine hearing it :P."", 'Mapping to the unit circle is just doing this but also taking the cosine. If you only take sine you have differential scaling between various points, rather than properly preserving equivalent intervals on the circle.', ""Eh, it's probably good enough.\n\nAnd I think the concept could hold, because if you really wanted to you could just assign fractional values to each possible day and hour entry. It's not like 24 mapped entries in one dimension and 7 mapped entries on the other dimension is a difficult thing to encode in your data processing.""]"
[Q] One way anova,"I did a one way anova in excel where i compared 8 groups. The p-value was non significant, but when I made the figure and inserted the standard deviations on the different bars some of the deviations on the bars did not overlap. Are they still significantly different from each other even though my p-value was above 0.05? Or does the deviations not matter when the p-value was non significant?",13oy36t,Brenn1vin,1684778333.0,0,0.5,"['""made the figure"" is highly ambiguous. You\'re going to have to explain the circumstances in more detail, and what you plotted.\n\nIf your overall anova p-value was not significant you would not normally go on to do pairwise comparisons. If you were going to do pairwise comparisons regardless, what was the point of the ANOVA?', 'If you had planned a priori to compare each mean with each other mean you could have used the Tukey hsd and not done an ANOVA. This is valid since Tukey controls the Type I error rate on its own. You would  slightly increases the Type I error rate if the Tukey test were done following a non-significant ANOVA  but for your own education you could see how it comes out.', ""If you're checking if the means of the 8 groups are the same, you can do K-way Anova.\n\nThen if not the same, you proceed to do multiple comparison which has methods including TukeyHSD, Bonferroni method, LSD. They basically carry out pairwise comparison of means of every 2 groups but with some correction."", ""I dont know anything about statistics, which is why I'm here. I did the analysis in excel, and used the means of the groups to make a simple bar chart. Then I calculated the standard deviation errors which gave me the error bars that was placed on the bar chart to look for overlap. As far as I know some groups in an anova can be different from one another and not, even though the p-value is significant. I did an anova instead of doing multiple pairwise comparisons. What I'm asking is; can the few groups that dont overlap still be significantly different from one another even though the p-value is not for the whole analysis.\n\nIm trying to see if the amount of prey delivered to a nest varies during the time of day. The day is segmented into 8 different groups and each group contain data from multiple nests""]"
[R] Another Bonferroni question! When to reset the FWER? Theoretical?,"I am reviewing my write-up Results section and thinking about how I can improve on the consideration I gave to random error in running so many tests. So I'm calculating Family-Wise Error Rate (FWER) so as to look back on results and demarcate what might be more likely to be a random result.What I'm trying to figure out is what constitutes a fresh analysis and fresh calculation of FWER? I'm presuming it is partly theoretical - for example when a hypothesis is looking for correlations, ANOVA, and a few linear regressions in order to answer the question, these might be considered the 'family of tests' relating to that hypothesis.But I'm guessing one might also view it as random Type I error risk relating to the tests run on the whole sample of participants, rather than per hypothesis.So, for example, let's say we had a sample of 100 people who sprinted from point A to B to C, producing time data.

1. Compare Point A results to B and C, and B to C. Also run correlations between A B and C.
2. Split the sample into Fastest Sprinters from A to B, and Slowest from A to B. Compare Fastest versus Slowest groups on A to B and C, and B to C.
3. Split the Fastest and Slowest Sprinters groups into people who drank Red Bull beforehand and didn't. Compare Fastest versus Slowest drinkers of Red Bull versus none on times between point A to B and C, and B to C. After looking at correlations, use Linear Regression to predict the time from B to C for Slowest Sprinters who drink Red Bull based on the predictor variables of red bull cans per week and time from A to B..
4. The same participants take the test 6 months later, compare the results again.

I am thinking FWER can be calculated for all tests, something like 6+6+23=approx 35 tests to be run, is a FWER of 1 - (1 - .05) to the power of 35 = 83% chance of Type I error and a proposed alpha level of .0014 to avoid Type I error.But could we also look at it as different hypotheses? Needing separate FWER calculations? For example the whole sample calculations, could be considered differently to part of the sample's calculations when putting them into a subgroup and looking at a different dependent variable.And what of the fourth circumstance - should we reset FWER completely given the test statistics were gathered 6 months after the first set of statistics?",13orsoc,Vax_injured,1684764133.0,1,1.0,"['All questions you ask of a sample must be part of FWER correction. Only exception is if you do the same analysis twice, e.g. you calculate a correlation between y and x and then you do a regression.. dont do the same thing twice. A new sample does not need to be corrected for hypotheses tested on another sample. The scientific method is hypothesis first, write them on a piece of stone and then test just those while correcting for FWER. For next experiment you can write other or new hypotheses. This procedure can be followed with data splitting. Say you randomly split data into 80% and 20%. You use 80% to discover say 10 genes out of 5000 that seem interesting and make hypotheses for just those 10 genes and then you test just those in the 20% while correcting only for 10 tests because this is a new sample. But the 20% is use once. If you go back and forth between whats discovery and confirmation, youll quickly get in trouble and cease doing science aka data leakage.', ""Just want to check I understand - so regardless of all the different ways I've split up the sample or whatever tests I'm doing, ALL must be accounted for in the one family of tests. This changes with a new sample - even if they're the same participants doing the same thing a week after the stats were done on the first sample. It's a 'luck' on the day kinda thing. Save the emerging hypotheses for another day's testing.\n\nWith correlation, if I run a follow up regression say to check for the amount of variance in the DV explained, isn't that only 1/2 of the tests a regression runs? It reports ANOVA, Pearson's - maybe that's what you meant, that regression counts as 1 test, although if there are a few predictors, would they each also be counted as a separate test? I'm guessing they would.\n\nIn my thesis, I'll be using ancillary hypotheses as a form of exploratory testing, I read somewhere that mightn't need the rigid Bonferroni correction as it is being viewed exploratorily, also would make sense I guess.""]"
[Q] Zipf: An Appropriate Statistical Distribution of Computational Power in the World?,"https://top500.org gives the top 500 supercomputers and their respective FLOPS (computational power).  
Do you think all computational power per device would be a Zipf Distribution or some other distribution? I want to use the data from Top 500 to estimate the total computational power in the entire world.",13oro8t,freedomisfreed,1684763866.0,1,1.0,"[""Standard test is to bin a bit then do log-log plot of x vs frequency of x. If the log-log plot is linear and negative, you've got a power law. (Or at least something like it, some people are very persnickety about saying nothing is ever _really_ a power law).""]"
[Q] Reasonable FIML estimates?,"Is it reasonable for a regression coefficient to have a +21% increase following full information maximum likelihood to account for missing data? For example, from 1.9 with missing data to 2.3 after FIML?",13op57w,alpacamaka01,1684757522.0,1,1.0,"[""A 21% increase in a regression coefficient following the use of Full Information Maximum Likelihood (FIML) to account for missing data can be considered reasonable, especially if missing data is related to the variables in the model. FIML is a method used to estimate missing data by considering all available information in the dataset. By incorporating this additional information, FIML can improve the precision and accuracy of parameter estimates.\r  \n\rThe increase in the regression coefficient suggests that the imputation of missing data through FIML has provided a more reliable estimate of the relationship between the variables. It indicates that the missing data, when properly accounted for, has influenced the parameter estimate and increased its magnitude.\r  \n\rHowever, it is important to interpret these results cautiously and consider other factors. Firstly, it's crucial to assess the statistical significance of the coefficient estimates and determine if the increase is statistically significant. Secondly, the specific context and nature of your data and research question should be taken into account when evaluating the meaningfulness of the increase. Finally, consider whether the change in coefficient aligns with your expectations and theoretical understanding of the relationship between the variables.\r  \n\rIn summary, while a 21% increase in a regression coefficient following FIML for missing data can be reasonable, it is important to consider statistical significance, the context of the analysis, and theoretical expectations to ensure appropriate interpretation.""]"
[D] Risk of studying from old textbooks,I got a bunch of statistics textbooks for free from work and they're published around 1990. The topics look really interesting and I've started skimming through some of them. What do you guys think the dangers are of studying from books that old.,13ome42,gvbd,1684749307.0,3,0.81,"['If the textbooks are about math or statistics, then theres no problem learning from old books. \n\nIn statistics, certain more recent trends like the Bayesian approach to statistics or bootstrap techniques might not be mentioned or emphasized.\n\nWhat matters most is just who the authors are and how well they presented the material.', 'I personally find old book are very good to read. They are harder to read too, but its good for you to train your ability to read mathematics, once you understand those the newer one are usually not hard to read.', ""I have much older books that I swear by. (An old copy of Feller for example. I wish I had a good copy of Kendall and Stuart Vol I -- 3rd edition would suit me fine)\n\nit depends on the topic; some topics have barely changed in 50 years, others have change a lot in the last 20.\n\ne.g. If you're learning mathematical statistics you're probably fine.\n\nIf you were trying to learn say MCMC or be current on Statistical Learning, you might want a more recent book since a fair bit has happened in 30 years."", 'Out of curiosity, which books did you receive?', 'I think its important to not learn from the most popular text books. Take some time to just visit the Wikipedia articles for some of the topics in your books and compare notation from the book to the wiki - youll get a sense of broad applications, and maybe the notation is slightly different on the wiki and you can reconcile the book to the wiki', ""I don't have them with me right now, I'll post a picture of the collection next time I go home - which is in 5 days""]"
[Q] Where to go next after finishing Statistical Rethinking by Richard McElreath?,"I was part of a reading group and finished Statistical Rethinking by Richard McElreath. 
What book/course would be a logical next step? (Preferably on the applied side, but not necessary)",13ojo95,zeoNoeN,1684740251.0,44,0.98,"['Perhaps Bayesian Data Analysis by Gelman:\n\nhttps://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954', 'Doing Bayesian Data Analysis by John Kruschke. I read this one first and just started on Statistical Rethinking. I found Kruschkes chapter regarding experimental goals, sample size, and power helpful for experimental design when you expect to perform Bayesian analysis post-experiment.', 'Solomon Kurzs re-work of Statistical Rethinking using the package brms', 'To piggyback off this (great) suggestion: the whole book is [available for free in PDF form](http://www.stat.columbia.edu/~gelman/book/)', 'There are some free PDFs available and also I think co-authour Aki vehtari has some course material available on his github', 'Thank you! That was also recommended by a PhD Student. Since Im from a non stats background, are their any extra fundamental one would need? I always heard that this book is considered hard', 'Havent heard of this before, Thanks!', 'And the authors website for the book: \nhttps://avehtari.github.io/BDA_course_Aalto/\n\nEdit: Also the other authors website: \nhttps://www.stat.columbia.edu/~gelman/book/\n\nEdit2: The lecture videos: \nhttps://m.youtube.com/playlist?list=PLBqnAso5Dy7O0IVoVn2b-WtetXQk5CDk6', 'You can always dive in and then backtrack to fill any gaps in knowledge as you go.\n\nAnother great book that covers probability fundamentals is Introduction to Probability by Blitzstein and Hwang: https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573\n\nHighly recommended!', ""Where'd you find a reading group?\n\n> recommended by a PhD Student\n\nAre you also a student? This would explain finding a reading group that would read a book like this. \n\nI'd love to find a similar book club."", ""Hard is always relative... Compared to many other graduate-level statistics books, Gelman is easy. Also, it absolutely is an applied book - theoretical statistics has a very different flavour. I'd say it is the logical follow-up to McElreath.\n\nHowever, I think the book is considered to be hard since it is often attempted by readers without the proper math background. You'll definitely want to understand probability at the level of Blitzstein and Hwang, and have a good handle of multivariate calculus and matrix algebra."", ""Ooh, I didn't realize there were lecture videos up!"", 'Thats the Harvard one right? I worked through some chapters already and love it', 'I found one that was offered by a PHD Student to other PHD and Master Students (Degree was non stats)']"
Regression Interpretation [Q],"In a little side project Im doing at work to look at some of the relationships impacting sick leave on the railway I have the following regression equation

Sick leave = -0.12 - Driver(0.039) + Age(0.005) + Stops (0.001) + Kms(0.002) 

Where,

Sick leave running count for corresponding diagrams.

Driver is a dummy indicating whether the employee is a Guard or Driver

Age is the employees age

Stops represents the number of stations they will stop at in the diagram

Kms represents the kilometres on there schedule.

With this equation, Im just a little confused on the interpretation.

If I wanted to evaluate the impact of 20 additional stops on increasing the chance of sick leave for a Driver of 50 years of age would I plug this in like this.

Sl = 
-0.12-1(0.039)+50(0.005)+20(0.001)
+0(0.002) = 0.125

Then would I just take the percentage change of this using the base case when stops equals zero, but age still equals 50.

Thanks for any input ",13oi7im,WIILLLZ,1684735485.0,0,0.5,"['The impact of 20 additional stops on sick leave, in this model , is the same if the driver is 6 months old or 100 years.', 'This takes me back to college days. Im a bit rusty but Im also curious to read what people say. Id also take the difference between stops equals zero and stops equals 20.', 'Difference should be 0.001 or .1%', ""From your equation, the incremental impact is 20*0.001. Basically if you hold everything else except the number of stops constant, the regression boils down to just the equation of a straight line with slope= 0.001. Intuitively, the slope is the rate of change (the result of the first order differential with respect to stops).\n\nGiven the model, though I'd suggest you look at multicollinearity issues with the model. Based on feature names, I suspect stops and kms are correlated. If so, you can't draw the conclusion I proposed above as the coefficient is biased."", 'I have no idea of what Im talking about, but shouldnt more stops also increase the km?']"
MS in Statistics make up for bad undergrad stats? (PhD stats admission) [Q],"I applied to PhD programs in statistics this cycle and didnt get into any. I got into one funded MS program in statistics at Miami university in Oxford oh. My undergrad stats were BS in stats with minor in mathematics, so I took math up to two semesters of real analysis. I had a 3.4 cumulative gpa as an undergrad. 

I am doing a funded MS, and hoping to reapply to PhD programs. If I get a 4.0 gpa, and do good in the coursework, can this boost my chances to getting into a whole lot more phd programs in 2 years? How do adcoms view students who did well in MS in Stats at another school? Id have to reapply, since Miami is a terminal MS, and theres no phd program there. So Id also like to hear what I should do in my MS to boost my shot at getting into PhD programs?

Love to hear your thoughts.",13ohk1z,AdFew4357,1684733427.0,1,0.6,"[""I guess masters in statistics would be more beneficial than just bachelor's no matter it's either academia or corporates.\nGood luck with your studies."", ""3.4 isn't great these days. A terminal masters program typically will prepare you for statistical consulting gigs. You may find that's the level you want to stop at.\n\nI'd encourage working up your GRE scores and get involved in research during the masters. I too had not a great undergrad GPA, but aced the GRE and had relevant experience. It's ok to be weak in one place (grades) if you make it up in the other areas."", 'Regardless, do a funded MS. Not in stats but I had a low undergrad gpa (although about a 3.9 in the last year after getting my shit together), did a MA and did well, got into a top 5 PhD program. \n\nAlso, PhD is research, so get research experience', 'So an MA would greatly improve my chances?', 'Could high grades like a 3.9+ in a MS help tho? Or would they really still focus on my 3.4 in undergrad? I mean, its a terminal MS, but its only because the department is small and doesnt have a PhD. Its highly theoretical and places kids in PhD programs.', 'Gotcha. Was ur PhD in stats?', 'Absolutely. Redemption stories exist and resonate because a lot of us needed redemption for ourselves. A PI offered me a fellowship and mentioned my horrific 1st yr grades followed by several years of all As as a strength that he identified with, not a weakness. But I wonder if he would have noticed my app if not for a professor I did a few papers with who lobbied for me. \n\nIt works each way. I had a student who had abysmal GREs, and frankly was the kind to do the bare minimum in the coursework. But cranked out papers like a Xerox.', 'Nope', 'Thats nice to hear. So do you think theres any connection between gpa/grades and research capabilities?', 'I see. But adcoms do almost always take students who did strong in an MS. Do you think the MS experience was the difference in making you competitive for top programs?', 'Ultimately hard to say but probably']"
[Q] Should we control for this variable?,"I will give a hypothetical example. I am not saying it is true, it is just a random hypothetical. Here are the assumptions for this made up example:

\- everybody will get depressed at some point in their life

\- there is a significant/observable cut off point between non suicidal depression and suicidal depression

\- anti-depressants have been shown to significantly reduce the chances of developing suicidal depression for a small subset of the population, around 5%

\- A significant proportion, for example, about half, of people with both non suicidal and suicidal depression end up with chronic nausea

\- The excessive crying in suicidal depression has been show to be able to cause chronic nausea (the level of crying in non suicidal depression is not sufficient to cause chronic nausea).

\- The lack of appetite in non suicidal depression has been shown to be able to cause chronic nausea. Antidepressants do not stop the lack of appetite in either non suicidal depression, Those with suicidal depression get chronic nausea due to excessive crying, and not due to lack of appetite.  

You are running a trial to see the effect of antidepressants on chronic nausea. You have a sample of 1000, you divide it into 500 being in the intervention group (these people receive the antidepressants) and 500 being in the control group (these people receive a placebo). 

Given that antidepressants only stop 5% of people from developing suicidal depression, given that suicidal depression uniquely can cause chronic nausea via its excessive crying, given that lack of appetite in both nonsuicidal and suicidal depression can also cause chronic nausea: shouldn't you exclude suicidal depression participant from the study?  

Think about it. Given that antidepressants can reduce chronic nausea in 5% of people, by stopping suicidal depression (and therefore stopping excessive crying, which can cause chronic nausea), wouldn't that mean if you don't control for the variable ""suicidal depression"" the antidepressant would mean less chronic nausea in the antidepressant group? 

Wouldn't that show that antidepressants have an overall effect on reducing the levels of chronic nausea? But given that they only do this for 5% of the population, if you don't control for suicidal depression, wouldn't it mean for 95% of people this reduction of chronic nausea from the antidepressant wouldn't apply? Because if they get chronic nausea, it would be because of the lack of appetite, which antidepressants have no effect on. So shouldn't you exclude those with suicidal depression from the study? Am I making a mistake here?",13og3en,Hatrct,1684728926.0,0,0.5,['This sounds like suicidal depression is a mediator. Do not control for it.']
What are the big research areas still around in statistical learning? [Q],"I was wondering what stats departments do active research in statistical learning. Furthermore, what topics are big or hot topics in statistical learning. Ive been reading quite a bit of elements of statistical learning and developed an interest in ensemble learning, as well as high dimensional regression and topics related to shrinkage for linear models. 

I was wondering if research, say in these areas still goes on? For example, what is the research going on with tree based methods / ensemble learning. 

Or is statistical learning an old research area with not much new happening?",13og069,Direct-Touch469,1684728651.0,7,0.78,"['I assume you mean something that is not deep learning. (Elements of Statistical Learning discusses neural networks, as does An Introduction to Statistical Learning, so I think neural networks count as statistical learning.)', ""You can't sell a VP you've hit the glass ceiling"", 'Here are some areas where I think some interesting research is being done-\n\n1. Leveraging computational power to explore methods that were computationally infeasible (eg lots of bayesian applications).\n\n2. Tried and true problems related to non parametric estimation.\n\n3. Missingness and small area estimates are still heavily studied.\n\n\nBut to be fair, the sexy research is leveraging computational power to do cool things in applied fields.', 'Yes, something which is not deep learning.', 'Please keep in mind that the field has progressed and some of those techniques/choices are outdated, and gelman will admit that himself!', 'What?', 'So like would 1) would be like Bayesian regression trees.', 'Really? Like the stuff in the elements of statistical learning book?', ""Gelman (who is awesome), isn't an author on either of those books...?"", 'I have seen lots of Bayesian applications. Bayesian structural equation modeling, Bayesian Meta-analysis, Bayesian estimation. \n\nPretty much, we are better able to create statistical models that better reflect our understanding and perception of the world. We intuitively understand that distributions of estimates are more reflective of reality that point estimates. The problem was that before calculating those integrals was no joke computationally.', 'Lmao whoops, got my quintessential texts mixed up', 'To be sure, Gelman would very likely say what you said he would day, though.']"
"[E] For graduate programs, does anyone have experience with substituting calculus or linear algebra coursework with CLEP test results?","Currently working in environmental consulting. Graduated with a MS in geology 12 years ago and currently use statistics in projects in my work. I am looking to enroll in a graduate statistics certificate program (currently looking at either Colorado State University or Penn State), which my job would pay for, however, I need to complete calculus prerequisites as I did not need to take these courses for my undergraduate program (also geology). I don't think my job would pay for prerequisites. 

To save time and money, I will ask any program I plan to register for whether they accept CLEP test requires to cover mathematics requirements. I was curious if others have had success using CLEP test results to fulfill math prerequisites.",13o6g1m,Geologist2010,1684703427.0,1,1.0,"['Will the clep for calculus fulfill all your requirements?\n\nIt depends on the school policy and program requirements, but my understanding is that the calculus clep will likely only give you credit for calculus I.\n\nIm the mod of r/clep and also a stats major.', ""That would at least save me at least $700, and time. Then I'll take Calculus II and III online at a community college. Some programs also require linear algebra credit, and some like Penn State only require the prerequisite knowledge and not necessarily formal credit."", 'Ah ok.  I never took the calculus clep, but it is one of the more difficult exams.  It has a DOD pass rate of about 34% - but there are some caveats.  DOD offers free exams to members of the military.  Many take the exams for promotion considerations, often without studying.  The DOD is the only place to find reliable pass rates, because the college board does not release this info.\n\nThe real pass rate is probably closer to ~45%.']"
"[Q] Trying to learn imprecise probability theory, most content seems too vague. Recommendations?","I've been trying to learn imprecise probability theory. However, most of the literature is focused on gambles, games, rational decisions, and I am just a poor statistician, trying to see how it relates to estimation theory. Are any such resources available?",13o4yzj,MinLikelihood,1684699915.0,1,0.67,[]
[Q] What is the best online course/series of lectures to go along with Casella & Berger/Hogg & Craig?,Would like it at a fairly high level. But cant seem to find good quality lectures. Thanks.,13o4pbn,Sorry-Owl4127,1684699253.0,33,1.0,"[""There is a YouTuber called StatisticsMatt, he's also a redditor! u/statisticsmatt. He has excellent material for inference. He's really the only guy who puts out high level statistics videos that hit the sweet spot for explanations and rigour.\n\nI got through Inference 1 and 2 by using C&B, the solution manual for it, and that YouTube channel.\n\nEdit: tagged Matt"", 'Had the same question, thanks. Following here.', 'Yes please, so many good math series on YouTube, where the stats?', ""These videos look promising: https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4\n\nI haven't watched them myself but I watched this guy's info theory videos years ago and found them useful.\n\nAn even less practical approach may involve starting with this course and then just reading Billingsley: https://www.youtube.com/playlist?list=PLUl4u3cNGP63micsJp_--fRAjZXPrQzW_\n\nThey are mostly useful for probability theory rather than statistics but they might still be useful."", 'If you are new to statistics, or need to freshen up on calculus techniques, don\'t both with casella and berger or hogg  and craig. Go with Mathematical statistics with applications, with the solutions manual, self learn. Professors from top schools will tell add their expertise, including overall concepts and philosophy, if those are available take it. I think harvard lectures are available, as an intro to probability, which are high-level.  But for statistical inference, go with youtube, lookup ""Statistical Inference"" and it should be the one for an indian university, channel: Nhtpel or something, graduate level inference course, if you really need it. But make sure to complement the lectures with actual problems, if you actually want to learn and not just ""get the jist"". At Texas A&M, I had a well-known bayesian statistician teaching me in undergrad he had us do ""the foundations"", deriving posteriors, conditional and marginal distributions, hierarchical modeling, the tests were all math, few coding questions, and the questions were like ""You are given this information (10 observations with this likelihood, and some prior distributions, derive some distributions and then explain how you would use software to sample from a particular distribution to get samples from other distributions, , and get estimates for the conditional posterior of parameters given your data, yadayada."" Anyways he mentioned that in the graduate level version he didn\'t have very much time to focus on the foundations, because some students need to know about sampling algorithms, some students need more applied work, and that he was glad he could focus on the foundations as its really the meat of Bayesian inference. Ah, I hope I\'m representing his thoughts right, anyways.. that is all.', 'Probably not the best one, but a while back I wrote some online notes to go with Casella and Berger while teaching. You can find them at [significantstatistics.com](https://significantstatistics.com). Happy learning :)', 'Is there a playlist for C&B? thanks', '>StatisticsMatt\n\nWow, excellent channel, thanks!', 'u/helloheyhowareyou, many thanks for your kind words about my YouTube channel. I love hearing that the videos are helpful. Much appreciated! Just an FYI, my next two playlists will be (1) complex variables (enough to provide a background for time series). and , (2) Introduction to Time Series. I hope to finish both playlists by the end of summer. Again, thanks for highlighting my channel!', 'Just replying to say me too (and since I think the Reddit algorithm shows content with more engagement to more people).', '>https://www.youtube.com/playlist?list=PLUl4u3cNGP63micsJp\\_--fRAjZXPrQzW  \n>  \n>\\_\n\nhey, think the second link is broken.', ""I don't know if he has  playlist that follows C&B exactly, but he does have several playlists that cover much of the material from two courses in mathematical statistics.\n\nCheck out parameters estimation, MLE, order statistics, exponential distributions, hypothesis testing, and any other topic you may be interested in."", 'This, lol', 'Following', 'How about this?\n\nhttps://youtube.com/playlist?list=PLUl4u3cNGP63micsJp_--fRAjZXPrQzW_', 'Nada, your first link works though (probability primer).', 'Weird, it works for me, even incognito. Anyway it is the intro to functional analysis course by MIT.']"
[Q] What kind of rasch Model should I use for a scale with likert scaled items? Dichotomous or Polytomous.,"Ive never heard of the method or IRT for that matter and gotta use it now so Im pretty uninformed :/
Also Id be happy if you could give me any sources to research using rasch models for item analysis thanks and excuse my english.",13o49gp,Labertran,1684698153.0,2,1.0,"['Why use rasch and not something like grm?', 'For polytomous data, you can use a Graded Response Model (Samejima, 1969).\n\nFor dichotomous, you can use either a Rasch, 2 PL, or 3 PL. depending on your research question and how much data you have available. \n\nI suggest the book The Theory and Practice of Item Response Theory by R.J. de Ayala. It has some good chapters on these.', 'My professor mentioned the rasch model so I kinda focused on that but Ill look into grm ^^\n(Sorry for being that uninformed)']"
[Q] What normality test could be used for a sample size n=2? Any advice will be greatly appreciated 🙏,I need to analyze the behavior of concretes using ACI 214. But havent been able to find a normality for sample size n=2.,13nzqjt,Maab26,1684687379.0,0,0.27,"['Its not possible to test general normality at n=2. \n\nI bet the response is not normal, though it might not matter', ""I don't think there is any way to statistically test normality when n = 2. You don't have enough information and the power to detect departures from normality would be basically zero. But if you look at your distribution, I think you can see that it's definitely not normal. The mean of your distribution equals the midpoint between the two data points. Unlike what you would find in a normal distribution, this mean value is not a peak but rather a valley."", 'Youre high bro', ""What are you measuring? If you have a device that measures something like deformation over time under increasing load, you can compare those measurements if you're able to gather a sufficient sample of measurements... n = 30 is the traditional recommendation. However if you only have static measures of two samples, your hands are probably tied and you can't really say more than one is less or greater than the other"", 'Could you provided more insight on why there is only a sample size of 2? Because otherwise there would be no good way to test normality other than to increase the sample size, which is an obvious answer', 'I\'m really confused by this comment. There\'s nowhere near enough information to make any kind of claims about what distribution that sample came from because it\'s just two numbers. It is not appropriate to even speculate about whether or not these two numbers came from a normal distribution.\n\nEditing because you made a comment and deleted it.... I just want to be really clear about this. The fact that the mean is not represented in the distribution doesn\'t tell you whether or not this is a normal distribution, because it\'s not a distribution. It\'s completely possible to have a normal distribution where the actual mean is not present in the sample. It\'s not even unlikely.\n\n This is not about ""sample distribution versus sampling distribution."" The issue is that two numbers don\'t give us enough information to make any kind of informed conclusion or speculation about the sample distribution  or sampling distribution. You just can\'t do much with two numbers. You can think of it as the null hypothesis being that the sample distribution is normal and the alternative hypothesis being that it isn\'t. You don\'t have enough information to reject the null hypothesis and your power would be very low.\n\nSometimes we just don\'t have enough information to do anything and that\'s okay. If it\'s possible to get more data than the OP should get more data, if not, they should report the data that they have.', ""That's what I said. Not enough information."", 'That\'s not what you said. Your comment says that the numbers don\'t come from a normal distribution because the mean isn\'t in the sample and it\'s a "" valley"" but it should be a "" peak."" That statement is false.', ""Look dude, it's Sunday afternoon and I'm not going to spend it arguing over a sample distribution with an n of 2."", 'I think both of you are making great points.']"
[career] Stats Consulting - How to get fast enough,"I started as a statistician at a small consulting firm about 6 months ago using SAS. I will be asked to run stats for a project that is typically a mixed design with a handful of outcome measures taken pre and post some manipulation with an experimental and control group. I'll be asked to generate all the results that would be needed for a paper - breakdowns of demographics, testing group differences at baseline, and testing the effect of the manipulation. Usually my company estimates these projects at around 30 or 40 hours. I'm finding it impossible to stay within the estimated time.  
  
I'm honestly not sure what is taking me longer than the other statisticians. I was new to SAS when starting, so my coding has been getting faster, and I have developed some macros for outputting tables that have sped things up a bit. The data typically needs some cleaning and it does take some time to for me to feel comfortable with understanding the data, and that it is being treated appropriately. It feels like there is always one more thing to check, normality of data, which particpants are missing data and why, does this measure have a range that is plausible, am I sure that merge was handled correctly, etc.  
  
Does anyone have tips for being more efficient?",13nx945,wsen,1684681300.0,25,0.94,"['You are new at the job. Previous  statisticians would possibly have worked on similar problems before either in your company or elsewhere. Hence it becomes a pattern recognition problem rather than thinking and resolving problem. With time and experience you will get quicker.', 'Build up a searchable code base of your previous projects, or at least save your work in such a way that it is easy for you to reference. That way each project you do you can copy paste and modify some previous code instead of starting fresh.\n\nEdit: autocorrect had me ""modify some previous coffee""', 'My guess is that the line items you describe are not of equal importance.\n\nProbably your seniors understand the relative strategic priority of each and can either instinctively or through client engagement know what actually ""matters"" and they\'re letting the rest of it fall through the cracks.\n\nMy advice? I\'m thinking the structure of the data won\'t change much, so you can build functional reporting/analysis structures on top of dirty data. Build the stuff that will put out client-visible and client-valued stuff first. Run the dirty data through it to see if it ""works."" THEN start trimming and cleaning up your data as much as you can, feeding it through the model/exploratory code as you go.', 'Believe me. When I was new to the job it took me more than a week to do all that. This is normal don\'t panic. You will develop your own framework that will speed up your analysis, you already have macros which is great. I use R software and I use ""gtsummary"" and ""tableone"" packages to help automate the tables. This cuts lots of time. But most importantly make sure that data is completely clean. In clinical research most projects have similar analysis plans, reuse the codes everytime and change what is nesseary.', 'Find out the code that others have used, and reuse it.\n\nCommonize it.  Possibly write macros.']"
[Q] Sum of values of a sample from normally distributed data,"Hello all,

I have a sample from normally distributed data. I would like to know the probability of the sum of the values of that sample being equal or greater than the mean of the normally distributed data.

How can I calculate that probability ? 

Thank you",13nvhjz,MxwellsDjinn,1684677204.0,0,0.5,"['[The sum of Normals is Normal](https://en.m.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables) with pretty straightforward mean and variance (the variance depends on whether the variables are independent or not).\n\nBut what do you mean by you have a sample from normally distributed data? Nothing in the real world is Normal. Some things are well-approximated by one. Some arent. Perhaps more importantly, the rules for the means and variances of sums do not require any assumption of normality.', '\nBy ""the mean"" do you mean the population mean?\n\nAre the values dependent? Or independent? If dependent, in what way?\n\nHow many values are you adding?', ""> The sum of Normals is Normal \n\nIf they're *jointly* normal, sure. OP's post seems to be unclear on that point. In fact the conclusion of normality seems to be based on looking at a histogram of a realization of (X1, X2, ..., Xn) taken together, which doesn't necessarily tell us that it's jointly normal."", 'I am not summing variables, I am summing the values of different instances of the same variable. Lets say X is a normally distributed variable. What is the probability of the sum of say, 35 randomly selected instances, being equal to or greater than the mean ? I am doing a risk analysis, this is why I would like to know', 'So you have 35 independent, identically distributed normal random variables. Their sum is also normal with the mean and variance described in the link.', 'What is an ""instance""?', 'Why is this information not in the posted question? Edit it', 'An individual, for example X=3500 is an instance of X. Maybe I am using the wrong terminology in this context', ""You are almost certainly having trouble with terminology. That's common, statistics has a lot of terms with meanings which can seem a bit arcane to the uninitiated.\n\nAs yodenaneda is saying, I'm pretty sure that the correct terminology here is that you have a bunch of independent and identically distributed random variables which you are summing. Keep in mind that whether you've _seen_ the value does not change whether or not it was a random variable. Consider rolling dice. We can ask what the probability that the sum of four six-sided dice is greater than 14 and figure out that it's 50%. Then we can roll four dice, sum them up, and see that we got 10. That's still a _realization_ of a random variable which is the sum of several (independent and identically distributed) random variables.\n\nAnd keep in mind, if there's no randomness, it doesn't make sense to ask about probabilities."", 'Thanks a lot for your help !']"
[[Q] Can I use studies with different confidence interval in a meta analysis?,"I have been collecting data for a meta analysis project. I found 4 studies with same PICO. But mean difference from 2 studies is calculated with 80% CI, 1 with 90% and last one with 95%. How do I move forward?",13nrkf0,mudassar05o,1684671562.0,1,1.0,"['Do you know the method the confidence intervals are calculated with?\n\nIf you know the method you can maybe reconstruct standard errors if the statistics are asymptotically normal or you know the assumed model.', 'Hopefully you had a protocol describing your statistical methods in detail before conducting a meta-analysis? What is the outcome? What meta-analysis method are you using?', 'Meta-analysis of what? What is your model?', 'I dont have access to raw data. I have mean differences and intervals. So, I dont know if I would be able to that.', 'outcome is continuous. intervention vs placebo. DerSimonian-Laird method.', 'well, it is to study effects of a drug compared to placebo for an outcome. According to pubmed, we found 4 studies with relevant PICO. Random Effects. We were able to extract mean differences from these studies. But these studies are conducted at different CI and thats where we are so far.', ""You'd have to extract the methods used from the text. But it could just as well be, that they're not well enough documented, that that's possible. Depends on the publications."", 'What software do you use? For most softwares you can set the desired CI for study-level and combined estimates.', 'I have used revman and open meta. Revman allows 90, 95, 95 CI. Not below 90.\nOpenmeta allows CI adjustment but requires mean change with SD along with mean difference and SE to shift between different CI levels unlike Revman (which requires only mean difference and SE or upper interval range and then adjusts SE as you shift between 90, 95, 99 CI options).', 'Can you supply a screenshot of your forest plot from RevMan?']"
[Q] Can someone explain to me how a one-way ANCOVA makes sense here?,"First, some information about the [study](https://doi.org/10.1080/15213269.2016.1257392):

> An online experiment was conducted to investigate the effect of manipulated Instagram photos on girls' body image. **The experiment has a 2 (Instagram photos: original vs. manipulated)  2 (social comparison tendency: lower vs. higher) between-subjects design.**

Because covariates were collected alongside this, I expected a two-factor ANCOVA.

> To test the hypotheses, **a one-way analysis of covariance** was performed with
body image as dependent variable, Instagram photo manipulation and tendency to make social comparisons as between-subjects factors, and level of
education as covariate. Participant age was excluded as additional covariate
because preliminary analyses showed no effects of age on the dependent
variable body image. Hypotheses were tested at the alpha = .05 level (onetailed).

Did they calculate *two* one-way analyses of covariance and just not write it down explicitly, or is there another possible trick that goes beyond my admittedly limited mathematical understanding?",13nquth,MightyKartoffel,1684669470.0,6,1.0,"['It definitely *should* be a two-way analysis, and unless there were other covariates, they didnt even covary out participants ages, so it looks like just a two-way ANOVA ended up being their final statistical treatment. Best case scenario, theyre bad writers. Worst case scenario, theyre bad writers and their stats are bass-ackwards.\n\nYoud know which is which from the results; if they discuss multiple main effects or an interaction, it wasnt a one-way analysis. If they discuss any actually implemented covariates, its an ANCOVA. If they somehow did an ANCOVA with no covariates, thats just an ANOVA.', ""Reading the results, it appears that they did a two-way. Maybe it's a typo?"", 'Thank you for your answer!\n\n\nThey included ""education level"" as a covariate', 'thank you for the answer - Probably. It has to be.\n\nSomeone over in /r/AcademicPsychology suggested that the authors may also have confused ""one-way"" with ""univariate""', 'How do the paper get published lol. Probably one of those publish anything journals.']"
[Question] Calculating the effect size from repeated measure ANOVA values,"\[Q\] Currently I am gathering data for a meta-analysis, but during my data collection I encountered really serious troubles with the data reported. Sadly asking authors directly was to no avail, as they are not answering my emails (most of them). I have the following issue: In a paper the results from reported as a repeated measure ANOVA, but no means or SD's are reported. Is it possible that based on the Degrees of freedom, F-value and p-value to calculate the effect size if I know the sizes of both study groups:  
E.G: (F6.4,105.8=5.1; p<0.001) n-active=12 n-placebo=12  
Can I calculate based on those values the effect size or should I rather try to contact the authors for the n\^k time.  
I would really appreciate a well argumented answer.",13nofbo,Safe-Safe-1498,1684661567.0,1,1.0,[]
What is the chance? [Q],"Honestly, I was gonna do this myself, but I forgot everything in AP Stats right after I took the class.

Question:

If I buy 30 rings, what is my chance of getting 2 Tri's?

The chance of getting Pri (First one) is 75%

The chance of getting Duo (Second one) is 45%

The chance of getting Tri (Third one) is 30%

The condition is that you have to move up the ladder, you have to meet the first requirement (getting Pri) then the second and then finally the 3rd. 

&#x200B;

Thank you!",13nje76,KanojoOkarishimasu,1684645161.0,0,0.29,"[""Please edit your question so it is comprehensible to someone who doesn't already know what you're talking about."", ""I can't figure out what you mean."", 'What is the probability of getting 2 of the rings to the level ""Tri""', 'What do you mean by ""the chance of getting Tri is 30%""?', ""Okay so this is how it works, you have to achieve getting Pri, then Duo then Tri, you have to get Pri to get Duo, You have to get Duo to get Tri. This is the scenario, I have 30 rings, I want to hit at LEAST 2 Tri, what is the probability? (So sorry, should've said at LEAST two, forgot about how that affects it with cdf and pdf)"", 'Oh, then you can get that from binomial cdf, x>=2, p= 0.75x0.45x0.3, n=30.', 'Thank you :)', ""0.82193, cant believe I just found such a good way to make money on my game lmao, I love mathematics (besides Diff Eq) + if this works out, my n will grow larger and larger, making it better od's for me"", 'Im actually not sure that the above response is entirely correct. If Im understanding correctly, theres three types of rings, and you have to get the first two before you can get the third, correct? So if you only buy two rings, theres a zero percent chance of you getting a Tri, correct?', 'You are correct but, looking at it, hes correct its a simple Binomialcdf', 'I dont believe so. Theres a few problems with their statement.\n\nFor one, n would not equal 30. Thats saying you have 30 tries to get a key with a certain probability, which we know is incorrect.\n\nSecond, getting a Tri is conditional based off of having a Pri and Duo. Because of this, the probability varies. Using a straight binomcdf would only be useful where the probability does not change. For example, using binomcdf, thats saying its possible to get a Tri on the first and second trial. We know thats not the case.\n\nIt would be best to run many simulations to find the true probability - once you get a Tri, do you have to get another Pri and Duo before a Tri, or can you keep getting Tris?', 'Another Pri and Duo']"
[Q]is this degree course good for my goal,"I work at a market research company as a product manager for a data analysis product.

I would like to establish a new role as head of data products and feel like being the in-house expert or at least having some formal training background would be useful.

I was looking for courses I could do online, is this a good course? 

https://www.nottingham.ac.uk/pgstudy/course/taught/statistical-science-distance-learning-msc#courseContent

(I meet the edu background needed no problem there)",13nhfei,kingkongjaffa,1684639159.0,2,1.0,[]
[Software] We've Built an AI-Powered SQL Query Builder - Looking for Feedback and Suggestions!,"Hello, fellow Redditors!  
  
As a software engineer, I've had my fair share of encounters with SQL queries. And let's be honest, they can be a bit daunting for beginners or cumbersome for the pros when they get too complex. That's why my team and I have been working on something we think could be a game-changer.  
  
We're excited to share with you [Loofi](https://loofi.dev/), an AI-powered SQL Query Builder we've built from scratch. This tool not only simplifies query building, but also provides real-time insights and recommendations, thanks to our AI algorithms.  
  
We're eager to get your thoughts on it and would appreciate it if you could try it out. Any feedback or suggestions are highly valuable as we continue refining our tool.  
  
Also, if you have any questions or need help, feel free to ask. We're here to support and learn from this wonderful community.  
  
Thanks in advance!",13ne6gp,Dale_Doback_Jr,1684629949.0,0,0.25,['So I hate to be that guy but how does this perform against GPT-4?\n\n(Im sure youve put a lot of work into this and I dont want to diminish that. Im just asking the obvious question most readers of this post are going to have.)']
[Q] Job opportunities,"Hello everyone I'm currently a second year undergrad major in statistics. What job opportunities do I have once I graduate? All I know rn is either Data Analyst or Data science, anything else where I can use what I'm studying and pays well?",13ncwdd,InterestingRemote745,1684626433.0,3,0.72,[]
[R] How do I estimate the parameters for this model,"I'm quite lost understanding how to produce the a.b and c parameters in these models. A typical regression model is something like y= intercept + bx (b is the coefficient using x as independent and y as dependent), now [these models](https://imgur.com/a/ASgQ46H) also should have just 1 independent and 1 dependent variable, yet the models should produce 3 parameters (a.b and c). Is anyone familiar with this, please? How do I achieve something like this in R.

Here's the paper link: [https://academic.oup.com/njaf/article/18/3/87/4788527](https://academic.oup.com/njaf/article/18/3/87/4788527). You can click on pdf at the bottom of the page to view the entire thing. I would really appreciate any help!",13ncile,brianomars1123,1684625400.0,0,0.5,"['These are non-linear regression models. If the conditional variance is assumed constant (which makes no sense for this application but it never seems to stop anyone) then see\n\n   ?nls\n\nfor fitting such models in R\n\nHowever, I strongly advise against trying to learn the ins and outs of nonlinear regression modelling just from that.', 'Thanks for your response. Im really not trying to understand non linear regression based off this. I just want to know how to replicate this on my end and get those parameters\nReading through, I believe what they did was get height and diameter measurements of trees and fit these models, the fitting provided the a, b and c parameters which they can then use to predict heights using the same model, basically just a typical linear regression process.\nMy confusion is that for typical linear regression, if I run this with lm in r, Id get an intercept and beta, I can the use that to predict height for any diameter in the data range. For this case, it still uses just height and diameter but someone gets 3 parameters which will be applied in the models.\n\nIm looking through the documentation for nls now.', ""If you have a small set of data I can try to show you an example.\n\nEDIT:\n\nlooking at the paper, they talk about *growth models* or *growth functions*; if the data are following individual trees over time, a plain nonlinear regression is unsuitable because of the time dependence within-tree; it's more complicated. If the data are all from separate trees across a variety of ages (no tree measured at two different times), dependence issues probably won't be so much of a problem (there can still be spatial dependence, but it would be less strong and the extent will depend on the situation).\n\nEither way, it looks like they did nonlinear least squares regardless. \n\nA warning in relation to the paper -- R^(2) is not a particularly meaningful quantity for nonlinear models."", ""Oh wow, this is very insightful. \n\nBased on their description, the data collection wasn't over time but over the same period. You're correct, the major variation in this type of data collection is a spatial one, basically variations at the forest stand level. \n\nI just created this with chatgpt  - [https://github.com/brian-o-mars/height-diameter-sim](https://github.com/brian-o-mars/height-diameter-sim). Not a very clear non-linear relationship but it may suffice. I really appreciate your help."", ""I could probably be more insightful if I knew anything about your area, but I don't.\n\nOn the data -- it's okay, I ended up making myself a small data set that seems to be slightly more realistic; I attempted to make it roughly consistent with the JP summary in the paper.\n\nMy problem right now is finding a good way to automate starting values for my example; this can be a bit of a bugbear if you don't have expertise in the area (where you might well *know* good starting values for one or more parameters already, greatly simplifying things)"", ""At the bottom is some code to illustrate using `nls` on a toy dataset which should\njust copypaste straight into R. The relevant output first:\n\n    > summary(cr.fit)\n    \n    Formula: y ~ a * (1 - exp(-b * x))^c\n    \n    Parameters:\n      Estimate Std. Error t value Pr(>|t|)    \n    a 21.90171    2.64130   8.292 5.82e-10 ***\n    b  0.10430    0.04084   2.554   0.0149 *  \n    c  1.99389    0.82235   2.425   0.0203 *  \n    ---\n    Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n    \n    Residual standard error: 2.722 on 37 degrees of freedom\n    \n    Number of iterations to convergence: 5 \n    Achieved convergence tolerance: 6.792e-06\n    \n...\n\nhttps://i.stack.imgur.com/Wa6QW.png\n\n#### CODE FOLLOWS\n\nI have avoided writing a function defining the expected values in terms of the parameters and the input  (well, I did write one but I omitted it here to show you all the steps I did explicitly); you should write one and use it in your code, because that will make your code more re-usable.\n\n    ## example of fitting Chapman-Richards to some JP-like data\n    \n    # simulated data:  (population a=23,b=0.097,c=2)\n    htjp = c(14.94, 8.61, 12.35, 8.26, 5.47, 2.27, 10.13, 15.93, 11.55, \n    15.47, 14.73, 20.32, 16.35, 9.96, 2.77, 22.32, 20.39, 24.37, \n    21.58, 11.85, 20.36, 13.22, 22.75, 11.48, 10.14, 12.24, 9.43, \n    8.47, 13.44, 9.21, 20.03, 8.89, 16.62, 16.53, 19.49, 20.56, 2.87, \n    16.62, 21.29, 18.44)\n    \n    dbhjp = c(11.6, 8.2, 15.5, 7.8, 5.9, 3.5, 12.6, 26.7, 13.4, 17, 11.6, \n    25.2, 23.2, 11, 3.5, 20.6, 19.8, 29.3, 21.6, 15.6, 27.3, 15.7, \n    36.3, 10.6, 11.5, 14.4, 7.5, 8.7, 13.7, 10, 14.4, 12.1, 21.9, \n    18.7, 12.1, 23.8, 3.7, 16.8, 27.1, 12.6)\n    \n    #--\n    \n    y=htjp-1.3  #shift back\n    x=dbhjp\n    n=length(x)\n    \n    # Example of one way to approach start values  for 3 parameter Chapman-Richards \n    # This is not guaranteed to always work well. Better approaches are definitely possible. \n    # e.g. the a-start value is very outlier-sensitive here. This is just a rough illustration.\n    # Finding good start values is often where you need effort and subject-area knowledge\n\n    a0=max(y)\n    y0st=log(y/a0)\n    xst=log(x)\n    nleft=min(n,max(n/5,10))   # pick smallest 20% of data if that's at least 10 points\n    xsmall = ( x < quantile(x,p=nleft/n) ) \n    coef0=coefficients(lm(y0st[xsmall]~xst[xsmall]))\n#    b0=exp(coef[1]/coef[2])   #\n    b0=exp(coef0[1]/coef0[2])\n    \n    y1=log(y)\n    x1=log(1-exp(-b0*x))\n    c0=coefficients(lm(y1~x1))[2]\n    \n    # 2. actually fit the model\n    # (this model can be partially linearized but it dramatically affects the estimates\n    # so I have avoided doing so)\n    \n    cr.fit = nls(y~a*(1-exp(-b*x))^c,start=list(a=a0,b=b0,c=c0))\n    \n    summary(cr.fit)\n    \n    coef=coefficients(cr.fit)\n    \n    af=coef[1];bf=coef[2];cf=coef[3]\n    \n    xs=seq(min(x),max(x),l=101)\n    ys=1.3+af*(1-exp(-bf*xs))^cf\n    \n    plot(dbhjp,htjp)\n    lines(xs,ys,col=4)\n\n\n####"", 'This is super helpful sir! I really appreciate this.\n\nI would def still need to learn the theory behind all these but just focusing on the practical, the big thing to implement this correctly is to get the right start values that would be used in the nls function right?', ""There's  no 'right'. You need to start with a roughly reasonable fit in order to be pretty sure it will converge.\n\nSo it's more 'what works' for a given situation.\n\nIf you only need to do it once, starting near good fits should usually work (for reproducing existing fits for the same data, you know where you're trying to end up - so anything not really far away should work). \n\n For some models the parameters represent a slope or a boundary value (a starting value or an asymptote) or other simple aspect of the plot. For those, you can often approximate them straight from a plot.\n\nIf you want to write code that nearly always works across data sets you may need a bit more back-of-the-envelope mathematical analysis to get good starting approximations. I sort of did a rough version of that. E.g. for thr CR model,  'a' is an asymptote of the mean function, so some sort of late average will be a rough start value. I started with the maximum because I used it to transform the data to get other start values. I then tried to back out a better approx for 'a' but that didn't work as well on these data, so I cut that update step out as a distraction from a simple example.\n\nIt's a bit of an art. \n\nSometimes (/often) someone else  will have done the work of producing good general  start values for you. In nls in R these will usually be called *self start* functions.\n\nDomain knowledge will often tell you where the values should roughly be for a given species \n\nSometimes a rescaling of variables is helpful  for getting convergence, as well.\n\nIt's useful to try multiple start values to make sure you end up in about the same place."", ""I in fact emailed a former stat professor and he did say exactly what you are saying. I however don't have much domain knowledge here, I'm fairly new to forestry research.\n\nI'm reading only and people are saying something like taking the log of both columns and running an lm on the log data. The coefficients can be used as the starting values. However, this would only provide a and b, missing c lol.\n\nIf you have some time, could you please explain how I can estimate this from a plot? I have actual data now for this [here](https://github.com/brian-o-mars/height-diameter-sim). HT and DIA are the height and diameter columns.\n\nI used the code you wrote to run this and it did work but a lot of the predictions (I used half of the data to build the model and tried predicting with other data points) were off and I believe that's because of the starting values. Also, I got errors about the starting values when I tried to implement other models in the paper.\n\nI understand I need to learn the theory behind this statistical tool before using it but I'm just trying to come up with a methodology for my research first and this seems like the closest I've been to figuring it out. If I can confirm that this indeed works, I can submit my proposal and spend the summer understanding the theory. Once again, I really appreciate your help so far."", 'Sorry about slow replies; I\'ve been ill. I see you\'re also posting on stats.stackexchange.com.\n\nLog of both (which I made some use of in my start values) would give you values for a and c (*if you already had a value for b*), rather than a and b:\n\nif y = ht-1.3 and x is diameter\n\n(edited the next line to fix up the \\^c part)  \n\nlog(y) = log(a [1-exp(-bx)]^(c)) = log(a) + c log [1-exp(-bx)]\n\nIf you already know b, then you have an equation of the form y\' = a\' + c x\'\n\nHowever, I already tried this; it\'s already in my code above --- it\'s how I got my start value for c. I also made an indirect reference to it in my comments above as well, when I said this:\n\n> I then tried to back out a better approx for \'a\' but that didn\'t work as well on these data, so I cut that update step out as a distraction from a simple example.\n\nThere I was explicitly referring to taking logs of both sides to get better a and c values given an initial estimate of b. However, it did not work particularly well on a for these data and I didn\'t have a large amount of real data to play around with it on to see if that was typical. \n\nI was not surprised to see it work poorly, though, because transformation entangles the error term with the estimates and when your data are noisy like these data are, that can really change the estimates.\n\nThe effect of the error term is what people who aren\'t statisticians *constantly* forget when they try to linearize these equations.\n\nDone correctly, it actually works like this:\n\ny = a [1-exp(-bx)]^c + e    . . . this is the nls equation we begin with   \nlog(y) = log(a [1-exp(-bx)]^c + e)  \n log(a) + c log [1-exp(-bx)] + e^*\n\nSo the ""log(a)"" and ""c"" you get out are muddled up by entangling the original e in with the x\' values.\n\n> Also, I got errors about the starting values when I tried to implement other models in the paper.\n\nYou need different starting value methods for every model. What I supplied was for the CR *ONLY* and it was just a rough example in order to show you how `nls` works. It\'s not even a general method for CR.\n\n>  I\'m just trying to come up with a methodology for my research first\n\nYou\'re asking me to teach you how to magically ""just do"" nonlinear least squares so you can avoid learning nonlinear least squares. There\'s no shortcut here. You can\'t ""just do"" it without learning what you need to learn to be able to do it.\n\nI sure can\'t spend the time to do it all for you so you can postpone learning it for a few months; that would be a ton of work for me (likely a couple of weeks, certainly many days); I can\'t spare that much time.\n\nAn hour here or there is fine when I can manage it, but days? Maybe more? I simply can\'t spare  that sort of time, even if you were paying me (no, I am definitely not asking you to pay me).\n\nI simply won\'t have time to figure out how to get start values for all six of your models nor to give you the experience you\'d need to do that yourself; just very roughly doing it for CR took me a couple of hours and I\'ve fitted plenty of nls models before (though not in this research area, mostly in ones where the noise variance was small, where linearizing transformations work better). Doing it properly for CR would take me much more time, and it would be the same for several of the other models. Some of them have simple shortcuts though; e.g. the Weibull should be easy. Some of those models may already have self start functions available for them.\n\nThere are some more automatic approaches (like brute force grid searches) to get start values, but they\'re computationally more expensive (and dodge using the domain knowledge you should be bringing to this).   The R package `nls2` should work for that, but you\'re still generally better with some analysis if you can do it.\n\nI\'ll try to come back and illustrate how plots can be used for some parameters for some models but I have a bunch of stuff to do in the next few days.', ""Thanks so much for this breakdown and I hope you feel better soon. I do apologize if I came off as just wanting you to do the work for me, really not my intention. I've indeed been posting on stackexchange, and have been spending the entire day trying to figure this out.\n\nI totally agree with you on your points. Forestry/Wildlife research data is known to be packed with a lot of noise so yeah, based on what you have been saying and also what I've been reading online, having domain knowledge would help get good start values for this. \n\nI really appreciate your time with this and once again, I apologize if I didn't have the right attitude with my questions.was to create a methodology like I've been saying and show my advisor to see his thoughts, but even if I can create one magically, I wouldn't be able to answer any question he may have.\n\nI really appreciate your time with this and once again, I apologise if I didn't have the right attitude with my questions.""]"
[Q] Business analyst vs Data analyst,"I really dont know what business analyst means on job postings. Just graduated with my bachelors in statistics and I recently saw a job for a business analyst position (contracted) even though I am mainly looking for a data analyst position. The job description doesnt list their preferred data analyst tool (SQL or SAS, etc), and the responsibilities section looks like its data analysis with more responsibilities. 

Im very curious from those in the industry, what is the difference between a business analyst and data analyst? Is this a good stepping stone for a data analytics role later on that might fit more what I did at university? 

This question is a little outside the scope of this Reddit but idk where else to ask like-minded people.",13nby3z,lewdjojo,1684623870.0,19,1.0,"['Youll come to find out that job titles mean nothing. At the end of the day you just need to look at what the job reqs are. If you have 4 years of sql experience, should you apply to a job that requires 1? Probably not, youll be fiercely underpaid', 'This might be bad advice as I\'ve never worked as a business analyst.  But I have worked with a bunch of business analysts and what they seem to do is act as the interface between software teams or other teams.  And they are usually expected to be ""experts"" on each of those subjects.  So for example in manufacturing, usually there will be a customer facing software and a manufacturing software system.  There needs to be a lot of communication between the two systems.  Or if your customer wants something special then the analyst will help to initiate the process of making sure stuff talks correctly.  Could it require tools you use as a data analyst?  Possibly.  Imagine production needs a list of items processed at certain stages so they can prepare the machines for them.  That could be a business analyst job to ensure all the requirements are met and that production has the list when they need it.\n\nI had a lot of these situations where we would allow some of our higher volume customers to do crazy special items.  Even though our software doesn\'t allow them to be input into the system, our BA would have to do a lot of testing and stuff to figure out how we enter it in our customer software so that our production team can ensure the materials are pulled with the correct sizes and amounts.\n\nAlso speaking of requirements, imagine a team wants to start using new software.  Well how will they software interact with other software already used?  What will be the limitations of this new software that might be unforeseen by the team wanting to use it.', 'I was Business analyst in Amazon for 2 years and my job was mostly creating sql queries, tableau dashboards, web scrapping and creating automated solutions using Python and VBA, for my 250 people large non technical team. There were 2 more BAs with me. My friend working in Wallmart labs as senior data analyst also does pretty much same except web scrapping part. \n\nOf course you have to coordinate with different teams like Data engineers, Business managers, team members who asked for a particular solution or feature. I was also responsible for creating, automating and maintaining the  business dashboards on which weekly meetings happened and there were a lot of ad-hoc requests from senior teammates every now and then.\n\nSo even in large organisations, BA may mean completely different. You should have checked with hiring manager for more clarifications regarding this.', ""As others have said, job title doesn't mean much. Just make sure you're getting paid adequately for the responsibilities that you do.\n\nMy first job out of grad school was a BA and it was originally just a dashboarding role. Over the course of the next year I got into SQL, than R to provide statistically derived insights for the department. By the end of year 2 I was doing a lot of data science, and never got a title change despite being referred to as one within our team. I'm now in a MLE role, but a lot of times I do data engineering to wrangle my data since it's a start up. Just goes to show you how much job titles can vary on responsibilities."", 'The business analysts on the team I work on do a lot of google analytics reporting and tableau dashboard stuff mostly plus some light forecasting type things I think using built in GA tools.', 'You should be ideally applying to both.', 'I am a business analyst \n\nI work as a data analyst, BI developer and have written some micro utility apps.\n\nTitles are BS.', ""I have met business analysts who act as really good data analysts and coders, and I've met data analysts whose role may involve serious quantitative understanding but little implementation of deep analytical methods and little digging upstream into source data.  All of those roles and duties are great but you gotta drill down to what's really likely to be asked of you by asking lots of questions about the team's niche and the role's niche within that."", 'then, how Product manager is different from an BA?', '[deleted]', 'Or when data analysts are actually data scientists (or data engineers) but get labeled data analyst to be underpaid. Looking at you, United Airlines', ""I've always understood product managers to be experts in a single thing, i.e. their product.  And they kind of control the direction of that product.  While BAs are typically very fluent in cross functional scenarios.  At least again this is my experience in dealing with the product managers I've dealt with."", 'Yes I should have put expert as ""expert"" as in my experience they pretty much are the translator between teams, so maybe fluent would be a better term', 'There\'s also the tactic where they call someone ""data scientist"" and it means ""an analytics coder they want to push harder by giving them a higher title and weaponizing their own impostor syndrome against them"".  (I haven\'t been in that position but I\'ve seen it.)', 'Oh noooooooo thats me']"
[Q] Variation of multinomial distribution,"As an illustrative example, imagine a trial where you roll a six-sided die. A multinomial distribution would treat each possible value separately, and describe the likelihood after *n* trials of rolling a 1 *a* times, rolling a 2 *b* times, etc. However, if all possible values are numbers, then you could instead reasonably ask ""what is the probability that, after *n* trials, the sum of the results is at least/exactly/at most *k*?"" and this corresponds to rolling *n* dice totaling *k*. Is there a probability distribution that answers this question, for dice of arbitrary fairness, side count, side values, etc?

edit: the side values would only need to be integers, if that helps simplify things.",13n46lb,ennyLffeJ,1684611437.0,2,1.0,"[""You're just asking about the general case of sums of discrete random variables with finite support. The arbitrary face values part means that the number of distinct values in the support of the sum grows very rapidly (as the product of the number of values in the support of each of the components in the sum). For example, consider if the first die has integers, the second multiples of 2, the third multiples of 3, the fourth multiples of 5, ... \n\nThe answer is no, there's no *general* formula that simplifies anything over computing the whole thing out term by term.\n\nNaturally in many special cases there are simplifications (e.g. if the face values are all on lattices with the same spacing things simplify a little because the support of the sum is on a lattice with that spacing -- the number of values in the support grows roughly as the sum rather than the product of that of the components)."", '[Sums of random variables theory.](https://www.statlect.com/fundamentals-of-probability/sums-of-independent-random-variables) There might be better resources, just an example walkthrough I found.  \n  \nSimulation python code showing an approximately normal(ish) distribution for adding 20 rolls of a 6-sided dice:   \n      \n    import matplotlib.pyplot as plt  \n    import numpy as np  \n    sums = []  \n    rolls = 20  \n    dice_sides = 6  \n    for rep in range(10000):  \n        s = sum([np.random.randint(1, dice_sides+1) for n in range(rolls)])  \n        sums.append(s)  \n    plt.hist(sums, bins=range(dice_sides*rolls))  \n    plt.show()', ""> For example, consider if the first die has integers, the second multiples of 2, the third multiples of 3, the fourth multiples of 5, ... \n\nI guess I was a little unclear, because this isn't an example of what I presented. Each trial would be exactly the same. I should also clarify that the faces only need to be integers, if that simplifies things."", 'When you say ""each trial"" do you mean each die is the same?', 'Yes', 'You could have say a d8 with faces like this, though?\n\n1,2,3,100,101,345,812,15197', 'This is a good example, yeah', ""I dont think there's a great general solution that's much more efficient than direct computation unless there's a *lot* of dice (more than enough to overcome the gappiness and potential skew, etc - in which case there's some numerical methods like FFT, and an asymptotic approximation via CLT).""]"
[Q] Intern/Career Options for Grad Students,"Hello, 

I just finished my first year of a Stats MS. I've taken Probability and Statistics I & II, Statistical Computing, and GLMs. By the end of July I'll have also completed Time Series Analysis and Multivariate Regression. My program is focused on the theoretical side of machine learning/prediction. Building ML algos from scratch but also learning relevant APIs. I'm just curious what companies/industries hire stats folk. 

Other than data analysis/science, I'm not sure what my career options are. My program boasts most alumni placement is in data science, but It's been a struggle to get analyst roles so far so Im looking to expand what I'm applying to. Any advice would be appreciated. Thanks!",13n1m2e,jadedgradstudent,1684605587.0,9,1.0,"['Finance because anything that impacts a customers financing has to be explainable. ML algos are difficult (not impossible) to explain, so a lot of financial institutions use traditional statistical models. That leads you to a statistician. You wouldnt believe the number of people coming out of ms in ds who cant explain a logistic regression.', ""Nope, I'm in finance and I studied computational solid state physics for my PhD, there are many others like me who studied either math / stats / physics / or have a CS degree focused heavily on the math side of things for their grad program. While I wish I would have studied business formally, it's not a requirement. Most finance firms have a research division that does machine learning and predictive analysis, which is where you would want to look for positions. Most large tech firms actually have similar divisions though, so if finance isn't your cup of tea you have many other options like healthcare (predictive analysis for diseases), telecommunications (multivariate analysis of consumer data, or if you have a good enough background in counter science the really fun problems involve reducing latency in streaming video apps), or anywhere that does marketing research analysis. If you can't find anything, you can always do your PhD while you look for something, that's what I did anyway until i could decide what to do with myself."", 'Interesting. I always thought the finance industry preferred people with accounting and business degrees out of tradition. What title does finance hire statisticians under?', '>\t Most finance firms have a research division that does machine learning and predictive analysis, which is where you would want to look for positions. \n\nFor math / stats / physics PhD students who are uncertain about whether they want to pursue academia or industry, what are your thoughts on spending a summer or two in an internship in such a research division (in finance or healthcare or telecommunications, as you suggest), especially as a first or second year? I ask because Im not sure how I should make such a trade off between making progress on my degree requirements and preparing for post-PhD things, e.g. by doing such an internship.', 'Typically its broken up into teams so you can see model validator, data scientist, or even just modeler. There are teams for accounting and finance but they dont build predictive models, the statisticians and data scientists do.', 'I did a research fellowship for my PhD, but it was part of a program I applied to and was accepted to participate in vs an internship specifically, but it was still working research experience. There are also plenty of opportunities for pure internships as well. I think you should absolutely do an internship to give yourself an idea of if you\'d prefer working in industry or academia, if you have the opportunity. Does your department allow for this? We had certain restrictions regarding ""working"" when I was in grad school because the department paid us and expected us to do a certain amount of research each week, but if you are allowed, now is actually the perfect time, and you might just take an easier load in the summer or plan on only doing thirty hours weekly for the internship and then spend your other thirty focused on study (again not sure how your program is, for me I spent about sixty hours weekly either doing research or studying - if your program leans more towards 40 hours weekly pick 20 / 20). Most internships are actually tailored to mentor students in your exact position and prepare them for careers, because these positions are actually quite sought after and it\'s pretty prestigious if you are able to complete one while getting your graduate degree. If you need specific pointers above and beyond this feel free to message me and I\'ll try to help!', ""I haven't seen a policy in writing that my (future) department allows/disallows internships, but the students of my intended advisor say that he is quite flexible with his students and allows them to intern as long as they are otherwise on track for their degree requirements.\n\n>Most internships are actually tailored to mentor students in your exact position and prepare them for careers, because these positions are actually quite sought after and it's pretty prestigious if you are able to complete one while getting your graduate degree.\n\nWait really?! That's actually quite fortunate; how does this preparation work? My impression from talking to professors right now is that (math) PhD's need to retool a lot if they go into industry, but these internships would definitely simplify matters.\n\n>If you need specific pointers above and beyond this feel free to message me and I'll try to help!\n\nI'm currently in the stage of trying to find out what I ought to know before starting my PhD, so I really appreciate the offer! I'll probably message you once I figure out what questions to ask :D""]"
What area of research describes these key words [Q],"Can someone help me identify what area of research this is in statistics? Basically some sort of area which includes topics such as high dimensional statistical learning + shrinkage estimators/priors and penalized high dimensional regression + sparse linear models+ optimization algorithms. 

Basically, an area of research which focuses on the development of statistical methodology for high dimensional learning problems. Involved custom shrinkage estimators, or certain algorithms for high dimensional learning, or topics from ESL that kinda go more in depth than lasso/ridge. 

Or any kind of research involving developing penalized regression or algorithms for high dimensional learning. 

Im trying to find what this area of statistics is called but I cant quite find it. Id appreciate if anyone can point me in the right direction.

If there is also a Bayesian flavor to this, Id like to know this as well. Thanks!",13mzcg7,AdFew4357,1684599915.0,1,1.0,"['Machine learning?', 'High-dimensional statistics', 'Thats too broad, wouldnt you think?', 'Yes yes yes! This is it! Thanks!']"
[Question] Odds of three people born the same day of the week?,"Team, please help out a guy who has made every effort to stay away from math over the past 20 years.

Trying to figure out the odds of three people being born on a Saturday.  I'm thinking it'd be 1/343 since I did 7x7x7.  Or I could just as easily be wrong. 

Thanks for your help.",13msv02,Amwrinkle,1684590772.0,0,0.5,"['Well, we can ignore the day of the week for the first person, all we care is that the second person matches the first person, and the third matches the first. \n\nYour probability would be correct if your were asking three people born on a Monday, for instance', 'Awesome, thanks for the help!', 'In the US, not sure about other countries, not all days have the same number of births. Weekends account for much less than 2/7 of births', ""But it still ends up a fairly close approximation. \n\nFor OP, if you don't want to assume a uniform distribution... You \n\nUsing US 2010 CDC data, with proportion for each day as:\n- Sunday: .0927\n- Monday: .1521\n- Tuesday: .1672\n- Wednesday: .1647\n- Thursday: .1629\n- Friday: .1560\n- Saturday: .1044\n\nYou end up with 1/44 rather than 1/49 that 3 random people share the same birth day of week.\n\nYou get there by doing the calculation for each day of the week, e.g., Sunday: .0927 * .0927 * .0927 = .000797, and then summing the result for each day of the week.\n\nThis is very similar to if you assume it's uniform: e.g., Sunday: .142857 * .142857 * .142857 = .002915, and then summing the result for each day of the week, which is equivalent to ignoring the first persons day of the week.""]"
[E] A little statistics help required please - assessing predictions,"If I conduct a study where I get a single group of people to guess what their IQ is, then get them to sit the test, how do I assess how accurate their guesses are? Im planning a study but dont know enough about statistical methods to know which ones to use.

Thank you so much in advance",13mlq2m,WholierThanMeow,1684569769.0,7,0.77,"['You plan on giving people IQ tests? Specifically which IQ test?\n\nDo you have IRB approval to conduct this study?', ""Hmm not sure what the exact objective is but i see three ways of doing this:\n\n1. Test the difference in distributions of the predictions and actual score for significance using a t-test or an equivalent non parametric test \n2. Compute a new variable as the difference between the predictions and actual scores and test whether it's mean is statistically different from zero.\n3. Use a regression model with the actual score as the dependent variable and prediction as  the independent variable and see of the coefficient is statistically significant as well as the direction of the relationship. Though in the absence of other variables, your estimate will most likely be biased.\n\nYou could also visually explore the data with a scatter plot. This might show you patterns . Like maybe people with high IQs predict lower scores for themselves  or something like that"", 'Im not planning on taking up too much of their time, so it will be any random old IQ test. Im more worried about how to run the statistical analysis afterwards though, as its not a simple experimental design Im not sure how to go about assessing their predictions.\n\nTo clarify - this isnt a real world study, its a practice in designing a study and correctly statistically assessing the results', 'In 1) and 2) if your model predicts perfectly, your test will be not significant.\n\n3) Makes more sense, but is very similar to just calculating the Pearson correlation between predicted score and actual score. I think this is pretty reasonable and will test the statistical significance if this correlation is different from 0. If it is not expected that there is a linear realtionship, but merely a monotonic one, one could use the Spearman correlation instead.', ""Great suggestions! It's always important to consider different approaches before deciding on the best one for your analysis. Additionally, the idea of using a scatter plot to visually explore any potential patterns in the data is a great suggestion. It will be interesting to see what insights can be gained from this study!"", 'I.e. not applied, only theoretical? \n\nBut then the mention of ""I\'m not planning on taking too much of their time"" is a sorta strange statement to make...', ""Yes but wouldn't insignificant results in 1 and 2  mean that there is not enough evidence to support the statement that people's predictions about their IQ scores differ significantly from their actual scores i.e people generally make good predictions about their IQ which might be what op is trying to check?"", ""3 makes plenty of sense as that's the general method for finding the relationship between a predictor and a Criterion and 3 is pretty much exactly that kind of set up. \n\nAlso appreciate your point on monotonic possiblity."", 'No, the research will take place - only in an educational setting (Im studying an elective from a course where all the other students did a lot of statistics work that I havent been exposed to). Thus not wanting to take up too much time for the participants\n\nEdit: whats with all the weird downvotes? Im not cheating on my studies or being a smart arse at all, I just want to understand the relevant statistical analyses', 'I think OP changed the question after my answer or maybe I read it too fast.. In the current form your approach 2) makes sense, and is the same as 1) if you use a paired t-test which is the appropriate here. I would recommend a non-parametric test instead though the Wilcoxon test.\nEdit: It depends entirely on what OP wants to prove. Either you want to prove that people predict their own IQ well, or you want to prove that people generally either overestimate or underestimate their IQ. One must choose a different null hypothesis for each question.', 'Because it is a real study, with real people. Its fine that youre a student and more in interested in the theory, but that doesnt diminish the realness of the people. \n\nExperiments involving humans typically need IRB approval. Did someone tell you that you dont? Or that you must do a study with real people?', ""Oh ok ok . Well be sure to look at the validity documentation as you may experience a dramatic change in error bands depending it's intended purpose and population, this making your study more likely to give you results that are not significant (I give this advice as a psychometrician)\n\nBut there might be some good short form iq tests out there for you"", 'I havent changed it, but thanks for your input!\n\nAnd Im only interested in the strengths of predictions, not the direction.', 'Yeah i know they basically mean the same thing but thought maybe it would be helpful for OP.', ""Studies done for a course don't require IRB approval. They aren't considered research activity. This changes if they intend on publishing but I doubt OP is considering that."", 'Source? I find it hard to believe all the protections the IRB provides human subjects is just thrown out the window if the study being conducted is ""for a course"".', 'The IRB only gives approval for research activities. The [US] federal government defines research as systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge [[Source]](https://ori.hhs.gov/content/chapter-3-The-Protection-of-Human-Subjects-Definitions).\n\nA class project does not fall under this definition as the project is not done to contribute to generalizable knowledge. It is done for pedagogical reasons. Unless the student intends on formally publishing the research, then there is no need to seek IRB approval. I thought this was pretty common knowledge, but, if you are skeptical, you can read about it at many different institutional websites. [[Example 1]](https://campusirb.duke.edu/irb-policies/irb-review-student-research-and-class-projects) [[Example 2]](https://research-compliance.umich.edu/human-subjects/human-research-protection-program-hrpp/hrpp-policies/class-assignments-irb-approval) [[Example 3]](https://msutexas.edu/irb/_assets/files/guidelines-for-class-related-projects.pdf) [[Example 4]](https://www.uab.edu/research/home/images/IRB/Giudance/Student-Information/IRB_Guidance_for_Student_Research_and_Class_Projects.pdf) [[Example 5]](https://cuhs.harvard.edu/undergraduate-research-and-course-projects)', 'So one could reproduce the milgram experiments as long as it were just for a class?', ""Milgram's experiments have been replicated hundreds of times. You can read through the compliance and conformity literature to see that. But generally, yes. If you aren't doing research activity, then you aren't beholden to the IRB. Your university might have general guidelines about what it expects to be appropriate for a class project though.""]"
[Q] ITT Estimation,,13m8raz,EdoDeo,1684532778.0,1,1.0,[]
[Q] Confounders and colliders haven't been accounted for,"I'm exploring the causal relationship between 10 different variables, and I noticed there are some confounders and colliders.

Although I don't have data, I do have other peoples equations (independent sources) when they looked at some of the parameters in groups of 2 or 3.

Although I can combine all their equations into one, is there a way of accounting for confounders and colliders (such as cancelling certain variables out, or ignoring the equations of certain relationships) or would I need to get my own data and start from scratch?",13m6woz,NoticedTriangularity,1684528319.0,11,0.92,"[""Confounders and colliders are theoretically determined, with input from the data. Try to create a DAG with the input from the other sources you reference. Figure out an equation from there. Keep in mind that DAGs aren't perfect. There are too many cyclical relationships, misunderstood relationships, and unobserved variables for it to ever be perfect."", 'You can use a bayesian network and account for colliders and cofounders. Basically, if properly set up, you can control for all variables and also unobserved variables', 'Can you recommend any examples or walkthroughs of this method?', ""Although the sources with the equations only conscider the variables in groups of 2 or 3, from other sources that I was able to create a DAG to understand to identify the colliders and confounders (though no quantification of these relationships are available). The problem is that these aren't conscidered in the individual research where analysis was done\nGiven that I have a DAG, and I have the equations for the relationships individually, what is the next step to combine it all together? For instancd, do I combine them all and re-arrange to remove colliders, or do I just ignore the equations for the colliders?"", ""Thank you for your response. I don't have access to the sources data, am I right in thinking that I can combine all the equations, and using realistic inputs - create the data that I can then use this on?"", 'Look up Judea Pearls work. The Book of Why is a easy read, non-technical intro to DAGs and causality.']"
[Question] Signal detection theory question for memory task,"I'm a stats novice seeking some advice on what extent d-prime is appropriate here. My lab is running an experiment where participants are exposed to sounds in an initial encoding phase, and then in the test phase they are given a subset of the old sounds (targets),intermingled with two types of new sounds: lures (very similar sounds to targets) and foils (clearly distinctly different sounds). Participants here rate each response as either old or new.  
  
Right now I've been calculating two different types of d-primes: 1) d-prime for target-lures, z(""old"" to targets - ""old"" to lures), and 2) d-prime for target-foils, z(""old"" to targets - ""old"" to foils). The reason for doing so is that these two types of d-primes represent different constructs in the field of memory (target-lure = mnemonic discrimination; target-foil = the conventional ""old-new' effect).  
  
My question is whether there is any issue of inflating or deflating d-prime values when there are three signals (targets, lures, foils) in the probability space? How would one best correct for response bias here or would the use of d-prime (or a-prime or A) be adequate? What other measures have been described in the literature?  
  
Thank you for your responses in advance, would appreciate any advice!",13m4xzd,love_your_ears,1684523743.0,1,1.0,[]
[Q] How can I test to see if two different groups answered multiple choice survey questions differently?,"I have quite a lot of experience with continuous data analysis, but have never worked with survey data before. I have the results of a quality of life survey where each question is multiple choice, and survey respondents can only give one answer per question. Some questions are scored 1-10, and some are categories e.g. [never, sometimes, quite often, very often]. I have some additional data that groups survey respondents according to a few other categories. What test can I use to see if different groups have answered the questions differently?

EDIT: I should say, I'm primarily interested in one group in particular, so it might be right that I want to compare that group to all the rest of the respondents pooled together. For other categories (e.g. age brackets) I'm interested in comparing across all of them (e.g. are some age brackets more likely to report problems sleeping as a result of a certain lifestyle intervention).",13m19pf,Apes_Ma,1684515555.0,6,1.0,"[""Immediately, Chi-squared test for independence or Fisher's Exact come to mind. \n\nI'm not sure how you're conducting this, but it would be a fun project for a DS reference.\n\nCheers"", 'For the ordinal responses, consider Kurskal-Wallis tests', '> more likely to report problems sleeping\n\nIf the issue youre interested in there is looking for *more* problems sleeping, with an ordered categorical response you should not use an analysis that throws away the ordering (such as plain chi squared homogeneity of proportions). Possibly a Wilcoxon-Mann-Whitney (for a two group comparison),  or a Kruskal-Wallis for more than 2 groups. There are other possibilities.\n\nWith your 1-10 variable youd have to explain more about what the score numbers represent.']"
[R] Comparing variances for sales purposes,"I have sales data for ten items (SKUs) across 24 months. The data is in revenue (e.g. $10,345) on a month by month basis. 

My goal is to determine how consistent sales are of each SKU relative to one another, with the idea being that SKUs with lower variability in sales are more reliable/consistent revenue streams for our company. 

My intention is to measure the variation of each SKU individually across 24 months and compare those values to one another. Is this the right way to go about it? Also, would it be a population variance or a sample variance? This is all the data I have for two years, but I suppose it could also be a sample of ten years of sales for the company. Im not sure which version to use.",13m0aib,cdelli01,1684513364.0,1,0.67,"['In your scenario, if the 24 months of sales data is considered a representative sample of the overall sales pattern for each SKU, you can use the sample variance formula. However, if the data represents the complete sales history for the SKUs, you can calculate the population variance.']"
"[Q] I want to make the claim that if you have no idea/prior knowledge that either event A or event B would occur, it would make the most sense to ascribe a 50% probability for each event. If true, is there some kind of rule I can refer to?",Thanks in advance :),13lubha,Wolfpaka,1684499966.0,8,0.84,"[""One of the early and famous ways of ascribing prior probability was the [Principle of Insufficient Reason](https://en.wikipedia.org/wiki/Principle_of_indifference), which I've usually seen ascribed to Laplace (but which wikipedia attributes to a number of people). \n\nThis leads to giving a coin flip 50/50 probability, or a dice roll 1/6 probability, which makes perfect sense, but often gives conceptual and empirical problems when applied to common continuous variable situations (e.g., a uniform prior on a logistic regression coefficient is probably a bad idea).\n\nMore recently, Edwin Jaynes proposed something called [maximum entropy priors](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy), which are a generalization of the principle of insufficient reason, and let you incorporate more complex constraints and additional info into the prior."", ""You would just be assuming a uniform prior, essentially. I'm pretty sure there's no, like, named rule or anything (like Occam's Razor or whatever)."", ""Assuming both events are mutually exclusive, your experiment can be modelled with Bernoulli distribution.   \n\n\nBernoulli distribution only has one parameter (we'll denote it as *p*), in your case that would be the probability of event A (or event B) taking place. The variance is maximised when *p = 0.5*. Since variance is essentially a measure of uncertainty about a parameter, we may argue that, by ascribing the probability of 50% to both events, we have maximised the uncertainty about the true probability.  \n\n\nThis is how I would defend you decision to assume *p(A) = p(B) = 0.5*"", 'If you set it at anything other than 50/50, it means you have some idea of which one is more likely. So no prior knowledge and equal probability are tautological, which is even better than a rule.', '""It would make the most sense"" is doing a lot of heavy lifting.... let\'s break that down a bit.\n\n- you have no idea what the probability of event A is ( that of B is 1 - Pr(A)), but you\'re insisting that you have to pick a number.\n\n- to justify a choice, you could define a loss function L(p, true p) and try to minimize expected loss E( L(p, true p)) wrt p. \n\n- Even if you do that, you still have to take a stand on (1) what kind of loss function to use (are overestimates as costly as underestimates? Does loss increase linearly with the error? its square? a higher power?), and (2) the distribution of ""true p"", about which you claim to have no information.\n\nIn general, you can think of many situations where this approach will give answers that are *not* p=0.5', 'actually my prior would be a uniform distribution on the probability between 0 and 1, also known as beta(1,1)', ""It does not mean you don't put extra information, it all depends on the parameterization. If you transform your parameter U as W=log(1/U), you would be putting exponential prior on the transformed parameter W. You could say, why do we even need this transformation? But who said your parameterization is the best / truest?..\n\nA prior, that is uninformative under one parameterization, might be very informative under another. This is what Jeffrey, and later Jaynes (1st answer) have tried to address.""]"
[E] Generating samples from custom CDF,"To my great shame, I must admit I was today's year old when I realized than I could use the uniform distribution and empirical cumulative density functions to simulate sampling for a distribution I do not have a closed form for.

Context: I've been digging a bit deeper into simulation, in particular Monte Carlo-type simulations where I need a probability function or cumulative density function to generate meaningfully random samples. Sometimes, the normal or lognormal distribution is sufficient, but sometimes the distribution is weird (not normal, multi-modal, etc.).

Generating an empiric CDF from data and using calculating values from it using the uniform distribution actual works to overcome the closed-form or not-normal function.

As example:

(Can't actually include the image on this subreddit, but the distribution overlap to a great extent, as can also be seen from the calculated mean and sd values.)

&#x200B;

    library(tidyverse)

n <- 1000

mn <- 25

sd <- 10

rnd <- rnorm(n, mn, sd)

cdf <- tibble(

x = seq(min(rnd), max(rnd), length.out = 100),

y = map\_dbl(x, \~ sum(rnd < .x) / length(rnd))

)

new\_rnd <- runif(n)

new\_rnd\_dist <- map\_dbl(

new\_rnd,

\~ approx(cdf$y, cdf$x, xout = .x)$y

)

mean(new\_rnd\_dist, na.rm = TRUE)

sd(new\_rnd\_dist, na.rm = TRUE)

ggplot(data = tibble(x = new\_rnd\_dist), aes(x)) +

geom\_density(fill = ""red"", alpha = .1, linetype = ""dashed"") +

Geom\_density(data = tibble(x = rnd), fill = ""green"", alpha = .1)

Note: markdown won't let me comment code with hash-tag, some some extraneous comments

Generating normal values with set mean and sdGenerating the CDF from the samplesGenerating uniform distribution sample with `runif()`.Matching the uniform samples onto the empiric CDF.Calculating mean and sd from newly generated sample transposed to normal

The cool things start to happen when you exchange the initial sampling with e.g. a bimodal:

`rnd <- c(rnorm(n, mn, sd), rnorm(2*n, mn/2, sd))`",13lty5h,peperazzi74,1684498996.0,12,0.88,"['Google ""Gaussian Copula"" - there are general frameworks for this kind of thing.', 'Just a note, you want to be careful when you talk about _which_ distribution you are sampling from -- in these scheme, you are most certainly not sampling from the underlying distribution that generated the data because of the fixed sample size. This is more or less just the procedure used in bootstrapping. In information terms, this does not contain any extra information than your original sample.\n\nThat being said, this scheme _asymptotically_ will sample from the correct underlying distribution of the data as the eCDF converges to the CDF almost sure (Glivenko-Cantelli).\n\nHowever, it is certainly very useful to use this sort of method.', 'Live and learn, I too only learned about the probability integral transformation 9 months ago.', 'Completely agree!\n\nHowever, for me it\'s a lot more useful to have data that\'s close to the ""real"" distribution than to accommodate unrealistic distributions for which I have a closed-form CDF. Guess I\'m too much of an engineer ', 'Just be careful in the tails. The approximation can get really crude near, and especially past, the minimum and maximum data points.', 'For the other problem, quantile estimation, I might use a fully empirical method in the bulk of the distribution, but fit a parametric model for the tails.  This would apply here as well, like above and below 1% (or whatever you choose) empirical CDF points, draw from a parametric model (possibly fat tailed if you expect that).']"
[Q] How to add trend back to the forecasted time series data?,"I am making forecasts on the quarterly Ontario population. It is not stationary, so, to make it stationary, I remove the moving average and divide by the moving standard deviation and train the model on that.

'''

rolling\_mean = data.rolling(window=4).mean()  
rolling\_std = data.rolling(window=4).std()  
  
y\_detrend =  (data - rolling\_mean) / rolling\_std

'''

To get forecasts in the original format, how do I add trend back to the forecasts?

Thanks",13l3zsc,PythonEntusiast,1684428114.0,2,1.0,[]
[Q] When should I add sql to my resume when I’m teaching myself,"So I just graduated not even a week ago with my bachelors degree in stats, but I was never taught anything besides R and a tiny bit of SAS. Because so many jobs love to see sql, I figured its best if I teach myself through YouTube videos. 

Considering how long these videos are and how simple sql seems at first glance, I figured Id be able to learn it within 3 or 4 days give or take if I truely dedicate myself to those videos and following along. Iv got nothing better to do. But people online say it takes 2 to 3 WEEKS, and even then its just the basics. This wouldnt be that big of a deal, but I really dont want to sit around for that long getting denied jobs because I dont have sql on my resume. 

So when can I comfortably put sql on my resume? Should I couch it a bit by saying basic sql or am I hurting myself by doing that? Is it unnecessary to couch it at all since the employer knows Im fresh out of college with no experience in the real world anyway? 

Thanks everyone!",13l0d6n,lewdjojo,1684419339.0,33,0.86,"['Just put SQL in and be prepared to fake it until you make it. Nobody expects you to know complex SQL formulas.', 'You can put ""familiar with SQL"" in right away, but, unless you\'re getting a certification, employers are going to want to see professional experience. I\'m too lazy, but you might google ""what do employers think of self-study"" (no quotes) for some tips', ""I have no idea how you CV is formatted, but just slap there some equivalent of SQL: Beginner and you are good, you can figure out details on the interview.\n\nA little story: several years ago when I was going through interviews on some Data analysts position, I had the same thing written in my CV, because I had an experience and training with R from university, but not much else. I had gone through some SQL tutorials and had like a half day of training at some recruitment event, but that was all, no real experience with SQL.\n\nThe interview didn't work out, because we discovered that they would like somebody who's more into ETL pipelines than me, that they hired a junior generalist data analyst three months ago and they were set ATM. But when we were ending our talk, their chief analyst gave me advice that if I am able to use GROUP BY statement, I should stop be humble about it and stop writing it in my CV, because I have a greater knowledge of SQL than majority of people their interviewed who claimed advanced knowledge.\n\nIt improved my self esteem quite a bit, tbh."", 'Add it now, and if you fail at some concepts in an interview, then you know what concepts to study next. Repeat until employed.', 'Tangentially related, but if you have used the dplyr package to manipulate tibbles in R, then SQL should come very naturally to you. This is because the main dplyr verbs almost have 1-to-1 analogues in SQL. For example:\n\ndplyr: select(), transmute(), mutate(), summarise()\nSQL analogue: SELECT f(x) AS new_x\n\ndplyr: filter( condition 1, condition 2, )\nSQL analogue: WHERE condition 1, condition 2\n\ndplyr: arrange(x, desc(y))\nSQL analogue: ORDER BY x, DESC y\n\ndplyr: group_by(x, y) \nSQL analogue: GROUP BY x, y\n\ndplyr: group_by(x) into a mutate(new_y = f(y))\nSQL analogue: Window functions, ie SELECT f(y) OVER (PARTITION BY x) AS new_y\n\ndplyr: left_join(), full_join(), etc\nSQL analogue: LEFT JOIN, FULL JOIN, etc.\n\ndplyr: bind_rows()\nSQL: UNION\n\nIn dplyr, the functions are called on dataframes/tibbles which allows you nest table transformations inside of each other. For example, lets say I want to take a dataframe called df2, select columns x and y, and left join onto another dataframe df1 using column x as the join key. In dplyr, this can be done with:\n\ndf1 %>% left_join(select(df2, x, y), by = x)\n\nWe nested the select() call inside the left_join() call. The same thing can be done in SQL and these are called subqueries:\n\nSELECT * \nFROM df1\nLEFT JOIN \n    (SELECT x, y\n     FROM df2)\n    ON df1.x = df2.x', 'My cv has two lines for software. ""Proficient"" for the ones where I can just start coding on a problem they hand me, and ""familiar"" where I\'d have to Google to refresh or get up to speed but have experience to draw on.', 'If it\'s the kind of position where you\'d be likely to use sql on the job (eg most data analyst positions, especially those for a company that makes software or is an IT co, or something like that), I\'d absolutely expect those questions to be asked in an interview, so clearly I disagree w/ the ppl telling you to simply not worry about it. \n\nWhat kind of programming/scripting experience (w/ other languages) do you have? Just the R and SAS you mentioned? Sql isn\'t technically a programming language, it\'s ""only"" a scripting language, and unlike other scripting languages like php, perl, etc, it can\'t do all that much - it\'s made strictly for interacting w/ a database. You might need more than a couple of days to learn it, but learning enough to pass an interview in that timeframe is certainly doable. \n\nI\'d also suggest some hands on experience rather than strictly watching YouTube vids  - if you\'re already running Linux, you can easily install MySQL/MariaDB with whatever pkg manager you\'re used to, or if you\'re on windows, I\'d suggest installing xampp (apache-friends.org) because you\'ll be able to easily interact with your database using phpMyAdmin and can  also get some hands on experience that way.', 'When youve done windows function/aggregation and join.\n\nMost analysis can be done on these two. Most hiring manager care more about if you can correctly join tables than deep knowledge in SQL and database.\n\nIf you prefer some kind of assessment, do exercises on sqlzoo.', ""Finished uni without ever touching SQL, now 95% of my job is writing SQL queries. Honestly, they work exactly how you would imagine, even most complex queries can be deduced intuitively, and if you need to write one yourself, there's infinite help on the internet. Most of the time you will be selecting some value from the database with some basic filter anyway..."", 'Congratulations on graduating with your bachelors degree in stats! It\'s great that you\'re taking the initiative to learn SQL through YouTube videos. I understand your concern about not wanting to be denied jobs because of your lack of SQL experience. However, it\'s important to remember that learning SQL is a process and it may take longer than you initially anticipated. It\'s better to take the time to truly understand the basics before adding it to your resume. \n\nI would recommend couching it as ""basic SQL"" on your resume since you\'re still in the process of learning it. It shows that you have some knowledge of the language but are still willing to learn more. Employers appreciate honesty and willingness to learn, so don\'t worry too much about not having enough experience in the real world. Good luck with your job search!', ""As an interviewer, I'm annoyed when people say they"", 'Sounds like we had similar backgrounds lol\n\nUnfortunately stats programs are light on the technical skills and SQLs use tends to be quite a bit different than R, but you should theoretically have a leg up because not every aspiring analyst will have the same rigorous math background \n\nI concur with the adding familiar with sql part. Pick up the basics (Select, from, where, joins, ect) and you should have enough to make it known it wont be a liability at least because youre really not going to learn it until youre forced to use it', 'SQL the language is simple, but relational databases not so much. I would not spend time learning SQL syntax, instead looking at the underlying concepts - normalization and de-normalization, indexes, join conditions, etc.\n\nGet SQLite and try doing something with it.', 'Tell them you did codeacademy or whatever. Sql is a joke compared to what youll be doing with it, the hardest part is thinking about what is right, now how to do it usually', ""I'd be thinking about whether you can achieve anything with your current knowledge of SQL. Can you do stuff that would be useful for the job you're applying for?"", 'You dont think Ill be asked about sql stuff in an interview?', ""I was asked a number of SQL question in my interviews. I even had a database normalization take home for one data analyst interview. If you put it on a resume then they're going to ask about it."", 'I think this is right, qualify your SQL level so its clear. A lot of people exaggerate their SQL fluency and then get surprised when they bomb a coding test. Im currently hiring and this has been a huge waste of our time.', ""> But when we were ending our talk, their chief analyst gave me advice that if I am able to use GROUP BY statement, I should stop be humble about it and stop writing it in my CV, because I have a greater knowledge of SQL than majority of people their interviewed who claimed advanced knowledge.\n\nOh, my. I'm not sure I could get by in a week if I wasn't allowed to use a couple GROUP BY statements every day. I sometimes don't bother with a SELECT DISTINCT if there's a chance I might need to probe at the underlying data some more..."", 'I almost exclusively used R during my time at uni. I had one class that was half R and half Python, but that was pretty early on. My last semester I did all my assignments for one class in SAS so Im a little familiar.  I mostly copy-pasted from my notes with some modifications I had to do myself. Occasionally I did have to write my own code so I have some building block knowledge. \nIn regards to R I gotta be honest, I dont even know how confident I can be. I did a lot with it, but I still feel unprepared for the real world. \n\nA couple years back I took a class on C++. \n\nWhen you say hands-on experience, what are you referring to? How would I gain such experience without an actual job? Besides YouTube videos or a classroom Im not sure where to find that experience.', 'If youre interviewing for analytics or data science roles you 100% will be asked to code in SQL.', 'You have to get the interview first, you can work on learning SQL while you hunt for jobs.', 'My argument is there will be time between an application for a job and the interview, not putting it in there will limit job prospects.', 'So Iv done a little work with SAS and even less work with Python. Im willing to bet that whatever I teach myself with sql I will probably be way better at coding with that than SAS or Python. Should I just lump them all together with the phrase familiar with even though I have varying levels of familiarity?', 'When I mentioned hands on experience, I was thinking you could set up a local environment (for Apache, MySQL, and PHP/Perl/Python as the bundle at apache-friends.org will set that up for you). If you can get XAMPP (or similar, MAMP, WAMP, etc are basically the same for this purpose - you just need a database that you can easily interact with using a web based interface like phpmyadmin). Once you have phpMyAdmin running, you can use that to play with your dB server and can, for example, run sql commands (and see their output) right in your browser - that would give you some ""hands on"" sql experience imo. If you need help getting that environment set up, feel free to msg me, though I think this tutorial will probably give you everything you need: https://www.ionos.com/digitalguide/server/tools/xampp-tutorial-create-your-own-local-test-server/', 'When I mentioned hands on experience, I was thinking you could set up a local environment (for Apache, MySQL, and PHP/Perl/Python as the bundle at apache-friends.org will set that up for you). If you can get XAMPP (or similar, MAMP, WAMP, etc are basically the same for this purpose - you just need a database server that you can easily interact with using a web based interface like phpmyadmin). Once you have phpMyAdmin running, you can use that to play with your (new) dB server and can, for example, run sql commands (and see their output) right in your browser - that would give you some ""hands on"" sql experience imo and you can easily try out stuff described in the videos. If you need help getting that environment set up, feel free to msg me, though I think this tutorial will probably give you everything you need: https://www.ionos.com/digitalguide/server/tools/xampp-tutorial-create-your-own-local-test-server/', ""Oh also, as far as your other background in Python, R, and c++, it's probably not going to be directly relevant, but it's good in the sense that you will understand how code is generally structured, and you understand some basic principles like how conditional statements, loops, iteration, etc, work. I wouldn't worry about not feeling terribly confident in any of those since that's not what you're trying to learn.\n\nDo you have an environment set up already to run your Python scripts, or are you using your school's infrastructure or something like that? Just asking as you may already have a webserver+dB server installed without realizing lol"", 'That was the plan until I read somewhere that it takes weeks. Do you think YouTube videos online cover what I might be expected to do in an interview?', ""Just say proficient in SQL.  Maybe you'll bomb the interview but you've lost nothing and gained experience.\n\nDo leetcode style practice questions over and over again and don't stress learning much else.  Technical screens are a skill in themselves and the more you do the better you'll be."", 'I still have anaconda and I am able to use Jupyter and do stuff with Python 3. I forgot how I installed it but I am able to access it afaik. \n\nYeah Im a little disappointed that R isnt used nearly as much as I was told it was. At least it looks good on a resume I guess', ""I think you're worrying about a non-issue. Put SQL on there as its a common keyword employers search for. Learn SQL the best you can in the interim, if you get an interview requiring some degree of SQL knowledge, do your best.\n\nThe job market for DS jobs is extremely tough. As a new grad you need to give yourself any and all opportunities just to get in the door."", ""Ah, I know Python is frequently taught as a standalone scripting language rather than as a web scripting lang, so I guess you don't have a web+dB server bundle installed then. I'd just go with XAMPP from apachefriends.org - it has a simple installer for windows that sets everything up for you. Once installed, point your browser to http://localhost/phpmyadmin and you'll be able to start interacting with your dB server."", 'Thanks so much for the tips man, Ill do that asap and msg you if I get stuck somewhere', 'No prob, you may also want to check sqlzoo.net as i hear that has some good practice questions etc']"
[Q] What is the best source/program to do power analysis?,"I have to perform a power analysis for my chi square and kruskal one-way anova tests to know the minimum sample size needed detect a difference in proportion and mean, respectively.

On spss (version 28) it seems more complicated than I anticipated (looked up YouTube videos but still was lost). 

Is there a good program that statisticians use and recommend? Or are any of the online calculators on google suffice? Using for a manuscript so want to ensure I do it correctly",13kznbv,imreadytolearn,1684417531.0,1,1.0,"['Check out G*Power', 'I would go for JASP, has a simple GUI and powerful, you can also add R code for more complex cases.', 'As another person recommended, for simple power analyses GPower is the best resource', ""There's a handy set of calculators here: https://select-statistics.co.uk/calculators/"", '""best"" is subjective and depends on your needs, interests and preferences.\n\nG\\*power seems to be very popular among social scientists. I\'ve never used it but as far as I am aware it\'s pretty comprehensive.\n\n> Is there a good program that statisticians use and recommend?\n\nTBH I\'ve not seen a statistician use one (though I am sure it happens). On the rare occasions I want power calculations, I generally use simulation (outside the cases simple enough to do algebraically -- and sometimes even then I\'ll use simulation). I have occasionally used the t-test power function that comes with R, though, such as to draw a power curve to compare with some other test on some set of alternatives.', 'Yep I downloaded and its good. The one question I have though is effect size supposed to be calculated? Or do we just choose a random number (ie. 0.3, 0.5, 0.7)', 'Yep downloaded appreciate the advice', 'Yeah I just downloaded g power and very user friendly', 'Is there tutorial for beginners to calculate sample size using simulation?', 'Just define your (assumed) data generation process (including effect size of the smallest effect you want to be able to detect), choose an appropriate test (one with appropriate type I error control), and then run the loop (generate data -> test data) to check how often your (true) effect is detected (i.e. this is your ""power"", which informs you on the expected type II error).\n\nYou probably might want to vary things like the number of measured samples, the noise level, the target type I error (alpha), the effect size and/or other important aspects of your data generation process, and see how it may affect your type II error (beta).\n\nYou ""calculate sample size"" by simply figuring out what should be the number of measured samples such that you get an acceptable type II error rate (often people choose 20% here, which is the same as power = 0.8) for the type (i.e. size) of effects you are aiming to be able to detect.\n\n(I guess you should probably also evaluate type I error rate, if you have the time to do so, by performing the same simulations but with effect\\_size=0, and checking how often the effect is erroneously detected as ""significant""; if the test is acceptable, this error rate should not be larger than the ""alpha"" you picked).\n\nHope it helps.']"
[Q] Creating a Excel Reward Program,"Hello All,

  
I'm new to this sub, I don't have any formal training in stats. I am an enthusiast looking to build a reward system for my sales territory. Attached is a copy of an excel sheet that I built but it seems to have a little snag in it, I was hoping to get some help on it.

  
Overall I was trying to create a score system for all of my accounts based off of 3 metrics. Each account receives a ""score"" based off of their actual data compared to the median of my sales territory. Essentially; If my median sales is $10 then a store that did $30 will receive a score of 3, a store that did $5 will receive a score of -0.5.

  
The snag I'm running into is the highlighted section, the function I have in the cell currently reads as =IF(C17/$C$2<1,-1\*(C17/$C$2),1\*(C17/$C$2)) Where my confusion lies is that this store is well below the median sales number, however they are receiving a better score than accounts who are still below the median but are higher than them.

  
Does anyone have any insight, videos I can go watch, terms that I need to study in order for me to fix this and improve upon it?

  
TIA!  
\*Picture will be in comments",13kyhku,Constant_Lab_1192,1684414653.0,0,0.5,"['You might want to rethink your scoring system. Using the score for other calculations will be simpler if your scoring distribution is continuous, but by using `if()` and two different functions, youve created a discontinuous distribution, and youll always have to handle positive and negative values differently. \n\nThree simple, continuous approaches to scoring that I can think of:\n\nDifference: `score_sales - mean_sales`. This gives you 0 when the store is on the mean, for every dollar over (under), the score increases (decreases) by one. You $30 store gets a score of 20, your $5 score gets a score of -5, and your $1 store gets a score of -9. The downside to this is that its not symmetric; the lower end is bounded by `-mean_sales` while the upper end is unbounded. \n\nRatio: `store_sales/mean_sales`. This gives you a factor that can be multiplied to give a result proportional to the store sales. a $10 store gets a score of 1, a $30 store gets a score of 3, a $5 store gets a score of 0.5, and a $1 store gets a score of 0.1. \n\nScaled Ratio: `(store_sales - mean_sales)/mean_sales`. This kind of preserves much of the scoring characteristics in your attempt, with stored below the mean receiving negative score and this above receiving positive scores. A $10 store gets a score of 0; $30 gets 2; $5 gets -0.5; $1 gets -0.9. This scoring system is bound by -1 on the low side and is inbound on the high side.', 'I think that you are not calculating what you want. With your formula if the sales would be $1, then you would have a score of -0.1 (which would imply that is doing better than a store that sold $5...)', '[Link](https://imgur.com/a/5b1FaPA) to referenced photo', 'This was perfect! Thank you for taking the time to sum all of this up. \n\nI believe what Im looking for is your scaled ratio definition. I will try that in a new version of my document', 'Is there a particular term of what I am trying to accomplish? It seems like that formula accomplishes the appropriate reward for all my accounts above the median. So, now I would just need to accomplish a function/equation to account for just how little stores do.', 'In simpler terms, I just want to find a function or formula to invert negative scores, make -.04 into -.96. Vice versa, if a store happens to be -.75 they should realistically be -.25', '(C17-C2)/C2 or equivalently C17/C2 - 1 leads to a score of -1 if there are no sales and 0 if the sales approach the median. There is a big step between the two cases, however - as soon as you reach the median the score jumps from 0 to 1. You can use C17/C2-1 everywhere, then 30 has a score of 2, 20 has a score of 1, 10 has a score of 0, and 0 has a score of -1. No jump in the score anywhere. That means your formula is just `=C17/$C$2-1`.']"
[Q] Linear regression of RCT data with multiple groups,"We are currently running analyses on data from an RCT. The primary outcome is continuous. We perform linear regression with adjustment for site and according to the group allocation/treatment (A/B/C). One statistician is performing the analysis on data subset by the exact comparison (i.e, A v B, A v C and B v C). I perform the analysis without subsetting the data. The results are almost identical. Is there any advantage or disadvantage of subsetting or performing the analysis without subsetting first?",13kxz3p,DocMaag,1684413418.0,13,0.93,"['You did a multi site study without discussing this in a data analysis plan?\n\nAlso was treatment randomized by individual or by site location (cluster randomized trial)? How many treatments and sites are there? Also comparing the three pair wise treatments sounds like a post hoc test rather than the primary testing that should be conducted.', 'It is not clear what the two different analyses are. You have 3 treatment groups, and the statistician you mention performs all pairwise comparisons between those groups.. But what are you doing that is not this?', ""If I understand correctly, your colleague is subsetting the data by site, and then evaluating every treatment contrast in each subset. If that's the case, this should provide equivalent effect estimates to fitting a model to the full data that adjusts for treatment group, site, and the treatment group-site interaction. However, the standard error associated with the estimates will differ across the two methods, because the model on the full data has more information (and makes a homoskedasticity assumption!). Based on your other comments you did not include the treatment-site interaction in your model, so that the two approaches provide similar results is likely because the interaction effect estimate is small.\n\nPersonally, I would adjust for site in a model rather than subsetting the data, if only because subsetting the data and computing the contrasts is a lot more work than just fitting the model!"", ""Sounds like you should use a mixed effects model, and you're both wrong."", '[removed]', 'We have a detailed SAP. I just never thought about the possibility of subsetting the data according to the specific comparison. The point estimates and CIs are close to identical and the conclusion doesnt change.\n\nTreatment was randomised by individual. Three treatments, three sites. We planned for pairwise comparisons and adjusted the threshold for significance accordingly.', 'So in R, I do:\n\nlm(outcome ~ site + group, data)\n\nHe does:\n\ndata_ab <- subset(data, data$group == A | data$group == B)\nlm(outcome ~ site + group, data = data_ab) \nAnd so forth for the other comparisons.', 'No.  OP\'s colleague is not subsetting the data by site.  OP\'s colleague is literally running a separate analysis for each contrast (A vs. B, B vs. C, and A vs. C) wherein they drop the patients from the group that is not part of that contrast.  So first they remove all patients in group C from the dataset and run ""lm(outcome\\~treatment)"" on the patients from just groups A & B, then they do that two more times (removing group B for the A versus C comparison, and removing group A for the B versus C comparison).  Why does everyone think the question here is about site?', 'Would you elaborate?', 'Computatuonal load is not an issue. We only have 175 observations coming from three treatment groups. The null hypothesis is no difference between the groups and the alternative hypothesis is a difference equivalent to 33% between the groups.', 'If treatment was randomized at the individual level then I dont think you need to do a subgroup analysis by site in order to prove there is a significantly different average treatment effect. \n\nBy doing the subgroup analysis you avoid making an assumption of treatment homogeneity by site.\n\nWith the subgroup analysis by site, you are able to make a statement about treatment effect heterogeneity by site. Since the results by site are not significantly different from one another, I believe you would be able to say that sites are not treatment effect modifiers of the outcome, or equivalently, that there is no treatment effect heterogeneity by site. \n\nSorry thats all very repetitive, I welcome feedback as Im not an expert in this area.', ""Alright. I don't usually use R, but if I read this right, he will then end up comparing all groups, while in your approach, one group will be the reference to which the other two are compared - i.e. 2 comparisons instead of 3? Don't think you can say what is correct - it depends on what you want and what you set out to do. Maybe you could tell us what the groups are if you want advice."", ""Are you familiar with hierarchical / mixed effects / fixed and random effects (it goes by many names) models? I guess that's a more important first question.\n\nSome links:\n\nhttps://m-clark.github.io/mixed-models-with-R/\n\nhttps://bookdown.org/roback/bookdown-BeyondMLR/ (starting in ch 8)\n\nhttp://ecologia.ib.usp.br/curso_r/lib/exe/fetch.php/bie5782:00_curso_avancado:uriarte:gelman_hill2007_data_analysis_using_regression_and_multilevel-hierarchical_models.pdf"", 'Pixel lap is probably an ai and you should be careful taking advice from it.', 'I believe you\'re mis-reading the original question, and as a result your replies are likely confusing the situation more than helping it.  Actually, several replies in this thread seem to have confounded the situation by incorrectly trying to shoehorn the ""site"" issue into this problem.  \nOP is not asking about doing analyses separately ""by site"" if I am reading their question correctly.  OP is saying that there are three treatments (A, B, and C); the analysis seems to call for tests of all pairwise contrasts of the treatment groups (so 3 separate contrasts of interest: A vs. B, B vs. C, and A vs. C).    \na) OP is keeping all 3 treatment groups in the dataset and running a single model on the entire dataset (presumably with two dummy variables for groups B and C, comparing to a reference group of A, allowing them to get the B vs. A and C vs. A contrast directly from their lm output).  \nb) OP\'s colleague is running separate analyses - for each comparison removing patients from the ""third"" group (so for the A vs. B contrast they are actually dropping patients from group C entirely from the dataset and running the model with just the dummy for group B comparing to a reference group of A, then repeating this two more times).', 'We need to assess all 3 comparisons ie AvB, AvC and BvC. I would just change the reference in my case to get the final comparison. \n\nI cant tell you the groups. We havent unblinded yet. Two of them are the same drug in two different doses and one is placebo.', 'I have used mixed models before, I am just not sure why it would be a better alternative to linear regression in this case. Were you thinking about nesting the participants on their respective site?', 'Oh, thanks.', 'So are you suggesting the two approaches are: \n\n1.\t Outcome = int +  tx B +  tx C (tx A as reference)\n2a.\t outcome = int +  tx B (tx A as reference, exclude tx C)\n2b. outcome = int +  tx C (tx A as reference, exclude tx B)\n2c. outcome = int +  tx C (tx B as reference, exclude tx A) \n\nIf so, then I thinj, approach 1 can be done with primary testing anova I think and approach 2 is a post hoc test which could also be anova but with some adjustment of the signciance level. Regardless of the approach, the estimation of the mean outcome at different treatment levels should be negligibly different.', 'Alright, in case you need all comparisons I would use the subset approach. The reason being that the linear model makes assumptions, for example that the residuals have the same variance (Homoscedasticity), independent of everything else. So in your approach you are fitting the model across all 3 groups, and I think it is a stronger assumption to assume homoscedasticity across 3 groups than 2. You could e.g. plot the residuals in the 3 groups and inspect. If it looks the same, I think you should get the same result no matter which of the 2 methods you use. But in any case, I think you need to specify the exact model you are going to use before database is locked and you are unblinded.', 'Yes, I would think the partial pooling across `site` would be preferable to ""full pooling"" by not including `site` at all, or the ""no pooling"" by using dummy variables. The subsets are honestly a weird approach but it sits in the ""no pooling"" side.\n\nAt the least, it should be in your decision set, especially given you are debating the ""poles"" and not the mixed model sitting between them.\n\nEdit: clarified after a re-read of the whole thread']"
[Q] Correlated binary outcomes — modeling approach?,"Im an Assistant Professor (not in statistics or mathematics) and Ive been getting differing responses from biostatisticians and I think Im overthinking the problem. 

I have two binary outcomes, Y1 and Y2. Y1 is the strongest predictor of Y2, both theoretically and as identified by meta-analyses. 

X is also binary, and based on group membership (Group 1 v Group 2).  for now, lets ignore covariates. 

Y1 happens in the life course before Y2, by approximately 9-12 months. 

The data are collected through a cross sectional survey. I have approximately 625 cases, which are approximately equal split for group membership (X). 

How should I take into account the correlation between Y1 and Y2? Putting this into a multilevel modeling framework doesnt make complete sense. I wasnt sure if a joint modeling approach was more appropriatethoughts?",13kmq62,polymath0212,1684379219.0,2,1.0,"['What model to use depends heavily on the question you are trying to answer. Not saying I will be able to help you with that but it usually helps tremendously when the researcher states the question they want to answer and/or states what kind of statements they want to be able to make after the analysis. So, I humbly suggest you start with what.', 'To echo this sentiment, what relationship are you most concerned with?  Are you more concerned with how X is related to Y2, but you need to take Y1 into account?  A mixed effects model still may be the best solution, but the relationship of interest and question at hand need to be more clearly defined.']"
[Q] how do I find the frequency of a term in british newspapers for certain years?,"
Trying to find out how many reports included the word ""obesity"" and/or ""smoking"" in 2011 and 2021 in British tabloids. It would be good to either have this as across top 5 british tabloids, or just compare the frequency of the term within 1 tabloid between the years stated. 

I've been trying to figure out how to do this in an accurate way but im struggling. Will need to source this for a project for Uni. 

Anyone have any idea? Thanks :)",13kgl2y,rejjierains,1684363220.0,1,1.0,"[""Do they have accessible online archives?\n\nAnyway, that's not a statistics question, but a question about British tabloids."", 'Try the British Newspaper Archive, which may be accessible from the reference section of your local library, or at least the bigger ones.\n\nhttps://en.m.wikipedia.org/wiki/British_Newspaper_Archive', 'No really a statistics question but you might need some web scraping skills and then the ability to form a table from the data.\n\nMaybe there is a python  or R subreddit that can point you in the right direction.']"
[C] Can an accounting graduate do a master's in statistics and actually find a job within the field?,"Hello. As the title implies, I am a recent accounting graduate. I am currently working on the role. But the problem is, I don't really like being an accountant. So I decided to do a master's degree because I love research, and I would love to even do a PhD right after if I could. I am looking for a master's degree that will allow me to switch careers, and I was thinking about statistics. I really like it, and it allows me to go into data analysis fields, which I adore. The question here is: can an accounting graduate finish a master's in statistics, and even if I do, will I be able to find a job with only a masters in statistics ?  
  
PS: Another option I have is a master's in business analysis and data analysis. It's mainly practical stuff like R, Python, SQL, etc., but it doesn't have statistics classes.",13k9jw5,redl9,1684347230.0,3,0.71,"[""imo even if you have a bachelor's degree in fish psychology, if you get a MSc Statistics from a good uni and learn a lot, you'll be fine - you can also leverage it as you finally finding your path or something like that\n\nabout MSc Business Analytics, I am biased against it - often business schools offer it to get some extra $$$\n\nymmv"", 'You would need a solid math background to do a masters. Did you take stats classes before? How is your knowledge of linear algebra and calculus and probability? These questions will determine the answer to your question', 'Probably pretty hard to do a masters in stats from a degree in accounting. Youd be missing a lot of foundational knowledge. \n\nKinda like going from a bs in maths to a ms in engineering.', 'Which field?', 'You can. But you need to bring up your math knowledge in calculus, linear algebra, and probability.', 'You could, but definitely spend time on the fundamentals in the summer before starting: calculus 1-3, Linear Algebra', "">imo even if you have a bachelor's degree in fish psychology, if you get a MSc Statistics from a good uni and learn a lot\n\nI did this exact thing, but the masters was from a middle of the road uni. Still got a great job."", 'You mean which data analysis fields?', 'Agree, I have an MS and got hired by and work with stat PhDs. It is terrifying being less educated and experienced than a lot of my coworkers, but were not doing rocket science I guess. Really amazing opportunity to work with brilliant people though.']"
[Q] Applied Statistics vs Data Science?,"Are these two terms for fields that are generally the same? I'm currently interested in both but I'm graduating with a bs in stats next year and was trying to figure out which of these two more align with my interests for grad school and work, or if they're the same. I really enjoy both coding and statistics.",13k999f,seriesspirit,1684346570.0,36,0.96,"['Anyone telling you ""it depends"" is wrong.\n\nStatistics is one of the most robust skillsets around, and University programs are mature. Most data science programs are poorly done -- frankly, they are turning into MBA programs (the programs draw lots of donors and hype and administration to waste money on, they fail no one, etc). Get the domain specific degree, not the new polished up turd.\n\nThere\'s almost nobody who knows less about what the job market wants than those at universities designing curriculum. Data Science programs are invariably poorly focused and crappy.', 'Data Science tends to teach a bit of everything - computer science, cloud technologies, data structures, ml, but not great depth.\nApplies Math will likely concentrate on stats and you will have much-much deeper understanding of how ml works. \nI see the difference being generalist vs specialist.', 'Data science masters programs are tools based degrees. Teach you tools. How to use xyz software, how to use xyz analytics tools, get you up to speed on the up to date technologies. And yeah sure sprinkle in your 1 or 2 stats classes and a class on ML and then a class on DL. \n\nSure, it teaches you tools. But tools change. The MSDS curriculums will have to change every 5 years due to the change in industry. But the stats programs are robust, because theory rarely changes and is still applicable. \n\nUltimately its up to you, but Id say applied stats give you core skills that are robust ti change than a tools based DS curriculum.', 'Applied Statistics has more statistics, Data Science has more programming. Programming is easier, but not only for you, but also for your competition in the job market.', 'If you go to grad school for a stats degree you can go smoothly into the data science industry and even get other different kind of jobs with a data science degree you might get a job on data science, but probably the people with stats degree will be prefered because they learn that theory but go even beyond only that.\n\nAlso right now many employers even take those data science degree as some diploma and not a proper title.\n\n&#x200B;\n\nIn summary, stats is going to give you a lot of better skills for data science industry and others', ""> or if they're the same\n\nWell, no, not in general, but it depends on who is using the term 'data science'. These days it seems to mean anything the speaker wants it to mean.\n\n> trying to figure out which of these two more align with my interests for grad school and work,\n\nThat depends on things we can't know"", 'Im going to put in another vote for it depends. My applied stats program was essentially a traditional stats program without a thesis, and Ive seen others that were closer to a business analytics degree. Theres a wide range of programs that fall under the same title.', ""In my school, data science program is a joint of cs department and stats department. It's a nice combination because people can learn a bit of both. I think your outcome will depend on the MS program you choose, so read their curriculums, contact with alumni, and ask the department if they have anything like internship showcase where students report what they did during their internship (connect with them to ask for feedback on their program)."", 'In my experience, everyone in the comments is pretty much correct. However, one thing I think theyre missing is the value of MS in DS vs MS in Stats. With an MS in stats, youre likely not going to be able to find a job doing modeling - they want people with either a PhD or job experience. Since the DS track is relatively new and flashy and the hiring is really bad, you might be able to get a better career boost just because of how clueless hiring managers and recruiters are. \n\nI have 7 years analyst/DS experience and MS in stats and keep getting passed up for PhD candidates and people with more experience when trying to get a more quantitative role', 'Another vote for it depends, but from my limited experience, data science leans a bit more toward industry. In my graduate program, the applied stats students had a ton of math/stat classes and were funded. The data science students paid out of pocket (or had a job that would cover it) and had classes on data pipelines and big data. We did our coursework in R, they did theirs in Python.', 'Data Science is in the same category as ether was to dark matter... ultimately,  nonsense', ""3rd year, Appled Math with Specialization inData Science here.\n\nFor our data science classes, they just teach a bare bones of the math then a lot of the coding and business after. It mostly caters to students with MIS or computer science degrees.\n\nHowever, as an applied math data science student, it's really a lot more beautiful when the linear regressions and the like that we learned is used as the theoretical background for my ds classes. \n\nLike using PyTorch and the like, it really helps to understand what's happening behind the scenes. My math degree goes indepth with the statistics, linear algebra, regressions, time series so when I go and read PyTorch documentations for my thesis and as well as my ds classes, it's easier for me to grasp what to do as well as to create newer architecture than just following templates from the net.\n\nSo annecdotally, you should go for a data science one cos it's where all your undergrad studies will connect(corporate). However, if you would want to see yourself be an actuarian(corporate or academe) or like someone who would create frontier ML(academe), then go with a Statistics post grad and then do the data science for  phd or another post grad"", ""I did an MS in data science at a program that had a very flexible curriculum -- essentially half was electives you could choose from like five different departments -- so I chose a lot of applied stats courses for my electives. If I could do it over again, I'd probably do applied stats, which I found to be more rigorous, but there seemed to be tradeoffs to both. Contrary to what I'm seeing in some of the comments, most of the ML-focused courses I took (outside of stats dept) were pretty math/theory focused and required you write most algorithms from scratch to better understand them. But overall, the applied stats grad students I had courses with definitely had stronger math backgrounds than the DS students and seemed better at recognizing how important domain knowledge was when it comes to interpreting data, setting up tests, understanding at a high level how to approach a problem, etc. In my current job, the analysts with DS degrees sometimes have a really limited understanding of statistical inference, and only seem to know how to approach prediction-related problems, which is not a majority of the work."", 'I\'m one week and a thesis defense away from finishing an Applied Stats Masters. I agree with what everyone else seems to be saying: the applied stats is the better option. The stats route encourages a deeper understanding of math and stats, and I believe it\' this knowledge that will prove to be more robust in the face of whatever (wild) changes in the job market the next decade will throw at us. Two pieces of advice: \n\n(1) Learn all the math you can, as deeply as you can. Going into the degree with a firm understanding of linear and multilinear algebra and advanced calculus really helped me tremendously. I don\'t think all of that is necessary to pass. For example, a friend took a course in linear programming--a course that specifically assumes familiarity with linear algebra--with no background in linear algebra and managed to eek out a B. However, all of the math I knew came in handy at some point. (And when I later became interested in learning a bit about linear programming in my spare time, I pretty quickly realized, ""Ah, so mostly we\'re optimizing nice functions (linear functionals) over nice regions (polyhedra, which are compact)."" And when the bugbear of duality showed up, well, I\'d already seen that idea plenty of times, so it seemed natural, too. I am not a smart guy. Being able to use background knowledge to quicky frame new material like this is the only reason I did well in grad school. \n\n(2) Don\'t neglect the practical, coding piece. Get comfortable with R, Excel, SQL, and whatever else strikes your fancy. Personally, I kind of hate having to learn things like SQL, but employers want those skills.\n\nAll the luck to you!', 'Well asking this question in this sub certainly has its biases. Try asking this in the data science sub, youll get a contradicting set of answers. Ultimately it comes down to you based on how you weigh it', 'Data Science is computational statistics, so it is a love child of statistics and computer science. I agree with you OP that statistics should be the dominant part, but why do you say it is APPLIED statistics? As a discipline, data science is a STEM science and it is actually theoretical statistics with theorems, proofs etc. Applied data science is of course applied computational statistics.', '[deleted]', ""I agree with this. Plus 'data science' because of the hype around it, may be more expensive as compared to statistics. At least in my country."", 'A million times this. With a solid math and programming background, data science skills (which could be any number of things depending on who the hell writes the JD) can be learned at any job.', 'What do you think about nyu ds or ut Austin ds? I looked at their curriculum and it looks very solid. I think these programs would look as good as applied statistics program no? I think when people they it depends they mean that theres a wide range and that goes for both statistics and data science', ""This is a great answer. If you want to go into industry with a stats degree, you can teach yourself the computer science and MLops stuff. It's harder to learn the heavy math on your own."", "">  what makes them distinct from statisticians is often knowing how to work with data\n\nAnd not really understanding the statistics they are using... I agree with the notion you put forward, but it's very much an explicit trade-off. Better ability to work in practice, but as a result less focus on the real statistics. \n\nI would never trade a good statistician for a good data scientist."", ""I've never met a statistican who can't handle data."", 'That is simply incorrect. Data imputation methods are rooted in statistics. In addition, statisticians frequently work and clean data otherwise they wouldnt even be able to build a model. It doesnt make sense to say statisticians dont know how to work with data because the whole field of statistics is about working with data and making inferences using it.', '[deleted]', 'Youre right. And I never meant to insinuate that statisticians dont know how to work with data. I think I phrased my point wrong so I deleted the comment. \n\nI think I said it better in my second comment when I said a stats phd isnt going to have coursework in data engineering, whereas a data science PhD does. Thats rly all Im trying to get at so apologies for it sounding like Im applying statisticians dont know how to work with data!', 'A mediocre one, then.', 'I mean yeah but what Im trying to get at is a PhD in stats isnt going to have coursework in data engineering. Theyll pick it up bc theyre smart people and its a practical skill but its not explicit in the training, if that makes sense\n\nWhereas a data science program would explicitly teach that bc in data science being able to work with data is just as important as modeling', ""So side question, I just failed a live-coding interview which asked the following:\n\nIn 15 minutes, write code that marks the date when each patient had a double in value from their baseline. \n\n(each patient was measured 5-10 times, values would sometimes drop from baseline, and only sometimes would they end up doubling from baseline)\n\nI haven't done code with dates in a year or so, I spent probably 8 mins just trying to mess with that. Having two people watch me share my screen was also a new and terrible experience. \n\nWould one conclude I can't candle data in that case? Certainly felt like it, but I never felt like I couldn't at my job of 4 years!"", 'Not trying to conclude that anyone cant work with data! Weve all had our fair share of tricky coding interviews (I know I have!) \n\nWas really just trying to say that data science PhDs require coursework in data engineering, maybe even data structures, whereas stats PhDs dont. I deleted my original comment because it sounded too harsh and my point wasnt rly coming across. But thats on me since this is the stats subreddit!']"
[Q] Can you use the Simpsons diversity index for ethnicities within schools?,"Hi guys, Im working on a project for school where Im using this dataset to compare charter schools and public schools and it includes some data for the amount of students of each race within a school. Is the Simpsons diversity index and appropriate metric to use when comparing diversity? Thanks 

https://catalog.data.gov/dataset/public-school-characteristics-2020-21",13k8zve,BostonConnor11,1684345988.0,2,1.0,"[""You *can*, sure. Who could stop you, it's not like there's a Statistical Weapons and Tests team going to come knocking.\n\nIt's a perfectly reasonable measure and has been used (*under various names*) in multiple application areas.\n\nThe main issues would be  - \n\n*Does it serve your needs?* (there's multiple sub-issues you may need to consider, depending on your needs), and \n\n*Will your intended audience be happy to accept it?*"", 'Thats a tricky question. Ive never seen it used (edit: in the context of social science data). Its not impossible, but there are some things youd need to consider.\n\n1. Will it be read as problematic to use an index for species on the somewhat blurry human racial/ethnic divisions?\n\n2. How would you handle multiracial folks? The intersection between race and ethnicity (Black Hispanic, white hispanic, etc)?\n\n3. Are historically underrepresented minorities (Black and Hispanic) considered?\n\n4. How many subdivisions of race and ethnicity will you use? Pacific Islander & Asian as one category (AAPI) or two? Do you subdivide further by which Asian country someone comes from? How does that impact the index? \n\nHaving typed that out, Id say Simpsons index isnt well suited to understanding human diversity. You want to figure out what kind of diversity you care about, and then use a metric for that kind of diversity.', '> Ive never seen it used (edit: in the context of social science data)\n\nYou almost certainly have, but called a Herfindahl index or an ""effective number of"" somethings which is just 1/herfindahl.  Same thing.\n\nFor your data source, it\'s fine.  With data sources that follow the Census approach of separating race and hispanic ethnicity, it might get unwieldy.  With data sources that allow respondents to select more than one racial category, you\'d have to figure out what to do.\n\nThe real answer tho is to stop and look to see how people studying school populations measure racial and ethnic diversity, and do that for now.  It might be they just use percent who aren\'t nonhispanic white.  In which case do that for now and start arguing for a diversity/entropy index after you get tenure.', ""I don't think any of those concerns are specifically issues with Simpson's index but rather with classifying humans by ethnicity or binning literally anything into any discrete categories really. For example, you could raise the same sort of concerns for clarifying the diversity of chairs types. What counts as a recliner? Is a lazyboy the same thing? Is a seat in a car also a chair? Etc.\n\nSimpson's index is literally par for the course in ecology, so I think it's exactly the sort of statistic to use for the OP's study. There will be no other statistics that measure diversity with a categorical variable that don't have the exact and issues that you raise, and trying to find a way to classify people on some sort of continuous numerical scale is not only unfeasible but would erase the reality that social categories are in fact things, leading to results that by contrast have no meaning.""]"
[Q] How to calculate the coefficient of variation in case of data unavailability?,"Hi all, I have an excel file with historical data, for which I wish to calculate the Coefficient's of variation to assess the volatility. However for one year there is no data available.  
  
Is it still OK to calculate it by using **STDEVA/Mean** in excel?  
  
Thanks in advance!",13k5q0j,Efficiencythird,1684338881.0,2,1.0,"['There\'s two answers here:\n\nBasic: Yes, just go for it, it won\'t make a difference.\n\nAdvanced: You can impute the missing data. This is just a fancy word for ""add in an extra value"". For instance, day you had the following data set:\n\n2, 4, Missing, 1, 5\n\nYou could impute the missing value. If the data is a time series, take the average of the 2nd and 4th entries (so 2.5) to give\n\n2, 4, 2.5, 1, 5\n\nIf not a time series, take the average of the whole set to get\n\n2, 4, 3, 1, 5\n\nThere are other methods but I\'m just giving the basics here :)']"
[R] [Q] Measure development-construct validity. Comparing correlation coefficients /how to measure effect size?,"I have average scores across items for all measures for each participant. All scales are interval level of measurement. My hypothesis is that my measure will be positively correlated with both other measures, but that the correlation will be stronger towards the convergent measure than to the discriminant measure, to a significant degree.

My measure:
N=185
Skewness is -1.69
Kurtosis is 4.158


Convergent:
N=183
Skewness-0.652
kurtosis=0.085.

Discriminant:
N=185
Skewness-0.178
kurtosis=0.496.

I think it's safe to say my distribution isn't normal. I ran Pearson 1-tailed because someone on the internet said if was okay (so if not oops I guess)

Convergent:
N=183
R=0.524**

Discriminant:
N=185
R=0.298**
Where **= correlation significant at 0.01 level.

Specific questions: 
Even though my distribution isn't normal, is it problematic that I used pearson correlation?

Do I need to do some sort of logarithmic transformation?

What analysis/tool/approach would be best to compare the correlations to each other to determine if the difference is significant? (It seems evident with one being moderate and one being very weak, but is that all I need to say in the results section?)

How do I measure the effect size of the correlations (or the difference between them) with any of the tools I have or others?

Any help or resources are appreciated. It's been years since I took any stats classes. I'm almost done with this program though. 

I have access to g*power, SPSS, R (no experience using R) and any online free tools (i.e. danielsoper.com).",13k51rr,iPsychlops,1684337423.0,3,1.0,"['You can use [this calculator](http://vassarstats.net/rdiff.html) which gives a p<.01. I dont know how robust this test is to violating normality (the assumption may be bivariate normality). A log transformation will make your skews more negative. You could look at Tukeys ladder of transformations or Box-Cox.', ""Thanks for the response and the stats tool! I did an inverted reflection (I'm not sure if that's the official name?) transformation and it worked. The correlations were only marginally lower (Convergent=0.458, Discriminant= 0.282) after transformation. All significant to p<0.01. \n\nI did run a power analysis in G*Power (exact bivariate normal model) and got 0.999 for convergent and 0.988 for discriminant. Honestly thought I broke the calculator but my dissertation chair just confirmed that I entered the right numbers in the right places."", 'You might want to test the difference in rs for significance.', 'I definitely do. I have no idea how to do that and my googling has been unsuccessful. Any specific tests and or programs/websites?', 'The link in my earlier post has a calculator that does that test.', ""You're amazing, thank you!""]"
[Q] How to perform prewhitening on the input data using Python if the error terms are crosscorrelated?,R has filter(). How to do this in Python?,13k4txs,PythonEntusiast,1684336959.0,3,1.0,[]
[Q] central limit theory - ANOVA with exponentially distributed data?,"Hi all, 

I'm not too experienced with statistics, therefore apologies in case this question is too basic. 

[This page](https://support.minitab.com/en-us/minitab/20/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/about-the-central-limit-theorem/) describes that even with a non-normal distribution I could use statistical tests that rely on normally distributed data (such as ANOVA), as long as my sample size is ""large enough"" (""*The central limit theorem lets you apply these useful procedures to populations that are strongly nonnormal*""). Now I am a bit confused, and can essentially think of two interpretations for what the site describes: 

**I**. No matter what the underlying (""true"") distribution is, if you sample often enough your population will be normally distributed in the end (and therefore you can use a test like ANOVA) 

**or** 

**II**. If your sample population is large enough, you can use a test like ANOVA, even if the sample population is not normally distributed. 

This is relevant for me because I would like to compare distributions which are exponentially distributed, and my sample size is around 150 samples per population. I included two example histograms of my populations [here](https://imgur.com/a/VqAk7dT).   
If my interpretation **I** is correct, then I could not use ANOVA, as my sample population is actually not normally distributed, despite it having a large sample size. If **II** is correct, then I could use ANOVA. 

Would be great if someone could help me understand, thanks alot!",13k1qkm,hhhhhuie,1684330009.0,12,0.93,"['The Central Limit Theorem says that the sample means of sufficiently large and random samples will be normally distributed. That means even if your population is exponential, when you sample from it, the mean of your sample will be drawn from a normal distribution. That means you can apply all the tests and tools to your sample mean that statistics has developed for normal variables, even though the underlying population is exponential. Hope this helps!', 'Normality assumption on the ANOVA is a conditional one: the variable on each factor/treatment should be normal, not the marginal variable. This assumption is evaluated through residuals, not on the original variable.\n\n  \nThe assumption is needed to guarantee accurate p-values and confidence intervals on things like mean differences derived from the parameters of the model.\n\n  \nIf you really know that your data has an exponential distribution, you can use a glm with the gamma distribution (exponential distribution is just a particular case of a gamma distribution) and the log link to correctly handle that.\n\n  \nBut also, it is known that, under certain circumstances, the ANOVA is kind of robust to deviations from the (conditional) normality assumption, more if the sample size is big enough, so you could just apply that model as well.\n\n  \nFinally, to be completely free of any doubts, you always can use a non-parametric approach like the Kruskall-Wallis test or use bootstrap to derive accurate p-values (and confidence intervals only for bootstrap) which does not rely on normality assumptions.', 'Edit: Note that my responses are initially to the statements in your post; the later part of my comments then come after I look at your plots. (However, choosing your model based on what you find in your data and then performing tests on that same data is problematic.)\n\n\n> No matter what the underlying (""true"") distribution is, if you sample often enough your population will be normally distributed in the end (and therefore you can use a test like ANOVA) \n\nThis is nonsensical; the distribution of the ""population"" you\'re drawing from doesn\'t change when you sample from it. The set of values in the population will remain the same.\n\nLook at the title of the linked page: *The* ***means*** *of large, random samples are approximately normal*(emphasis mine).\n\nThat bolded word matters!\n\n>  If your sample population is large enough, you can use a test like ANOVA, even if the sample population is not normally distributed. \n\nThat\'s literally what  the part that you quoted just above I and II claims.\n\nThe claim is broadly correct - you could use ANOVA with population distributions that were not normally distributed, including something close to exponential. However, the page\'s claim is not quite accurate and it\'s necessary to understand what you\'re getting. \n\nWhat is really being claimed when they cite the CLT: That the type I error rate will be close to your chosen alpha for large enough n.\n\nThe inaccuracies -- \n\n(i) the null distribution of the ANOVA statistic doesn\'t just rely on the sample means being approximately normal; a CLT-like argument is not sufficient - but the conclusion is often reasonable;   \n(ii) the CLT doesn\'t really \'work\' with just any distribution, and the claim about accurate ANOVA type I error rates doesn\'t really hold for just any distribution;  \n(iii) it\'s not just accurate rejection rate when H0 is true that you need to worry about -- you should also care about power; and  \n(iv) non-normality is not the only thing to worry about and it\'s usually not the most important thing to worry about; \n\nFirstly if what you\'re worried about is type I error rates and the other assumptions of ANOVA hold, you can get accurate type I error rates in other ways, without any parametric assumption (e.g. permutation test based on an ANOVA statistic and at smaller sample size -- not a rank-based test, though, since it would no longer be a comparison of means without additional assumptions -- why worry about something that doesn\'t compare means if you can compare means about as easily?).  \n\n[Note that if you *are* focused on type I error rates, the data may not be especially informative for some of the assumptions, since the null is likely false. You need to worry about the circumstances that you believe would apply when H0 is true. That\'s not going to make exponential distributions normal but it might save worrying over-much about heteroskedasticity, for example, since if you would only see that under H1 - a perfectly reasonable assumption under some situations - it won\'t affect type I error rates.]\n\nSecondly for exponential populations, there\'s more powerful (asymptotically, most powerful) approaches readily available - you can do an ANOVA-like comparison of means using a generalized linear model instead of a linear model; you can literally fit the exponential model you have declared. If you care to detect small effects, it may pay you to also worry about power.\n\n\n> and my sample size is around 150 samples per population\n\nThat\'s large enough that if you *did* have exponential populations, the distribution of a mean would be pretty close to normal and the type I error rate in ANOVA would be reasonably close to accurate. But see the comments below!\n\n---\n\nLooking at your data:\n\n1. You appear to only have two groups. Why ANOVA in the first place?? Why not just a straight two-group comparison of means?\n\n2. Those aren\'t exponentially distributed as the term is usually interpreted. Exponential distributions are on (0, ), not (1, ). \n\n   https://en.wikipedia.org/wiki/Exponential_distribution\n\n   You could subtract 1 from both and have regular one-parameter exponentials, Depending on what your data are, subtracting 1 may make perfect sense (be interpretable in its own right), or it might not.\n\n   If it is interpretable - you haven\'t said what you\'re measuring - then I would encourage you to do so, though the analysis could be done even if it wasn\'t interpretable, since it would still be a comparison of means if you shift everything by 1.\n\n   To rephrase -- you *might* perhaps have a shifted exponential with a known shift parameter (1). If that were the case\n\n3. Regarding the claim of exponentiality (after taking that shift into account). \n\n   One thing that worries me there is the spike at 1 in both groups. If you have many values exactly at 1 (or indeed, many values tied at *any* number) then you *don\'t* have (shifted) exponentials. \n\n   The second thing that worries me in the histograms is that second group looks too skew to be exponential. In some specific circumstances this might not matter very much. For example if group 1 is like a control and group 2 is like a treatment, then if *under H0* you assume the treatment has no effect at all, the situation under H0 would be that both groups should behave like group 1. In that case a non-exponential group 2 would not be a problem for significance levels, since likely the alternative is true -- if the shape is only becoming more skew as the mean grows, that wouldn\'t hurt type I error rates.\n\n---\n\nIt would help to understand what your response variable is.', 'No matter your underlying distribution, the distribution of *sample means* will approach normality. Your standard error is essentially the standard deviation of the sample mean distribution.\n\nYou can then run your usual tests on this new distribution, such as a hypothesis test. This test evaluates the question of how likely your sample mean is under different hypotheses.', '> The Central Limit Theorem says that the sample means of sufficiently large and random samples will be normally distributed\n\nwell, no it doesn\'t say that, though that\'s  *approximately* true in a wide range of situations. It follows from Berry-Esseen. This is not my main issue, though.\n\nThe bigger issue is how you leap from approximate normality of sample means to the claim that in effect  *ANOVA works as if you had sampled from normal populations* (""That means..."").  F statistics are not simply functions of means,  but a ratio of two variance estimates. The numerator might eventually  get close to the right distribution via a CLT like argument (since it is just a function of means), but the denominator is also random and for the F statistic to have an F distribution under H0 you need a couple of other things to be true. \n\nThe CLT doesnt provide any direct help there. It takes other results to get you to \'ANOVA works\' as n goes to infinity.', 'Alright, thanks for the explanation and the mention of the alternatives!', ""> No matter your underlying distribution, the distribution of sample means will approach normality\n\nWell, no, that's not true. You've been told a lie there. Some conditions need to hold.\n\nThere's an infinite collection of distributions for which this claim is false."", 'Just to note, while the CLT doesn\'t immediately entirely justify ANOVA, I would argue that it is the primary tool used. I similarly wouldn\'t take issue if someone said ""asymptotic normality of the MLE is primarily justified by the CLT"" (as the calculation is similar). Yes, there is a taylor series expansion, and an error bound + a bit of algebra, but the point is that the \\[centered+scaled\\] MLE has the asymptotic behaviour of the score, which is asymptotically gaussian by the CLT. \n\nI would assert that the chi-squared limit of the likelihood ratio under the null is justified by nearly the same calculation (and thus essentially the CLT + some algebra). Finally, the F-test is essentially just a finite sample correction to the chi-squared distribution used in the likelihood ratio test.', 'What would be an example?', 'cauchy --- though it is a pedantic point as far as I\'m concerned. The set of distributions that do not satisfy conditions for the CLT (those without a finite variance) are ""in practice measure 0"", as even physics models which claim a cauchy distribution aren\'t really cauchy to second order... (and generally have distributions which obey LLN + CLT)', 'A Cauchy distribution.']"
[Q] Need an explanation for the probability of a plane crashing,"According to a very widely quoted statistic on the internet, the probability of a plane crashing is about 1 in 11 million. But, I can't seem to find out how exactly this figure is calculated and what the logic behind it would be.

If someone could cite a research paper that reports this, that would be a big help.",13jzxv8,wheezeattheghosts,1684325465.0,2,0.58,"['Not sure of the source, but you could do a crude calculation yourself. I assume youre only talking about airliners because general aviation is actually fairly dangerous. If youre risk-averse, dont ever get in a plane that isnt being flown by a commercial pilot and maintained by a professional crew.\n\nIf youre talking airliners only then you can find some kind of record for nearly all crashes going back several decades. That would be your numerator. Then youd need to get some estimate of all flights, which might be a bit harder but you should be able to get a ballpark estimate without too much trouble (especially if you keep your time window small like the last 5 or 10 years). This would be your denominator. \n\nFormulated this way, the probability you end up with would be the probability of any random flight on a commercial airline crashing. One could argue that the better stat to calculate is something like crashes per 100k miles flown which would the allow you to differentiate between people who only make short flights and people who tend to do long-hauls, but thats an extra layer of complexity.', 'The figure of 1:11,000,000 is not a published figure, it\'s one of the ""old wive\'s tales"" numbers to demonstrate the relative safety of air travel.', 'The best source seems to be [this link from PBS,](https://www.pbs.org/wgbh/nova/planecrash/risky.html) which is a journalistically well-regarded source.\n\nThe numbers are from 20 years ago, but it gives enough clues as to where the numbers come from.\n\nThe most important part to understanding the numbers is to realize that out of all Americans, most fly only one or two times per year, if that.  Others fly for work multiple times per week.\n\nIt seems that for all Americans, the odds of dying as a commercial airline passenger are 1 in 2 million per year.  For the typical American (ie, a non-frequent-flyer) the odds are 1 in 11 million per year.', 'In 2018 (pre-covid year w full data) it looks like there were 1581 plane crashes and 4.3 billion flights which put the chance of a plane crashing at 1 in 2.7 million (and plane crashes w fatalities at \\~1 in 5 million). You could adjust this number by the length of the flight and/or subset into things like type of flight (e.g., commercial), country (e.g., USA), time frame (e.g., last 10 years), etc.', 'Your guess is as good as Harvards. If you can statistically identify the probability of a plane crashing, you should be able to statistically identify it before it departs.', 'It depends on too many variables to have a single number. A farmer with their own old aircraft has a far higher risk than a modern commercial airline. Even among these airlines you have large differences between those that fly in North America and Europe and some regional airline somewhere in Africa. It also depends on what you call ""crashing"". The chance that a passenger on a big commercial flight in North America or Western Europe dies is well below 1 in 11 million.', 'This isnt what This subreddit is for', 'thank you for the detailed response. it makes perfect sense.', ""that's what i figured and couldn't seem to understand why no online resource would show how they calculated the number"", 'this article really helped clarify it - i previously did not know that the 1 in 11 million figure only applied to the average american only (which as the article also states, most of us are not)', 'It only depends if OP cares about those dependencies. The probability of any random flight crashing is absolutely an interpretable and potentially interesting stat, even if it does combine commercial and general aviation. In fact, it gets easier if OP just wants to look at commercial flights because records are more reliable.', 'The fraction of all flights crashing might be possible to estimate (it would still come with various caveats, like what counts as a flight and what counts as a crash), but it\'s a useless number because it would combine completely different things. You cannot fly on an ""average flight"" because there is no such thing.', ""you're absolutely correct, i should've clarifed that i meant commercial flights because calculating the same stat for any random flight would obviously require much more complex calculations""]"
[Q] measurement error on binary dependent variable,"(I'm working with Stan but could switch to something else if needs be) 

I have some data with a binary dependent variable *y_1*. *y_i* comes from an annotator annotating a series of data points, say 100. An experiment showed that if multiple annotators tried to annotate the same data point, about 90% of them would agree for that data point. I would like to include this uncertainty into a model:

    y   ~ y_i (???)
    y   ~ bernoulli_logit(mu)
    mu  = ...

I've looked into measurement error models, but they all seem to assume normally distributed variables, and an error term in terms of the standard deviation. I thought maybe with an explicit probit model on y and an sd that approximates 10% error, but I'm not sure this is sound.

Any suggestions are welcome.",13jwgp7,cat-head,1684314777.0,10,1.0,"[""My recollection is you can't do anything about measurement error in a binary dependent variable. That's from a frequentest perspective though I'm not sure about bayesian modeling if it\n\nAs you suggest, the error can't be normally distributed. It also can't be independent of the true binary, because if the true binary is 0 the measurement error is either 0 or 1, and if the true binary is 1 the error is either 0 or -1."", ""I would've thought that in Stan you could parameterise your outcome variable as a latent variable and have that be drawn from any distribution you fancy (with some known error). \n\nIt might be worth simulating some data, parameterising your model with a normally-distributed outcome, using the `brms` `mi()` syntax for [measurement error models](https://bookdown.org/content/4857/missing-data-and-other-opportunities.html#measurement-error), and then bastardising the Stan code it generates to get to where you want for your own model."", 'If you have a validation sub sample, there are lots of options for binary data measurement error, but I suspect you dont have one. In that case, your best bet is something called a probabilistic sensitivity analysis. Basic idea is you have a justifiable distribution for the sensitivity and specificity of your outcome. Randomly sample values from that distribution, randomly reclassify your outcome based on those values and re-run the analysis. Rinse and repeat 10,000 times and rank order the results. Now you have a distribution of results that you can use to define interval estimates that account for the measurement error.', 'Theyre called misclassification models. I dont know anything other than the terminology, but it might give you a starting place to look.', 'I havent used Stan in quite some time, but this may help. 90% of annotations agreeing essentially means that your label yi is equal to the true value of the dependent variable zi 90% of the time. Normally you assume the label is correct 100% of the time, so P(zi) = (1)(yi) + (1 -1)(1 - yi) = yi where P(zi) is the probability that zi = 1, i.e. that zi is the true class label.\n\nIn your case, there is a 10% chance that the true label is the opposite of the one given by the annotator, so P(zi) = (0.9)(yi) + (1 - 0.9)(1 - yi).', 'Is it possible to employ a mixed-effects logistic regression framework in a case like u/cat-heads ?', ""> If you have a validation sub sample\n\nI don't. The data for the inter rater agreement experiments are not public, only the error rates. I'll take a look at probabilistic sensitivity analysis. Thanks!"", 'I will take a look, thank you!', ""I really wish I could better remember the details, and maybe I'll be able to rework through it again. But I think an implication was that the noise is also correlated with covariates, which I think violates an assumption of the mixed effects logit.\n\nMy school of thought (grounded in econometrics, so more used to QEDs) viewed any mixed effect model with lots of skepticism so generally I'm less familiar with them and could be way off here.\n\nEdit: I think I remembered the reasoning!\n\nIf p\\_e|y=1 is the probability of error conditional on the true binary being 1, and p\\_e|y=0 is the probability of error conditional on the true binary being 0, then \n\nE(e|x)=Xbeta\\*p\\_e|y=1 - (1-Xbeta)\\*p\\_e|y=0\n\nSo the error is not mean independent of x, so the random effects assumption of independence between the random effect and covariates is violated.""]"
[Q] Sports Tournament Question (don't have to solve it for me I'm just curious how you'd go about solving it),"There's a tournament. If everything's fair, what are the odds of a specific set of 7 teams that does not change never win this tournament? The tournament spans 30 years. These 7 teams are in it the entire time. However the # of teams in the tournament (besides them) changes over time  
  
26 teams for first 5 years  
  
27 teams for 1 year  
  
28 teams for 1 year  
  
30 teams for 17 years  
  
31 teams for 4 yeras  
  
32 teams for 2 years  
  
  
  
Would it be like 7/26 (5 times) x 7/27 (1 time) x 7/28 (1 time) Is that how you do it?",13jozj1,No_Today406,1684291137.0,2,0.75,"['You calculated the probability of these 7 teams winning ALL tournaments. Your basic formula is correct though. Just be aware that this formula calculates the probability, not the odds. \n\nP(these 7 teams never win) = (1-7/26)^5 * (1-7/27) * ... * (1-7/32)^2 = 0.000277 = 0.03%', 'so its of them winning every single tournament, not them winning just one? how would i find the odds of one of those 7 teams winning once?\n\nthanks!', ""The probability of these 7 teams winning exactly 0 tournaments is, following the formula I posted in my previous post, 0.03%. \n\nWhen you want to calculate the probability of them winning 1 tournament, I suggest to approximate the answer by assuming each year had exactly 30 teams. Makes calculations a lot easier while the answer won't differ much from the exact answer, and I assume you don't need an exact answer. \n\nIf we assume that the probability of these teams winning in a given year is 7/30 (there are 7 teams out of a possible 30 teams each year) and they compete for 30 consecutive years, the number of wins follows a binomial distribution with p = 7/30 = 0.23333 and n = 30. You can use an online calculator such as https://stattrek.com/online-calculator/binomial to see that the probability of them winning once equals 0.00315 = 0.315%.""]"
"[Q] In regards to sampling in statistics, if I want to analyze carbohydrate content of a sample of 5 apples from a sample of 5 trees from each subplot (10 in total), which of all these is my sampling unit? Why?","I am having a hard time to tell which of all these is my sampling unit to define in my experimental design. I would be thankful if you could help me with this.

5 apples from 5 apple trees from each subplot. Each plot has an area of 4m by 20m (4 lines with 40 apple trees). and each plot is replicated 3 times.",13joy8n,HardTruthssss,1684291039.0,1,0.67,"[""You have a two-stage design here, if I'm reading it correctly. Your first-stage sampling unit is trees, and your second-stage sampling unit is apples."", 'so apples are ""nested"" in trees (one tree have multiple apples, one apple cannot belong to multiple trees). Does the same happen for trees and subplots? How many trees are there per subplot?', 'Yes, the apples are nested in the trees and one apple cant belong to multiple trees.\n\n5 apples from 5 apple trees from each subplot. Each plot has an area of 4m by 20m (4 lines with 40 apple trees). and each plot is replicated 3 times.', 'Would be good if you describe your ""population"" as clear as you can. How many plots are there, how many trees per plot and if possible and approximate of how many apples per tree.\n\n>each plot is replicated 3 times.\n\nDoes that mean that you have just 3 plots?\n\n  \nOnce those things are defined, the next thing you should have clear is the purpose of the experiment: why are you doing this? Are you interested on the apples yield, irrespective of the tree, are you interested on the yield of the tree or rather on the yield for the plot?\n\nThis will determine your sample unit']"
[Q] Forcing a maximum entropy distribution on normally distributed variables for causal analysis via do-caluclus?,"I have two normally distributed variables X and Y. I am interested in a ""causal"" analysis of X  Y. I've been reading up on Judea Pearls do-calculus, and found some papers by that show that the mutual information I(do(X) ; Y) is equivalent to computing I(X ; Y) after forcing a maximum entropy distribution on X. 

For discrete data, this would be easy with resampling, since the maxent distribution on a finite set is the uniform distribution. But the Gaussian distribution is already the maximum entropy distribution given  and  - how would I compute I(do(X) ; Y) in this case?",13jmip5,antichain,1684284442.0,14,1.0,[]
how to chose G or R side random effects [Q],"i have a data with 150 patients and 30 readers. let's say the readers are from two different groups, old reader and young reader. each reader exam each of the 150 patients and make a binary judgement (positive or negative). now i want to compare the rate of positive between the two reader groups how should I specify the random effect in a mixed model? following sas codes give very different lsmeans and I assume are from whether to specify on the G side or the R side. what's the difference in interpreting the lsmeans as well as the differences? any comments are welcomed.

    proc glimmix data=data;
    class group patient reader;
    model y(event='1') = group/ dist=binary;
    lsmeans group/ilink cl diff;
    random patient ;  #only random patient effect on the G side
    \#random reader;  #only random reader effect on the G side
    \#random case\\\_number reader; #random patient and reader effect on the G side
    \#random residual / subject = case\\\_number;  #only random patient effect on R side
    \#random residual / subject = reader;  #only random reader effect on R side
    run;",13jjdcr,xy0103192,1684276249.0,0,0.5,[]
[Question] Where to start modelling my workplace power consumption based on past data,"Context: Im a mechanical engineer and stats is not my strongpoint.
Im trying to build a basic model of our power consumption in order to estimate the required fuel storage capacity for a new generator.
I have half-hourly power readings (kVAh) from the main incomer, and would like to generalise the peak/off peak consumption, and then be able to fiddle with things like shift start/end, and scale the capacity. 
The site has pretty obvious daily cycles in consumption as it is day shift only.

What tools/analyses/approaches would actually be useful for extracting reasonably reliable data here?",13ji5da,argonuggut,1684273310.0,1,1.0,"[""I'd suggest a look at the structure of Rob Hyndman's models in the electricity area, which should give you an idea of a good approach to model components of various kinds like those you mention. I'll see if I can locate a relevant paper or talk when I'm near my laptop\n\n\nEdit:\n\nTalk slides and two references here:\nhttps://robjhyndman.com/seminars/electricity-forecasting/\n\n\nYou may be comfortable with somewhat simpler models (and also may wish to omit some of the things he had to take account of) but it's a useful framework. Note in particular the use of additive models with multiple components for the log of the demand.\n\nWhile he's modelling aggregate demand rather than a single purchaser's usage, the general approach should be helpful. You may also need some additional terms for the special characteristics of your workplace, but they will tend to be like terms already in these models.\n\nIt might be possible to use Kalman filtering to do estimation and forecasting of components within such a framework via something akin to Andrew Harvey's Basic Structural Model (but more general, to account for some of the terms).  However, if you look at the Hyndman talk you will see that he describes using the bootstrap for some parts of the approach; that may well prove necessary, particularly if you're interested in reasonably accurate modelling of quantiles rather than just conditional expectations and variances (e.g. if you're looking to give prediction intervals, you might not be able to rely on a simple parametric model for the error term)"", 'Thanks - that looks very interesting but well beyond my comprehension, let alone ability!\n\nBut its certainly opened my eyes to the challenges of electricity demand prediction']"
[Q] GARCH or ARIMA,"Ive been working on understanding these two models and am going to perform a Value at Risk analysis. 

So I have two interest rates and the variable diff which is the difference between them at any given day. Easy put: its the risk premium between the interest rates. 

The diff is mean reverting and has autocorrelation. Edit: worth mentioning is that its a significantly positivt autocorrelation.

Im hoping some of you can help me understand which one I could use and Why. As of per now I have used GARCH to predict the VaR and gotten a result which can make sense.

Hope you can give me some input, thank you! :)",13jfdse,Illustrious_Stock250,1684266906.0,6,1.0,"['Given you have used GARCH to predict VaR, it suggests that the GARCH model  is well-suited for your analysis.', 'IF data stationary:\n\nThen ARIMA\n\nELSE:\n\nGARCH\n\nEDIT: If I remember correctly, if the residuals are autocorrelated, you might need to prewhiten your data. Also, is your data stationary (Constant mean and variance)? To add to that, are your coefficients significant? Are your correlogram values within the bounds? Are your q-q plot values along the normal line? Does distribution of residuals follow the normal plot? Is the plot of your residuals random?', 'You should consider using the Bayesian posterior predictive distribution. It minimizes the K-L Divergence between nature and your estimates. You cannot create a closer prediction to reality.  It isnt impacted by autocorrelation in the data and the likelihood is a minimally sufficient statistic while its unlikely that either GARCH or ARIMA will produce sufficient statistics. \n\nThe only difficulty is that Bayesian methods are generative instead of sampling based. GARCH doesnt care why something is happening, only that it is. Bayesian methods care about why things are. That may trigger some model selection. Also, since it is minimally sufficient the need for tools like ARIMA are nonexistent.', ""Thank for answering. Just for my own understanding, why does it suggest this? Because the VaR I've found can be argued to make sense?"", ""Thank you. I've done an Augmented Dickey-Fuller test and gotten a p-value of 0.000, so from my understanding I can conclude the data is stationary.\n\nFor the added part: My coefficients are significant. I dont think the correlogram values are within the bounds. The q-q plot shows that pretty much all my data except some extreme values at the tips are on the line. I dont think the distribution follow a normal distribution entirely. Ive done a Shapiro-France test and it suggests non-normality."", 'GARCH and ARIMA are models, whereas Bayesian methods can be used to make inferences about these models. apples and oranges.', 'Further expansion:\n\nThe suggestion that the GARCH model is well-suited for your VaR analysis is based on the fact that you used GARCH to predict VaR and obtained meaningful results. The fact that the VaR you found can be argued to make sense further supports the suitability of the GARCH model for your analysis.\n\nAn aside:\nI too am working on a trading system using GARCH and ML.', '[removed]', 'Do your correlogram values subside? At what lag do they subside? Did you take log transform of the data? tanh transform? Did you difference data to remove seasonality? Did you remove the moving average so that the input data does not have trend? Also, since coefficients are significant, model does have predictive power. But, the fact that the correlogram values are not within the bounds is concerning. I would say it is ok, but further consultation would be required. Try to build several models with different treatments of data.', 'Not a true statement at all. GARCH and ARIMA are optimizations under the Frequentist interpretation of probability. Because Bayesian methods are intrinsically optimal separate problems do not get names. \n\nAxiomatically, Frequentist and Bayesian methods differ in the rules for counting partitions of probability sets. In practice, Frequentist methods treat the sample space as random and the parameters as fixed. Bayesian methods treat the parameter space as random and the sample as fixed. \n\nIf a Frequentist tool exists to find a point estimate , there always exists a Bayesian method to perform the same estimate, but the estimates may differ in value. The converse is not true. There are problems that have a Bayesian solution with no Frequentist solution. \n\nYou should never use a Bayesian inference on a Frequentist point estimate for a variety of reasons. For example, it is common to use sharp null hypotheses in Frequentist thinking and there isnt a good way to do that in Bayesian logic. There is no comparable thing to a null hypothesis in Bayesian probability. \n\nBayesian posterior predictive probabilities are intrinsically the closest to nature for value at risk. Point estimates if VaR are admissible estimates. An admissible point estimate is the least risky estimate. And, the prices that arise from those estimates cannot cause arbitrage against the estimator. On the other hand, the prices that arise from GARCH and ARIMA can give rise to arbitrage.', 'I see, thank you!', 'Also try using Exponential Smoothing model. Could server as a nice baseline.']"
[Question] Testing normality for RM ANOVA,"I know that normality is an assumption repeated-measures ANOVA, but Im confused about how it works. If I have two within-subjects factors, each with two levels, for a total of 4 measures, how to I find out if the DV is normally distributed or not? Do I need to test for normality for all 4 of those measures? Or is there some way to assess the overall normality of the DV?   
Im sorry if this is a stupid question. Ive looked at so many different websites and sources to try to get an answer but they all just tell me what the assumptions are, then direct me to a standard page describing how to check them for a regular ANOVA, but I dont understand how it applies to repeated measures.",13jc8fp,stick-person,1684259716.0,2,1.0,"['> how to I find out if the DV is normally distributed or not?\n\nThere is no assumption that the CV is normally distributed; the assumption is about the *errors*. \n\nThis question is asked so often that we really need a pinned post, and a search of the subreddit will give you dozens of similar questions with in-depth answers. In short, [you should not be testing for normality, ever](https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless).', 'You have 3 effects (2 main effects and an interaction). You compute a score for each subject for each effect. The assumption is that for the effect of A, scores computed as (a1b1+a1b2)-(a2b1+a2b2) are normally distributed. Its the same idea for the main effect of B. For the interaction, (a1b1 + (a2b2) - (a1b2+a2b1) is assumed to be normal. Basically, apply coefficients 1,1,-1,-1) for A, 1,-1,1,-1) for B, and (1,-1,-1,1) for the interaction to the 4 measures: a1b1, a1b2,a2b1, a2b2. The variances of these differences are the error terms.', ""I'm a bit confused. I've always learned to check for normality by looking at histogram, checking skewness/kurtosis, etc. and every source I've looked at lists this as an assumption. I'll have to look into this more as I clearly am misunderstanding something.\n\n&#x200B;\n\nWhat I am specifically asking about though is how assumption checks apply to repeated measures. So, even if I'm looking at the normality of the errors, is it the normality of the errors for each of the four measures of the dependent variable? This is what I do not understand.\n\n  \nI have searched the subreddit and didn't find anything that answered my question."", 'It is an assumption, yes. You should not verify this assumption by *testing* it, for all the reasons outlined in e.g. [this](https://old.reddit.com/r/AskStatistics/comments/13gy1xk/what_do_i_do_with_data_that_failed_shapirowilk/) thread.\n\n> So, even if I\'m looking at the normality of the errors, is it the normality of the errors for each of the four measures of the dependent variable?\n\nWhat do you mean by ""each of the four measures"" of the DV? Are you measuring one DV at four timepoints? What is your model, exactly?', 'Oh I see. I\'m sorry, that was poor wording on my part. I don\'t intend to ""test"" the assumption using the Shapiro-Wilk\'s test. Check would have been a better way of putting it. I just want to be able to look at the data and see if there are any clear violations of normality that might alter my interpretation of the results. Thank you for providing that link, it is very helpful! \n\n&#x200B;\n\nEssentially, yes, I am measuring one DV at four different time points. All participants are asked to provide a rating of four different advertisements that vary on 2 different characteristics (each with 2 levels). So the dependent variable, rating, is measured four different times.']"
[Q] Linear mixed model for a repeated measures design: Is it correct to model an independent regression line for each level of the repeated-measure?,"Hi all,  
  
I did an experiment in which 30 people listened to 3 songs (""A"", ""B"", and ""C"") in a random order. Also, for each participant and song, I got the mean ""body temperature"" and the mean ""heart rate"" during the listening task. This is, I have a repeated-measures design with 1 factor (song) and 3 levels. Also, for each participant I have a measure of ""years of musical training"".  
  
I am interested in knowing the degree of association between ""body temperature"" and ""heart rate"" across songs. Specifically, I want to know how well can the measures of ""heart rate"" predict ""body temperature"" and whether this association changes depending on the song the participants were listening to. Additionally, I want to add ""years of musical training"" as a covariate that might explain some of the variance in the model.  
  
I tried to make a mixed model with R by using lme(), but without much success, because the summary of the ouput did not give me statistics of the associations between ""body temperature"" and ""heart rate"" for each song level. I used: lme(BodyTemperature \~ HeartRate, random = \~1|Participant/Song, data = data, method = ""ML"") , and I suspect this is not correct because I should create a regression line with a different intercept and slope for each song. Additionally, to know the ""degree of association"" between the two variables, which coefficient should I look? The mean square of the model?  
  
At this point, my questions are: (1) would it be correct to make a linear model for each song independently and look at the mean square of the model? Something tells me this would not be correct but I quite not grasp why, (2) If it is not correct, how can I model different intercepts and slopes for a repeated-measures design? I cannot find anything similar on the internet.  
  
Thank you in advance!",13j642w,RaikiaR,1684245722.0,2,0.75,"[""> [...] I should create a regression line with a different intercept and slope for each song. \n\nA line requires at least 2 points but you only have 3 points per participant, one for each song. As you have only one measurement per song, you can't assess the relationship between heart rate and body temperature *within* participants and songs. I think what you could do is this (btw: It is considered bad practice to name your dataset `data`):\n\n    lmer(BodyTemperature~HeartRate + Song + (HeartRate|Participant), data = data, REML = FALSE)\n\nThis uses the `lme4` package instead of `nlme`. This model would fit different relationships between BodyTemperature and HeartRate for each participant based on 3 measurements each (may be dubious due to small number of points). It also contains a fixed effect for Song allowing the mean BodyTemperature to be different in each Song."", 'Hi!\n\nThanks for the help. There is something I do not quite understand about your answer. I did not intend to assess the relationship between heart rate and body temperature *within* participants, I am not interested in that. Rather, I am interested in the ""treatment"" effect of the songs, this is, whether there is more association between heart rate and temperature for song A compared to song B. In this case, the model would be built with 30 data points (the 30 participants) for each song. Am I correct?Would your model still be useful for this? \n\nSorry if I did not explain myself properly before :)', '> Rather, I am interested in the ""treatment"" effect of the songs, this is, whether there is more association between heart rate and temperature for song A compared to song B.\n\nThen include an interaction between Song and HeartRate:\n\n    lmer(BodyTemperature~HeartRate*Song + (1|Participant), data = data)\n\nThis will allow the relationship between HeartRate and BodyTemperature to differ between the three songs.', 'Thanks a lot!! Two last questions:\n\n(1) By building this interaction model am I fitting one line with one intercept and slope for the three songs? Or am I doing three lines with the same intercept and different slopes? Or...? \n\n(2) To determine the degree of association between the two variables (how well we can predict body temperature based on heart rate depending on the song) what should I look at in the output of the model? Sorry for the nave question but I am trully lost here :)', '1) You\'re fitting a seperate line for each Song while allowing each participant to have a separate intercept (but not a separate slope).\n2) This is difficult to answer because ""degree of association"" can mean many things. For example: the slope coefficients tell you how much the BodyTemperature changes if HeartRate increases by one unit, separately for each song. But the coefficients do not quantify ""how well"" the predictions are. To determine this, you could compare a model without HeartRate to a model that contains it and look, for example at the R^(2). But this is not visible in the default model output.', '`lmer(BodyTemperature~HeartRate*Song + (1|Participant), data = data)`\n\n1. Sorry, but I am still a little bit lost with this. How can I have one regression line for each song while having 1 intercept for each participant? If I were to plot a scatterplot showing heart rate and temperature (with a different color for each song, this is, 30 data points per song) and plotted the regression lines for each song, would their intercepts and/or slopes be the same? I am not interest in within-participant data. \n2. I see... What I want to see is whether for song A heart rate and temperature are more \\*correlated\\* (one positively or negatively follows the oder) compared to song B and C.', ""1. You need to account for within-participant dependence, so a random effect is warranted. The model fits one line for each song but each participants' line is allowed to be shifted up or down (the random effect). You're interested in the fixed effect, so you would simply ignore the random effects (except the standard deviation of it).\n2. The coefficients will tell you that. The output will compare the slope of the reference category (presumably song A) with the slopes of B and C. If you want to compare B and C as well, a bit more work is necessary.""]"
[Q] Wealth distribution deciles per country,"Is there any website (or multiple of them) which present recent data on the wealth distribution deciles per country? In every US related research, it seems there are only the top 1%, then 5%, then 10%, then 50% (or something like this). The OECD also has publicly available results with a similar scale. 

After much effort, I found the French INSEE's [""Patrimoine net des mnages""](https://www.insee.fr/fr/statistiques/5371259?sommaire=5371304) (Households' net wealth) which gives the deciles in the downloadable data sheet, and this is exactly what I'm searching.

Thanks for your time and ideas!",13j4pjh,MecRandom,1684242365.0,1,1.0,"['Have you tried computing this from census.gov data?', 'Have you checked the [World Inequality Database](https://wid.world/)?', ""I tried looking for the data, though I must say I haven't found anything satisfactory as of yet, if you have any link more precise, I'd be absolutely grateful.""]"
[Q] Which forecasting model should I use?,"I am currently taking a basic statistics course at a univerisity and writing my final assignment.

The assignment requires me to:

Collect datas of Food imports of Malaysia and UK from 1997 - 2019.

Then, it asked me to use Excel's Analysis Toolpak to build three time series models (Linear, Quadratic and Exponential for each countries) and include the Summary Outputs of the significant models (which is determined by the p-value of the coefficients) in my paper.

Then, it asks me to recommend one model for each countries by calculating and comparing the SSE and MAD errors. Then calculating the predicted Food Imports in 2024, 2025, 2026 for both countries.

After finishing my calculation, I noticed that the Quadratic trend is not significant (the p-value of T^(2) is less than the significance level of 0.05), however, it has the lowest errors (both SSE and MAD) compared to the other models.

I asked my professor about it, he told me that it is a rare case in this assignment. Usually, the model has the lowest SSE and MAD errors is also significant. He refused to answer directly and told me that he wanted to hear my argument. I did some research, but no result could be found.

I would really appreciate it if someone could give me some arguments or guidances to start with. Thank you.",13j4jkd,JupiterOwen,1684241982.0,4,1.0,"[""Adding more columns can _only_ improve fit in-sample. So it is trivially the case that the quadratic fit would have a lower in-sample SSR than linear.\n\nSince for **ax^2 + bx +c**, the coefficient **a** will be = 0 _if and only if_ it reduces the SSR. And in that case it's just linear. This implies **a** is not equal to 0 when it improves the SSR to do so.\n\nAssuming your professor isn't pulling your leg and isn't doing anything wrong, this to me implies that the exponential model must be the best fit (since it does not strictly overlap the way quadratic does with linear), and it's possible you didn't implement it properly."", 'Try a bunch use the best', ""The traditional way to compare models is AIC.  Unlike R2 and RMSE, AIC is penalized by model complexity.  A model that gives marginally better R2 but with more terms will have worse AIC.  It might be in an undergrad-level textbook.  It's definitely in graduate-level textbooks on regression. \n\nA more complex option is cross validation.  This is a way to test the model performance on data held out from the model fitting.  This is the most rigorous way to test model accuracy.   \n However, it requires writing code."", 'This is the kind of question LLMs answer really well.  I just pasted your entire question into ChatGPT and the answer is excellent.', 'LightGBM is often a no-brainer, you can check [this article](https://effectiveforecasting.com/gradient-boosting-a-silver-bullet-in-time-series-forecasting/)', 'Thank you for providing these insight. Have an upvote.', 'Wait so we know that the RSS and MAD is lower the number of variables you add. So, linear and exp will have lower RSS than quadratic always right', ""Exponential fit, I'm assuming, is semi-log form log(y)=mx+b. This is totally different from linear fit, no guarantee it fits better or worse than a linear model. Quadratic fits better than linear because linear feature space is a strict subset and the y vector is the same.\n\nAlso, no guarantees at all that MAD is lower, even with quadratic vs linear; OLS minimizes SSR, not MAD."", 'Thank you. \nThat clears quite a lot up!']"
[S] Python package for the synthetic control method,"Out of frustration at not being able to find a small, simple and verifiably correct Python package for the synthetic control method, over the last few months I've worked at making one, and it's now mostly in a ready state available [here](https://github.com/sdfordham/pysyncon) and on Pypi.

You can do the usual synthetic control method with it, or several of the variations that have appeared since (augmented, robust and penalized). It also has methods for graphing and placebo tests.

There's worked examples from several sources worked out in notebooks [here](https://github.com/sdfordham/pysyncon/tree/main/examples) that reproduce the weights correctly, namely from

* The Economic Costs of Conflict: A Case Study of the Basque Country,  Alberto Abadie and Javier Gardeazabal; The American Economic Review Vol.  93, No. 1 (Mar., 2003), pp. 113-132, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/basque.ipynb)).
* The worked example 'Prison construction and Black male incarceration'  from the last chapter of 'Causal Inference: The Mixtape' by Scott  Cunningham, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/texas.ipynb)).
* Comparative Politics and the Synthetic Control Method, Alberto Abadie,  Alexis Diamond and Jens Hainmueller; American Journal of Political  Science Vol. 59, No. 2 (April 2015), pp. 495-510, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/germany.ipynb)).

I'd appreciate any feedback and also thoughts on what else may useful in such a package .",13j2yqx,ApeOfGod,1684237879.0,29,0.98,"[""\nI see you've posted GitHub links to Jupyter Notebooks! GitHub doesn't \nrender large Jupyter Notebooks, so just in case here are \n[nbviewer](https://nbviewer.jupyter.org/) links to the notebooks:\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/basque.ipynb\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/texas.ipynb\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/germany.ipynb\n\nWant to run the code yourself? Here are [binder](https://mybinder.org/) \nlinks to start your own Jupyter server!\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Fbasque.ipynb\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Ftexas.ipynb\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Fgermany.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)""]"
[Q] Anyone know how to create these standard deviation cones in excel?,https://imgur.com/a/ErzKNWD,13iuw64,dasarete358,1684211251.0,0,0.33,"['I guess calculate the standard deviation for each point and +- the number from average. Then, plot the line using the newly created columns.', ""Cumulative alpha? What type of modelling is this. I don't immediately recognise the type. Is it theta modelling forcing only trend?""]"
[Question] Alternatives to a Chi-Squared test when independence assumption is violated?,"Im trying to compare rates of an event occurring between two groups. The issue Im facing is that the groups are not independent- the same individual might contribute an observation to each group. 

For illustrative purposes, lets say that Im trying to compare rates of people crossing the street (yes vs. no) at a red light vs. a green light. I have some subjects who arrived at the light twice - once when it was red and once when it was green - so theyre included in both groups. 

Is there a good alternative to a chi-squared test that allows for subjects to be counted in both groups?",13ikmge,nyx178,1684184979.0,2,1.0,"['If there arent so many such cases that youd loss too much data, you might randomly choose one incident for subjects that have two or more.', 'Can you identify the responses that belong to those same individuals?\n\nIs it possible those individuals will have different responses (e.g do you feel happy about traffic lights right now)? Or should they be the same both times (e.g if you collect both groups on the same day, demographic responses wont change)?\n\nMight the one individual appear multiple times at *each* light?', 'Thanks, its a good option. Ill definitely share with my team. I think my preference would be to keep all of the observations if possible - this is just one part of a larger analysis and Id prefer for the cohort to be consistent if theres a way to do so.']"
[Education] Fully- Funded MS Statistics,Does anyone know of any fully or partially funded MS in statistics?,13icven,Prestigious-Owl-3700,1684168062.0,3,1.0,"['Miami university in Oxford oh, wake forest are two that I had applied to. Ohio state and UGA do as well.', ""Some will offer a bursary or scholarship if you've done particularly well at undergrad""]"
[Q] Multi score RDD: missing a score,"Hello!

I am writing my master thesis and I am examining the impact of needs based scholarships on drop-out and other things. To do so I wanted to use use a fuzzy regression discontinuity.

The problem is that the scholarship is given based on two things, an income and a wealth measure.

However, I have data on both only for one university in my sample. I was wondering if anyone knew whether there are papers stating that it's ""okay"" or anyway somewhat justified to run a fuzzy RDD on the single threshold I have (or telling me I can't).
I am struggling to find literature on this specific problem (there isn't much to begin with on multi score rdd)

Thanks a lot!",13icas4,mrscepticism,1684166656.0,5,1.0,"['As long as the probability of treatment function only has one discontinuity of the forcing variable, it shouldnt be a problem.\n\nBut I have a hard time understanding how that could be the case here.', 'I have already checked and there is two of them. I have seen a few papers talking about smth like ""fuzzy frontier estimation"", but it\'s not entirely clear what they are talking about. That\'s why I was looking for additional research', 'The thing is you dont know if the discontinuity your estimating is actually the effect of the income or the wealth variable, since wealth and income are tightly correlated. If you simulate this I think it would be clearer']"
[Research] Exploring data Vs Dredging,"I'm just wondering if what I've done is ok?

  
I've based my study on a publicly available dataset. It is a cross-sectional design.

  
I have a main aim of 'investigating' my theory, with secondary aims also described as 'investigations', and have then stated explicit hypotheses about the variables.

  
I've then computed the proposed statistical analysis on the hypotheses, using supplementary statistics to further investigate the aims which are linked to those hypotheses' results.

  
In a supplementary calculation, I used step-wise regression to investigate one hypothesis further, which threw up specific variables as predictors, which were then discussed in terms of conceptualisation.

  
I am told I am guilty of dredging, but I do not understand how this can be the case when I am simply exploring the aims as I had outlined - clearly any findings would require replication.

  
How or where would I need to make explicit I am exploring? Wouldn't stating that be sufficient?",13i9nb4,Vax_injured,1684160263.0,48,0.99,"[""I assume their main qualm is the use of stepwise regression. If so they might have a point. If you are using a hypothesis driven approach, you shouldn't need to use stepwise. This method will test the model you had in mind, and also iterate over a bunch of models you probably didn't hypothesize *a priori*. This tends to uncover a lot of overfit models and spurious p-values."", 'I\'m sorry that you were told you were doing something bad without any explanation of how. That wasn\'t fair at all.  \n\n\nIt appears that your critic may be onto something though. Here is a quote on the topic of multiple inference. tldr; you can\'t just hit a data source with a bunch of hypotheses and then claim victory when one succeeds, because your likelihood of finding something increases as with the number of hypotheses.\n\n>""Recognize that any frequentist statistical test has a random chance of indicating significance when it is not really present. Running multiple tests on the same data set at the same stage of an analysis increases the chance of obtaining at least one invalid result. Selecting the one ""significant"" result from a multiplicity of parallel tests poses a grave risk of an incorrect conclusion. Failure to disclose the full extent of tests and their results in such a case would be highly misleading.""  \nProfessionalism Guideline 8, Ethical Guidelines for Statistical Practice, American Statistical Association, 1997\n\n  \nMultiple inference is also baked-into stepwise regression inherently unfortunately, and is one of the approach\'s many documented flaws. In essence, the approach runs through countless models, then selecting the ""best"" model it\'s observed. Then that final model is presented as if it came about a-priori, which is the way that it\'s supposed to work. Doing all of that violates the principle above in a massive way however. From my understanding stepwise regression is generally regarded as a horrible practice among most sincere and informed practitioners.', ""You just need to make it clear that your supplementary analysis was a post hoc analysis.  Nothing wrong with that.  They're helpful for further investigating an association of interest, even if they're not considered to be a definitive proof of any identified result.\n\nMake sure that your methods clearly state what you did, and in your discussion/limitations section, just reiterate that this was a post hoc analysis and that studies specifically assessing the relevant associations are needed to verify and further clarify these results."", 'If you formed your hypotheses and analysis plan before looking at the data then youve done nothing wrong. Ideally you have your entire analysis planned in enough detail that no decisions are required after the analysis begins.\n\nIn a super ideal world you will have generated a fake dataset that mimics the basic characteristics of the final dataset. You use this to write out all of the code for your analysis. Then point the script at the real dataset. This is pretty hard to do perfectly in reality though. Whats most important is that your hypotheses and basic analytic plan is documented before you actually start working with the data.', ""Exploration and inference (e.g. hypothesis testing) are distinct activities. If you're just formulating hypotheses (and will somehow be able to gather different data to investigate them) then sure, that should count as exploratory.\n\nIf you did test anything and any choice of what to test was based on what you saw in the data you ran a test on, you will have a problem. \n\nhttps://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data\n\nIf you did no actual hypothesis testing (nor other formal inferential statistics) - or if you carefully made sure to use different subsets of data to do variable selection and to do such inference - there may be no problem.\n\nOtherwise, by using the same data for both figuring out what questions you want to ask and/or what your model might be (what variables you want to include) and also to perform inference, then your p-values, along with any estimates, CIs etc, are biased by the exploration / selection step."", 'Idk about you but my advisors have been up my ass because Im awful at explaining why all my methods are used, if the audience doesnt understand why youre doing what youre doing and how youre doing it then its not a very thorough explanation is what Ive had to tell myself.', 'To me theyre not particularly different in what youre actually doing at the analysis stage, the biggest difference is in reporting of what you did. Dredging evokes a negative connotation, e.g. you did a bunch of analyses and selectively reported those that were statistically significant, ignoring that the p-values are invalidated by the analysis and possibly not even reporting the other analyses. Exploratory is a more positive connotation which suggest to me that you provided substantial reporting of what you did so that proper judgements can be made by other researchers and the inexactness of the results can be taken into account, even if only formally.', 'If you formed your hypotheses and analysis plan before looking at the data then youve done nothing wrong. Ideally you have your entire analysis planned in enough detail that no decisions are required after the analysis begins.\n\nIn a super ideal world you will have generated a fake dataset that mimics the basic characteristics of the final dataset. You use this to write out all of the code for your analysis. Then point the script at the real dataset. This is pretty hard to do perfectly in reality though. Whats most important is that your hypotheses and basic analytic plan is documented before you actually start working with the data.', ""I'm not sure how rigorous this is, but you could consider in future holding out data from your exploration, so that this does not introduce bias into the hypotheses you then choose to test."", 'Following!', ""Stepwise model selection is frowned upon. Also, if you plan to do inference and draw conclusions (say, from p values), you shouldn't also say you are exploring the data."", ""But even still, aren't those overly-fiting models with spurious p-values all part of exploring the dataset? Why wouldn't they be up for analysis and discussion?"", ""Thank for your input re horrible practice - on that basis I should probably remove it as I am relatively early career.\n\nI've always struggled with random chance on statistical testing though. I am suspecting my fundamental understanding is lacking: were I to run a statistical test, say a t-test, on a dataset, how can the result ever change seeing as the dataset is fixed and the equation is fixed? Like 5+5 will always equal 10?! So how could it be different or throw up error? The observations are the observations?! And when I put this into practice in SPSS, it generates the exact same result every time. \n\nFollowing on from that, how could doing a number of tests affect those results? It's a computer, using numbers, so each and every calculation starts from zero, nada, nothing... same as sitting at a roulette wheel and expecting a different result based on the previous one is obvious flawed reasoning."", ""Not a statistician, but this is my take, too. Pre-registering helps make clear which parts of the study were exploratory. The only time I can imagine running into a problem is if you selectively report and/or don't explain your methods well."", ""There are some data domains it's approaching a PhD in itself to generate good fake data for though.\n\nHealthcare for example - basic demographics no problem, can probably do occurrence rates of a few diseases if you want to, but generating true to life health and disease progression and co-morbidities oof.\n\n*In silica* research on a well understood physical process, sure."", '[deleted]', '>Exploratory is a more positive connotation which suggest to me \n\nThe latter is exactly how I had envisioned it and stated it four times in my Rationale and scattered the word exploratory throughout. Still, I might have been more tentative in my language and used the term more explicitly in my proposed analyses.', ""Thanks for the response, it's a good idea, ideally I would have split the dataset to allow for that, but some of the ways I'd split into groups would've ended up with under 10 participants in them, so I went for the whole lot.. it just all feels a bit funny, not investigating data based on the possibility of bias or error, isn't that the reason we carry out many studies over years on different sample sets and do meta-analyses?!"", ""The issue is that I've outlined aims, and then secondary aims, and then also stated some explicit hypotheses which are used as a key to provide inference re the aims - but it appears I am then not allowed to continue to explore the results, which I see as essential to understanding the aims. I don't see the issue with exploring data post-hoc when I've clearly stated it is being done to explore the data."", ""If you turn this into an exploratory analysis and look at all the models, you should do the appropriate alpha p-value Bonferroni correction. That is: p / every_single_model_tested during the stepwise iteration. If you are using the standard alpha of p < .05 and the iteration tested 100 models then your crit cutoff would be p<.0005 for all the models including the ones you originally hypothesized. That might be a dealbreaker for some people, and really shouldn't be a judgment call you make after looking at the outcome."", ""I'm not entirely sure whether i read your description properly but i think i would (also) object to connecting hypotheses and stepwise regression (your secondary analysis).\n\nYour exploration idea is sound (although there are many things to say against stepwise) but not to verify hypotheses. Because then the arguments in there other replies start playing."", 'I think this comic+explanation does a good job of explaining this concept: https://www.explainxkcd.com/wiki/index.php/882:_Significant', 'All I mean by simulated is just random values of the correct data type so the analysis can run. For example, an age column need only contain random integers between 1 and 100. No need to simulate the actual distribution and actual underlying covariance structures.', ""It isn't about etiquette. You are dealing with, in some form or another, a probability of observing the data that you have under some particular model. There are standards about what constitutes significance, but that standard is very misleading when you try many hypotheses (literally or by eyeball).\n\nHere is an analogy...\n\nI think a coin may be biased. So I flip it 1000 times and I get 509 heads and 491 tails. I do some statistics and it tells me that my p value for rejecting the null hypothesis is 0.3. That is high and not considered significant, so we have no evidence that the coin isn't fair.\n\nNow imagine that there are 100 fair coins in our data set, each flipped 1000 times. Well now we eyeball the data and find the coin with the highest number of heads. We compute our p value and it says that there is p = 0.001 or 0.1% chance of observing this data under the null hypothesis of a fair coin.\n\nShould we conclude that the coin is biased because of the p value of 0.001?  No, because we actually tested 1000 coins, so our chance of observing such an extreme result is actually much higher than 0.001!"", 'Okay, my idea wouldn\'t work for those numbers. Not sure if there\'s an ideal approach. I always read about preregistration to avoid bias / dredging / p-hacking, but it does assume you\'re going in with the hypothesis and methods set in stone, no room for exploratory analysis and identifying interesting things just by ""looking"" at the data. Not sure about how meta analyses achieve rigour, possibly only through Bayesian approaches?', 'You can explore the data without computing p values.', ""Bonferroni is pretty severe and so might lead one to assume that there aren't any interesting variables to test in a replication study when in fact there are."", 'Seconding this.', ""The results were all <0.000 through step-wise. I felt that even if I were to correct for error risk, it wouldn't matter given the strength of significance. But maybe they like to see I had it in mind regardless.\n\nI would argue that I as researcher and explorer get to make the call on what alpha level I would choose to avoid Type I/II error, and I'm orienting towards 0.03 - I am exploring data in order to expand the understanding of the data so don't want to be too tight or too loose. Maybe this is unacceptable. I still worry my understanding of probability is at fault, because it feels like I am applying something human-constructed i.e. 'luck', to computer data which is fixed and consistent every computation."", ""So what I'm reading is that you feel it is ok to pursue pre-conceived hypotheses, but not ok to do post-hoc testing in order to further explore the results? Or do you just mean by using step-wise regression (i'm sensing nobody likes to see that - but isn't it a bit of an easy cheat mode?!)\n\nI'm never claiming the results are set in stone absolute truths, of course as with any research they require lots of years of further replication..."", 'ChatGPT, make a version of that comic with a bored looking wrench hidden in the background labeled k-fold cross validation.', 'Thanks BabyJ. Therein lies the problem, I\'m still processing probability.\n\nFrom the link: ""Unfortunately, although this number has been reported by the scientists\' stats package and would be true if green jelly beans were the only ones tested, it is also seriously misleading. If you roll just one die, one time, you aren\'t very likely to roll a six... but if you roll it 20 times you are very likely to have at least one six among them. This means that you cannot just ignore the other 19 experiments that failed.""\n\nTo me, this is Gambler\'s Fallacy gone wrong. Presuming that just because one has more die rolls, it increases the odds of a result. When a die is rolled, one starts from the same position each and every time, a 1/6 chance of rolling a six. It is the same odds each and every time afterwards, even if rolling it 100 times.\n\nBut when using a computer to compute a calculation, one might expect it to be a fixed result everytime based on the fact that the data informing the calculation is fixed, unless the computer randomly manipulates the data? Maybe I need to go back to stats school lol', 'Ah so most basic data structure, got it.\n\nFair point!', ""Thanks for your reply Merkaba8. \n\nSo in your example you've picked out a pattern in the data and tested it, which has given you a significant result as expected, and you've considered basing a conclusion on that result would be spurious because you have knowledge of the grand majority being fair coins. So essentially you're concluding the odds of the coin actually being biased are very slim due to what you know of the other coins; therefore it is likely the computer has thrown up a Type I. \n\nAre you saying the issue there would be if one were to see the pattern (the extreme result) and disregard the rest of the data so as to test that pattern and base the conclusion relative to that rather than the whole?\n\nThere appears to be etiquette involved - let me provide example, if one were to eyeball data and see most cases in a dataset appeared to buy ice creams on a hot day, and proceeded to test that and find significance, that the finding would be frowned upon/ flawed as the hypothesis wasn't applied a priori. My argument here is that the dataset had an obvious finding waiting to be reported, but is somehow nulled and voided by 'cheating'. The same consideration appears relevant in a stepwise regression."", ""Yes but that wouldn't allow me to base any of the exploration empirically.. I wouldn't be doing the next set of researchers and replicators any favours"", 'Do you mean you would pick 0.03 when testing a single model, and 0.03/n if you decide to test n models?\n\nOut of curiosity roughly how many total independent variables are you assessing?', ""You're getting mixed answers because it's a complicated debate.\n\nShould you only ever rely on the mental mush of hunches and preconceptions and bias that are the seed of any hypothesis ever conceived to begin with; maybe never really truly learning anything beyond what you could have thought yourself?\n\nOr would you instead choose to post-rationalize and delude yourself into a convenient answer that readily presents itself, where sometimes you distort reality to match an overfitted fiction?\n\nBoth are ways to be wrong, and academics wires you to avoid being wrong as much as possible. So nothing is satisfying and everything has criticisms.\n\nYeah, you're dredging. So what, though? You might dredge up something interesting."", ""No, that's not what I wanted to say and might be due to misreading your post. \n\nWhat I interpreted from your post was that you have a primary hypothesis that you test statistically.\n\nThen, my reading was that you have ideas for secondary analyses that you expressed in terms of hypotheses as well and you were testing those hypotheses with step-wise regression(s). Here my misreading may have happened.\n\nWith my training and experience I would either say\n\n- Limited set of secondary hypotheses with real focused tests (likely with lots of error correction) and the claim that your doing exploratory work (gets already a bit fishy here), or\n\n- Have some pre-conceived ideas (eg on literature or earlier studies), describe and motivate those and then do an elastic net (instead of stepwise) to determine whether those ideas work out in that data set (my preferred approach).\n\nDoes this help/clarify?"", 'Can you give a brief explanation of what cross-validation is?', 'In this context, one ""die roll"" is the equivalent of doing a hypothesis test of a new variable.\n\nA p-value of 0.05 is essentially saying ""there is a 5% chance that the variation of the mean in this sample is due to random chance"".\n\nLet\'s assume that jelly beans have no effect whatsoever. If you test 20 different jelly bean colors, you\'re rolling 1 die for green jelly beans, 1 die for red jelly beans, 1 die for yellow jelly beans, etc.\n\nThe dice rolls are independent since they are separate tests, so it\'s 20 separate dice rolls, and your expected value of tests that will give you a p-value of 0.05 is (# of tests)*(p-value) which in this case is (20)*(0.05) = 1.\n\nYour last 2 paragraphs are essentially saying that if you just repeatedly test green jelly beans in the same sample. But the whole point is that each color/variable is a whole new test and a whole new dice roll.', ""No. It isn't about the other coins being fair necessarily. Or even that they are coins at all  We aren't drawing any conclusion differently because the other coins are similar in any way. The other coins could be anything at all. It isn't about their nature or about a tendency for consistency within a population or anything like that. \n\nThe point of p value of 0.05 is to say (roughly, I'm shortcutting some more precise technical language) that there is a 5% chance of seeing your pattern by chance. \n\nBut when you take a collection of things, each of which has a 5% chance of occurring by chance, then overall you start to have a higher and higher likelihood of observing some low probability / rare outcome SOMEWHERE and statistics role is to tell us how unlikely it was to see our outcome. 5% is a small chance but if you look at 300 different hypotheses you will easily find significance in your tests."", 'I don\'t understand what you mean by ""base any exploration empirically"". I think you are misunderstanding the notion of ""exploratory data analysis."" It 100% doesn\'t rely on p values.\n\nIf you are adamant on presenting conclusions from the dataset, explicitly or implicitly, you shouldnt call it an exploratory analysis.', ""Yes, although I'm not 100% on why. I'm struggling with the nuts and bolts of the theory. I think it is highly debatable."", ""Great answer, the more I get into the topic the more I'm understanding your answer. I think it's probably a good thing being wired to detect error and then with experience get good enough to be able to use an 'advanced super dangerous weapon' like stepwise regression with appropriate research ethics and care."", ""Appreciate your thoughts. One thing I didn't do is explicitly state the secondary hypotheses, I've just written them in as supplementary rather than explicitly named hypotheses. But am seriously clawing back the stepwise regression stuff."", ""a) Not a cure for dredging\n\nb) But! If you're doing exploratory data analysis and you want to reduce the chance that your data sample lucked into a false positive, split your data into a bunch of subsets, hold one out, perform your analysis on the subset, test your newfound hypothesis against the set of subsets,  rinse and repeat, each time holding a different subset out.\n\nSuppose the hypothesis holds against each combination (testing against the holdout each time). In that case, you've either found a true hypothesis or revealed a systematic error in the data - which is interesting in its own right. If the hypothesis appears and disappears as you shuffle the subsets, you've either identified a chance happenstance (very likely), or there's some unknown stratified variable (not so likely.) \n\nc) I look forward to the additional caveats and pitfalls the experts here will illuminate."", '>But the whole point is that each color/variable is a whole new test and a whole new dice roll.\n\nThat\'s precisely what I\'m saying. I\'m lost here because I don\'t understand how these independent variables are connected. When you say ""The dice rolls are independent since they are separate tests, so it\'s 20 separate dice rolls, and your expected value of tests that will give you a p-value of 0.05 is (# of tests)(p-value) which in this case is (20)(0.05) = 1."", I\'m ok with the first premise but you then connect each separate test by placing them together in the probability equation. I guess I\'m trying to understand the \'glue\' you\'ve drawn from to make that assumption. How can they be separate new tests every time if you connect them for calculating random chance? It feels like Gambler\'s Fallacy.', 'No worries, I think there is confusion as you might be referring to the process of *Exploratory Data Analysis,* whereas I am just doing a follow on exploring-of-data through supplementary computations.\n\nBy ""base my exploration"", I\'m referring to drawing on actual data from testing the hypotheses to go on to further test as supplementary analyses.', 'The Bonferroni correction is a surprisingly simple concept, as far as statistical methods go. I suggest looking it up.', ""That's not a probability equation; that's an expected value equation.\n\nIf I flip a fair coin 30 times, each coin flip is independent and to calculate the expected number of heads I would get, I would do 30*0.5 = 15."", ""I've actually taken more time to study up on it again, and this time have 'got it', your comments make much more sense to me now :))"", '[deleted]', ""Thanks - I've done a bit more studying on it, and realise my confusion now - it's in the 'range' of space to have errors. Reducing alpha level to .01 for example means instead of having a possible range from 0.00 to 0.05 in which to have a lots of **expected** Type I errors, we will have fewer in the range of 0.00 to 0.01 (or whatever a correction gives us). My understanding was just a little under-developed, guess I shouldn't have gone drinking instead of attending the stats classes, whoops."", 'nice', 'I get the feeling a lot of math and stats at a high level is people saying ""this number moves the way that I want it to move when it does this thing I\'m observing.""']"
Bounding difference of expectations [Q],"Let X and Y be two discrete random variables. Assume sup_a |P(X = a) - P(Y = a)| < b for some b in (0,1). That is, the difference in distributions is bounded under the l-infinity norm. Is there some way to bound the distance between the expectation of X and the expectation of Y?

It makes sense intuitively that as distributions get closer, their expectations will also. But I cant quite figure out how to do that.",13i8ewx,sk81k,1684157461.0,4,1.0,"['No. A simple example would be to pick any n>2 and let \n\n> P(X=0) = P(X=n^(2)) = 1/2 \n\nand \n\n> P(Y=0) = 1/2 - 1/n, P(Y=n^(2)) = 1/2 + 1/n\n\nThen for any given b n can be chosen large enough to satisfy your condition, but E(X)=n^(2)/2 whereas E(Y)=n^(2)/2+n, meaning E(Y)-E(X)=n. You can make the difference in expectations arbitrarily large, despite the difference in probabilities being arbitrarily small.', 'EX = sum_a a P(x = a) and EY = sum_a a P(Y = a), then we can consider | EX  - EY | = | sum_a a (P(X = a) - P(Y = a)) | <= sum_a |a| |P(X = a) - P(Y = a) | <= b sum_a |a|.\n\nWithout further restrictions, I believe this bound is sharp, e.g. consider two Rademacher random variables with parameters p and q, so P(X = 1) = p, P(Y = 1) = q, then we have EX = 2p - 1, EY = 2q - 1. b = | p - q |, and our bound gives  2| p - q |, which is exactly | EX - EY |.\n\nEdit to clarify: This just reframes a bound on the absolute difference in expectation as a bound on the sum over the absolute value of elements in the image of the random variables. There is no guarantee that this sum is bounded. If, for your particular random variable, it is bounded, then you get a bound on the absolute difference of expectations.', ""So we've got a proof of a bound and a counterexample to any bound in the answers. The counterexample looks very convincing to me."", 'Probably better to write ""difference in pmf"" rather than ""difference in distribution"" since the later can readily be taken to mean ""difference in distribution function"", (i.e. cdf).\n\n---\n\nNo, the expectation is clearly not bounded if the support of either X or Y (or both) is not bounded.\n\nTake the distribution of X as given; assume for simplicity of exposition that X has bounded support (this is not necessary but makes it easier to see the counterexample).\n\nChoose some m up beyond the support of X (i.e. larger than the largest value X can take)\n\nJust put P(Y=m) = b/2 (say). Otherwise, take the pmf of Y to be the same as for X but scaled down equally everywhere ... which should be P(Y=t) = P(X=t) * (1-b/2) for all t in the support of X.\n\nIf I didn\'t screw up somewhere we have a valid pmf for Y, and it\'s one that satisfies the stated bound on difference in pmf everywhere in the union of support for X and Y\n\nNow push m up past any finite bound. Clearly the contribution to the difference of expectation within the support of X is fixed and finite, but the contribution to the difference of expectation from the pmf at m is not bounded. \n\nSo the absolute difference in expectation is not bounded.', 'The condition you have doesn\'t imply the distributions are closer in any way. \n\nIn fact, if you take any non deg distribution X (meaning it is not a point mass at any point) and any point mass Y they will still satisfy your condition for some b. \nBy spreading out X and Y appropriately you can create a large difference in expectation.  \n\nBtw, ""getting"" closer is usually phrased in terms of converging sequence of random variables, not just two rvs.', 'That sum can be infinite though, no?', 'I might be wrong, but in the ""proof"" wouldn\'t the upper bound be an infinite, divergent sum if it wasn\'t a finite sample space?', 'Yep. If the sum converges either by having a finite sample space or it is simply a convergent sum, then you get a finite bound. Otherwise the bound is infinite and you get nothing of value.']"
[Q] Currently getting to know about Machine Learning. Got to Know Statistics plays a Pivotal role in this field. Can you recommend some of the best books to Learn Statistics properly?,,13i3bnb,kundhavai,1684143319.0,2,0.55,"[""I'm just at the end of my statistics degree, I was recommended a book during my thesis which I thought was better at explaining core topics I learnt in my first couple of years it has a nice foundation then builds into generalised additive models which are very common in statistics \n\nGeneralised additive models by Simon n wood"", 'Data Scientist here.  Only took a few stats courses in mathematics undergrad but taught myself probability theory and applied statistics on the job to learn how they were the same/different from the ML side of the fence.  I can say that most of the best learning I had came from reviewing Shalizis course notes and his book.  For most bread and butter stuff its pretty good as a starting point into deeper topics. :-)\n\nhttps://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/', 'I used this to learn about stats and machine learning with R: http://www.statlearning.com', 'depends on what you mean by ""learn statistics"" (it\'s a huge subject; what do you want to learn?) and by ""properly"" (at what depth are we talking?)\n\n\nA lot of people coming from outside stats seem to imagine they can just pick up one book and read it and they know all there is to know. That\'s an error of scale of a similar order to Douglas Adams\' *the entire battle fleet that was accidentally swallowed by a small dog*. If you want to be done in less than a few lifetimes, you\'re going to have to be more specific about what you want to learn about and what you want to understand about it -- what you want to be able to do.\n\nIf that (read one book) is what you imagined, maybe start with Wasserman\'s *All of Statistics* (it it pretty broad but isn\'t remotely close to \'all\'; indeed it\'s not even his only ""All of"" book on statistics, and there\'s a ton of stuff outside anything Wasserman has written about). It is aimed at ML people but it\'s very much on the theory side; if you want practical stuff you\'ll need a very different kind of thing. If you want to know about topics it doesn\'t cover, you\'ll need more.\n\nIf you\'re seeking something more basic, the usual books used for mathematical statistics subjects subjects aimed at undergrads might be a starting point, since that will get you basic inference, and you can branch out from there to whatever topics you need.', ""It's an excellent book, to be sure.""]"
"[Q] Can someone critique my shiny dashboard? I'm a new grad looking for a job, did this for a capstone project. Uses data from the Armed Conflict Location and Event Data project and exploratory point pattern methods. I didn't really get any critiques from my professor. The dashboard isn't hosted.","Would you hire me? Rate dashboard 1-10? It has kernel estimation, measures of interpoint interaction, a lot of plots and tries at explanations. 

[https://www.youtube.com/watch?v=rkb\_C4Q0X20](https://www.youtube.com/watch?v=rkb_C4Q0X20)",13hz3r9,DetectiveFeline,1684129639.0,1,0.55,"[""This video isn't available anymore"", 'It is for me?']"
[Q] Cows are 20x deadlier than sharks: what are some commonly quoted but wrong/stupid/manipulative stats?,"I'm looking into writing a character for a short story, and the character quotes these sorts of statistics. Like ones that are either wrong, or don't make sense when compared with whatever else, etc. I imagine you'll know what i'm talking about. What are some other examples of these?",13hyfaz,Caterpillar_Negative,1684127521.0,5,0.61,"['Freakonomics.\n\nI said what I said.', '76.5% of statistics are made up.', 'dihydrogenoxide is by far the most harmful chemical on earth and is commonly found in all aquatic enviroments. All cancer cells require dihydrogenoxide oxide to survive. It is estimated that dihydrogenoxide directly kills (number here) people yearly etc etc', 'People only use 10% of their brain at a time', 'there was also this wrong statistics that cigar companies used for a while to say that there ls no correlation between smoking and lung cancer but cant remember now. though that may be more subtle than outright non-sense.\n\nalso this: https://www.tylervigen.com/spurious-correlations', '""At the time of Americas founding in 1776, the average newly-minted American citizen could expect to live to the ripe old age of 35, giving them a few months to run for the presidency before they keeled over.""-[source](https://longevity.technology/news/usa-embrace-longevity-or-grow-old-fast/)\n\n[Or any of these](https://en.wikipedia.org/wiki/Simpson%27s_paradox#Examples)', 'Lower life expectancy of left-handed people', 'Relationship between vaccines and autism, 97% of scientists agreeing on climate change, true holocaust death count, women earning 77 cents on the dollar, swallowing 8 spiders on average every year, people only use 10% of their brain.\n\nCould list a million relationships as well like the false relationship between video games and violence, but dont know if thats what you are looking for.', 'There\'s a nice [xkcd](https://imgs.xkcd.com/comics/conditional_risk.png) on this.\n\nY2K conspiracism would probably also be a good fit for your character. That and the ozone hole. *""Nothing bad happened so there was never anything to worry about""* (obviously, ignoring the fact that a lot of resource was expended to prevent anything bad/worse happening).', 'Ive been in some psychology related groups on fb and a lot of common folk dont seem to understand averages and absolutes.\n\nSo youll get gems like: Life expectancy of women cant be 80, because I know a woman who is 82.', '#BANN DHMO!!!', 'Theres also people lose half of their heat through their head. Think it was measured with people not wearing a hat but wearing other warm clothing', 'Why downvoted??', 'If I recall correctly, the fasted way to cool down is by putting your hands in ice water', 'Who knows lmfao.']"
[Q] basic stats question,"Apologies for the basic question but I just cant find a clear answer anywhere.

I am doing my dissertation and I need to test for significant (either via T-Test or Mann Whitney), but for some of the data Ive encountered group A would be parametric and then group B would be non-parametric. Do i then go with Mann Whitney? T-Test? Something else? I have no idea.",13hortk,donn_12345678,1684101699.0,11,0.92,"['> , but for some of the data Ive encountered group A would be parametric and then group B would be non-parametric\n\nData cannot be parametric nor nonparametric. Those are descriptions that apply to *models*, and from there can be applied to tests, estimates, intervals or other inferential procedures.\n\nDo you mean ""you tested normality and rejected, concluding that some B samples may be non-normal""? That might not be particularly important nor particularly relevant. I expect that none of the populations you sampled can actually be normal (what are these measuring?) so a test that rejects is only telling you what you should already be able to tell for sure by other means (and the rest will be type I errors). What you need to know is how much impact whatever non-normality there is in the populations (not the samples) could have on the properties of your test (a test-dependent effect size, not significance). If you\'re mainly focused on correctness of significance levels then you need to worry about the impact *when H0 is true*. The data may be pretty much irrelevant to that consideration (because H0 is very probably strictly false).', 'A permutation test might be a good way to go as well.', 'We sampled 2 populations (group A and group B) and measured a bunch of variables. For one or two of the variables Group A would be normally distributed (via Shapiro wilks) and Group B would not be normally distributed (via Shapiro Wilks). My supervisor said to me if normally distributed use T-test and if not use Shapiro Wilks. What do I do if one is normally distributed and one not?', 'Sorry, Im a biology student so stats arnt my strong suit, the data sets has been tested for normality via a Shapiro wilks and data set A was >.05 but data set B was <.05', ""Just use the non-parametric test. Is the safest one if any one is going to judge your statistical analysis. Don't forget to inspect and present a boxplot comparing both groups for interpretation"", 'Failure to reject normality doesn\'t mean you have normality; it just means you didn\'t detect the non-normality you have. A rejections wouldn\'t tell you whether your test will work as you want, and rejecting normality does not tell you that the test will not work as you want. The normality test is not answering the questions you need answers to.\n\n>  My supervisor said to me if normally distributed use T-test and if not use Shapiro Wilks\n\nYou don\'t mean what you wrote there. I presume you mean ""Mann-Whitney"". \n\nBriefly, this is fairly poor advice for substantial list of reasons.\n\n> What do I do\n\nAre you asking \n\n(i) ""what makes sense to do?"" (for which the answer is mostly ""uh, don\'t do what you were told, you really need to focus on different things than what you\'re focusing on""), or\n\n(ii) ""what would my supervisor want me to do?"" \n\n for which the answer is ""don\'t ask me, I don\'t know them, nor what other unhelpful things they might believe or expect"". I might *guess* that they\'d expect a Mann Whitney, but if they didn\'t tell you already and you don\'t ask them, there\'s no way to know for certain. Maybe they imagine a t-test is okay. Maybe they would consider still other criteria -- e.g. maybe they expect you to test still other things first, like whether the distributions are the same shape, as I sometimes see people doing before a Mann-Whitney? Maybe they\'re aware of more than two tests? I cannot guess what mix of sense and nonsense they\'re carrying around. Based on what you\'ve said so far, I don\'t anticipate that their expectations will make a lot of statistical sense and I cannot be sure what they might want.\n\nI could t seems like you missed a lot of the information in what I wrote before, so perhaps more details on what it would make sense to do would be more or less moot. On the other hand, if you want to do what your supervisor would want, that\'s easy -- just ask them what that is.', 'non-parametric test would be the best option', ""If the p-value is greater than the threshold, that doesn't mean that you accept the null.  It means you do not reject the null.  \n\nA better question to ask yourself is whether the data looks normal.  Fit the data with a normal distribution and plot the fit along with the histogram and see if the fitted distribution."", 'Thank you! Uni is stressful so things like this are the little bits of help that make a big difference, I will put a couple box plots in my paper as well.', 'How would the OP deal with the potential critique (from the supervisor, or from some other person looking at the work) that ""if one is normal and the other is not (as we concluded from the goodness of fit tests), the Mann-Whitney assumption that they have the same distribution shape is not the case"".\n\nMaybe you know how to respond to that, but the person you just gave that advice to will not, and yet that\'s a very likely\\* criticism of that advice.\n\n\\* albeit badly misplaced, on several grounds', 'I have never heard about such a restrictive assumption on this test. Would you mind to provide a source? This is one of those assumptions that should hold in order to make valid some type ""shift"" interpretation?', 'My understanding is that a ""same shape"" assumption is required to interpret the result as applying to a location/shift parameter. Without that assumption, the test is comparing the overall probability distributions of the two groups.']"
[Question] Forest Plot save as Word,"Is there any way to generate Forest Plots and save them as word so that the text is searchable?
I couldn't find the option for that in RevMan, and am searching in R but with no results. 
Sometimes I save them as pdf and convert it to word but that basically destroys a lot of the text and makes me end up with something unpresentable.
Thank you.",13hbro9,AmrKhaledM,1684069584.0,1,0.67,"['Set.wd\n\nLibrary(meta)\n\nMeta-function\nPdf(file = name.pdf, width = your.width, height = your height)\nForest.meta(meta-function object, code for your desired forest plot)\nDev.off()\n\nI know you asked for word, but you can search in a pdf, so I am not sure why you require word.', ""If you use the emf device from the devEMF library then you get a figure which Word understands as vector graphics. The text will be like text boxes.\n\nThat said, just having the text there as text doesn't necessarily mean that Word will include it in searches. The internal structure of a Word file is quite different from that of a pdf, and neither even remotely resembles what is displayed."", ""I am submitting a paper to a journal and one of the reviewers had that exact request.\nWe went back and forth trying to convince them that you can search in pdf but they still returned the paper with that request.\nIt's the only remaining comment on the paper.\nAcademia!!"", ""Talk to a senior editor about it. That's not a reasonable demand by a reviewer. I'd submit it with the pdf and if it's rejected on that basis then move to a different journal."", 'That is very unfortunate. How have you tried converting so far?', 'I actually might do that but I was just trying to do my best and seek every possible way to fulfill that demand. That way when I take it up with a senior, I would prove to them that I have exhausted every solution.\n\nBut most likely, yes, it will come to that.', 'I exported them as pdf and tried the Ilovepdf website with activating OCR and it worked for one file but not for the rest. Other sites miss up the files.\n\nSo I think the only way is to generate the files as word from the software itself.', 'Have you tried using Markdown and Knit to word?', ""No, haven't tried that.\nI read about Markdown before, so will go through both of these packages and check what I can do with them.\nThank you."", 'If it doesnt work, let me know. I will gladly try to get the file working for you, if you give me your data or some dummy data to work with.', ""You're too kind.\nI will definitely let you know how it turns out.\nThank you so much.""]"
[Q] What is the correct way to write t-test ?,"Okay so this is a very niche and spefific question but what is the correct way to type t-test. 

I have seen it typed as ""T-Test"" ""T Test"" ""t-test"" ""t Test"" ""*t* Test"" etc ect 

what is seen as the ""correct"" way to write this ?",13h9jmu,Bittersweetcharlatan,1684062923.0,7,0.82,"['It may depend in part on who is judging the correctness, but since Fisher invented the thing we call the t-test\\* (in the form that gets the name ""t""), if we\'re going to argue that someone gets to claim that one form is more correct, I think he gets priority about how to name it.\n\nIf you accept that basis, I offer this quote from *Statistical Methods for Research Workers* (the edition I looked at was published in 1934; this quote is from page 122 of that edition):\n\n> The validity of the *t*-test, as a test of this hypothesis, \n\nNote the italicization of *t* in *t*-test and the hyphen. When I write the name of the test, I don\'t typically reproduce the italicization, but I use the lower-case ""t"" and usually include the hyphen; which is to say, I often write ""t-test"". Sometimes I just write ""t test"".\n\nSorry, I don\'t seem to have an earlier mention of the test written in that form ready to hand (though I think his 1927 Metron paper would be a good candidate for an earlier mention of it).\n\n\n\n---\n\n\\* yes, I definitely mean *Fisher*; the statistic for the test that Gosset (Student) invented was *not* called ""t"" and used a different form of statistic from what we do call t. Take a look at his 1908 paper, it\'s not so hard to get hold of. Fisher (rightly) gives Student priority on the test (since he invented an equivalent test and correctly identified the distribution - albeit without formal proof), but the t-statistic in the form we use it seems to be due to Fisher.', 'A capital T is sometimes used to refer to other distributions, so I always use a small t for clarity.', 'There may be guidelines following a particular style guide.  For example, I believe it was APA guidelines I was following for one particular article I was writing, and the editor was very particular about what Greek letters and statistical quantities were were italicized or not.  It didn\'t make much sense coming at it cold. *\n\n***EDIT, addition***: In APA ""t-test"" doesn\'t have a hyphen, and the t is italicized: *t* test. \n\nSo you might look at what style guide is used for journals in your particular field.  If you can get used to those conventions, it may save you some mental distress in the future.\n\n______\n`*` For example, Greek letters were italicized, but not if they were commonly used in English, so some were and some weren\'t.', ""The Student's t-test."", 'I dont think it is too strictly when it comes to the question of how to write it correctly. You just need to know that the concept come from a scientist who during the time working for the Guiness brewery had found it. Because the company dont want others competitor to know that they have a statistician as worker, he have to publish it under a code name which is Student T.', 'The names of statistical procedures in general are never proper nouns, even if they contain proper nouns (e.g., the name of the procedures inventor). This rules out any variation that capitalizes test. The test statistic itself appears in mathematical notation as *t*, lowercase and italicized, so that rules out variations with a capital T.\n\nWere now left with the issue of whether or not to hyphenate, and perhaps whether to italicize the letter when naming the procedure in running text. Following the less-is-more hyphenation principle that some style guides advocateincluding them where they reduce ambiguity and facilitate reading, and omitting them when they dont further those purposesI prefer *t* test (italic lowercase T, hyphen absent) where possible and t-test (upright lowercase T, hyphen present) where the italicization may be lost or imperceptible. For me, the italicization is enough of a signal to treat the letter as more than an ordinary letter standing alone, so the hyphen doesnt add anything in that case.\n\nI support giving *p* value the same treatment, by the way.', 'tee test\n\nor\n\n\nThe-letter-after-s test', 'Just Students t-test, not the Students t-test. Its a pseudonym, so you treat it the same way you treat given names and their possessive forms. For comparison, we have the Fourier transform (name used attributively) and Bayes theorem (possessive), but not the Fouriers transform or the Bayes theorem.']"
[Q] book recommendation,"Hi,

Looking for a book on use of statistics in real world applications. Not too academic, more focusing on real world data collections and modelling. Fun to read.

Thanks,",13h7jpz,Ok-Break7558,1684056029.0,30,0.98,"['Larry Gonick - Cartoon Guide To Statistic\nhttps://amzn.asia/d/hya77zP', '- The Art of Statistics\n- Mastering Metrics\n- Statistical Rethinking', 'I like Applied Predictive Modeling by Kuhn and Johnson!', ""Tim Harford or Nate silver's book would do."", ""I have a book coming out in the fall that fits this description, called *Probably Overthinking It*:\n\n[https://www.goodreads.com/book/show/123226972-probably-overthinking-it](https://www.goodreads.com/book/show/123226972-probably-overthinking-it?from_search=true&from_srp=true&qid=cudMcFIYGB&rank=1)\n\nYou can get a sense of what it's like from this talk:\n\n[https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/](https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/)"", 'For what level of prior knowlegde?', '*Introduction to the Practice of Statistics*.  Any version 6 or later will be good for a beginner to learn about applying statistics to real world problems.', 'My intro to stats class used The Art and Science of Learning from Data and it was very engaging and informative.', ""Naked Statistics: Stripping the Dread from the Data\n\nThe Drunkard's Walk: How Randomness Rules Our Lives \n\n The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century"", ""*Naked Statistics* by C. Wheelan\n\n[Here's a presentation by the author](https://youtu.be/cIbr_AksnAc) to give you an idea of the content."", 'Ive really enjoyed 2 of Adam Kucharskis books:\n\n- The Rules of Contagion: Why Things Spread - and Why They Stop\n- The Perfect Bet: How Science and Math Are Taking the Luck Out of Gambling', 'You might enjoy ""Dataclysm: Love, Sex, Race, and IdentityWhat Our Online Lives Tell Us about Our Offline Selves"" by Christian Rudder. The book uses data from online sources like dating sites and social media to explore human behavior and preferences. It\'s a fascinating read and demonstrates the power of statistics in understanding real-world phenomena.', 'Silver helped convince me to get into the field. Great book.', 'Sophomore undergrad', 'That can mean widely different things. Ie. having completed 101, having not completed 101, having completed 101 and understood 101.\n\nMy two go to introductory books are Naked statistics for the theory outline and The Signal and the Noise for the real world practice of using statistics and thinking about probability.\n\nFor something above that, Mastering metrics is a go to for causal inference that does a good job of throwing a wide net in terms of accessibility and depth.']"
[S][Q] Excel Polynomial Regression Confidence Level,How can I use the Linest function in excel (hopefully some data from the additional statistics output option in this function) to compute the two sided confidence interval curves for a polynomial regression?,13h719c,Chronozoa2,1684054263.0,1,1.0,"[""I can't answer your question, but I have to ask, why are you doing this sort of analysis in excel?\n\nPresumably you could recover the confidence intervals the old way with Pe+-1.96*SE"", 'Fair point! Only a portion of overall the calculation is statistical in nature and the rest needs to be accessible to my coworkers. Python, matlab, R are options but not ideal in this case.']"
[Q] Dual Form of class weighted soft-margin SVM,"I am familiar with the dual form of the soft margin SVM when there is only one parameter C, but I cannot find the dual form of the class-weighted soft margin SVM which has the following objective with parameters C\_1 and C\_2: 

[https://i.stack.imgur.com/BZCGB.png](https://i.stack.imgur.com/BZCGB.png)

Can the dual form of this problem be derived?",13h6z15,tmen7,1684054041.0,1,1.0,"[""I think there's a nice derivation of the Lagrangian Dual SVM in the Boyd/Vandenberghe Convex Optimization textbook"", 'What does the \\xi_i : y_i notation mean?', 'I see, but is it for a class weighted SVM?', 'It essentially only adds up the slack variables belonging to that class']"
[D] Shapley values as a collection of experiments,"I have written about causal Shapley values on this blog:

https://mkffl.github.io/2023/04/20/causal-shapley.html

I look at  Shapley value as a collection of experiments that measure the effect of adding a feature on the final prediction. The effect can be total or incremental, or a combination of. This perspective helps understand asymmetric Shapley values, which add constraints from causal assumptions.

Any feedback appreciated!",13h4fbz,mkffl,1684045036.0,13,1.0,"[""Looks like a great read, I'm saving it to read tomorrow.\n\nConcerning the use of shapley values for causal inference, this part of the Shap documentation gives advice on what to be careful about, as shapley values are state of the art for model explanation, but come with some caveats in causal inference.\n\nhttps://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html"", 'interesting, thanks. the problems raised broadly overlap, though the solutions are different. The examples of observed confondouded variables are similar: ad spend is a non-interventional variable and interactions is a mediated variable. The CausalML solution with doubleML looks great and makes me wan my to try this library. It is interesting that lundberg wouldnt even mention solutions that directly improve the logic of SHAP like the one I used.']"
[D] floats in statistical computing,"I was reading comments in this [post](https://www.reddit.com/r/ProgrammerHumor/comments/13gt6co/standagainstfloats/) and I was wondering why I never had any issues with floats when doing statistics. Is it important for statistical computing? in which cases one should be worry about this?  
  
I code mostly in R and the most complex thing I had done in terms of programming was to implement a Bayesian estimation algorithm using metropolis Hastings and Gibbs sampling for a complex nonlinear model with mixed effects. I just didn't worry at all about floats and precision.",13gyzef,nmolanog,1684028312.0,3,0.67,"['>  Is it important for statistical computing?\n\nYes, absolutely.\n\n> in which cases one should be worry about this?\n\nmostly when writing (designing or coding) algorithms; its important to consider loss of significant figures when doing so. Catastrophic loss of accuracy can be a real problem and *has led to problems* many times in the past.\n\nOne thing to be particularly aware of (because it comes up pretty often in stats) is the cancellation that results from subtracting *almost-equal* quantities, and why it\'s important to avoid that.\n\nAnother thing that does come up (this is one that\'s pretty common in MCMC work) is the issue of underflow and overflow (a pretty extreme version of accuracy loss) when dealing with likelihoods of different models, such as when performing Bayesian model averaging for example. It\'s important to know the log-sum-exp trick for dealing with that (I had to rediscover it on my own; I wish someone had mentioned its existence to me before having to figure it out myself).\n\nhttps://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n\n\n>  was wondering why I never had any issues with floats when doing statistics\n\nHow do you know that you didn\'t? Just because you didn\'t *notice* an accuracy problem doesn\'t mean there wasn\'t one. MCMC in particular has a tendency to happily chug on even when its answers are poor and it can be hard to spot any problem if you don\'t already know what the results should have been; how would you know the difference between an inaccurate calculation (say one with barely a single digit of accuracy) and an accurate one?\n\n>  the most complex thing I had done in terms of programming was to implement a Bayesian estimation algorithm using metropolis Hastings and Gibbs sampling for a complex nonlinear model with mixed effects. \n\nYou can easily hit it just coding up a sample variance.\n\nhttps://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\nThings are way better than they once were; the IEEE 754 (1985; most recent revision is 2019) standard for floating point calculations sets out a number of things that really do help floating point calculations work better, and the use of ""math chips"" for calculation where the calculations are done to considerably more precision than the final result needs to be stored in both help quite a bit (e.g. On generations of Intel machines doing mathematical calculations in 80 bits when the double precision floats store 64 bit words, though of course the digits of the fractional part - the mantissa or *normed significand* - will be less than the storage, because you lose bits for exponent and sign).\n\nWhile important, neither of those helpful things can fix an algorithm that causes a catastrophic loss of accuracy.\n\nsome reading related to floating point numbers:\n\nhttps://floating-point-gui.de/ -- light, lacks details\n\nand  (though you don\'t have to read further if you don\'t wish to, the first one does mostly cover the main issues)\n\nhttps://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html  -- not so light, lots more detail\n\nhttps://en.wikipedia.org/wiki/IEEE_754\n\nhttps://pages.cs.wisc.edu/~markhill/cs354/Fall2008/notes/flpt.apprec.html  (has a detailed discussion of guard, round and sticky bits if you want to get into why 754 helps preserve accuracy over what came before)', 'There are so, so many things in computation that can bite you that you don\'t find out about until you\'re bitten. For many, this is one of them. For R users, many other topics are covered in [The R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf). The deeper you look, the more you realize that you\'re skating on a very thin sheet of ice over a very deep ocean filled with unspeakable horrors. \n\nThe first time I became aware of the fuckery that is floating point was coding numerical comparisons, things that look like `if (x == y)`. Depending on what you\'re doing, this can lead to a world of pain (or it can be, somewhat shockingly, okay in particular circumstances). If `x` and `y` are some quantity computed two different ways, you\'re in for a world of hurt. More recently I was made aware that floating point [breaks some of the rules of math](https://stackoverflow.com/questions/10371857/is-floating-point-addition-and-multiplication-associative) we\'re all used to. Is this a problem in many contexts? No. But it can be! Though practically for me it most often means that I need to be sure whether I should be using `if (x == y)` or `if (abs(x - y) < threshold)`.\n\nAs efrique has pointed out, there are plenty of other places it can become very important to understand what\'s going on under the hood. I\'ve found some of this is a bit easier to wrap one\'s head around than others. The implications of finite precision for numbers makes ""work with log-likelihoods"" and ""log-sum-exp will save your life"" pretty clear consequences. Catastrophic cancellation took a bit more to get through my skull.', 'That post was a bit extreme (maybe a bit satirical?). The basic idea is that computers work in binary, and some numbers can be perfectly represented in binary numbers, just like some numbers cant be perfectly represented in decimal. Float and double are two standardized methods of storing such numbers with finite precision. The difference is that doubles use double the size of floats, and as such, can represent numbers be represented with greater precision.\n\nIn statistical computing, computations often come down to differences in the 3rd, 4th or even higher decimal digit, or else need precision to a high level for intermediate calculations. In lots of practical applications, float precision is good enough that and loss of precision is inconsequential. However, computers are dumb and its easy to forget this notion so we may end up making these precision errors when programming. Some languages abstract way some of this difficulty, but none are immune.', '[These notes](https://www.stat.umn.edu/geyer/3701/notes/arithmetic.html) are the minimum a statistician should know about computer arithmetic.', 'Comparing equality of doubles in C++ comes to mind. 1 could look like .99999997 or 1.000000004. Other than that, the errors are so small that its largely insignificant. \n\nI have no idea if the errors are normally distributed. They may be, because of the volume of different types of computations, so it may tend to balance out. \n\nFloating point numbers become more inaccurate as they get larger because the bucket size theyre put into increases, so the uncertainty grows. Multiple large multiplications of doubles may compound in meaningful ways. I havent noticed it happen though.', ""There's been a lot of good discussion. I definitely second all the comments about working in log space and being careful when evaluating likelihoods. \n\nProblems can also arise during certain matrix calculations. This was the underlying issue from a recent interesting Statistics Stack Exchange post. [https://stats.stackexchange.com/questions/615068/error-in-gaussian-process-implementation](https://stats.stackexchange.com/questions/615068/error-in-gaussian-process-implementation)"", 'I agree about the problems with MCMC. I coded an extension for JAGS a while ago, and because this was heavily using some math functions that made it slow I coded some approximations. The difference was only after a large number of digits.\n\n\nStill bad idea, because it gave me really bad results and was off by a lot. I think I remember it did not even become ergodic in some cases. All these problem disappeared as soon as I disabled the approximations.', 'It\'s a meme format, [""stop doing X""](https://knowyourmeme.com/memes/stop-doing-math). Some of them are pretty entertaining, even borderline educational.', 'wow, great resource thanks for sharing. I feel a mix of shame and anger since none of my teachers talk about that back in the day. This would have been very useful when I was doing my MSc thesis.', ""were those due to floating point issues, or just that your approximations weren't precise enough?\n\nI do a fair amount of computational stuff and rarely run into floating point issues (the only real issue was when coding up partial-likelihood stuff using clever updating tricks in either overfit models, or problems with an absurdly large amount of signal)"", 'Very few statistical computing courses cover this material.  IMHO all should.', 'Ok, misunderstanding. No, the problem were not floating point issues, but the approximations were also very good. This was just too illustrate the fact, that MCMC can be extremely sensitive even to very small errors.', 'It\'s one of those things that doesn\'t matter until it suddenly does. I ran into it when setting up some optimization problems involving tabular data and the model choked when floating point error stopped the table adding up properly to the margins.\n\nI think it gets blown off as a ""computing theory"" thing and so it\'s considered less critical to cover in courses that aren\'t directly focusing on the intricacies of programming. Which is bad, because it\'s the kind of thing that\'s going to trip more and more people up as we\'re dealing with more sophisticated algorithms like ML.', ""ah! I didn't mean to give you a hard time about it! (I recently had a nasty approximation problem myself --- trying to get a good+fast approximation of f(x,y) = erf(x)-erf(y) over the whole complex plane --- and need good relative error rather than absolute).\n\nI mostly just think people often over-estimate the effect of floating point error in statistical problems. It obviously can absolutely be a big issue, but I find that in many cases there is something else funny happening under the hood."", 'It\'s not just nowadays that we have ""sophisticated algorithms"". Yes. Naive users are completely unaware of this.  But the only reason that the statistics programs they are using work at all (if they do, some are broken) is because the programmers that wrote them did pay attention to this stuff.']"
[Q]Tails distribution question,"Probably this is a dumb question but i want to know if this is correct, till so far, all the statistical test i have seen for rejecting hypothesis focus on the tails following some threshold to see if we reject or fail to reject the null (this in a very supercial way).

But i do not understand why the tail if in distributon like the normal most of the data is in its center, my main thought is because of the outliers that focus on the tails, I am also not sure if this is true, but if it is, then why we should focus the test on the outliers to check an hypothesis would not it make more sense to the where most of the data is gathered?",13gqdpq,Unhappy_Passion9866,1684006004.0,1,0.67,"['The hypothesis test is testing how likely it is we would see scores as extreme or more extreme (further from the mean / median) if the null hypothesis is true.', ""We're trying to decide  if a sample tells us information that's not consistent with H0.\n\nSomething that would behave like what we typically expect to see under H0 would not lead us to doubt H0.\n\nIf I  want to see if I have an unfair coin, I might toss it a bunch of times and look at the proportion of heads. \n\n If H0 is true, then that sample proportion should be close to 0.5, but will vary from it a bit due to the sampling variation of the sample proportion. The result of the experiment will be a single draw from the sampling distribution of the proportion of heads under H0. \n\nIf I see a proportion that's far into the tails of that \ndistribution, it becomes hard to maintain the null position - that its a fair coin.\n\nE.g \n\n- if I toss 30 times and get 18 heads , the explanation 'fair coin with random variation' is pretty plausible. I don't have strong reason to doubt it\n\n- if I toss 30 times and get 25 heads, 'fair coin with random variation' is pretty implausible (in that we should almost never see a result as or more extreme if H0 were true). That explanation based on the null is not tenable.\n\nhttps://i.stack.imgur.com/wfedw.png\n\nThe situation shown on the left is not something that would surprise us if H0 were true; we'd see something at least as weird as that (at least as far from 50% heads) *pretty often*.\n\nThe situation depicted on the right would be quite surprising if H0 were true; we should almost never see something like that if H0 were true."", ""The tails aren't actual data. They're the extremes of the theoretical distribution of results if the null is true (and all assumptions are met).\n\nThe observed result is compared to this theoretical distribution of results we'd see purely by chance to help decide if what was observed is consistent with chance, or might be evidence of a real underlying difference.""]"
[Q] The use of detrended correspondence analysis and canonical correlation analysis?,"Hi everyone! So bear with me here, statistics is my worst subject! Is there anyone here who could help explain detrended correspondence analysis (DCA) and canonical correspondence analysis (CCA)? I've tried looking it up but I still can't understad if these are two different analyses as in they are always used separately (sort of like t-test and ANOVA are two different analysis) or do you commonly use them together, as in one is usually followed by the other?  On what type of data do you usually use these for and is one analysis better than the other or does it wholly depend on what kind of data you have/question you want answered? Thanks for any and all help!",13gnrjb,BWMartell,1683999586.0,3,1.0,[]
[Q] Questionable statistics in medical research,"I have noticed a trend within the covid literature. This post is not about the decision to vaccinate/not vaccinate, I am using it as an example to ask if I am wrong about the statistics, because this is what stood out to me. So I am not stating what I am saying about the vaccine/virus are factual, that is not the point here, the point is, am I making a mistake with the statistical analysis? Because I find it bizarre that I would be right and 100s of PhDs who publish in reputable academic journals don't understand basic statistics.

We keep hearing that vaccination reduces long covid. From what I have read, these studies typically range from around 15-30% reduction in long covid after vaccination. But none of the studies control for this variable: severity of illness.

From my own research (again, the point is not whether or not these are facts, focus on the statistics), there appear to be 4 generally agreed upon unique causes/mechanisms of long covid, which appears to be a heterogeneous condition:

* damage from severe acute covid itself
* autoimmune issues
* persistent viral load
* clotting/inflammation from the spike protein of the virus

So, if ""damage from severe acute covid"" is not controlled for, I am unsure how it makes sense to say ""vaccination reduces long covid by 15-30%"". Wouldn't this mean that that this is likely because that 15-30% comes from reduction in rates of severe acute covid itself, given that we know vaccines do a good job at preventing severe acute covid in the first place?

But if it is a heterogeneous condition, and if the majority of people don't get severe acute regardless (and instead get mild/asymptomatic infection, in which you are still susceptible to long covid), and if the vaccine does not prevent the other 3 mechanisms, and thus the majority of people who get long covid get it from one of the other 3 causes, doesn't that mean the statement ""vaccination reduces rates of long covid"" or ""vaccination reduces the chances of getting long covid by 15-30%"" is meaningless for people who don't get severe acute covid, aka the vast majority of people? Isn't that 15-30% only applying to those who get severe acute covid? Shouldn't we keep in mind the base rate? Wouldn't saying ""vaccination reduces the risk of long covid by 15-30%"" assuming that all people have the same base rate risk of severe acute covid? (which is definitely not the case)? So is it statistically correct to give the blanket statement ""vaccination reduces the chances of long covid"" to people as a whole? How much practical/statistical utility does it have?

Am I missing something here?

EDIT: I am being downvoted: can anybody please specifically cite which part I am wrong about? I posted here asking if I made a mistake with the statistics. I am not sure why I am getting downvoted. If you think I am wrong, I don't know why you would downvote me, that is why I posted. If I am wrong, please show me, that is why I posted this. I am not sure why you would downvote + not show how I am wrong.

**EDIT: I understand this can be confusing. I am adding this example I made in a post to clarify things: It is basically like telling 100% of the population of a country that winter tires will reduce the risk of a car accident, yet 90%+ of the population lives/drives in a place where it does not snow. And you said this because you took a sample of 1000 people, and divided them into 2 groups, 500 people with winter tires, and 500 people without winter tires, but DID NOT CONTROL FOR THE VARIABLE ""snow on the road"", and found an OVERALL 15-30% reduction in accidents in the group that had winter tires on their car. Well obviously that is because SOME people in both groups live in areas with snow, and those in the winter tire group would be less likely to get into an accident compared to those in the no winter tire group who live in snowy areas, meaning that the OVERALL rate of accidents is lower in the ""winter tire"" group. Then based on that concluding ""winter tires reduce the chances of accidents"". TECHNICALLY you would be correct, but this statement would be MEANINGLESS for those who live/drive in an area where it does not snow.**

&#x200B;",13giv5p,Hatrct,1683987661.0,11,0.6,"[""Wouldn't damage from severe covid be a mediator in this setting (meaning, it's one of the mechanisms through which vaccines reduce long covid)? In that case, you don't want to control for it if you're interested in the total effect of vaccination on long covid."", 'Consider this analogous example (if I understand your point correctly):\n\nIn an analysis of whether parachutes (vaccines) prevent death from hitting the ground (long covid) after falling out of an airplane, would you want to adjust for speed at ground-impact (severity of covid infection)? Youd quickly prove that parachutes dont _directly_ provide protection against dying from the fall.\n\n(the data might even suggest that parachutes are dangerous if it contained some poor fellas who tripped and fell out of stationary, grounded aircrafts and some of those with parachutes got tangled up in the cords and suffocated, while those without parachutes had nothing to choke on and survived).\n\nSeverity of covid infection is on the causal pathway to long covid, and vaccination causes less severe infections. As another poster stated, by adjusting for severity of covid, you could isolate the direct effect of vaccination beyond the effect through severity of infection - but that would not be useful or representative of their effect from a public health planning perspective.', 'Vaccination almost certainly has an effect on severity of disease, which means controlling for it is almost certainly a bad idea. This will give an inappropriate effect estimate that cannot be interpreted causally and does not have any real policy relevance.', 'I dont think youre missing something, I think youre just getting stuck on something that isnt important. The question is: does the COVID vaccine reduce the risk of long COVID? Its not asking _how_ it reduces the risk, just whether it does. Disease severity is altered by vaccination, and **we have no reason to think people who are vaccinated are predisposed to mild disease separate from vaccination**  I think thats what youre getting hung up. If anything, people aged 65+ and immunocompromised people  ie, the groups most at risk for severe disease  are generally _over-represented_ in the vaccine group, not under-represented.\n\nMaybe a made-up example could help. Say theres a drug, Drug A, that reduces the risk of heart attack by lowering cholesterol. In this world, its impossible to measure someones cholesterol until after the experiment began (so the Drug A groups exact cholesterol levels before they were given Drug A are unknown). Youre trying to find out if Drug A reduces the risk of heart attacks. Controlling for cholesterol levels would not necessarily be a good idea, because youre negating the main reason Drug A reduces heart attacks.', ""I can't speak to the specific issues that you are pointing out here. Really this sort of criticism needs specific context of the study before it can be discussed. **If the issue is simply that a lot of bad research gets published and gets public attention, then yes,** but that goes across academic fields sadly, though it might be larger for the social sciences. Welcome to the reality of academia, nobody actually thinks every peer reviewed study is valuable.\n\nSomewhat tangentially and purely anecdotally, I will add that a lot of academics have no issue badmouthing epistemology after covid for a fairly abysmal performance and inability to take any policy considerations beyond the purely medical into account in their analysis. Lots of self-censorship on this however, given academics who have spoken out on these issues have caught a lot of flak. The early evidence for the consequences of some of the recommendations made during covid is pretty disastrous, particularly for school closures."", 'Hiya. Im a PhD quant social scientist who consults on a COVID research study. Ill readily admit that data and stats are my forte, not medicine, biology, or immunology, so I dont really have the subject matter expertise thats needed to give you a full picture. That said, Ill do my best to answer your question.\n\nTo include vaccination, severity of COVID, and long COVID in the same model would introduce post-treatment bias. Richard McElreath has a good bit on this in his book *Rethinking Statistics*. In this instance, vaccination would reduce severity of COVID, which in turn would reduce long COVID. However, including both vaccination and severity variables in a singular model would reduce the effect of vaccination on long COVID to zero. \n\nNow, does this mean that the effect of vaccination on long COVID is actually zero? No. Once we already know the severity of COVID, including vaccination doesnt give us any additional information, thus its effect is artificially deflated. But if we want to know the effect of vaccination on long COVID, we cant include severity of COVID, since vaccination produces that post-treatment effect.\n\nAs others suggested, you could test severity of illness as a mediator, but thats an entirely different question and experimental design. \n\nNow I will tell you firsthand theres a lot of sloppiness in this line of work (largely because it is done so fast, and because most physicians have minimal formal training as researchers). But this is not such an instance.', 'It is absolutely true that vaccination reduces the chances of long covid for the population at large. It could also be true that the reduction is primarily occurring in people at risk for severe acute covid, but that doesn\'t change the fact that rates of long covid would go down overall if everyone was vaccinated. This is practically useful because, well, it\'s nice to have fewer people debilitated by long covid in society. \n\nFundamentally though, you\'re right. It would be much more informative to know \\*who\\* receives protection from long covid due to the vaccine. As someone not at any substantial risk for severe acute covid, I would also love to know whether the vaccine is protecting me from long covid. And yes, the available data should allow researchers to get at least some sense of the answer.\n\nIn short, it\'s not wrong to say ""vaccination reduces the chances of long covid"", because the statement refers to the entire population. It would also be very useful to know more about who is most helped and why.\n\nIt\'s also true that know who it helps and how doesn\'t change that much public policy. The vaccine will still be recommended to everyone currently eligible for the vaccine. Therefore there\'s no strong urgency to publish on the who and how.', ""I would point out that vaccination likely has an effect on most of your indicators not just severity. For example viral load is often reduced by can vary greatly between the vaccine and variant.\n\nSecondly, I'd note it can be difficult to compare between studies due to the temporal nature of vaccines and variants. Getting the vaccine for one variant and getting sick with that variant versus another will change how each of those indicators (severity, viral load, etc) appear. \n\nIt also appears that viral load measured by amount of virus in the nose is a weak indicator of actual viral load within the individual. It's better for determining transmission risk than it is for being a stand in for vital load within a person. Some people get it deep in their lungs causing more severe illness and some people get it much more in this upper respiratory system. So this confounding factor is far harder to measure and varies alot between people. It would be hard to control and impossible to measure for large populations."", ""I think you're asking a question about research framing, not a statistics question. Statistics are a tool you use to answer a question. The problem isn't the tool that researchers are using, the problem is the question that they're asking. Obviously it is proven that the vaccine does prevent long covid to some degree in the population. But as you have pointed out, that decreased risk is probably not distributed evenly across the population.\n\nI think your question is an important one to ask. But I think it's more a question about how epidemiological research questions are framed. And then how that epidemiological evidence gets translated into public policy."", ""I mean, without you citing the actual studies and summarizing just what you read, it's hard to comment. You also don't really cite the full statistics that you want feedback on..... \n\nI know multiple studies exist that *did* control for disease severity and how that affected development of long COVID, so there seems to be a lot of info you are missing in your analysis and likely not properly considering."", ""Without fully reading what you've written, I agree that soft sciences tend to misuse statistics a lot, both in terms of how they obtain data and how they interpret it. I've previously encourage statistics students pursuing a PhD to look into this as a possible thesis topic."", 'I\'ve had to do stats for medical articles and although we always start out doing a good job, the reviewers always seem to know better and will ask smt bayesian or smt only a handful of people (including the reviewer) know about. Bayesian is one of those annoying trends that is currently being overused.\n\nI\'m curious to hear how you would determine severity of covid with limited EMS. It sounds like a lot of the things you mentioned like viral load aren\'t usually put standardly (is that a word?) in an EMS that I\'ve seen so far. Autoimmune issues are standard. I\'m surprised if they didn\'t control for that somehow, unless you mean if it developed during treatment.\n\nThey usually have to pull from several EMS systems who may not have the same information, unless they\'re doing an international study where everything is set up at first. But one of them with ten million people sounds like they were limited in what was in the system.\n\nIn addition, some theoretical techniques don\'t allow you to add anything from DURING treatment as a control variable. So viral load and damage etc would be out in those cases.\n\nSidenote:\nI worked on one very large international covid study but the data was absolute garbage: the main coder set up a missing value as a zero, and a no is also a zero. I found almost no one was able to publish with it, either, except one group who did not even know about that and assumed it was all a ""no"".', ""So I worked in a clinic as an Army medic during covid so bare in mind most of my paitents where young men in good physical condition but none of my covid paitents died, none had long lasting side effects and around 50%-60% of them didn't even experiance symptoms."", '100s of PhD who publish in reputable academic journals dont understand basic statistics\n\nBingo!!!', 'I think the stats youre describing are appropriate to answer some specific questions. You have a different specific question and so might need to specify a different model. \n\nas an aside, the number of and if statements in the conceptual model and the potential number of causal pathways suggest a structural equation model to me', 'First of all, thanks for replying instead of downvoting/censoring+not refuting.\n\nWhat you say is not wrong, but I don\'t understand the practical utility of not keeping the base rate in mind, especially in such an extremely skewed example? Again, I am not saying this is a fact or that this is a precisely proven number, but from my analysis of the legitimate sources, I don\'t think it would be unreasonable to say something like 95%+ of reasonably healthy non-elderly people did not get severe acute covid even before the vaccines came out, and perhaps around 90%+ of the total population on average.\n\nYet 5-40% of all people who get infected get long covid, and the severity of illness does not seem to be significantly associated to the chance of developing long covid (perhaps it is associated with the intensity, but not the frequency). That is, whether you get mild/asymptomatic or severe covid, it seems like more or less you have similar chances of developing long covid. Obviously, if you don\'t get severe acute covid, how can you possibly get long covid via the mechanism ""damage from severe acute covid?"" So if people with mild/asymptomatic infection get long covid, it must be from the other 3 mechanisms, none of which there is no evidence for, and does not appear to be any rational basis to expect that the vaccines protect against (as they were not made to do so, they were made to stop severe acute covid, which they do well).\n\nSo if roughly 90-95% of the population would not get severe acute covid anyways, what is the practical utility of limiting studies to the OVERALL correlation between vaccination and reduction of long covid, without controlling for severity of illness? Isn\'t it completely useless, or doesn\'t it even set up a false sense of security for 90-95% of the population to tell them ""vaccination reduces the chances of long covid"" or ""vaccination reduces the chances of long covid by 15-30%""? Wouldn\'t you expect people would assume that THEIR risk of long covid would go down by 15-30% if they get vaccinated? Is this accurate though? If the above is true, wouldn\'t vaccination actually reduce chances of long covid 0% for around 90-95% of the population? So again, what practical utility does it have to make these blanket statements to the public? Can this be considered an accurate or honest statement, if it does not apply to 90-95% of people? Would it not set them up with a false sense of security? Is this consistent with informed consent? Shouldn\'t it be specified that that 15-30% doesn\'t necessary apply to them (and that for around 9/10 people it doesn\'t?) Shouldn\'t the focus/message be on what the vaccines have been proven to do: that is, vaccinate if you want to significantly reduce your chances of severe acute illness? But if you want to protect yourself against long covid, you need to prevent infection in the first place as well?', '>In an analysis of whether parachutes (vaccines) prevent death from hitting the ground (long covid) after falling out of an airplane, would you want to adjust for speed at ground-impact (severity of covid infection)? Youd quickly prove that parachutes dont directly provide protection against dying from the fall.\n\nThis is so awesome. Excellent explanation. ', ""We all know that it's not the fall that kills you. It's the landing. Well done!"", 'No you don\'t understand me correctly. You, just like all others, don\'t appear to understand that long covid is a heterogeneous condition, and are operating based on the faulty premise/ASSUMPTION (which there is no evidence for) that mild acute covid aka 4.1/10 severity can have significantly higher long covid rates than mild acute covid 4.0/10. **You are making the classic mistake of ASSUMING that correlation = causation (the ONLY thing the studies did was divide people into 2 group: vaccinated vs unvaccinated and did NOT control for illness severity, so ALL we know is vaccination is OVERALL CORRELATED with lower rates of long covid, those studies DON\'T SHOW HOW, so you can\'t just ASSUME that 2 people with mild covid 1 vaccinated 1 non vaccinated will have different long covid rates when this was not CONTROLLED for in the study, what likely happened is that there were more people with SEVERE acute covid in the ""unvaccinated"" study, which caused the OVERALL rates of long covid to be higher in the unvaccinated group).**\n\nThere is no evidence for this. There are 4 INDEPENDENT mechanisms/causes of long covid. ONE is lasting damage from ""severe"" acute covid. HOW can you get long covid from this mechanism if you get MILD or ASYMPTOMATIC covid? Can you answer this? That means if you get long covid you got it from one of the 3 other mechanisms/causes of long covid, which are NOT RELEVANT to illness severity. They are, autoimmune issues, persistent viral load, and microclotting/inflammation from the spike protein. None of these require severe illness. There is no evidence any of them are made worse with more severe illness. In fact, some of them are NEGATIVELY correlated with severe illness. That is why loss of taste/smell and cognitive issues is significantly NEGATIVELY correlated to age: they affect young people much more than older people, which means they are negatively correlated with illness severity.\n\nRead this example to see what I mean, replace winter winter tires with vaccines, car accidents with long covid:\n\nIt is basically like telling 100% of the population of a country that winter tires will reduce the risk of a car accident, yet 90%+ of the population lives/drives in a place where it does not snow. And you said this because you took a sample of 1000 people, and divided them into 2 groups, 500 people with winter tires, and 500 people without winter tires, but DID NOT CONTROL FOR THE VARIABLE ""snow on the road"", and found an OVERALL 15-30% reduction in accidents in the group that had winter tires on their car. Well obviously that is because SOME people in both groups live in areas with snow, and those in the winter tire group would be less likely to get into an accident compared to those in the no winter tire group who live in snowy areas, meaning that the OVERALL rate of accidents is lower in the ""winter tire"" group. Then based on that concluding ""winter tires reduce the chances of accidents"". TECHNICALLY you would be correct, but this statement would be MEANINGLESS for those who live/drive in an area where it does not snow.', 'OP, this is the only answer you really need.', ""They're a psycho anti vaxxer, any attempt they make to present as logical is a front. Just read their post history and don't bother wasting your time"", 'Vaccination prevents severe acute illness, but it does not prevent breakthrough mild/asymptomatic infection. The rate of long covid in those with mild/asymptomatic breakthrough or non-breakthrough infection appear to be virtually the same. Also, a high proportion of those with mild/asymptomatic infection get long covid, presumably because it is through a mechanism OTHER than severe acute covid. I think you are not giving enough importance to how long covid is a heterogeneous disorder. For example, someone who gets autoimmune issues from mild breakthrough infection would NOT be expected AT ALL to be protected by the vaccine in terms of long covid from the mechanism ""autoimmune issue"". In fact, some studies show the vaccine (many vaccines, not just this one) itself can cause autoimmune issues. So severity of acute illness is not relevant here.\n\nAlso, I don\'t understand why you say it would not be a good idea to control for severity of illness. Again, long covid is a heterogeneous condition with DIFFERENT causes. So even if you gave a self-report scale, having people rate their initial acute illness from 1 to 10, that would be better than nothing, with a large sample size. Any differences would be expected to more or less even out (though there is always the possibility of the placebo effect in the vaccinated group). That way you can match vaccine vs non vaccine group on illness severity. If you do this and find the average illness severity in your vaccine group was for example 3.2, and 4.9 in the unvaccinated group, you would know that one of the reasons the vaccine is associated with lower levels of long covid is because it causes less severe acute illness in the first place.\n\nInstead, what these studies are doing are superficially making ""vaccine"" group vs ""unvaccinated"" group, and then superficially checking for overall rates of long covid in each group, then making a global statement based on that ""vaccination is associated with a reduced risk of long covid""... for a HETEROGENEOUS disorder, this is not much of a meaningful statement to make, unless you know BY WHICH mechanism this reduction is being achieved.\n\n>The base-rate fallacy is people\'s tendency to ignore base rates in favor of, e.g., individuating information (when such is available), rather than integrate the two. [https://www.sciencedirect.com/science/article/abs/pii/0001691880900463](https://www.sciencedirect.com/science/article/abs/pii/0001691880900463)\n\nThe SAME mistake appears to be made in studies that claim omicron has lower rates of long covid compared to previous variants. It is likely that this difference is due to omicron causing less severe illness itself, and the vast majority of people who will get long covid after getting omicron won\'t get it from the mechanism ""severe acute illness"", rather, they will get it from a mechanism that is similar in omicron vs other variants. So it is counterproductive to say this to the public, because for the vast majority, omicron will be just as likely to give them long covid.', '>Ill readily admit that data and stats are my forte, not medicine,biology, or immunology, so I dont really have the subject matterexpertise thats needed to give you a full picture.\n\nThanks for answering, but without that background knowledge, your answer is unfortunately not relevant.\n\n>This is why:  \n>  \n>In this instance, vaccination would reduce severity of COVID, which in turn would reduce long COVID.\n\n**You are operating based on this premise, which is not necessarily true. You arbitrarily assume it is true, without checking for it. That is why I am saying we need to control for it. You can\'t just randomly assume things. There is no evidence that someone with mild acute covid before vaccination will have a lower risk of developing long covid if they got long covid after vaccination, if the vaccination slightly reduced the ""mildness"". There is simply no evidence. We would need to study this. It has not been studied. But based on what we know, the hypothesis point toward: no, there won\'t be a significant or practical difference. Why? Because long covid is a heterogenous disorder with 4 DIFFERENT causes/mechanism. Severe acute covid is ONE of the causes.**\n\n**The other 3 causes appear to be DIFFERENT/INDEPENDENT OF/IRRELEVANT to illness severity. And mild or asymptomatic covid in the majority of cases is not expected to cause long covid DUE to the mechanism called ""severe acute covid"" BECAUSE MILD/ASYMPTOMATIC INFECTION IS NOT SEVERE. HOW CAN YOU BE DAMAGED BY ""SEVERE ACUTE COVID"" IF YOU DON\'T GET IT IN THE FIRST PLACE? So how on earth would vaccination reduce long covid through the mechanism ""illness severity"" if REGARDLESS OF VACCINATION one DOES NOT get severe acute covid, and mild/asymptomatic infection is not strong enough to cause lasting damage to the body due to ""severe"" illness, and instead if mild/asymptomatic people get long covid it would be due to 3 OTHER/DIFFERENT/UNRELATED causes/mechanisms of long covid (aka autoimmune issues, persistent viral load, microclotting/inflammation, all of which appear to happen due to INFECTION ALONE, UNRELATED TO/IRRESPECTIVE of ILLNESS SEVERITY).**\n\n**How can I explain this so people understand. Imagine you get hit by a baseball to the face. If the hit is NOT STRONG ENOUGH, it CANNOT break your skin. But let\'s say there is something poisonous on the ball, which can touch your skin then that can cause an infection that breaks your skin. These are 2 DIFFERENT mechanisms. So then how can you say wearing a faceguard (that doesn\'t prevent skin to skin contact of the poison) will ""reduce"" the chances of breaking your skin on your face? Yes it does reduce it, because it protects against hard hits, but IT DOES NOT PROTECT against the poison that causes infection. So obviously, if you take 2 groups, divide them into ""faceguard"" and ""no faceguard"", there will OVERALL be LESS cases of infection in the ""faceguard"" group, but let\'s assume 90% of people play with their children who cannot throw hard enough to break the skin, but they ARE susceptible to breaking of skin on the face if the poison touches their face. WHAT PRACTICAL UTILITY would it then have to tell 100% of the population ""wearing a faceguard reduces the risk of broken skin on the face during baseball"". This would be MEANINGLESS for 90% of the population.**\n\nThe only way they did these studies is that they grabbed a bunch of people, divided them into ""vaccinated"" vs ""unvaccinated"", and saw a slight (15-30%) reduction in long covid in the ""vaccinated"" group. This DOES NOT answer my mild vs mild question above. ALL it shows is that OVERALL, there is less long covid in the ""vaccinated"" group. ""vaccination"" was the ONLY variable they compared. They DID NOT CONTROL FOR SEVERITY. Now, yes, we KNOW (based on other studies) that vaccination reduces illness severity. However, this STILL does not answer my mild vs mild question. What is likely happening is that BECAUSE they did not control for illness severity, there are MORE PEOPLE with SEVERE ACUTE COVID in the ""vaccinated"" group, so obviously, there will be less OVERALL rates of long covid in the ""vaccinated"" group compared to the unvaccinated group.\n\nAGAIN, this DOES not answer the question ""if I get mild covid without vaccination, will my risk of long covid be lower if I had gotten the vaccine and also got mild covid but slightly less mild covid?"" And even if the answer was yes, I highly doubt that there would be a PRACTICAL difference. There are also studies showing that for the most part vaccination useless against preventing long covid after mild breakthrough infection: the long covid rates are more or less similar in mild breakthrough cases compared to nonvaccinated mild cases.\n\nAlso, I don\'t get why you say there is ""post-treatment bias"" if you were to simply CONTROL for illness severity. WHAT is wrong with giving a likert scale, having the ""vaccinated"" and ""unvaccinated"" group both rating their illness severity out of 10, then matching them on that and comparing long covid rates based on that. How is that post-treatment bias? How is there a practical problem with that? At the end of the day, it would control for illness severity would it not?', 'Right, but things get a bit more complicated when you consider the adverse effects (known + unknown/long term) of the vaccine. We can\'t ignore this in the risk/benefit analysis. The question to vaccinate, is, for EACH/ANY PARTICULAR individual, will the protection gained against severe acute covid OFFSET the potential adverse effects? Obviously, the risk/benefit analysis for a 70 year old with diabetes will be different than a risk/benefit analysis for a healthy 12 year old. Yet the policy was to vaccinate every single person on earth, without any individual risk benefit analysis. And they used ""vaccination reduces the chances of long covid"" as an additional push factor for ALL ages/demographics to influence their decision to vaccinate, even though I showed how it seems to be virtually ineffective for 90%+ of the population, because if these people get long covid, which is a heterogeneous condition, it would be due to a cause/mechanism OTHER than severe acute covid, which is what the vaccine protects against.\n\nI don\'t understand this, because up to now virtually every single medical intervention required an INDIVIDUAL risk/benefit analysis, not a risk/benefit analysis from a POPULATION level, that will NOT APPLY AT ALL to MANY individuals.\n\nI will give you an example of myocarditis. It is known that both the virus and the vaccine can cause myocarditis. During the omicron wave, they were urging all healthy young people to get a booster. That is, young, healthy, people, including children/teens who already had 2 doses, and some had natural immunity on top of that. How many cases of severe illness did that cut? And did that OFFSET the KNOWN (myocarditis, POTS, similar neuro/cardio symptoms of long covid etc..., potential increased chances of heart attacks years down the line? How do we know this won\'t happen?) risk of adverse effects from the vaccine? I highly doubt it. Some of these young healthy people who had to get boosted got myocarditis, because they had too much spike protein in their body at one time (from vaccine + virus). A Harvard study already proved that those with vaccine-induced myocarditis had high circulating levels of spike protein. And we know this can happen from the virus as well: the spike protein from the virus can also leak into the blood and cause this issue. Later on, they partially realize their mistake, and said to wait a few months before/after infection before getting boosted (presumably to reduce chances of myocarditis). So was it worth it to give a 3rd dose and increase the chances of myocarditis for a healthy young person who already had 2 doses + natural immunity on top of that? Look below on some stats that might shed light on this.\n\nHere is a government source, go to page 12:\n\n[https://www.publichealthontario.ca/-/media/documents/ncov/epi/2020/05/covid-19-epi-infection-children.pdf](https://www.publichealthontario.ca/-/media/documents/ncov/epi/2020/05/covid-19-epi-infection-children.pdf)\n\nFrom Jan 2020 to July 2021 (there were no vaccines available for children in this jurisdiction at this point), it shows 39 out of 70 187 recorded (actual infections likely 2-4x that amount due to the fact that not everyone who was infected tested+asymptomatic infections) infections in people under 18 resulted in severe acute covid (as per ICU admission). Also, keep in mind this was even BEFORE omicron (which is less virulent). My guess is that the majority of these 39 had risk factors/were immunocompromised, which means in the healthy child/adolescent population, even less got severe acute covid. So something like .01% of healthy unvaccinated children/adolescents got severe acute covid. Would it make sense to vaccinate them? And then boost them on top of that? According to the health experts in government, yes. They said their cost/benefit analysis said yes. Yet they did not reveal their calculations for the cost benefit analysis. Keep in mind, myocarditis and POTS are KNOWN adverse effects. Long covid is still an ongoing thing, and more and more studies are showing that the spike protein is problematic. There is no evidence that the spike protein in the vaccine too cannot cause these long term low grade damages (if there is please let me know, but I have researched 100s of papers and extensively searched and found nothing), they are refusing to do the studies. So how can this statistically meet the risk/benefit analysis? Maybe I am missing something.\n\nHere (and in most places) the govt encouraged people to get vaccinated then crowd inside high infection risk areas without masks such as restaurants (therefore encouraging people to get infected), in which you scanned a QR code showing proof of vaccination otherwise were not allowed in. They also made the blanket statement vaccines reduce chances of long covid (which we disussed the problems of it already), instead of preventing infection + funding studies to combat long covid. So they told people who had very little chances of severe acute covid to go get infected, while doing nothing for long covid, and these people had a lot more to lose from long covid after mild infection as opposed to severe acute covid, and some of them are still suffering from long covid. Is this rational from a medical point of view? Moral?', 'Perhaps on ""persistent viral load"".. as a small% of people report less long covid symptoms after vaccination, perhaps the vaccine is ""clearing up"" the persistent viral load in these individuals, but this constitutes quite a small % of people with long covid and even of those only a small% of them have this issue fixed after vaccination. But from my review of the literature, vaccine is quite unlikely to have any effect on autoimmune issues, and can cause autoimmune issues of its own. Same with clotting/inflammation from the spike protein, vaccine does not appear to have any mechanism to clear this up, and it contains the spike protein itself + vaccine injured report similar symptoms to long covid patients so use some inferential logic here to guess what might be going on in this regard.\n\nIf many studies report 15-30% reduction of long covid after vaccination, and don\'t control for illness severity, it would be reasonable to conclude that the other 60-75% is being caused due to the other causes of long covid that the vaccine does not protect against.', ""Correct. I guess technically this falls under research methods. But I thought it would be relevant enough to statistics. I mean if mainstream experts don't know which statistical tools to use that is related to statistics."", ""I don't know the studies you're talking about.\n\nHowever, there are in general a number of different causal effects that might be interesting in any given setting. The most common ones are the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The former tells you the average treatment effect in the population, i.e., the expected effect for a randomly drawn individual. The latter is similar, but for those who actually took treatment. This might be different if there is selection into treatment.\n\nIf I understand you correctly, you're saying that the ATE is not the expected treatment effect for any particular individual, because the effect is heterogeneous and will be much larger for some, and much smaller for others. This might well be the case, but the ATE is still informative, because it tells you the expected reduction in long covid if everyone took the vaccine compared to if nobody took it. \n\nThat said, it's always interesting to estimate heterogeneous treatment effects for different groups, but it is often challenging (for reasons ranging from statistical power to study design). \n\nFinally, controlling for severe covid would not solve this problem. At best, it would tell you the effect of vaccines through the other mechanisms, which might or might not be interesting (but it's more likely to tell you nothing useful at all)."", '>First of all, thanks for replying instead of downvoting/censoring\n\n""Censoring"" yikes.\n\nDownvoted for crying about being downvoted lol.', ""Using basic inferential logic from studies from Harvard and such means being an 'anti-vaxer'? Since when? Nice straw man. But I won't get into the details, this is not the place. You can choose to read my post history with an unbiased lens. I did not make any claims of facts, I simply used basic inferential logic to compare the findings from high quality studies. If this subjectively upsets you, that is your problem, not mine, please keep childish straw mans out and misguided personal insults due to your own lack of reading comprehension/high level of emotion/low level of logic of there. If you have specific refutation against any of my points, post in the appropriate sub/topic."", 'Why is it not a meaningful statement to make without knowing ""by which mechanism"" the reduction is achieved?', '>Thanks for answering, but without that background knowledge, your answer is unfortunately not relevant.\n\nYou asked r/statistics. You\'re not going to find that subject matter expertise here. I suggest you ask on another sub if you want to get an answer from someone with that domain experience. \n\n>That is why I am saying we need to control for it. You can\'t just randomly assume things. There is no evidence that someone with mild acute covid before vaccination will have a lower risk of developing long covid if they got long covid after vaccination, if the vaccination slightly reduced the ""mildness"". There is simply no evidence. We would need to study this. It has not been studied. But based on what we know, the hypothesis point toward: no, there won\'t be a significant or practical difference. Why? Because long covid is a heterogenous disorder with 4 DIFFERENT causes/mechanism. Severe acute covid is ONE of the causes.\n\nThe presence of vaccination does in fact suppress COVID-19 severity \\[[1](https://jamanetwork.com/journals/jama/article-abstract/2786039), [2](https://www.europeanreview.org/wp/wp-content/uploads/1770-1776.pdf)\\]. In addition, COVID-19 severity increases the risk of developing long COVID \\[[3](https://www.nature.com/articles/s41418-022-01052-6)\\]. \n\nI\'m also not 100% sure I\'m understanding your question. However, if you\'re asking whether someone who gets mild COVID without vaccination is more likely to get long COVID than someone who gets mild COVID after vaccination, solely based on the severity of the disease, the answer would be no. \n\nDoes that make vaccination useless? Absolutely not. If vaccination decreases the likelihood of getting severe COVID-19, then vaccination can be protective against long COVID. \n\nSomething else to note is that illness severity, immune dysfunction, persistent viral load, and inflammation from the spike protein are all probably going to be interrelated. Any model that treats these as independent mediators of COVID-19 is likely not reflective of reality.', 'Among many other things, you are assuming that vaccine policy is made based on individual risks and benefits. Thats not the case with vaccines for deadly infectious diseases. Controlling infectious disease requires policies for everyone likely to transmit that infectious disease,  not just those most at risk for being seriously harmed by the disease. We have a lot of rules and laws that follow similar principles; manufacturing plants arent allowed to spread deadly waste to their neighbors, grocery stores can sell possibly contaminated food, farmers have to cull entire flocks/herds with certain infectious diseases. \n\nHowever, most of us on Reddit live in very individualistic societies. The population demands that the individual risk for protecting all of our grandparents and neighbors stay low. Vaccines are therefore carefully monitored and very safe. Not perfectly safe. But for the vast majority of the population, the burden of covid far outweighs the very small risk from the vaccine. And its absolutely true that there would be a lot more death and suffering in the entire population without the vaccine.', '> I don\'t know the studies you\'re talking about.\n\nhttps://www.nature.com/articles/s41591-022-01840-0\n\nAround 10 million sample size. They saw around a 15% reduction in long covid in the vaccinated. I did not see them controlling for severity of acute covid.\n\nLook how the news phrases these types of studies:\n\nhttps://www.usnews.com/news/health-news/articles/2023-03-23/study-vaccination-cuts-risk-of-long-covid-19-nearly-in-half\n\n>The research, which reviewed 41 studies involving more than 860,000 people, found that vaccination reduced the risk of long COVID by nearly half. Other factors like older age, increased body mass index, smoking and being female increased the risk of long COVID. Health issues like asthma, diabetes, coronary heart disease, anxiety and depression were also associated with a higher risk of the condition.\n\nFrom the actual study:\n>Findings  This systematic review and meta-analysis of 41 studies including 860783 patients found that female sex, older age, higher body mass index, smoking, preexisting comorbidities, and previous hospitalization or ICU admission were risk factors significantly associated with developing PCC, and that SARS-CoV-2 vaccination with 2 doses was associated with lower risk of PCC.\nhttps://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2802877\n\nWell obviously old age, increased BMI, etc.. would be associated with higher rates of long covid, because one mechanism/cause of long covid is ""damage from severe acute covid"" and because people with those risk factors are much more likely to get severe acute covid in the first place. Again, this ""50%"" (according to this study) reduction in long covid is absolutely meaningless for the vast majority of people, around 90%+ of the population, because this study didn\'t control for severity of acute covid.\n\nLook at this study in the Lancet:\nhttps://www.thelancet.com/journals/laninf/article/PIIS1473-3099(21)00460-6/fulltext\n\nCheck out Figure 3 and Figure 4. It is clear that there is a large correlation between severe acute covid and long covid symptoms in this study. Why did they not control for it? It seems that the long covid in this case was largely due to the severe acute covid. But what does this study serve in terms of the majority of the population who won\'t get severe acute covid to begin with?\n\nLook at this study:\nhttps://jamanetwork.com/journals/jama/fullarticle/2794072\n>The number of vaccine doses was associated with lower long COVID prevalence: 41.8% (95% CI, 37.0%-46.7%) in unvaccinated patients, 30.0% (95% CI, 6.7%-65.2%) with 1 dose, 17.4% (95% CI, 7.8%-31.4%) with 2 doses, and 16.0% (95% CI, 11.8%-21.0%) with 3 doses. Older age, higher body mass index, allergies, and obstructive lung disease were associated with long COVID.\n\nAgain, why did they not control for severity of acute illness? They show the more vaccines you get the less long covid you get, and that variables like older age, higher BMI, risk factors, were associated with higher levels of long covid. You need to control for these. Again, obviously if ""severe acute covid"" itself is one of the causes of long covid, OVERALL rates of long covid will always be less in ""vaccinated"" group compared to ""unvaccinated"" group. But how meaningful is any of this for vaccinated or unvaccinated people who don\'t get severe acute covid?', '> If I understand you correctly, you\'re saying that the ATE is not the expected treatment effect for any particular individual, because the effect is heterogeneous and will be much larger for some, and much smaller for others. This might well be the case, but the ATE is still informative, because it tells you the expected reduction in long covid if everyone took the vaccine compared to if nobody took it.\n\nCorrect.\n\n> That said, it\'s always interesting to estimate heterogeneous treatment effects for different groups, but it is often challenging (for reasons ranging from statistical power to study design).\n\nIn many cases, yes. But in this case, it doesn\'t particularly seem difficult to control for severity of acute covid. So the fact that this was not done in these studies is strange to me.\n\n> Finally, controlling for severe covid would not solve this problem. At best, it would tell you the effect of vaccines through the other mechanisms, which might or might not be interesting (but it\'s more likely to tell you nothing useful at all).\n\nI am not sure how it wouldn\'t solve the problem? If we control for severity of acute illness, it would indeed tell you the effect of vaccines on the other mechanisms/causes of long covid, and yes that would be interesting: that is what we are looking for/that is what would be relevant for approximately 90% of the population (given that roughly 90% of the population didn\'t get severe acute covid in the first place without vaccination). So I am not sure why you are downplaying this or saying it will likely not tell you anything useful at all.\n\nAgain, correct me if I am wrong, but if these studies are simply taking ""non vaccinated"" group vs ""vaccinated"" group, then saying ""vaccination is associated with a 15-30% reduction in long covid"", and if vaccines don\'t prevent the other mechanisms/causes of long covid (except severe acute covid itself), and if around 90% of people do not get severe acute covid without vaccination, is that not an example of the base rate fallacy?\n\nhttps://en.wikipedia.org/wiki/Base_rate_fallacy\n\nInterestingly, the diagram example in the wiki page is about covid itself: it is saying how it is wrong/committing the base rate fallacy to say that there are more vaccinated in the hospitals than the unvaccinated, because the base rate is different: much more of the population is vaccinated to begin with, and this leads to a larger OVERALL number of vaccinated individuals being hospitalized, but it would be wrong/meaningless to say that ""vaccination is associated with increased hospitalization"". So isn\'t my example similar? Why would it go one way by not the other? Isn\'t science and statistics supposed to be objective? Why should we pick and choose? How can it be base rate fallacy in one case, and not the other? Again, if my analogy is wrong, please let me know how.', ""No one is going to try to reason with someone who has time to type 80000 paragraphs of misguided nonsense and clearly has an agenda dude. You're not a millionth as smart as you think you are. I won't waste my time."", '> You asked r/statistics. You\'re not going to find that subject matter expertise here. I suggest you ask on another sub if you want to get an answer from someone with that domain experience.\nr/statistics.  \n\nMy question was contingent on people understanding the medical premises. That is required in order to talk about whether the statistical procedures are correct or not. So it was directly relevant for me to clarify. Also, statistics is used for a purpose, not just for the heck of it. \n\n> The presence of vaccination does in fact suppress COVID-19 severity [1, 2]. In addition, COVID-19 severity increases the risk of developing long COVID [3].\n\nAgain, you are using incorrect/irrelevant research to shape your initially faulty premise. You have it backwards. That ""research"" you point to, all it does it take vaccinated and unvaccinated group and show lower OVERALL rate of long covid in unvaccinated group. This is because there are obviously less people with severe acute covid in the unvaccinated group. This is NOT sufficient to claim that if someone has mild infection without the vaccine, their long covid risk will go down if they get slightly more mild infection after the vaccine. This MIGHT be true, but it HAS NOT BEEN CONTROLLED FOR. THAT IS WHAT I AM TALKING ABOUT: WHY DON\'T WE CONTROL FOR IT? \n\n> Something else to note is that illness severity, immune dysfunction, persistent viral load, and inflammation from the spike protein are all probably going to be interrelated. Any model that treats these as independent mediators of COVID-19 is likely not reflective of reality.\n\nFirst you tell me to not even post this here because this sub is about statistics, now you are making all these medical assumptions? There is not enough studies to 100% PROVE this but, on the balance, the 100s of studies I looked at, these causes do seem to be largely or entirely independent of illness severity. I even gave an example: cognitive impairment and loss of smell/taste is actually INVERSELY correlated with age and illness severity. That is, older/more ill people have LESS of it compared to younger people with more mild covid. Also, vaccine injured people report the same symptoms as those with long covid, and vaccine injured people do not get severe acute covid.  There are many other studies. But EVEN IF you were right and all these causes were interrelated, STILL, the RATE at which long covid is reduced in someone with mild covid who is unvaccinated vs someone with mild covid who is vaccinated seems to be either none or very small, which implies that the vaccine either fully, or largely does not protect against the other 3 mechanisms, therefore the 3 other mechanisms are either unrelated to, or very slightly correlated with illness severity.  \n\nHere is a legitimate source about the different mechanisms:\nhttps://www.theatlantic.com/ideas/archive/2022/10/long-post-covid-symptoms-mild-cases/670469/\n>he cases of long COVID that turn up in news reports, the medical literature, and in the offices of doctors like me fall into a few rough (and sometimes overlapping) categories. The first seems most readily explainable: the combination of organ damage, often profound physical debilitation, and poor mental health inflicted by severe pneumonia and resultant critical illness. This serious long-term COVID-19 complication gets relatively little media attention despite its severity. The coronavirus can cause acute respiratory distress syndrome, the gravest form of pneumonia, which can in turn provoke a spiral of inflammation and injury that can end up taking down virtually every organ.\n\n>...Recently, a second category of long COVID has made headlines. It includes the new onset of recognized medical conditionslike heart disease, a stroke, or a blood clotafter a mild COVID-19 infection. It might seem odd that an upper respiratory tract infection could trigger a heart attack. Yet this pattern has been well described after other common respiratory-virus infections, particularly influenza. \n\n>...Instead, for some the term may invoke a chronic illnessa complex of numerous unexplained, potentially debilitating symptomseven among those who may barely have felt sick with COVID in the acute phase. Symptoms may vary widely, and include severe fatigue, cognitive issues often described as brain fog, shortness of breath, internal tremors, gastrointestinal problems, palpitations, dizziness, and many other issues around the bodyall typically following a mild acute respiratory infection. If the other forms of long COVID seem more easily explainable, this type is often characterized as a medical mystery.\n\n> ..Teasing apart which kind of long COVID a person has is important, both to advance our understanding of the illness and to best care for people.\n\nHere is another study showing just how prevalent heart issues are in those with mild or asymptomatic infection (which also implies there are other mechanisms than illness severity):\nhttps://publichealth.jhu.edu/2022/covid-and-the-heart-it-spares-no-one\n\nRemember, illness severity is about RESPIRATORY symptoms. These long covid symptoms are VASCULAR/NEURO symptoms. This again implies that there are DIFFERENT MECHANISMS involved. So whether or not someone is vaccinated, if they have mild covid and get the sniffles, why on earth would you logically expect that a ""slightly less"" severe sniffle would cause any meaningful reduction in long covid rates?\n\nHere is another study showing that the spike protein can directly damage the heart (again, this implies it has nothing to do with respiratory illness/severity of acute covid illness):\nhttps://newsroom.heart.org/news/coronavirus-spike-protein-activated-natural-immune-response-damaged-heart-muscle-cells\n> Our study provides two pieces of evidence that the SARS-CoV-2 spike protein does not need ACE2 to injure the heart. First, we found that the SARS-CoV-2 spike protein injured the heart of lab mice. Different from ACE2 in humans, ACE2 in mice does not interact with SARS-CoV-2 spike protein, therefore, SARS-CoV-2 spike protein did not injure the heart by directly disrupting ACE2 function. Second, although both the SARS-CoV-2 and NL63 coronaviruses use ACE2 as a receptor to infect cells, only the SARS-CoV-2 spike protein interacted with TLR4 and inflamed the heart muscle cells. Therefore, our study presents a novel, ACE2-independent pathological role of the SARS-CoV-2 spike protein,  Lin said. This research takes the first step toward determining whether the SARS-CoV-2 spike protein affects the heart. The researchers now plan to investigate how SARS-CoV-2 spike proteins cause inflammation in the heart. There are two potential ways: the first is that spike protein is expressed in the virus-infected heart muscle cells and thereby directly activates inflammation; the second is that the virus spike protein is shed into the bloodstream, and the circulating SARS-CoV-2 spike proteins damage the heart.\n\nHere is proof from Harvard that post-vaccine myocarditis is due to circulating spike protein, this has NOTHING to do with illness severity, the vaccine did not give these people acute respiratory covid symptoms:\nhttps://www.tctmd.com/news/free-spike-protein-mrna-covid-19-vaccines-implicated-myocarditis', 'Vaccines don\'t prevent infection though.\n\n\\>  But for the vast majority of the population, the burden of covid far outweighs the very small risk from the vaccine.\n\nDid you ignore my entire post that you replied to? Are you saying children with 0.01% chance of severe covid should have been vaccinated + boosted?\n\nYou are assuming that it is either vaccinate all or nothing. This is not the case. There should be individual risk/benefit analysis. If it is too impractical to do a literal individual risk/benefit analysis, then at least for demographics, for examples healthy young children, healthy adults 30 to 65, elderly, immunocompromised, etc...\n\n\\> And its absolutely true that there would be a lot more death and suffering in the entire population without the vaccine.\n\nyou are using circular logic. You didn\'t address my arguments. Nowhere did I say there shouldn\'t have been vaccination. I said specific demographics may not have passed the risk/benefit analysis. We can\'t just pick and choose statistics.\n\nExperts/media are quick to use correct statistics when it benefits vaccines, but seem to forget elementary statistics when it comes to anything that shows the vaccine in a bad light. This is not science, this is subjective human bias.\n\nFor example, they keep saying how ""just because  there are more vaccinated people in hospitals doesn\'t mean the vaccines don\'t work, it is because there are more vaccinated people in the overall population than unvaccinated people, so the rate of hospitalization is actually lower for the vaccinated"". This is true, but then they come and say ""vaccination reduces long covid"" without specifying that this statement doesn\'t apply to 90%+ of people. We can\'t just pick and choose, that is not science. Science and statistics is supposed to be objective. Objective statistics should drive out opinions, it should not be that our biased/pre-existing opinions shape our statistical methods.', '>Again, why did they not control for severity of acute illness?\n\nBecause that would not answer the question. Another poster gave a great example using parachutes. I suggest you go back to that one and make sure you understand it.', "">I am not sure how it wouldn't solve the problem? If we control for severity of acute illness, it would indeed tell you the effect of vaccines on the other mechanisms/causes of long covid, and yes that would be interesting: that is what we are looking for/that is what would be relevant for approximately 90% of the population (given that roughly 90% of the population didn't get severe acute covid in the first place without vaccination). So I am not sure why you are downplaying this or saying it will likely not tell you anything useful at all.\n\nThe problem is that severity of the acute illness is itself an outcome of the vaccine, and controlling for an outcome is very tricky. As I said, at best it gets you the partial treatment effect through other channels, but that require additional assumptions.\n\nWhat you want to do instead is to condition on pre-determined variables, such as age or physical fitness. That is both uncontroversial and useful, although as I said you need to be careful to get it right statistically.\n\n>So isn't my example similar?\n\nNo, because your example is about a variable which is itself an outcome of the treatment. Furthermore, the base rate fallacy is about failing to consider the full population. This issue is different - a population ATE of vaccines on long covid is a well defined statistical object, and requires no adjustment of the sort you're talking about. Again, it can be more or less interesting, but it's not wrong or bad statistics."", 'Also ""what\'s wrong with having them rate the severity of their COVID""\n\nSo for one thing, we don\'t have a consistent definition of severe covid so far. If you look at hospitalized vs non-hospitalized and the effects, there are studies doing this. \n\nYou also seem to think long COVID is the only thing vaccines prevent, and they also prevent death and other systemic effects that aren\'t classified as long COVID. \n\nIf you are set on doing a literature review and critique you need to do a full pull of the literature not just cherry pick studies.', ""Me: uses inferential logic from studies published from Harvard and such.\n\nYou: you are an anti-vaxxer\n\nMe: how so?\n\nYou: I won't waste time.\n\nGot it. Move along please."", 'This is my final comment. Engaging in these conversations takes time and energy, and based on your other responses, you seem to be willfully obtuse. \n\n> My question was contingent on people understanding the medical premises. That is required in order to talk about whether the statistical procedures are correct or not. So it was directly relevant for me to clarify. Also, statistics is used for a purpose, not just for the heck of it.\n\nMDs usually do not have sufficient training to run statistical analyses. In many cases, they rely on statisticians to aid them. That is where we come in. We usually work synergistically to solve these problems.\n\n> Again, you are using incorrect/irrelevant research to shape your initially faulty premise. You have it backwards. That ""research"" you point to, all it does it take vaccinated and unvaccinated group and show lower OVERALL rate of long covid in unvaccinated group. This is because there are obviously less people with severe acute covid in the unvaccinated group. This is NOT sufficient to claim that if someone has mild infection without the vaccine, their long covid risk will go down if they get slightly more mild infection after the vaccine. This MIGHT be true, but it HAS NOT BEEN CONTROLLED FOR. THAT IS WHAT I AM TALKING ABOUT: WHY DON\'T WE CONTROL FOR IT?\n\nI think youre getting the relationship reversed: there is an overall lower rate of long COVID in the vaccinated group. In addition, multiple COVID infections is something that was never discussed in your original research question. But I already told you why we cant control for disease severity when examining the effect of vaccination on long COVID. Doing so would show a null effect of vaccination, when in fact vaccination is protective against long COVID. \n\nBasically the question that you are asking*if a vaccine does not protect against severe disease, then can it still protect against long COVID?*is impossible and not scientifically useful to answer because vaccines protect against severe disease. \n\n> > Something else to note is that illness severity, immune dysfunction, persistent viral load, and inflammation from the spike protein are all probably going to be interrelated. Any model that treats these as independent mediators of COVID-19 is likely not reflective of reality.\n\n> First you tell me to not even post this here because this sub is about statistics, now you are making all these medical assumptions? There is not enough studies to 100% PROVE this but, on the balance, the 100s of studies I looked at, these causes do seem to be largely or entirely independent of illness severity. I even gave an example: cognitive impairment and loss of smell/taste is actually INVERSELY correlated with age and illness severity. That is, older/more ill people have LESS of it compared to younger people with more mild covid. Also, vaccine injured people report the same symptoms as those with long covid, and vaccine injured people do not get severe acute covid.  There are many other studies. But EVEN IF you were right and all these causes were interrelated, STILL, the RATE at which long covid is reduced in someone with mild covid who is unvaccinated vs someone with mild covid who is vaccinated seems to be either none or very small, which implies that the vaccine either fully, or largely does not protect against the other 3 mechanisms, therefore the 3 other mechanisms are either unrelated to, or very slightly correlated with illness severity.\n\n> Here is a legitimate source about the different mechanisms: https://www.theatlantic.com/ideas/archive/2022/10/long-post-covid-symptoms-mild-cases/670469/\n\nI make these medical assumptions because Ive worked in analytics long enough to know that mediators often are associated with each other. If we treat them as possibly related and find out theyre not related, then thats fine. We learned something new. But if we force them to be unrelated when in fact they are related, we miss valuable information that helps us determine how these relationships work. \n\nThe relationships between cognitive impairment and loss of senses could be floor effects (i.e., perhaps theres less measurable decrease in cognitive function because of lower cognitive function in older adults).\n\n> Remember, illness severity is about RESPIRATORY symptoms. These long covid symptoms are VASCULAR/NEURO symptoms. This again implies that there are DIFFERENT MECHANISMS involved. So whether or not someone is vaccinated, if they have mild covid and get the sniffles, why on earth would you logically expect that a ""slightly less"" severe sniffle would cause any meaningful reduction in long covid rates?\n\nBecause I cited a study that said so?', 'You say\n\n>There should be individual risk/benefit analysis.\n\nI said\n\n>Among many other things, you are assuming that vaccine policy is made based on individual risks and benefits. Thats not the case with vaccines for deadly infectious diseases. Controlling infectious disease requires policies for everyone likely to transmit that infectious disease, not just those most at risk for being seriously harmed by the disease. \n\nThis is our disagreement. Yes, children should be vaccinated. They transmit the disease to their parents and grandparents. Just like people around infants should be vaccinated against the flu and whooping cough. This is even true in the case of a vaccine that primarily limits the duration and infectiousness of a disease, like the COVID vaccine. It still limits transmission.', 'How about that poster and you answer why you don\'t understand this:\n\nIt is basically like telling100% of the population of a country that winter tires will reduce therisk of a car accident, yet 90%+ of the population lives/drives in aplace where it does not snow. And you said this because you took asample of 1000 people, and divided them into 2 groups, 500 people withwinter tires, and 500 people without winter tires, but DID NOT CONTROLFOR THE VARIABLE ""snow on the road"", and found an OVERALL 15-30%reduction in accidents in the group that had winter tires on their car.Well obviously that is because SOME people in both groups live in areaswith snow, and those in the winter tire group would be less likely toget into an accident compared to those in the no winter tire group wholive in snowy areas, meaning that the OVERALL rate of accidents is lowerin the ""winter tire"" group. Then based on that concluding ""winter tiresreduce the chances of accidents"". TECHNICALLY you would be correct, butthis statement would be MEANINGLESS for those who live/drive in an areawhere it does not snow.', 'If we assume that 90% of people get mild/asymptomatic illness regardless of the vaccine, why would it be a problem? That would mean the vaccine is not having an effect on these people no? Unless you are implying there are different degrees of mild/asymptomatic illness, in that it is possible a vaccinated individual can have ""relatively"" less mild/asymptomatic illness compared to an unvaccinated person with mild asymptomatic illness? I wouldn\'t give much credibility to such a theory, because we see that a large proportion of people who get mild/asymptomatic illness, whether they were vaccinated or not, end up with long covid, and there seems to be no practical difference in terms of frequency/intensity of their symptoms based on observational studies. But even assuming there is/even if the vaccine makes ""mild/asymptomatic"" illness ""more"" ""mild/asymptomatic"", then we can still measure and control for severity of acute covid can we not?  \n\n> No, because your example is about a variable which is itself an outcome of the treatment. Furthermore, the base rate fallacy is about failing to consider the full population. \n\nYou lost me there. The base rate fallacy is about failing to consider the full population, and if 90% of the population don\'t get severe acute covid anyways, and then you tell people ""vaccines reduce chances of long covid by 15-30%"" WITHOUT controlling for severe covid, meaning that this would not be relevant for 90% of the population, how is that not committing base rate fallacy? You are basing that 15-30% on those who had/will have severe covid, which is less than 10% of the population. How is that not related to the concept of base rate fallacy? I don\'t know, maybe it is not a perfect or classical example of base rate fallacy, but it seems like there is still a sort of base rate bias going on here, because that 15-30% number is based on a small % (around 10% or less) of the population), then being applied to the population as a whole (90% of whom it does not apply to). \n\n> Again, it can be more or less interesting, but it\'s not wrong or bad statistics.\n\nThat\'s an interesting way of wording it. It would also not be ""technically"" wrong to say that something reduces the chances of something by 0.1%, then saying ""x is associated with a reduction in y"". What practical utility does that have? It doesn\'t seem ""less interesting"" to me, it seems practically useless/misleading to me.\n\nHere is the practical importance of what I said. Check out this study:\nhttps://www.nature.com/articles/s41598-023-32939-0\n\nIt is saying that long covid causes cognitive impairment in a significant proportion of younger people, and not older people (or at least very rarely relatively to younger people). Yet older people are obviously more likely to get severe acute covid. And vaccines are strong at preventing severe acute covid. So logically, wouldn\'t that mean that the vaccines are not protecting against the mechanism/cause of long covid that causes cognitive impairment, as there is an inverse relationship with cognitive impairment and age (older are actually LESS likely to get cognitive impairment, while significantly more susceptible to severe acute covid)? So again, what practical utility is there/how honest is it to blanket state ""vaccination reduces the chances of long covid"" to the population.', 'My question is, do vaccines protect against the OTHER 3 mechanisms of long covid (that are UNRELATED to illness severity). We need to CONTROL for the variable (illness severity) to find this out. It is not that difficult to control, it is that they didn\'t bother to control it at all. For example, excluding hospitalized from the study is one way of partially controlling for this, but bizarrely the studies did not do this. Of course it is medically useful to control for this. If 99.9% of a given demographic do not get severe covid without vaccination, and anyone in this group gets long covid, it would be due to the 3 OTHER mechanisms (UNRELATED to illness severity) that would cause their long covid. So how does it make sense to run a study in which you take 1000 random people, divide them into 2 groups of 500, DO NOT CONTROL for illness severity, obviously find lower rates of long covid in the ""vaccinated"" group (because obviously, vaccines reduce severe acute covid, and severe acute covid itself is ONE cause of long covid) then make a global health policy decision that tells EVERYBODY in the population that their risk of long covid will be reduced with vaccination, even though this applies to 0.01% person of certain demographics. If you can\'t understand this basic research methods problem, I don\'t know what to tell you.\n\nThis is basic correlation does not necessarily equal causation stuff. WHY do you think it is stressed so many times that correlation does not necessarily equal causation? Because if we blindly assume correlation=causation, we don\'t know WHAT is causing WHAT (all we know is that SOMETHING(S) is/are causing an OVERALL increase/decrease in something, and that is not useful for INDIVIDUALS. Humans operate at at individual level, we are not all literally connected. You can\'t have 2.3 people sitting at a table. If you say on average a restaurant can hold 2.3 people a table, that extra .3 is ABSOLUTELY USELESS to the INDIVIDUAL 3rd person who WON\'T get a seat at a table of 2. So both from a common sense point of view and a medical point of view, causation is important to establish.\n\nThere is nothing wrong with using these correlation studies initially, but afterward we need better quality studies that try to look for causation. It doesn\'t have to be perfect, even if we partially control for certain variables it is better than nothing. **Don\'t you find it interesting that the same people who AUTOMATICALLY downplayed ANY other POTENTIAL treatment for this virus by saying ""NOT RCT=ABSOLUTELY MEANINGLESS CORRELATIONAL STUDY. WHO CARES if x drug is 80% associated with a reduction in covid severity, study is ABSOLUTELY useless because it is correlational and not an RCT. WILL NOT LISTEN."" are the SAME ones who are not even TRYING to control for basic confounders like this illness severity in the long covid example, and purely relying on low quality correlational studies and then very confidently and vocally saying ""vaccination reduces the chances of long covid"" to ALL demographics, including those who have less than a 0.01% chance of having their long covid risk reduced by vaccination? You don\'t find the double standard strange?**', 'Does OP genuinely think vaccines are solely to protect the person getting the vaccine?', ""Again, you give an example which is different in important ways from the vaccination case. \n\n(I'll assume that treatment assignment is as good as random in the following, in order not to introduce additional confusion).\n\nIn your winter tire example, the overall accident reduction is a population average treatment effect, as I've explained before. However, there are clearly heterogeneous effects which are interesting - the treatment effect of winter tires will be larger than the ATE in snowy areas, and close to zero in areas without snow. This is interesting and important, and one way of figuring this out would be to perform the analysis separately for the two groups. Note, however, that there's nothing wrong with the ATE, and it's not a base rate fallacy (as you've argued elsewhere). \n\nNow (and this is a crucial point), you *CANNOT* do the same thing in the vaccine case, because severe illness is directly affected by vaccines. Many of us have tried to explain this to you again and again,  but you somehow refuse to accept it. Since you're still confused by this, I suggest you read up on causal inference."", ""Your premise you are setting up here would be saying 90+% live in an area where they can't catch COVID..... \n\nAnd you don't seem to understand how public health or vaccination work at all based on the comparison you are trying to make."", ""| If we assume that 90% of people get mild/asymptomatic illness regardless of the vaccine, why would it be a problem? That would mean the vaccine is not having an effect on these people no?\n\nI've been trying to understand what you're talking about for like 10 minutes now and I think this is the statement you're misunderstanding. The claim the studies are making is that if 90% of people get mild/asymptomatic illness regardless of the vaccine, the vaccine reduces the the probability of then developing long-covid by 15-30%. The causal effect in this case is independent of whether or not people will have mild covid regardless of the treatment of the vaccine.\n\nWithout having a background in this literature, I'd image the reason why people aren't conditioning on damage from acute covid is that acute covid is an outcome of the vaccine treatment. If I have some set of covariates X that describe a patient pre-vaccine and pre-covid, I give them a vaccine Treatment D and observe outcome Y, acute covid seems to me like it would just be an outcome, where almost always you only condition on your covariates X."", 'Fine. I\'ll quickly bite.\n\nThe design I proposed earlier in which I allowed all mediators to covary (i.e., examined relationships between disease severity, immune functioning, etc.) does all of the ""controlling"" you\'re seeking. In fact, it answers everything you want to know.\n\nThe question about whether vaccination decreases likelihood of long COVID while controlling for disease severity cannot be answered, but the question about whether vaccination decreases likelihood of long COVID by decreasing disease severity can be answered. (This language is a bit more causal than it should be, but I\'ll keep it for clarity.) These are two different questions, to be clear.\n\nIt also ameliorates that post-treatment effect by acknowledging the sequential nature of events vaccination triggers a change in disease severity, immune functioning, etc. which in turn decreases risk of long COVID. Whether vaccination in and of itself decreases long COVID apart from these risks is a question of partial vs. full mediation.\n\nI don\'t think there\'s any more to be had from this discussion. You\'ve had your question answered multiple times by multiple experts, and yet you are still replying to us with walls of text that are extremely difficult to read. I don\'t mean to sound rude, but I have a full-time job and a life outside of Reddit. I love sharing knowledge, but to have it ignored time and time again despite explaining it in multiple ways isn\'t respectful of my boundaries. Thank you.', ""Yes. And doesn't know how to read statistics or medical literature."", ""I can't believe I have to type this/that there are still people like you out there, who lack basic elementary knowledge on this issue this late. If you believe that vaccinations are meaningful for preventing transmission, then you lack absolutely elementary medical knowledge on this issue. It looks like you have taken your info from mainstream media early in the pandemic and are stuck there. Because even the CDC/mainstream media has admitted for quite a while how weak vaccines are at preventing infection. **This is common sense: how many people do you know who are vaccinated? How many of them were infected? Almost all right? So on what basis are you making your bizarre claim that vaccines are sufficient or meaningful to prevent infection? You can downvote and personally insult me all you want, this does not change the facts.**\n\nI suggest you read up on the difference between relative risk reduction and absolute risk reduction. At no time did the vaccines have an absolute risk reduction of higher than around 5%.\n\nSince omicron, vaccines provide a slight reduction in risk of infection for a few months, then they wear off to 0 protection. So, unless you get boosted every 3 months or so, AND wear masks/take other precautions, the protection from vaccines in the omicron era will not be sufficient to prevent infection. So for all practical purposes, the vaccine does not protect against infection.\n\nThere are tons of research studies that could teach you this, if you bothered to read them.\n\nEVEN the CDC admitted this as early as Delta (keep in mind vaccines offer much less protection against infection against Omicron):\n\n[https://www.cnn.com/2021/07/27/politics/cdc-mask-guidance/index.html](https://www.cnn.com/2021/07/27/politics/cdc-mask-guidance/index.html)\n\n>The US Centers for Disease Control and Prevention on Tuesday changed its masking recommendations as it grows more concerned over the Delta variant of Covid-19, urging vaccinated people in certain areas of the country to resume wearing masks indoors in public areas. In recent days I have seen new scientific data from recent outbreak investigations showing that that Delta variant behaves uniquely differently from past strains of the virus that cause Covid-19, Walensky told reporters.\n\n[https://www.nature.com/articles/d41586-022-00775-3](https://www.nature.com/articles/d41586-022-00775-3) [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9828372/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9828372/) [https://www.ctvnews.ca/health/coronavirus/protection-against-infection-offered-by-fourth-covid-19-vaccine-dose-wanes-quickly-israeli-study-finds-1.5850153](https://www.ctvnews.ca/health/coronavirus/protection-against-infection-offered-by-fourth-covid-19-vaccine-dose-wanes-quickly-israeli-study-finds-1.5850153)\n\nShould I keep going?\n\nIf you had any basic knowledge on the issue, you would know that a vaccine in the arm would be extremely unlikely to provide sterilizing immunity (protection against infection) against a virus that enters in through the nose/mouth in the first place. Rather, you would typically need a live attenuated virus vaccine administered nasally to get meaningful protection against infection from such a virus. This is basic, elementary medical knowledge. If you had any clue on this topic, you would realize that it was absolutely a surprise that the vaccines initially (against the first few variants) even had some degree of protection against infection: this was not expected (though of course, their RATE of protection against infection was deliberately inflated/overstated in the media/by certain organizations, such as by misleading the public about relative risk reduction and absolute risk reduction).\n\n[https://thehill.com/changing-america/well-being/prevention-cures/501677-what-is-sterilizing-immunity-and-do-we-need-it/](https://thehill.com/changing-america/well-being/prevention-cures/501677-what-is-sterilizing-immunity-and-do-we-need-it/)\n\nSo don't insult others when you have no clue what you are talking about. You did nothing to refute any of my claims in this thread, all you did was throw irrelevant childish incorrect straw man arguments and childish insults and even that ended up backfiring for you and showing your ignorance."", '>\\ Now (and this is a crucial point), you CANNOT do the same thing in the vaccine case, because severe illness is directly affected by vaccines.\n\nI understood this from the beginning, but I am talking from a PRACTICAL point of view, because the chances of severe acute covid are SO LOW for the vast majority of people. That is why I gave the winter tire example. I know what you are saying, but you are being pedantic and hiding behind general technical statements and what you say has no practical utility.\n\nLet me give you an example. If a healthy 12 year year old child has a 0.01% chance of getting severe acute covid, YES, I UNDERSTAND, TECHNICALLY, the vaccine will further reduce that risk to maybe 0.001% or something like that, and in doing so, reduce the OVERALL risk of long covid for that individual as well. But PRACTICALLY, this would be NEGLIGIBLE, because if that person gets infected and gets long covid, there is a 99.9% chance that the cause of long covid will be UNRELATED to ""severe acute covid"", because there is a 99.9% chance that individual will NOT get severe acute covid without the vaccine. So if they get long covid, they will get it via one of the OTHER 3 cases/mechanisms of long covid that are INDEPENDENT of ""illness severity"" to begin with. So HOW does it make practical sense to tell this individual ""vaccination reduces the chances of long covid"", when there is a less than 0.01% chance they will get long covid via a mechanism that vaccination protects against?\n\nhttps://www.reddit.com/r/statistics/comments/13giv5p/comment/jk4fqi2/?utm_source=share&utm_medium=web2x&context=3', 'No. That is not what I said. Literally read my posts. I said 90%+ will NOT GET SEVERE COVID without vaccination.', '>I\'ve been trying to understand what you\'re talking about for like 10minutes now and I think this is the statement you\'re misunderstanding.The claim the studies are making is that if 90% of people getmild/asymptomatic illness regardless of the vaccine, the vaccine reducesthe the probability of then developing long-covid by 15-30%.\n\nNo wonder you are confused. This is WRONG.\n\nAll the studies did was take ""vaccinated group"" and ""unvaccinated group"". And found that OVERALL, there is 15-30% less long covid in ""vaccinated group"". They DID NOT CONTROL FOR severity of illness, as I said a thousand times. They took 100% of the population (via their sample), and saw if there is a difference in terms of long covid rates between ""vaccinated"" and ""unvaccinated"".\n\nOBVIOUSLY, if ""severe acute covid"" ITSELF is ONE (of 4 different) causes of long covid, we would expect long covid rates to be lower in the ""vaccinated group"" (because vaccinated reduces the rate of severe acute covid, so how on earth can you get ""long covid via the mechanism of severe acute covid"" if you don\'t get severe acute covid in the first place?), but not by much, because there are 3 other DIFFERENT mechanisms/causes of long covid, that vaccines DO NOT protect against. And that is EXACTLY why we see only a 15-30% reduction in long covid rates in the ""vaccinated group"", and not higher: this 15-30% reduction must be because the vaccinated group is 15-30% less likely to get severe acute covid in the first place.\n\nSo then (THIS IS A SEPARATE PART): if we assume that 90%+ of the population won\'t get severe acute covid EVEN IF THEY WERE NOT VACCINATED, that means that for 90%+ of the population as a whole, the vaccines are useless at reducing long covid, because if any of these people get long covid, it must logically be due to a cause OTHER than severe acute covid, because they DIDN\'T/WON\'T get severe acute covid, but they WILL get infected (therefore they will get mild/asymptomatic infection).\n\nThat means only less than 10% of the population will have their risk of long covid reduced by being vaccinated, because these people will get severe acute infection, and need the vaccine to not get severe acute infection, which can lead to long covid via the mechanism ""damage from severe acute covid""\n\nDoes that clarify things?\n\nSo then how does it make logical sense to make the blanket statement ""vaccination reduces the rate of long covid"" to the population as a whole, when this is not the case for 90%+ of people?\\\\\n\n**It is basically like telling 100% of the population of a country that winter tires will reduce the risk of a car accident, yet 90%+ of the population lives/drives in a place where it does not snow. And you said this because you took a sample of 1000 people, and divided them into 2 groups, 500 people with winter tires, and 500 people without winter tires, but DID NOT CONTROL FOR THE VARIABLE ""snow on the road"", and found an OVERALL 15-30% reduction in accidents in the group that had winter tires on their car. Well obviously that is because SOME people in both groups live in areas with snow, and those in the winter tire group would be less likely to get into an accident compared to those in the no winter tire group who live in snowy areas, meaning that the OVERALL rate of accidents is lower in the ""winter tire"" group. Then based on that concluding ""winter tires reduce the chances of accidents"". TECHNICALLY you would be correct, but this statement would be MEANINGLESS for those who live/drive in an area where it does not snow.**', 'Good job failing to read even one sentence from me lmao. No wonder you have such a hard time reading research papers. Why should vaccination policy for vaccines that you\'ve just agreed do protect against infection and reduce spread of infection across the population only be determined based on individual risk of infection?\n\nFrom your first article ""Vaccines protect against infection from Omicron subvariant  but not for long""\n\nLiterally says that vaccines protect from infection. Are you that bad at reading you couldn\'t even start the title?', ""\\*shrug\\*. Someone here is stuck in a media bubble. I've read the studies you cite. I think they say that the vaccine limits transmission of COVID. It doesn't prevent infection or transmission entirely, but it limits transmission. I think those studies say that the vaccine is an important tool to reduce the burden COVID places on society. A lot of PhDs agree with this reading of the evidence."", ""Again, the ATE is a well-defined concept that is often useful and informative (yes, in this case too). You're arguing that the treatment effect is different for different groups, and that is entirely correct - but this is true for basically *every* treatment effect we can think of, in medicine, economics, psychology etc. Nobody is trying to hide this. It's just hard to estimate heterogeneous effects in a convincing manner."", ""But your analogy the actual analogue would be they don't live where they could catch COVID (live where it doesn't snow). \n\nAnd vaccines don't only have to do with long COVID prevention."", ""So you are making things up to fit your narrative.....your entire premise is just off base to start. I'd suggest trying to actually understand the correct and lengthy responses people are giving you rather than be contrarian to match something you've decided is true."", '> *shrug*\n\nThis, along with your/other downvotes, does nothing for your arguments. You can downvote, name call, shrug all you want, but you provided ABSOLUTELY ZERO refutation of my arguments so far. You can\'t magically/randomly change objective reality by subjective emotionally driven downvotes/attempts at censorship because you personally got emotionally triggered and were unable to provide logical refutation of a point. This makes 0 logical sense. \n\n> Someone here is stuck in a media bubble. \n\nNot sure why you would write this to me. Clearly, the person I replied to is in the media bubble (that too not even updated). Again, 0 refutation of this argument with this line of yours.\n\n> It doesn\'t prevent infection or transmission entirely, but it limits transmission. \n\nNow you are shifting the goalposts to agree with me, but you are being deliberately vague to not lose face. I said that it PRACTICALLY or MEANINGFULLY does not prevent infection at this point. Again, 0 refutation on your part. The sources I backed my argument up with all clearly show that there is SOME protection from infection, but it wanes after a few months. What I said: vaccines do not practically/meaningfully prevent infection, because if you want meaningful/practical protection against infection, then you need to get boosted literally every 3 months for like AND wear masks + take other precautions. Do you know anybody on this earth who has gotten a booster every 3 months for the past 2 years? If so, how many? Is it safe to say less than 1% of humans have done this? Again, 0 refutation on your end of my argument. Again, downvoting/trying to censor me via downvotes does not change the facts. If you want to target my argument do so, but downvoting and shrugging your shoulder or shaming me does not change the facts. This is not about me or you, this is about objective reality, the natural laws of science/the earth. I am simply stating it as it is. You shrugging your shoulders is not going to change photosynthesis, neither will it change the facts about viral transmission and these vaccines, as factually observed in real life in recent history, and as scientific studies show.\n\n> I think those studies say that the vaccine is an important tool to reduce the burden COVID places on society. A lot of PhDs agree with this reading of the evidence.\n\nThis is irrelevant. I never doubted this. Also, you are using appeal to authority fallacy with the PhD thing, and are setting it up like a straw man to imply I said something that I didn\'t and a lot of PhDs disagreed with me, which is simply not true. I never disagree with this, so it is irrelevant. I simply said we need to be HONEST about WHAT the vaccines help with, instead of hiding behind GENERAL/VAGUE/MISLEADING statements due to PICKING AND CHOOSING when we use statistical procedures/when we call out statistical fallacies (e.g. correlation=causation, base rate fallacy) and when we don\'t, then doubling down and using appeal to authority fallacy to say we are right and anybody who used objective science to criticize us is wrong because they are not formally experts. Experts are not god, if they are picking and choosing and not being objective, or hiding behind ""technical"" or general formal rules/being deliberately vague and being impractical, then they are wrong, regardless of their previous ladder climbing on the formal education system, which becomes irrelevant it is not being used/applied properly: the point of their expertise is to be experts, if they are picking and choosing and not being objective, then practically they are no longer experts, and are serving personal/subjective/political agendas. All I am asking for is consistently and scientific objective, and informed consent. Vaccines clearly show significant reduction of severe acute covid. But when you have a demographics that has less than a 0.01% chance of being protect against long covid with vaccines, I don\'t see how it makes sense to use vague statistical studies that don\'t control for necessary variables, then blanket state to EVERYONE that ""vaccines reduce the chance of long covid"", implying that each and every individual will have a practical/meaningful reduction in long covid DUE to getting vaccinated. I have clearly shown that this is not the case, and not a single person has refuted me. \n\nThe closest anyone came in this thread is to use vague generalities like ""you cannot technically use this statistical prodecure because it introduces bias in the treatment effect"" or a similar vague/technical statement detached from practical reality in this case, and even then they did not specifically prove their case, because they don\'t seem to be factoring in that there are 3 OTHER/INDEPENDENT/UNIQUE causes of long covid that vaccines DO NOT protect against and that have NOTHING TO DO WITH SEVERE ACUTE COVID, which is the only thing the vaccines protect against. I have no idea why these people in the ivory towers claim excluding people who are hospitalized for example will introduce ""treatment bias"" and thus will lead to a meaningless study. It makes no practical sense. If there are 3 OTHER/COMPLETELY DIFFERENT causes of long covid, and if 90+% of people don\'t get severe acute covid without vaccination, then practically, I don\'t see the problem with running a study that excludes all hospitalized people, and then checking what is the hospitalized rate in the vaccinated or unvaccinated group: this would be PRACTICALLY more useful for nonhospitalized people/people who didn\'t/won\'t get severe acute covid EVEN WITHOUT VACCINATION, which constitute the overwhelmingly majority of the population.\n\nI don\'t get how this study would not be practically more relevant/clinically/medically useful for 90+% of people, compared to a study that simply takes 1000 people, divides them into 2 groups of vaccinated vs unvaccinated, DOES NOT CONTROL for illness severity (so for example maybe 50 people hospitalized in the unvaccinated group, and only 5 in the vaccinated group, and 99% of the difference in long covid rates between the groups is caused by these hospitalized people, meaning long covid rates in the non hospitalized people in both groups, aka the vast majority, is virtually similar, then claiming VACCINATION PROTECTS AGAINST LONG COVID.. well no... all this study shows is vaccination protects against long covid FOR THOSE HOSPITALIZED/with severe acute covid... it does NOT prove that vaccination reduces long covid for those who were not hospitalized/did not get severe acute covid). But every time I say this they use appeal to authority fallacy, they say they are the ""experts"" in statistics, and that this would ""introduce treatment bias"" ""therefore the results would be meaningless"". How can they be meaningless? Use some basic logic and common sense, how can it be meaningless to compare vaccination and unvaccinated group of non hospitalized patients and compared their long covid rates? This would practically be a much more useful study, given that practically, the vast majority are not hospitalized/do not get severe covid, but many of them still get long covid after even mild/asymptomatic infection.', '&#x200B;\n\nWhat would be wrong with asking people in the vaccinated and unvaccinated group to rate their acute illness severity on a scale of 1-10? To match them before comparing long covid rates? How is this different to, for example, excluding hospitalized from the study? Why did the researchers not do this? Isn\'t this common sense?\n\nInstead what they are doing is taking vaccinated vs unvaccinated, then seeing OVERALL CORRELATIONS between ""vaccination status and long covid"". While this is good for an initial study just to see if there is a correlation, obviously, the problem is, we know that vaccines reduce the chances of severe acute covid, and severe acute covid can cause long covid itself, but the vast majority of people will not get severe acute covid without vaccination, so this overall correlation is useless to them without more info. So why did they stop at just that overall correlation and not control for illness severity like I mentioned in my above paragraph?', ""It's not that they can't catch it. It's that  they won't develop severe acute illness. It's called an analogy for a reason: it doesn't have to match directly/exactly.\n\n\\> And vaccines don't only have to do with long COVID prevention.\n\nHow is this relevant?"", ""Rather, others don't understand how there are 4 different mechanisms of long covid, and are operating based on a faulty premise (that mild acute covid vs mild acute covid = difference in long covid rates, when there is no evidence for this). That is why they don't understand me. I wrote the winter tire example, but you and others ignored it and censored me/downvoted me instead. Rather strange. How will censoring something solve it? Did you read the winter tire example? Which part of it is wrong?""]"
[Q] What type of statistical test should I use to compare between time variables?,"Hello! I am analyzing some survival data using Kaplan-Meier Curves and log rank tests. This part was easy enough to do in SPSS but I now want to compare the overall survival of some patients with the prognostic time they first received. I can't do this with K-M and log rank

My question is which statistical test should I use for this nonparametric data: basically comparing ""time A"" variable with ""time B"" variable. 

I'm not a statistician sorry if this question may seem dumb to y'all I'm still learning this stuff",13giryz,aeon_throwback,1683987439.0,4,0.84,"[""There are parametric survival methods that exist.  I don't use spss so I don't know the specifics of what it is capable of.  The link I am sharing is an overview of those models, but using Stata instead of SPSS.  I imagine SPSS has similar capabilities.\n\nhttps://stats.oarc.ucla.edu/stata/examples/asa2/applied-survival-analysis-by-hosmer-lemeshow-and-maychapter-8-parametric-regression-models/"", 'Cox model?  https://www.r-bloggers.com/2016/12/cox-proportional-hazards-model/', ""Thank you I'll look into it""]"
[Q] Is this enough info to calculate the sample size?,"All I am given is the list of variables, their estimates, variances and covariances.  I swear this is not enough information, and am stuck.",13gepbu,Bigg_UN,1683976246.0,8,0.9,"['Those are definitely not going to reveal the sample size. Unless you have the actual sample, or the reported sample size, its going to be difficult to determine the sample size', ""I give homework exercises and quiz questions where I give the mean & std error and ask for n. Or % and MoE and ask for N. It's not particularly clever, but just another way to make sure they know the formulas. Is this homework, or how the heck do you end up with this question?"", 'No but you could guess ', 'Are you trying to do a prospective, planned sample size calculation, or back calculate what the sample size must have been?', 'I think I know what kind of question this is. \nWrite formulas for all like R2, covariance etc and try to solve for n.', 'Its just R output and the question wants me to find the sample size using the info, but i think it hasnt all displayed properly', ""The output most likely contains a confidence interval, based on a standard error, and the corresponding variance(s) or standard deviation(s) from which that standard error was derived.\n\nFind the values you can link to some algebra and solve for sample size. If you think the output is missing something, explain what you were looking for and why you can't find it."", '@D-Juice\n\nOkay guys I was thinking, sometimes the output has Degrees of Freedom. Depending on the test you might be able to find the sample size. If its a 1 sample r test or a dependent samples t test, df = n-1. If its a simple linear regression, df=n-2.']"
[Q] How to conclude if multiple slightly different trendlines can be described by one overal trendline for the data?,"Hello,

for my research i gather data and for every series i can make a trendline (it's just a simple linear regression so excel trendline is fine). This results in me having about 5 trendlines that are nearly identical. how can i prove that fitting one trendline through all the data is correct (opposed to having 5 slightly different trendlines)?

i have calculated the prediction interval which says something about what the spread at a certain x value is. i also calculated the confidence interval which says something about what the mean y is at a certiain x. 

yet i don't think these are the correct calculation to conclude fitting one overal trendline is correct. what can i use to conclude this?

thank you!",13gcfwj,KalibuPorter,1683968998.0,3,0.81,"[""Pool the data, and create an indicator variable for each series. Regress the outcome variable on the time variable, along with the set of indicators and a set of interactions between time and the series indicators (excluding one reference group to avoid perfect collinearity). Now each interaction term estimate the difference in slope between the reference series and the others. To test if they're all the same, you can test all of the interactions jointly."", 'well in both cases you have residuals, either with respect to the five curves or in the latter the latter case the one curve. You can look at the distribution of residuals in both cases and if you have some prior expectations about how measurement error should look like, you can see if having more parameters just make the residuals unrealistically small compared to expected noise or whether if they are justified. you can also do some sort of cross-validation (leave %30 out fit to the rest look at how well the your model predicts, do for many times for both models and compare to each other and see if extra parameters give ""significant"" improvements). Since this is linear regression I think you can also directly set some sort of chi2 test.', 'The model with one trend line is nested within the model with 5 trend lines.  You can test that one is sufficient vs. 5 with an F-test.', 'Its early but doesnt it seem like you want a model reduction F test or something to see that you dont lose much information when reducing the predictors in your regression?', 'A video  came my way which reminded me this question and some of the things discussed here so I am posting it here:\n\n[https://www.youtube.com/watch?v=CqLGvwi-5Pc](https://www.youtube.com/watch?v=CqLGvwi-5Pc)\n\nIn particular after 11.19 is exactly what you are asking for I think. If it sounds interesting you can watch some of the preliminary videos such as t-test, linear regression etc to get a better idea on what he is getting at.', 'This sounds nice, is this much different than doing 5 seperate linear regresions, one pooled regression and comparing where the slopes from 5 regressions stand with respect to the parameter estimate of the pooled regression and its standard error', 'I recommend the same approach as standard_error. This is probably the outcome you are looking for', ""hmmm, i don't think thats what im looking for. \n\nbasically \n\nH0: intercept and slope of each trendline = intercept and slope of generelized trendline\n\nHa:intercept and slope of each trendline =/= intercept and slope of generelized trendline.\n\nthis is kind of what im trying"", 'Why an f test? I ultimately went with a t test where I compared the slope of one trend line with the overall slope', 'yeah i just looked it up again and read it a bit better and this is what i needed. i need to do it five times to compare them against the overall trendline. i think the t test i did also worked in the end. thanks though', 'The fully interacted regression I suggest (assuming no control variables, or that all controls also are interacted with the group indicators) will give results identical to what you get from running five separate regressions. The advantage of the pooled approach is that you get inference (standard errors, p-values etc) on the differences in slopes directly from the regression.', 'Arent you basically trying to say having multiple fitted lines do not improve predictions when compared to a single fitted line? that kind of analysis is usually done with either looking residuals or doing some sort of cross-validation. With such analyses you can get a feel for whether the extra parameters you add to the model are redundant or not.', 'if you really want to work with fitted parameters you can do bootstrapping:\n\n1- add noise to your system or do bootstrap resampling of your data or assign normally distirbuted weights to your residuals \n\n2- refit parameters in both models, do this many times\n\n3- compare the distribution of obtained parameters for single line to those of five lines.', ""Another thing that comes to mind is Bayesian linear regression. In this way yous get credible intervals on your slope and intercept and see if one models' intervals and the other ones overlap significantly. But I think the first method with residuals is the more intuitive one."", 'I think u/iavicenna is giving you sound advice.    It sounds like you might be interested in a nested likelihood ratio test.  In the restricted model, your data all follow the same trend line.  In the full model, each data set follows its own trend line.      \n\nIf you assume residuals are normally distributed, then you can use the residual variances to obtain an F-statistic or chi-squared (using Wilk\'s theorem).  \n\nThe outcome of these tests won\'t ""prove"" anything, but they will let you get a p-value.', 'If you want to test whether one overall line is okay relative to 5 different lines then that leads to an F-test.', 'Great! I believe multiple individual t tests have less power than a grouped test (F test, etc). However as you have said they agree so things seem explainable.', 'no i have a chemical system and everytime i slightly change the composition and measure the same thing. i want to determine if the relation changes. so i need to be able to say that the trendline stays the same even if i change the composition.', ""yeah i think i just might've done that (sorry, statistics was a looong time ago...) but i performed a t test where i took the difference of the two slopes and saw wether or not it was different from 0. is this basically an f test?"", 'Right in this case look for my post above either about Bayesian linear fitting or bootstrapping.  Or alternatively (and more easily) calculate the SD of your parameter estimates (for the single trend line) via  \n\nhttps://stats.stackexchange.com/questions/289457/proof-for-the-standard-error-of-parameters-in-linear-regression\n\nand see within how many SDs are your other parameter estimates from the ""fit one curve to one trend line"" model is.']"
[Q] What’s the term to describe low relationship strength due to low population variability?,Is there a specific term that describes the phenomenon of sample variability being too low to detect relationships?,13g610x,thebigmotorunit,1683948810.0,1,1.0,"[""It's called [range restriction](https://stats.stackexchange.com/questions/404954/significance-of-correlation-with-range-restriction-in-x)."", 'If Im understanding your question correctly, you might be referring to [power](https://www.scribbr.com/statistics/statistical-power/)', 'Thanks a ton!']"
Stats roadmap for CS student [E],"

The title says it all. I'm currently in bachelor degree in CS, and already doing an internship in data engineering. What is the path in statistics that I should take in order to understand ""the data science"" better? I can fit curves, do regressions and etc, but I genuinely want to learn all the statistics around this stuff. Thank you very much.

TLDR: CS student wants to learn all the basic of statistics to work with data.",13g3iez,squ1rt13,1683941666.0,1,1.0,"['When it comes to statistics, alot of the basic starting points revolve around probability distributions (discrete and continuous) and what you can do with them. \n\nThen when you get comfortable with that you can move along with point estimates and finding efficient/sufficient/consistent estimates. \n\nI dont know if hypotheses testing would be of benefit for you, however it is critical to the statistical processes in which you want to test the probability  of a parameter is true or not. Typically by calculating most powerful test(MPT) for type 1 and type 2 errors.\n\nIf anyone else wants to fill in any gaps I missed please feel free. Things like confidence intervals, sample size, etc.', 'Im also a CS student going into ML. Starting out w Ross is great, I loved his probability book. The next step would be mathematical statistics, the most common undergrad recommendation is Wackerley. If your math is good - for ex a first course in real analysis, you can go to a more rigorous math stats book, such as Casella & Berger or Hogg. \n\nFor more applied modeling stuff, Ive heard really good things abt Elements of Statistical Learning. Theres also great ML books too, such as Bishops PRML.', ""Thank you very much and noted. It's good to have a place to start. Currently studying Ross's A First Course in Probability. Trying to get the grasp.""]"
[E] - ANOVA on multicenter clinical trial - Exercise on 3 models,"I'm doing an exercise on a multicenter clnical trial to train when to apply specific types of ANOVA or mixed models.  
  
The exercise goes as follows:  
  
""  
  
A clinical trial on hypertension was randomized with double-blind to compare three different treatments:  
  
New drug (A-Carvedilol) Existing drug 1 (B-Nifedipine) Existing drug 2 (C-Atenolol). 29 centers were selected and patients were randomized into treatments. The study consisted of 6 visits: 2 considered pre-treatment and 4 during treatment:  
  
Visit 1 (week 0): measurements (including diastolic blood pressure, dbp) were taken to verify if patients met inclusion criteria, and a placebo was administered for hypertension treatment during one week. Visit 2 (week 1): measurements were repeated, and patients who met the inclusion criteria were randomized into one of the 3 treatments. Visits 3-6 (weeks 3, 5, 7 and 9): dbp measurements were obtained in 2-week periods. 311 patients were recruited, 288 were considered eligible for randomization, and 30 patients dropped out of the study until visit 6. The primary endpoint was diastolic blood pressure (dbp).  
  
The proposed analysis consists of analyzing the dbp value at visit 6 with the following models:  
  
Model 1 (fixed effects only): i =mu + dpbBase + treat + centre Model 2 (with center as a random effects factor) Model 3 (with center and center\*treatment interaction as random effects)""  
  
Given that we're ""fixing"" at visit 6, we could see this as a ANOVA of one block, right? In this case, we'd have 1 quantitative variable (dpbBase) + 1 categorical as independent factor (treat) + 1 categorical as independent factor (centre), correct?  
  
Thus:  
  
Model 1 - We'd have a model of type y = u + bi + tau\_j + error where b\_i would be the effect of block i (in this case 6), and thau\_j represents eht effect of treatment j. Would this logic be correct? Here we'd apply a randomized block model with fixed blocks (of fixed effects).  
  
Model 2 - Now, we make center as a random effects. Here, since we now have a mix of fixed + random (fixed would be treatment and random the center), we start to get in ""mixed models"" territory, right? From my understanding, this would be an hierachical model, right? Since we could start from the Center factor and then branch out onto the treatment factor. I don't think this would be a random coefficients model since we have no repeated measures in this analysis (from what I think at least) and we're fixing on visit 6 (we don't have a ""time"" factor here)  
  
Model 3 - We now have interaction + fixed and random effects here. Which type of model would be applied here? Would it also be an hierarchical model? Can we even evaluate this interaction in an hiearrchical model? I believe it would be DBP = mu + dpbBase + Treat\_k + center\_j + center\*Treat\_(j)\_k + error, correct?  
  
Thank you in advance!",13g0nvt,Independency,1683934220.0,1,1.0,[]
[E] Motivating Example to (Benevolently!) Trick People into Understanding Hypothesis Testing,"I'm a PhD student in statistics and wanted to share a motivating example of the general logic behind hypothesis testing that has gotten more ""oh my god... I get it"" responses from undergraduates than anything else I've tried.

My hunch - almost everyone understands the idea of a hypothesis test inherently, without ever thinking about it or identifying it as such in their own heads. I tell my students hypothesis testing is basically just ""calling bullshit on the null"" (e.g., you wake up from a coma and notice it's snowing... do you think it's the summertime? No, because if it were summertime, there's almost no chance it would be snowing... I call bullshit on the null). The example I give below, I think, also makes clear to students why a null and alternative hypothesis are actually necessary.

The Example: Let's say you want to know if a coin is fair. So you flip it 10 times, and get 10 heads. After explaining the p-value is the probability, under the null, of a result as / more unlikely than the one we observed, most students can calculate it in this case. It's p(10 heads) + p(10 tails) = 2\*\[(0.5)\^10\] = (0.5)\^9. This is a tiny number that students know means they should ""reject the null"" at any reasonable alpha level, even if they don't really understand the procedure they are performing.

I then ask: ""Do you think this is a fair coin?"" To which they say, of course not! When I ask why, most people, after some thought, will say, ""because if it were fair, there's no way we would have gotten 10 heads"". I write this on the board. I then strike out ""because if it were fair"", and replace it with ""if the null hypothesis were true"", and similarly replace ""there's no way we would have gotten 10 heads"" with ""we'd see ten heads/tails only (0.5)\^9 percent of the time"". Hence, calling bullshit.

This is usually enough for them to realize that they use this thinking all the time. But, the final step in getting them to understand the role of the different hypotheses is by asking them how they got their p-value of (0.5)\^9. Why didn't you use P(heads) = 0.4 instead of 0.5? The reason is because the null hypothesis is that the coin is fair, meaning P(heads) = 0.5! This is the ""aha"" moment for most people, in my experience - **by getting them to convince themselves they HAD to choose a certain P(heads) to calculate the odds of getting 10 heads, they realize the role of the null hypothesis. You can't calculate how likely/unlikely your observed statistic is without it!**",13fu17a,sample_staDisDick,1683918364.0,104,0.97,"['I love this sort of thing.\n\nA huge problem with understanding hypothesis testing is just the absolutely bizarre language that it uses.\n\nAn intuitive notion of the null hypothesis IMO is the ""devil\'s advocate"" who\'s job is to always argue. ""Nope, nothing to see here folks. Be on your way!"" This devil\'s advocate however, can only make arguments based upon shared knowledge that both they and you have about observed likelihoods. So they\'re always limited to marking arguments in the form of something like:   \n\n\n""Common, you\'re telling me that these are two different groups? There\'s a 15% chance of seeing what you saw if they were a single group. You can\'t honestly tell me that\'s good enough"".\n\nor \n\n""Common, you think that this observation didn\'t come from that group? There\'s a 2% chance that it did. That\'s 1/50. Are you willing to risk that?""\n\nIt\'s up to us to consider the devil\'s advocate\'s argument and decide whether or not we\'re persuaded by them, or we think that they\'re being overly cautious.', 'Maybe a daft question on my end but if you observe ""10 heads"", why then is the following calculation made:\n\n> p(10 heads) + p(10 tails)\n\nIs that not saying ""10 of the same is observed"", i.e. ""10 heads or 10 tails"", not just ""10 heads""?', 'I love this! Nothing against ""Active learning"" but I love a good, clear walkthrough of a concept.', 'Here is an OLD video of mine dong something similar-- it is a much better thing to do in person, where I trick a student into thinking they got 10 guesses in a row correct... though after 5, 6, or 7 flips almost everyone thinks that something is up. https://youtu.be/Y5UPmUN1w94', ""It's great that you've independently discovered this approach - here are some slight wrinkles on it with [weighted dice](https://www.researchgate.net/publication/230192798_A_Classroom_Demonstration_of_Hypothesis_Testing) and [playing cards](https://www.tandfonline.com/doi/full/10.1080/10691898.1994.11910464)."", 'Love this, OP! Youd have a great time checking out simulation based inference (called randomization based inference too) approaches to intro stats! Lots in the journal of data science and statistics education indicating this approach works well for all levels of learner.', 'I use the following example. Joe tells me he is a good driver. I wonder if that is true, so I ask Joe how many accidents he was in last year and he says ""two."" I then get the students to decide that a good driver is not likely to have two accidents in a year. Then I put all of that thinking into hypothesis testing language.', ""Great explanation. I recently saw a similar question and I believe most of the answers were wrong, let me know what answers you all get and how?  \n\n\nHow would the answer change if we saw 1 head and 9 tails. Assume the null and alternate are the same (two-sided alternate). I'm thinking we calculate the probability of seeing 1 head and then add the probability of seeing 0 heads as well(because this is more extreme) and multiple this by two to account for the tails side of things, just like in the original post. Is that correct? Many of the answers seem to miss the 'or more extreme' part and thus fail to include the probability of seeing 0 heads"", 'nice, i might borrow that!', '> I then strike out ""because if it were fair"", and replace it with ""if the null hypothesis were true"", and similarly replace ""there\'s no way we would have gotten 10 heads"" with ""we\'d see ten heads/tails only (0.5)^9 percent of the time"". Hence, calling bullshit.\n\nBut what if you get three heads in a row? ""If the null hypothesis were true, we\'d see ten heads/tails only (0.5)^2 percent of the time""\n\n0.25% seems very low - less than the magic 5%, for sure. So do we call bullshit on the null? Why or why not?', 'This is why I come to reddit', ""My go to explanation is to call the null the boring hypothesis. It's almost always the one where everything is the same and nothing changes. The p-value is then the probability of seeing something at least as interesting as what we observed, under the assumption that everything is truly boring. A small p-value then suggests that there is at least one interesting thing going on."", 'Following!', 'Stealing this', 'This is a great way to put it, and also taps into the idea that this stuff is more familiar to people than they might think! Even though it gets obfuscated by terribly confusing language.\n\nThe language is so confusing that you can really understand it and still accidentally mess it up when talking about it - I do all the time. Please don\'t take this to mean that I think you don\'t understand this, but I honestly think that\'s what happened in the two examples you gave.\n\nI fully agree with the first phrasing. The second phrasing, I believe, is not true. The first is ""Probability(observing something as/unlikely as what we saw **| (given) |** null hypothesis is true)"". The second is ""Probability(null hypothesis is true **| given |** we observed what we saw)"".\n\nI think I know what you\'re getting at though/perhaps meant to say, because the ""are you willing to risk that?"" concept is a great way to think about hypothesis tests IMO, because really, that\'s the alpha level. I believe an accurate interpretation of alpha = 0.05 is ""if the null actually **is** true, you\'ll make the wrong decision 5% of the time (by rejecting)"". But this doesn\'t mean you\'ll make the wrong decision 5% of the time overall, because the probability the null is true isn\'t 1.\n\nWhat clarified this for me (and something I honestly didn\'t believe) is the fact that when doing a test for difference of means, for example, when the null actually is true and the means are the same, *the p-value is uniformly distributed between 0 and 1.* This is super bizarre to think about - it\'s **not** the case that when the null hypothesis is true, you should expect large p-values. They are just totally random!\n\nSo, what\'s the probability of getting a p-value less than alpha = 0.05 when the null is true and the distribution of p-values is Unif\\[0, 1\\]? Well, that\'s 0.05... meaning you\'ll erroneously reject the null hypothesis 5% of the time *when the null is true.* You\'ll make this mistake 0% of the time *when the null is false*. \n\nOnce again, apologies that this post reads like a lengthy correction - it was intended for the thread as a whole because I think you inadvertently pointed out a really easy pitfall that exists in large part due to the awful language you described!', 'the p-value is the probability, under the null, of a result _as/more unlikely_ than the one we observed i.e. the probability of a result as unlikely plus the probability of a result more unlikely.', ""Suppose you observed 2 heads, then 2 tails, then 1 head, then 3 tails, then one head and one tail. The probability of this happening is also (0.5)^10, but it's not as effective at making the null hypothesis seem unlikely."", 'This is absolutely true! Here\'s the direct calculation. Recall the null here is p(H) = p(T) which makes the null distribution of then number of heads out of 10 tosses a symmetric distribution, which means we can cheat and multiply tail probabilities by 2. You wouldn\'t be able to do that if, for example, you wanted to test against the null hypothesis that heads is twice as likely as tails. But for now let\'s stick with the null being equal probability of heads and tails.\n\nYou get 1 heads and 9 tails. The probability of this event under the null is (1/2)\\^10 times the number of ways to rearrange (i.e., TTTTTTTTH vs HTTTTTTTTT...) of which there are (10 choose 1) = 10. There are ""ten places to place the H out of ten slots"".\n\nTurns out this has probability 0.0098. Doing the same thing with 0 heads gives probability 0.00098 (can you convince yourself of why this probability is exactly 1/10th of 0.0098?). Adding these up and multiplying by 2 gives us 0.01074. Multiplying that by 2 yields a **p-value of 0.0214, meaning getting 1 heads out of 10 would cause us to reject the null hypothesis using the typical alpha = 0.05 level.**', 'This is a subtle point, but I think it hopefully answers your question. The null distribution is the distribution of.... what, exactly? It\'s the distribution of *your chosen* ***test statistic,*** *under the null hypothesis that p(H) = p(T).*\n\nWhy is this important? Well, in the original example, the test statistic is quite specifically *the number of heads thrown out of 10 tosses.* What if instead we chose our test statistic  to be the exact sequence of H/T out of 3 tosses, which is the statistic implied by your question, I think? (note: I\'m kind of abusing the word ""statistic"", now, since this isn\'t a number and really just an outcome, but the math is still valid).\n\nWell if we observe 3 heads out of 3 tosses, under the null, and our test statistic is the sequence HHH (as opposed to our test statistic being **3**), the probability of that event under the null is (0.5)\\^3 = 12.5%. But of all possible outcomes from 3 tosses under the null (there are 2\\^3 = 8 of them), **all combinations of H/T has probability 12.5%, so the sum of events ""as/more unlikely than the one we observed"" is the sum of all outcomes with probability under the null equal to 12.5 or lower.** Well, 12.5 is ""equal to or lower than 12.5%, so our p-value is 8(0.125) = 1.\n\nWe wouldn\'t ever be able to reject anything with this test statistic, because **the p-value of** ***any*** **outcome would be 1!** This is the exact issue you hear about when it comes to ""statistical power"", which is determined by sample size, choice of null hypothesis, and importantly, **choice of test statistic,** and relates to the ability of a hypothesis test to detect a difference in the event that the null is actually false. The example above I have has no power at all.\n\nThe coin could literally have heads on both sides and the above procedure would always give you a p-value of 1.', ""> It's almost always the one where everything is the same and nothing changes.\n\nI thought it was the negation of your claim. For example, if you wanted to prove that a drug was ineffective, would your null hypothesis wouldn't be that the drug was ineffective (i.e. the same as your experimental hypothesis)?"", 'Sorry if I am being a bit slow here, but in this case would the P(10 heads) be the ""as likely"", and the P(10 tails) be the ""more unlikely""? \n\nI am struggling to attach those descriptors to the calculation of the probability values since they seem a bit detached / independent for me i.e. the descriptions wrt the corresponding p-values.', 'Sorry could you step through this a little more slowly? I always have (have had) brain farts when it comes to working with hypothesis testing. \n\nDo you arrive at the (0.5)^9 value in a similar way to OP? As in 2*(0.5)^10? In that case, I can\'t realize where this ""2 multiplier comes in"".\n\nAlso sorry but I am struggling a little bit to link your example to what is being discussed by OP?', ""u/sample_staDisDick, just incase anyone reads this in the future. Your answer of 0.0214 matches mine (specifically, 22/(2\\*\\*10) = 0.021484375). However, in\n\n> Adding these up and multiplying by 2 gives us 0.01074\n \nI think your writing has an extra 'multiply by 2' after adding the 0.0098 and 0.00098 because you also say \n\n> Multiplying that by 2 yields\n\nAnyways, answer if right but just want to avoid confusion for others.\n\nAlso, given that this is a two-sided test using and assuming we are using a significance level of 0.05, of course we reject the null if the p-value is 0.0214, but if the p-value was something like 0.04, am I correct that we fail to reject the null?"", "">  Well, in the original example, the test statistic is quite specifically the number of heads thrown out of 10 tosses\n\nAnd in my example the test statistic is, like yours, the number of heads thrown out of 3 tosses, not the exact sequence.\n\nAre you sure you didn't mean to reply to [this](https://ol.reddit.com/r/statistics/comments/13fu17a/e_motivating_example_to_benevolently_trick_people/jjyadqx/) comment, instead?"", 'Why would you want to prove that a drug is ineffective? Not much profit in that.\n\nThe standard approach is to assume that it is ineffective, and check whether the data provides evidence against the assumption. If not, we continue to assume it is ineffective, but if the evidence is there then profit might come from an effective drug.', 'What is more likely: to find 10 heads or 10 tails?', 'Sorry, I made a mistake. The probability of that specific sequence of 10 coin flips occuring is (0.5)^10, not (0.5)^10', 'Ack, sorry! This is my first reddit post and I clearly got confused with the thread/also think that I truly merged your two comments in my head when replying to you... I unfortunately spend last night in an airport terminal after getting booted from an overbooked flight and am quite tired.\n\nTo answer **your** question - I made a mistake by using the word ""percent"" lazily in my initial post (""(0.5)\\^9 percent of the time"" should have been ""with probability (0.5)\\^9""). The p-value you calculated should be 0.25, i.e., 25% of the time, not 0.25 percent of the time. So 3 heads out of three tosses isn\'t enough to reject any any alpha level below 0.25, certainly not 0.05!\n\nSorry for what probably felt like a poorly-aimed/condescending response.', 'Both would be equally as ""unlikely"" as the other. But hasn\'t the hypothesis posed explicitly ""10 heads"". \n\nI mean isn\'t any arbitrary sequence of H and T as equally unlikely as observing as the other?', ""See my other reply to u/anonymousTestPoster, above, and let me know if it's still unclear!"", 'Exactly, both are as likely. So P(10H) is the _observed result_, and P(10T) is a result _as likely_ as the observed result, following the naming above in the definition of p-value.\n\n> But hasnt the hypothesis posed explicitly 10 heads?\n\nRead again the definition of p-value. If still not clear, check out the Statquest video about p-value.', 'Not being ""slow"" at all! Happy to try and map the outcomes you\'re describing to the relevant probabilities, and let me know if it\'s not sticking and I\'ll try it another way.\n\nWhat you said is absolutely true - for example, HHHHHTTTTT is equally likely (under the null, that is - where H and T are equally likely on any given toss) as HHHHHHHHHH or TTTTTTTTTT. However the null distribution in question here is a particular distribution for *the* ***number*** *of heads thrown out of ten*, as opposed to the distribution of exact sequences of H/T of length 10. It just so happens that when you have 10 H or 10 T, *there is no difference between the probability of ten heads, vs. the probability of HHHHHHHHHH, because there is only one way to get 10 heads - namely, the exact sequence above.*\n\nSo under the null where p(H) = p(T) = 0.5, the probability of HHHHHTTTTT is 1/(2\\^10), but the probability of getting *5 heads out of ten throws* is actually (10 choose 5)/(2\\^10) = 24.6%.\n\nYou can try out all the other numbers of heads (0 through 4, 6 through 10) and realize that all of these probabilities will be lower than 24.6%. So if you got 5 heads, and added up all the probabilities that were ""as / more unlikely than getting 5 heads, which has a probability of 24.6% under the null"", well, you\'d be adding up the probabilities of every number between 0 and 10 heads because they are all as/more unlikely than getting 5 heads. **So your p-value here would be 1.00  and we would not reject the null at any alpha level!**', 'Another quick point - the hypothesis is that p(heads) = p(tails) = 0.5. The explicitly ""10 heads"" part is the outcome we observed, where the ""outcome"" is the specific observation of our *chosen* test statistic (the number of heads explicitly out of 10 coin tosses).', 'Thank you I think that helped to clear up a lot of things. It clicked when I saw the *(10 choose 5)* part, and realized I was obsessing too much over ""specific sequences"" rather than a question such as: ""what is the probability of observing 5 heads"" for example, rather than the example you gave of the following:\n\n> So under the null where p(H) = p(T) = 0.5, the probability of HHHHHTTTTT is 1/(2^10), but the probability of getting 5 heads out of ten throws is actually (10 choose 5)/(2^10) = 24.6%.\n\nThank you very much for persisting with me through this exercise :)\n\nEDIT: Actually I do have a follow up question, I suppose the application of p-values in such a discrete setting has opened up a small can of worms for me:\n\nSay we are looking at the p-value for observing three heads in ten coin tosses. As you stated (I believe), the p-value is the probability of observing events *as/more unlikely* than three heads in this case, so it would be calculated as (I think):\n\np-value = p(3 heads) + p(2 heads) + p(1 head) + p(0 head) + p(7 heads) + p(8 heads) + p(9 heads) + p(10 heads) ???\n\nI went from 3 --> 0 and 7 --> 10 under the assumption of symmetry of calculating [n Choose r]\n\nIn this case, my question is, why define the p-value in this way? I am primarily concerned in my hypothesis with *3 heads*, why introduce 9 heads, or 2 heads in this calculation? I don\'t care about those values (in this example). It seems like I would be convoluting the problem in this way, and simply be adding more complexity to my hypothesis test no?', 'This is a great question! To briefly address your question about calculating the p-value for observing three heads, your calculation is correct! Minor thing to note is that the reason symmetry worked for you here isn\'t because of the symmetry of (n Choose r), but because of the symmetry of the remaining terms of the binomial formula:\n\n(n Choose r) \\* ***\\[p\\]\\^r \\* \\[1 - p\\]\\^(n - r)*****,**\n\nstemming from the fact that p(heads) = p(tails) makes (1 - p) and (p) both equal to each other at a value of 0.5.\n\nFor your main question, it makes more intuitive sense in the continuous case where probabilities only exist for ranges of values (e.g., P(x > some value)) and don\'t really exist for single points. This is the ""P(X = x) = 0 for any particular value of x when X is a continuous random variable"" thing you may have run into. The ""density"" of X at the value x is really a proportional representation of the probability of finding a value between (x - epsilon) and (x + epsilon) where epsilon is arbitrarily small - it\'s a ""tiny little neighborhood around x"".\n\nIt\'s less obvious why we would represent a p-value in this way for a discrete variable, where we can directly calculate the probability mass of, say, X = 3 in our example where X is the number of heads thrown out of ten tosses. The way to think about, in my opinion, why we define the p-value as the sum of all the probabilities of events as / more unlikely under the null (in our case, the p-value is p(0) + p(1) + p(2) + p(3) + p(7) + p(8) + p(9) + p(10) = 0.344), is thinking about it as:\n\n*a p-value of 0.344 indicates that, if the null hypothesis were true, only 34.4% of observed events* ***would provide more evidence against the null than the outcome we observed.***\n\nThinking about it in this way allows us to see our observed outcome in comparison to all the other outcomes we could have seen that would have provided even more evidence against the null hypothesis. So, if we get a p-value of 0.01, for instance, by calculating the p-value in the way we do, we can talk about our observed outcome being in the ""99th percentile of all outcomes in terms of providing evidence **against** the null hypothesis"".']"
[Q] SPSS Custom Table (Crosstab) Macros?,"The firm I work for uses SPSS to create crosstabs of each question. We use the ""custom table"" option and manually drag each question and then export each question to excel. Is there a macro, or a way to automate this? It would save a lot of time.",13ft8i3,pballa2099,1683916484.0,0,0.33,"['Firstly, use syntax instead of manually dragging things. Typing it out once and then changing variables is already much faster\nSecond step is then to standardize your tables, so you can create a macro and only feed it the variables. Thirdly, you can use the Output Management System (OMS), or simply export all viewer content into a specified excel document.', 'May I add an SPSS question here as well\n\nWhat test should i do after chi-square? I have one data set and two questions with multiple responses: silly example; A) Do you own fish (y/n)?, and, B) How happy do the fish make you (scale of 1-4)?', 'This is helpful, if youre willing to hop on a call next week Im happy to pay for an hour of your time. This will save me a lot of time in my workflow, I am unfamiliar with how to write the syntax but I am sure with a little guide I could figure it out.', 'You can look at the standardized residual for each cell to see what is generating the sign effect.', ""Dm me. It's past midnight where I live, but i can show you the details another time.""]"
[Q] How can I calculate/present this data most accurately? Surface area and volume ratios.,"I'm a doctor analysing some data from patient scans to try to gain some insight into various parts of the disease process I'm interested in.

  
I am analysing some data using 3D modelling files. The software is a commercial medical tech company's, so it's proprietary and only certain parameters are available for me to use.  
Lets say I have a number of 3D models of various objects, and I have the volume of each model.  
On the surface of these objects are abnormal patches, for which I have calculated the total surface area abnormal patches on each model.

  
I do NOT have the total surface area of each model, only the volume.

  
I wish to perform some comparisons of different objects in two groups, by comparing how much abnormal patches they have. However, because each object has a different size (volume), these comparisons are meaningless unless I find some way to standardise or index them.  
I thought about doing a very crude surface area in cm2/volume in cc or ml

  
However the problem with this is that at lower volumes, the ratio becomes exaggerated (due to high surface area to volume ratio) and at higher volumes, the opposite occurs.

  
Can anyone think of a better way to measure/compare these data with the given parameters I have? I appreciate there may not be a good answer to this, but I'd be willing to listen to bad answers, ones with lots of potential inaccuracies, as long as I can justify that those were the best way to do these calculations given the circumstances, when I present them to my colleagues in the hope that we can collect some preliminary data in order to potentially come up with some research studies that may add to our knowledge in this disease area.",13fsbgv,Pow_Surfer,1683914340.0,3,1.0,"[""If the models are all approximately similar shapes - doesn't matter what shape as long as they're all more or less comparable - then the square root of the volume will be roughly proportional to the surface area. If the shapes are quite different that's not going to be very accurate but it'd still be a justifiable way to make the volume: surface area ratio a little more consistent across sizes, and fully consistent for any given shape.\n\nE2A: if the models are organs, you could probably find some data on typical volume:surface area for each organ and tighten it up that way."", ""> square root of the volume will be roughly proportional to the surface area. \n\nI'd have thought 2/3 not 1/2 since surface area would be proportional to a linear dimension squared and volume to its cube, (unless we're dealing with fractals)""]"
[Q] Need help finding research papers published between 1967-1968 that neglected their effect size,"Based on research published in the early 2000s, older psychotherapy research papers commonly had those mistakes and especially in the 60s Would love some help from anyone who can come up with a few names, even the later ones from the 70s and 80s could do :) Thanks",13fq5py,Adventurous_Taste_38,1683909524.0,0,0.5,"[""Presumably the 'research' cited examples. Start there."", ""I looked at one examples and went to the research, but then the research takes me to other research that analysis similar mistakes, yet they never explicitly say which papers they used, because they disproved them or something\nThis should've been an easier task but it's taken me way too much time and effort..."", ""If you don't like time and effort, don't do research. No one here is going to have a mental list they can handily download straight into your brain.\n\nFocus on a topic area and type of research, do a literature search, and plough through the results."", ""I literally just need a resource or something to help me find those old research papers to get an unaddressed statistical error from it to use on my course work.. I thought I found it when I found articles about psychotherapy. I am very inexperienced and I am tried really hard but I still don't know how to properly read research papers I guess..\nEven the retractiondatabase website is no help"", ""Are you saying that you have the reference but you can't find the paper?\n\nTry scihub.se. You may need a VPN to acces it, depending on where you live.""]"
[D] Linear Regression for Beginners,"Linear regression is a statistical method used to determine the relationship between two or more variables. In particular, it is used to find the best-fit straight line that describes the relationship between the dependent variable and one or more independent variables. The dependent variable is the variable that is being predicted or explained, while the independent variables are the ones that are used to explain the dependent variable.
https://link.medium.com/DNDBCdzfKzb",13fps9y,Confident_Western,1683908691.0,0,0.11,"['This is not a statistical description of linear regression.  Using a statistical software package, we obtain the following output. This needs to be explained.  How do you use such software to perform a linear regression? When you do so, what is the computer doing?']"
[D] Please share any career or educational advice for rising senior,"I am a rising senior, double majoring in economics and statistics. I do not plan on attending graduate school right after I graduate, so I understand the following year is crucial as I will be going into the job market. Can y'all please share any general advice you have to help me finish strong and put myself in a good position upon graduation next year? For example, when and how I should search for jobs, specific skills I should pick up within the next year, and anything you wish you did before your senior year of college. Thank you for your time.",13fpdsv,average-at-average,1683907790.0,3,0.71,"['If you have not had any formal public speaking courses, take them now. The professional world, and doubly so any technical field, is over run with weak communicators. Having both economics and statistics degrees (which I happen to also have, btw) you are expected to both be capable of complex analysis as well as explain your analysis to those without your specialist education. You will find many of your educational and professional peers are not formally trained presenters, they are learning on the fly, and not presenting their best selves. Presenting is a seriously valued skill for audiences as small as one and as large as a televised broadcast. Formal communications also includes learning how to listen, learning how to debate for solutions and not to ""win"", as well as learning how to moderate a group conversation for equality of voice and inclusion of all participants - for the purpose of the communication and the purpose the group exists at all. \n\nI expect you\'re aware that economists typically work for major corporations or governments. While statistics is the foundation of artificial intelligence. You could parlay your education to become an advisor of AI technologies at the economic level, for a major corporation or government agency. I recommend getting gregarious, as you could end up at some quite interesting places, which you may not be aware even exist yet. Start a personal program of talking to anyone you can find with a similar set of degrees and 7-10+ years post graduation to get an idea of what\'s out there. It is vast.']"
[Q] Working with weighted survey data,"I'm working with survey data about how customers are satisfied with a service, they can answer from 1 to 10. I have a weight vector for each respondent to represent the actual population share. I want to create a box plot with the satisfaction scores, but if I use the weighted score (the score multiplied by the weight) then my scale goes outside the original 1 to 10. What is the best way to rescale the weighted score to build the box plot? Even further, does this process I'm doing make sense? Or the weighting only works when summarizing results?  
  
  
  
Thanks for the help",13fosdt,dongorras,1683906476.0,2,1.0,"['You could transform the weighted data to fall within a range of 1 and 10 while maintaining the weighted distribution. You could plot the unweighted data but this becomes disjointed with the summary statistics calculated on the weighted variables.', ""I used a MinMaxScaler from scikit learn to bring it back, from 1 to 10. But, does really high outliers kind of messed up the central values. I'm not sure the weighted distribution is kept after the rescaling.""]"
[Q] Can someone identify this regression algorithm?,,13fohff,nemc2,1683905785.0,1,1.0,"[""We're sorry, your account hasn't gathered enough karma to post in /r/Statistics yet. You might try first posting in /r/AskStatistics, /r/homeworkhelp, or asking a question at [CrossValidated](https://stats.stackexchange.com/questions). \n\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/statistics) if you have any questions or concerns.*""]"
[Q] Statistics World Data Cache _ Website?,"I'd like to look up some peer review general knowledge data on stuff like mobile phone ownership but unsure where to start?

Pew maybe?",13fnh0b,SDEMaestro,1683903412.0,3,1.0,"['Try r/datasets', 'Thanks so much!']"
[Q] Should I use Bayesian or Frequentist approach to network meta-analysis? Good walkthrough resources for network meta-analysis in R for non-statisticians?,"Hi everyone, thank you for taking the time to read my post!

I'm trying to conduct a network meta-analysis (NMA) in order to eventually perform a cost-effectiveness analysis (CEA). I'm trying to decide between Bayesian and Frequentist approaches for the NMA. My research question is basically to assess different treatment options (five total) for a specific type of pain. For all but two of these treatment options (three of the ""established"" treatments), there's about 4-5 randomized controlled trials (RCT) with common comparators (namely placebo) that total about a thousand patients. However, for two novel treatment options that we're interested in, there's only one RCT for each that is suitable for the NMA (i.e., there's only one RCT with a common comparator); this totals about 50 patients per study per novel treatment.

Therefore, there are few data points for the two novel treatments, but there are many for the other, more-established treatments. Would the Bayesian or Frequentist approach be more suitable for NMA?

Could you refer me to a good walkthrough resource for conducting this NMA in R? I'm looking for a resource that would be able to explain (as succinctly as possible) the different variables that can/should be optimized or changed and the rationale behind them, if possible.

Thank you so much for any help with this--very much appreciated!",13fipca,pantaloonsss,1683891664.0,15,0.86,"[""use netmeta library and review the package documentation. If you're still having trouble, I mean, there's a post in /r/rstats that Google Bard and ChatGPT are both pretty good at walking through R code."", 'There are many differences between the Bayesian and frequentist framework. Most are ""philosophical"". \n\nYour choice should depend on your data imo. If it is very limited (small) and unbalanced (missings, truncations) I would consider looking more in to Bayesian techniques. If the data is quite neat and complete, then stick with frequentistic methods. \n\n(This is the ""Reddit"" version of my answer, short and clear, we could write several pages about it ;) )', 'In my experience, non-statisticians might be better served by Bayesian analysis. Although it may seem a little more difficult to understand at first, once youve gotten past the initial learning curve, everything becomes much more intuitive. With frequentist approaches, its easier to think you understand something when, in truth, youre mistaken. Consider how confusing p-values are, for example.', 'With meta-analysis, the big choice is not so much Bayes vs frequentist, but fixed vs random effects. \n\nWith a random effects model, you allow for ""shrinkage"" of your effect sizes towards some common mean (which ideally should lead to estimates for the lower-power studies that are a bit more realistic/generalizable), and a mean whose confidence/credibility intervals more accurately accounts for relative uncertainties of the individual studies.\n\nIn practice, Bayesians will nearly always go for a random effects model (it\'s very natural to write and estimate hierarchical models, like random effects models, in Bayesian statistics), but you can do them just fine in frequentist approaches as well.\n\nIf you actually have meaningful (i.e., ""informative"" or ""mildy informative"") priors you can elicit (e.g., maybe you know there is a very low likelihood of effect sizes greater than X or less than Y), then it\'s best to incorporate that via a Bayesian analysis. If not, then I would just choose whichever is simpler to run.', ""I'd add that it also depends on what you're doing with the output. \n\nFrequentists will give you an estimated value (+ eventually confidence intervals). \n\nBayesians will give you a distribution, which can be useful for specific use cases. Are there assumptions I want to make? e.g. Strictly positive parameters, specific target distribution,.. Is there an overlap between 2 parameters distributions? Etc...\n\n\nI think a good practice is to first quickly reach a model using a Frequentists approach, then switch to Bayesian and compare the results.""]"
[Q] Excel Help!,"Hello,  
  
I have bullshitted my way to a statistical analysis promotion, although I am very comfortable with SPSS. I am pretty much illiterate in everything else. I am currently assigned to organize a excel spreadsheet by matching codes with criminal charge descriptions. It wouldn't be that bad of a manual task buuuuuut there's about 5k different Texas criminal code charges. An additional issue is that there are duplicates in the charges but there will be slight differences such as misspellings and added spaces. Any suggestions for formulas or ways to facilitate the process? I have tried the find and replace and it's working but it's a little slow. Any help would be appreciated! Thanks in advance fam!!",13f1uut,rebochebo,1683843175.0,0,0.22,['Dm me I think I can help. This isnt really statistics though lol']
[Q] Explaining a Mixed Effect Model to a Non Statistician/Mathematician,"I'm not a statistician, but I do have a basic understanding of biostatistics in the context of Medicine and Clinical trials. However, recently I came across a trial that is using a Statistical Method that I am very unfamiliar with and was hoping someone could help.

Here is the study. It's in the ""Statistical Analysis"" section: [https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext](https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext)

Where I'm getting very confused is with this sentence: ""Reproxalap was compared to vehicle via a **MIXED EFFECT MODEL for Repeated Measures with baseline area under the curve as a covariate** and **treatment group and minutes post-challenge as factors**. A generalized estimating equation procedure, with baseline area under the curve as a covariate and treatment group and minutes post-challenge as factors, was used to compare responder proportions for the key secondary endpoint.""

I'm trying to understand what the terms in this entire paragraph actually mean and why this would be a VALID statistical method to use for this given trial.

If someone could explain, non-mathematically, the actual intuition and reasoning behind what a Mixed Effect Model for Repeated Measures is(and what the significance of the covariate and factors are in this model) with some examples or point to some sources that explain it well to someone with a basic understanding of stats, and even perhaps then proceed to explain it mathematically, I'd be very grateful. Thank You.",13f1rud,ayazasker,1683842992.0,36,0.97,"[""I think the other answers are better on the details, but my big picture advice is, mixed effects models are useful when you observe a sample of instances from the true population (like people, if you ran the experiment again, you wouldn't use the same exact people for it) and you think individuals have some special differences that modify that general relationship.\n\nTo statisticians, mixed effects means there are fixed effects (the same for every observation) and random effects (different for every observation). It might be more intuitive to think about them as *varying* instead of random. \n\nSo maybe you have a bunch of children and you're measuring their heights as they age. You might have a fixed effect for the average rate of growth, but varying effects for each child, cause they start at different heights and grow at different rates.\n\nOften we model those varying effects as if they're random draws from a normal distribution and we estimate the (possibly very complicated) covariance structures based on how individuals are related to each other in nested groups or hierarchies (which is why you'll sometimes hear these called hierarchical or multilevel models) to make statements about those effects."", 'The mixed effect model for repeated measures (MMRM for short) is a very common model used in clinical trials and often supported or even ordered by regulatory agencies for registration trials. The basic regression model treats all of the repeated measures of the outcome variable as a multivariate outcome, for example from baseline all the way till the stated primary outcome timepoint. This allows for the use of all available data and is more efficient than just looking at a change score or the final time point, where missing data can really impact the available data. Next, the model assumes no structure on the time points, in effect modeling them all as individual factors. (I couldnt open your link, and the description didnt explicitly state this, but this is the most common setup.). Technically, there should be an interaction between treatment and time, so that you are in effect modeling each groups mean value at each time point. Because this model doesnt impose any structure on the time trajectory or profile of how the means change over time, it allows for a great among of flexibility to let the data represent themselves without making possibly unjustifiable or unrealistic assumptions, such as both groups changing in a linear or quadratic manner over time. The mixed effect refers to allowing the residuals for each mean to be modelled differently (though they dont specify the structure). This adds to the flexibility of the mean modeling and doesnt impose further restrictions, such as homogenous variance over time, as outcomes tend to get more variable over time. \n\nLastly they also include a couple of covariates, presumably to increase statistical efficiency.', 'I didnt read the article but a mixed model is one with both fixed and random effects. Every repeated measures ANOVA is a mixed model because subjects is random and trials is fixed. More complex varieties of mixed models are required if there are two or more random effects or missing data. Random effects allow the generalization to the population. Thats why subjects is always a random effect. Psycholinguists were among the first psychologists to use mixed models with more than one random effect. Specifically, words and subjects would both be random effects so the results could be generalized to the population of words and the population of subjects.', 'Following!', 'The ""mixed"" refers to including both fixed and random effects in the model. The link is broken so I can\'t help you about the specifics. \n\nThese terms ""fixed"" and ""random"" are used differently depending on who you ask or what literature you\'re reading. \n\nIn my field of psychology, we typically call models that include both between-group and within-group effects ""mixed"". This makes sense in repeated measures data because you have variables that vary between persons but not within an individual (fixed; e.g., biological sex) and you have variables that vary within persons (variable; e.g., reaction time).', 'mind trying again with that link?\n\nEDIT: NM, found it: https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext', 'Fixed effects are variables for which the researcher decide which value it is going to take. For example, I want to test the effect of pizzas on mood. Im giving a pizza to one group and no pizza to the other. The variable pizza is fixed. They also are variables for which the degree of change is constant. Time can be a fixed effect (I know, time is relative according to physics) but the rate to which time advances can be seen as constant. \n\nRandom effects are variables we cant control. For example, the weather is something that we cant control (I think). If you want to study the effect of weather on mood, you cant decide that the weather is going to be rainy for a group and sunny for another group at the same time and place. Therefore, you have to measure weather. \n\nNow, time can be a random effect even if it is constant. However, it is not the time that is random but the measure of another variable at different points in time. It is a simplified way of saying that the weather is different when measured at two different points in time (even if slightly). The weather is not constant so if you you measure it more than once (repeated measures), you have a random variable.\n\nI hope that my explanation is clear and correct! :)']"
[Q] Help with guidance on exercise problem using r studio,"Guidance on solving an exercise problem using RStudio

The question is as follows, using a stimulation approach, approximate the chance that Bob will pass his 25 question test. 
The rules are:
1. create at least 10,000 sets of 25 single digit numbers. 
2. Each number is the answer to each question. 
3. An even number is a correct answer, while an odd number dictates a wrong answer. 

Going back to the question, it is asking for the relative frequency of 17 or more correct answers from the 25 questions. 

This is to be done on RStudio software. 
Guidance is appreciated and not sure if this is the right post as my options are being exhausted",13excc3,ipapijoe,1683833338.0,0,0.4,"['Start with this function:\n\n    runif()', 'given the way the instructions are stated, `sample` (with the default value of `replace` altered) would also work']"
[Q] mixed models - parameterization,,13et7pa,majorcatlover,1683824677.0,1,1.0,[]
[Q] Can i perform a post-hoc test to identify on which day the samples become significantly diiferent if i have two samples only (control and experimental)?,"Hi. Im working on my undergrad thesis and we're having a hard time analyzing our data as were also not good with statistics. In our study, we wanted to know whether the two samples only (control and experimental) are significantly different in the parameter chosen. We used kruskal wallis test (we used SPSS) as the data are not normally distributed and we only have two samples, hence Anova cannot be carried out. However, we have a total of 6 different analysis period (storage time/ day 0-21). The result says that significant difference exist within the samples but we cannot perform post hoc test as we only have two samples. Is there any way we can identify on which day the samples have significant difference?",13eqhzl,EntranceOne8046,1683818581.0,2,1.0,"['I think the short answer is you can\'t without replicates. You can plot the data and point it out qualitatively by saying ""the experimental set appeared to diverge from the control at day X"". \n\nAs a side note you it\'s the residuals (how far the real response deviates from the models predicted value) that need to be normally distributed. This is not necessarily linked to whether the actual data appears to follow a normal distribution', 'hi, thank you for the detailed response. We conducted the experiment with three replicates each sample. Right now, I was thinking of setting the data per storage time as different treatments and compare each to the control and run a post hoc...but im not sure if that would be okay though.', 'If you have three replicates for both time points across 6 time periods you have a two factor design - time X treatment. Therefore, I think you could possibly use a mixed model in SPSS, or just a two way ANOVA. You would not be able to say that the treatment had an effect at a specific time period unless there was a significant interaction between time and treatment.\n\nThe version of SPSS I used you (pre-2020) had to mess around input a tiny bit of script. Hopefully, they changed this in the version you are using.']"
"A test for the ""jaggedness"" of a graph [Q]","I made a big ol' excel sheet where I counted up all my favorite songs of all time and found that 1966-1975 had a very clear bias towards the odd years. Are there any tests I can do to test the significance of the ""jaggedness"" of the graph.  The first thing I thought of was just to add up the odd total vs the even total and assume a p=0.5 and test how unlikely it is to see such a bias. Are there any methods more oriented towards testing the jaggedness of a graph. I attached a zoomed in graph and zoomed out graph",13eor0w,yaakg25,1683814726.0,1,1.0,"['You could probably do something along the lines of testing for a seasonal trend', 'With e.g. Monte Carlo simulation on some model (random walk for example) you might find that this particular type of weirdness is unlikely. However, there are uncountable types of weirdness, and the probability that at least one of them will occur in a data set just by chance is very high. So testing for a particular pattern spotted a posteriori is a dubious excercise (cf. the ""Texas sharpshooter"", who fired at random, hit some random objects, and afterwards claimed he had aimed at these in particular).', ""If it's just odd-even bias, why would the consecutive jaggedness be interesting?\n\nHowever, whatever you do, you have a problem, in that you're testing hypotheses *generated by the same data* that you're testing on. Your p-values will not be meaningful.\n\nhttps://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data"", 'It is possible to measure how ""noisy"" a distribution is with kernel density estimation - https://en.wikipedia.org/wiki/Kernel\\_density\\_estimation.']"
[Q] Example of collider bias? regression of wage of years of education,"I saw an example online of regressing wages on years of edu to find the causal effect. They suggest experience is an omitted variable correlated with both, which makes sense.

But as I have been learning about colliders, I wonder if this is an example of collider bias, where we wouldnt want to control for experience.

My thought process is education may cause more work experience, but wages may also cause someone to work for more years (or health /social connections or some omitted factor may be related to both experience and wages)

In this case, controlling for experience would introduce bias, no?

If someone with more edu got more experience, then comparing rhat person to someone else who already has that level of experience without the edu would then no longer be a good 'control' because the latter was able to get that experience without more education (work ethic, discpline etc.)

Does that logic hold or is this not a good example of a bad control?",13emv0x,Whynvme,1683810308.0,16,0.88,"['Build a DAG and experiment with it. Try daggity or dagitty, cant remember how its spelled haha.', 'At this point, experience being a confounder is a reflex, from the famous mincerian equation. \n\nI think what you say about experience being caused by education and wages is plausible but  the timing is important, if you think wages and education today are affected by experience today but affect experience tomorrow,  there is no collider bias, as you are looking to explain wages today and you are not including experience tomorrow as the control.\n\nIf you think the relation is simultaneous, meaning, experience today causes and is caused by wages and education, then yes, you are in a pickle. I think this perfectly ilustrates why modern economics is abandoning controls altogether, relying instead in pseudo-randomization to obtain causal estimates, as relations between socioeconomic variables are extremely complex and more often than not, controls are endogenous themselves.', ""Yes, I think that's a bad control. The standard example of an omitted variable in this setting is some form of innate ability, which impacts both education and wage."", 'how you define the causal structure, makes experience indeed a collider. And indeed controlling for experience introduces bias between education and wage, and you will likely overestimate the true causal effect.', 'These thing are always difficult to answer. While you explanation on why salary has a causal effect on experience makes sense, I wouldn\'t say that\'s the case in any real data set. Your data is collected at specific time points. Let\'s say it\'s collected today and I have started working 42 years ago, so I have 42 years of experience. If my salary was (counterfactually) lower than it currently is, I might need to work longer, but you can\'t see that in my current years of experience. If my salary was counterfactually higher, I might retire earlier, but again, we don\'t see that in my current (counterfactual) experience. No matter how high my salary might be, I will always have 42 years of experience right now. \n\nNow, I\'m not smart enough to think about the consequences of ""counterfactual censoring"", in the case where I\'d already be retired by now thanks to a high salary, and not included in the data at all. So that maybe destroys my whole argument. If somebody wants to destroy my argument, I\'d be happy to hear that.', 'Good qestion mate. First, it\'s importnat to remember that collider bias, or selection bias, occurs when there\'s a common effect (the ""collider"") of two variables, and controlling for this common effect can introduce spurious correlations.\n\nIn ur case, u\'re right that education n experience are likely correlated n both impact wages. However, it\'s not clear that these are necessarily a collider scenario. Experience isn\'t necessarily an effect of education and wages simultaneosly, which is the set-up you\'d need for a collider bias.\n\nIf you think about it, experience might be an intermediate variable on the path between education and wages. Education -> Experience -> Wages. If this is the case, controlling for experience would block some of the effect of education on wages, which would lead to an underestimation of the total effect of education.\n\nSo, it\'s a bit more complex than a straightforward collider scenario. But you are thinking in right direction. Sometimes, it\'s not best to control for a variable, even if it\'s related to both your predictor and outcome, if it\'s an intermediate variable on the causal path. And yes, there could be other unobserved factors contributing to both education and wages. If these factors also affect experience, then it might be a confounding variable, not a collider.  Hope that helps!']"
[Q] Method for forecasting a one step ahead time series based on another time series that has a strong relationship and causality with it and has been observed already?,"Hello,

I have 2 time series that have a strong relationship and causality to one another - one is ""total invoiced in a given month"" and the other is ""value of purchases paid by credit card in a given month"".

Let's say that I have the observed value for the last month for the second series, but I don't have the information for the last month of the first series yet. 

What are good methods to predict the total invoiced last month based on the payments of the last month that I already have the information?

I've been doing some research and I've come across Dynamic Regression Models, but I'm trying to find other methods that can be good as well and to compare their performances in my case.

Do you guys have any suggestions? Thanks!",13emti5,Maekkar,1683810207.0,4,1.0,"[""You want the transfer entropy here, probably. For two time series X, Y, the TE(X -> Y) = I(X\\_{past};Y\\_{future}|Y_{past})\n\nWhere I( | ) is the conditional Shannon mutual information. If TE(X -> Y) is significantly greater than 0, then you can just use the maximally likely estimate for Y\\_{future} given X\\_{past}. \n\nIf your data are Gaussian, use a parametric estimator, if it's not, use a nearest-neighbors based KSG estimator."", 'The simplest is also the best. The Bayesian posterior predictive distribution minimizes the K-L Divergence between nature and the model. It isnt possible to build a closer model. \n\nIt will give a probability distribution for each value. It also has a simple model selection mechanism, so if you are unsure of how much information may be in lags, it will provide a probability for each potential model. \n\nBecause accounting data is highly correlated by the design of accounting systems, you can readily add other information streams that improve predictive quality such as changes in GDP, local unemployment and so on. It takes very few accounting variables to estimate the others. Again, model selection methods allow you to determine if it is an improvement. \n\nThere are also a set of scoring rules to determine if your predictive model is good. Likewise, there are validation methods in the literature. \n\nThe only thing that might be an issue is creating what is called the prior distribution. The prior encompasses information about the location of parameters that comes from sources that are outside the data itself.', 'regArima or VAR.']"
[Q] Probability of at least one of two different rolls given X amount of tries.,"Hi, is there a formula to do something like this? For example:

Say I have a three sided weighted dice with 1 and 3 having a 10% chance with 2 having a 80% chance. I want to calculate the probability of getting at least one 1 and one 3. I calculated it using a tree diagram seen here

https://imgur.com/a/A8LNz3g

Binomial distribution does not work because the events are different?",13ed6df,xSyzygy,1683780130.0,0,0.5,"[""Binomial doesn't work because binomial only allows for two different states. Since there are three sides to the die, binomial won't be an appropriate distribution. \n\nWhat you want is multinomial. https://online.stat.psu.edu/stat504/book/export/html/667"", 'You can calculate the chance as sum of two groups:\n\n1, 2, 3, in any order (6 options)\n\n1, 1, 3 or 1, 3, 3, in any order (2*3 options)\n\n=> 6\\*0.1\\*0.8\\*0.1 + 6\\*0.1^3 = 0.054', 'Thanks, this does help but is there an easy way to use this if I want one of the outcomes at least an x amount of times? It looks like I would have to do it for multiple cases then add up the probabilities together', 'Is there a formula that extends this to larger number of trials and options? Seems like it would take long to do this if say the dice had 6 sides and I did 10 rolls', 'In some cases you can use [inclusion-exclusion](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle), but in the most general case you only have the options to go through many cases (in one way or another), to simulate it for an approximation, or make some approximations for a direct (but not exact) calculation.']"
[Question] Probability of surviving?,"[Question] I dont know where to ask this so I figured some of you could help me with this. I have a genetic condition that only ten people in the world (including me) have been diagnosed with. Three of them have passed away, all between the ages of 20-22. I am now the oldest living diagnosed person with my condition. I am 22. So thats 7 of us still alive today. Now since there could be others out there who have it but arent diagnosed, I have no idea how to figure this out. But so far only ten cases have been discovered worldwide. What is the probability of me making it to 23? 

(I know not to base any judgement on the probability of this outcome; however I do realize that it is a possibility and I have to be aware of that. Yes it does scare me, but anything could happen. I just am curious what the probability of it would be)",13ec4cu,psychicfemme,1683776613.0,3,1.0,"['I hope it\'s good, but we cannot tell with the given information. Even if we make the (absurd) assumption that all cases behave in the same way and they are indistinguishable except for deaths you are in an age range where we don\'t have data.\n\nHere are three extreme scenarios that are all consistent with the given information but lead to completely different outlooks:\n\n* 1000 people have this genetic condition, in over 99% of the cases it doesn\'t lead to notable symptoms ever, in the remaining cases it\'s often deadly before the age of 25. Some of the 10 diagnosed people showed symptoms and died or will die young, for others it was discovered while looking for something else. If you are in the second group, you have an over 99% chance to live a normal life, just like the 990 people who never get diagnosed.\n* The genetic condition always* leads to death at some point before the age of 25.\n* The genetic condition is most dangerous in the age range of 18-22, people who survive that range have a good chance to survive to old age.\n\n\\* there is always the chance of outliers. Stephen Hawking survived a ""0% survival chance"" disease for over 50 years.']"
[Q] [C] What kind of careers do a statistics degree come with?,"What career should I consider with a statistics degree?

Very curious what kind of career fields that comes with statistics. I know statistics is very broad so if anyone wants to share their experience with their jobs that uses statistics, I would be grateful! Currently a stats major and super curious about what I could get into :)

I was thinking maybe getting into public health and be a biostatistician? Idk, still early in my degree so I still have a lot of time to think about it.",13e99fr,Aegisquarz,1683768085.0,27,0.83,"['A friend of mine has become theater actor after stat degree but Idk if this is statistically significant', ""I am a statistician who works in global health as a scientific researcher. That's the best part of being a statistician - you get to play in everyone's backyard.\n\nIt's a rewarding career with a lot of interesting problems."", 'If I could go back, Id do a lot more programming courses and do my masters in stats not math. Algebra did me no favors ', 'Quant roles in banking', 'A/B testing and causal inference is a fun and increasingly-important field.', 'Public health biostatistician sounds like an awesome career! Although just a heads up, most of the jobs I\'ve seen with a ""Biostatistician"" title require at least a masters degree.\n\nI\'m a data analyst in the healthcare industry (private). I don\'t do a lot of statistics on a day to day basis but I build datasets and spend a lot of time in SQL and Python. I also have an undergrad in statistics, and I\'m finishing up my masters in statistics right now.', 'Some jobs on Wall Street love people with Statistics backgrounds, especially from a top schools.', 'Yup... Biostats is the way. Making white overweight old men live longer is where the money is at.', 'actuary', 'Biostats / public health / epidemiology is an interesting field. Not quite the same thing, but there\'s lots of lucrative jobs in medical statistics, i.e. clinical trial statisticians.\n\nAlso, just a warning that ""data scientist"" is really an umbrella term for many different roles. It\'s mostly a buzzword to attract statistically literate graduates. Nothing wrong with most of these jobs (it\'s my job title), just wanted to point out that it\'s too broad to aim for as a career choice.', 'I have a statistics degree and Im gonna be a software engineer', 'Most people cant do stats right out of college I find unless they have PhD or other advanced degree. Youll likely need to find a data analyst position to get some experience and will have to clammer for modeling experience so you can one day use what you studied, hopefully. Just being real', 'Data analyst / scientist in Insurance, finance or anything related to risk (risk analyst).\nActuary if youre specifically interested in Insurance.\nAlso if youre into research, you almost always need statistics in research whatever it is the domain / field.', 'Ok, this is weird ""career"", but my colleague (Bachelor\'s degree in Stats) have been writing thesis and solving university assignments for students. Started with a huge learning curve even after 4 years in university; currently, after years of doing it, I see her as qualified as my stats professor, haha.', ""I have a Master's in stats and I've spent the last 10 years as a data analyst. But I got laid off so I'm following your post for ideas too!"", 'I worked as an actuary right out of uni. Pension actuary in particular.', ""I'm a data scientist in marketing for now.\n\n I had a bit of experience as a data analyst before where I could basically automatise my basic tasks with R. This gave me time to work on data science/operation research projects.\n \nMostly Python programming and SQL. I get to do some modeling. \n\nA sound knowledge of stats seems tremendously important and underrated even among data scientists (see the subreddit for example).\n\nI'm always finding ways to apply linear and generalized linear models to make recommendations. \n\nA lot of jobs now hire for data scientist but are less modeling oriented. I'd be very cautious. \n\nData cleaning/data engineering is fine however and good experience. A useful analysis starts at the data ingestion step."", 'I currently work in economic forecasting. Its pretty interesting, would recommend!', 'Ford, Visteon, University of Michigan School of Nursing, UM Health System Quality Improvement Office, Nissan, Volkswagen Credit, Ford, Michigan State University.', 'I minored in statistics, now I do community theatre. There is clearly a correlation between statistics education and level of theatre attainment', 'OOOO that sounds cool!! What do you do on a day to day basis?', 'Sadly and annoyingly, not all backyards are pretty and neat.', 'Great Tukey reference :)', ""Omg I was a stats grad and had to do some pure math mods and woah it was tough. Was it real analysis, can't remember anything about it now. Haha"", 'Why more programming classes and why math and not stats?', 'Do you mean linear algebra?', ""I'm interested in doing something related to this when  I finish college. Do you have any advice?"", ""What's that?"", ""Woaah that's awesome!! How do you like being a data analyst so far?"", 'oh wait what? I thought biostats would be more meaningful', 'Only if you want to work for insurance companies, right?', 'What does a software engineer do?', 'I know I need to get a masters, but in what cases would I need to go for a phd?', 'Since you asked - the website is called www.homeworkhelponline.net\nBut it seems to be a few years from automation by AI, so I would not base my career on it.', 'Does your stats professor have stat degrees?', ""From what I've seen, would you say in general, data analysts don't do a lot of statistics on a day to day basis?"", 'What does a pension actuary do and how do you like so far? :)', ""Depending on the day, my work may consist of a combination of literature reviews, meetings with my peers and other teams who would like to use my models, writing Python and R code to build pipelines from raw data to forecasted outputs (and the usual debugging sessions), Bash scripting to run big tasks on our cluster computer, and creating presentations to share our findings. There's also a periodic crunch for publication deadlines. \n\nI'm currently working on methods to obtain population health and poverty data from areas that aren't easy to survey via satellite data.\n\nIt's a great sector because you get to work on big problems that feel meaningful."", 'Sometimes people get mad Im in their backyard', 'Yeah, measure theory and homological algebra are no joke. But theyre a joke if you want a job. \n\nOnly thing I really got out of my program is that there isnt a damn thing that scares me anymore and Im always looking for something to learn / do. I think way more critically now too, even if its hindsight 20/20 vision.', 'Almost certainly not. Linear Algebra is foundational. I would assume he means more abstract stuff like group and ring theory.', ""My experience is within the UK. It's hard to give specific advice as there are different types of quant roles e.g. development, research, validation, trading etc, my general advice is as follows. A MSc equivalent or higher is generally required. The most fundemental relevant skill I've found is to be able to apply your knowledge to a wide array of scenarios by generalising solutions to problems and produce these solutions quickly and efficiently. You learn this naturally from studying a quantitative degree e.g. Maths, however this does require truly grasping and understanding the material, so called building neural pathways in the brain. Additionally, experience with programming languages is a must, specifically C++, Python and SQL. Of course you want knowledge of standard applied Mathematics, ODEs, PDEs, Linear Algebra, Statistics and probability theory. Further knowledge of stochastic calculus, Geometric Brownian Motion etc would be a bonus, especially if you are looking to get into derivative pricing. Regarding Statistical methods, you want a firm grasp on ML techniques, classics being linear and logistic regression, decision trees/ random forest, SVM and more recently Neural Networks.  Data is extremely important, so an understanding of the nuances and intricacies of data analytics and the way data is interpreted would be advantageous. The quant realm is highly competitive, however once your foot is in the door you are truly set. I didn't have any banking experience, just a strong degree in Mathematics (applied) from a top UK Uni with abundant knowledge/experience in ML and programming. If you lack experience (like I did) then your dissertation is key, focus on mathematical modelling and applications to real world scenarios."", 'Heres an overview paper:\n\nhttps://www.amazon.science/publications/top-challenges-from-the-first-practical-online-controlled-experiments-summit', ""I like it! Pay is a little under what I was expecting ($75k MCOL) but for the time being, the company is good & respects work-life balance. I have a good team and my responsibilities align with my long term career goals. It's an exciting time to join the healthcare industry! It's also more stable than other industries in terms of layoffs."", 'Different types of insurance and reinsurance;\nPension administers;\nRisk management firms;\nBanks;\nPrivate practices.', 'They build software such as apps or things like SPSS', 'Have a look at modeling positions at most companies. Many of them are manager level at least, requiring 3-5 years of work experience. The education usually also calls for a PhD - they would rather have someone that understands the models at the most deep level academically rather than someone who doesnt. Therefore people with PhDs get those positions while you have to be lucky to beat someone with a PhD without one', 'I would say the definitions of ""data scientist"" and ""data analyst"" can mean a lot of different things. In general, yes, I\'d say that data analysts don\'t do much beyond basic descriptive statistics.', 'So that was over a decade ago for me.\n\nI ultimately ended up leaving after 6 months because I realized I didnt want to work in a traditional office. I wanted to go jump out of airplanes and so I joined the military.\n\nAs a side note, dont overlook government jobs where you dont directly use your degree. Look into becoming an officer in the military, NSA (if youre American), or any of number of government agencies.\n\nAs for what a pension actuary typical doesA pension fund is essentially a giant pool of money (assets) and the members (liabilities). Investing the pool of money is one aspect of managing a pension but then theres also managing the liability side. If a member typically retires at 60 years old, its the actuaries job to quantify how much that is likely to cost the pension fund. You see the member will draw out a certain amount until they pass away (plus maybe their spouse continues to draw from the pension - every pension is different). So you could look at actuarial models of life expectancy and through a discounting process estimate how much money the fund needs to be solvent. Its a big mix of statistics, economics, finance, legal and fiduciary regulations, etc.\n\nIs it a good job? Yes it actually is. Being an actuary is a great balance between being ~~payed~~ paid a bunch of money and having a relatively stress free job (at least compared to similarly paid jobs). \n\nYou could also work as a consultant, which seemed to be a popular route. From what I can tell they mostly worked with divorce lawyers to figure out the value of a pension in a divorce settlement.', 'I thought not', "">from studying a quantitative degree e.g. Maths\n\nIs a degree on statistics enough? I know algebra, a lot of probability theory, R language, sthocastic processes ... But what I'd like to learn more about is ML, I know nothing about it\n\nThank you for your answer. I'll save your comment so I'll think about it again once I finish college the next year"", 'I started at 75k and got a bump to 95k after MS so Im just staying (wfh is very competitive and I feel very grateful). My role is the same with the addition of some studies with traditional methods (regression, chi square). Anyone with a pulse can fit an ML model to the iris dataset so Im happy with the general domain data and statistics skills Im gaining.', ""Haha that's awesome! How do you like being in the military?"", ""> between being *paid* a bunch\n\nFTFY.\n\nAlthough *payed* exists (the reason why autocorrection didn't help you), it is only correct in:\n\n * Nautical context, when it means to paint a surface, or to cover with something like tar or resin in order to make it waterproof or corrosion-resistant. *The deck is yet to be payed.*\n\n * *Payed out* when letting strings, cables or ropes out, by slacking them. *The rope is payed out! You can pull now.*\n\nUnfortunately, I was unable to find nautical or rope-related words in your comment.\n\n*Beep, boop, I'm a bot*"", '>Is a degree on statistics enough?\n\nYes, I would say even preferable.', ""Good to know!! Hopefully I'll be getting a bump next year"", ""Omg where you guys from? the US? 75k is a lot in my local currency. I started at around 40k (thats about 30k in USD) with a degree in stats. Now at 7th year, with master in stats, I'm getting about 60k (thats 45k in USD). Sigh this is pathetic. I'm mostly involved in observational studies (health services/clinical research) in a government hospital in my country."", 'Yea I\'m from the US. In the city I live in, $70k is the socially accepted ""minimum"" for living comfortably as a single person. With $1.2k rent ($2.4k with roommate), $1k medical bills per month, and $90k total student loan debt, I still won\'t have a ton left over. I\'m at least glad my car is paid for already. \n\nI definitely recognize that the US is better than a lot of places, but I\'m still disappointed with my salary. I\'ll be looking for another job when things settle for me if I don\'t get the raise I need.', 'US salaries for data professionals are insane since demand for info is so high here. Also a lot of Americans cant code or do beyond basic math. I looked into moving to Singapore and salaries were really low there- they value experience and theres a lot of technically smart people, you have to grind to the top like finance in the US.\n\n95k is like just enough to survive in HCOL where I live. Idk how people do it. My company is in a LCOL area and I wish there more job prospects in case I get fired so I could move there.']"
[R] I think I've made a huge mistake in my systematic review,"\-So, my dissertation is due in 5 days. I've left it late but everything is written except for the results and discussion. Maybe too late for the issue that I've just run into.  
I am writing a systematic review on infanticide (killing of infants) in primates. In all of the cases I have included, infanticide has occurred. I have made certain predictions on what I think I will find in my research.

&#x200B;

  
\-My original predictions were that infanticide would most occur on unweaned infants, most after a takeover event and most in one male group as seen in previous research.  
Of course, only after I've collected all the data and placed it in SPSS have i realised- if infanticide occurred in all my cases, its going to be very hard to run a statistical test that actually says anything about the influences on infanticide. Yes I can be descriptive but in terms of saying anything significant- its going to be difficult. i feel like I've backed myself into a corner. I spoke to the statistics tutor at my school who told me I could shift my prediction to be ""Unweaned infants are more at risk than weaned infants after a take over event or in a one male group"" and do a logistic regression analysis on my data. But she told me she was unsure and google is saying that this is maybe not possible. I think it would be worse to do a test that means nothing and base my whole paper around it. My dissertation leader is away and unavailable.

&#x200B;

  
\-I have data on: date, species, region/site, sex of attacker, age of infant  
I also have binary yes/no data on if infanticide occurred in one male group, an unweaned/weaned infant and after a takeover.  
I also have a lot of data that looks at the study design of each of the studies.  


&#x200B;

My question is: How can i use this data the best I can ? Obviously its looking like its going to be extremely descriptive. Is logistic regression analysis possible?  
Luckily I've done a cluster network analysis on the authors so I at least have one complex method but It would be great to have more.  


I tried to be as clear as I can but I am freaking out a bit, please be kind ! Thanks in advance",13e4jd6,bedshock11,1683755627.0,0,0.44,"['If infanticide occurred in each of your data points, is there variation in the intensity (?) of infanticide (maybe # of offspring killed)? If so, you could for example make your predictions be like infanticide of unweaned more likely at higher intensities.', 'Don\'t beat yourself up about your thesis. Getting this far is a huge accomplishment that the majority of undergrad students do not reach. I\'ve seen students in worse shape than you 5 days before scrape together a passable thesis. And don\'t blame yourself for having a fatal flaw in your analysis plan, either.- honestly, whichever faculty member is advising your thesis should have signed off on your analysis plan. \n\nYou\'re right that you can\'t do logistic regression predicting infanticide from your predictors, since all of your studies involve an occurrence of infanticide. I think it\'s hard to predict much about infanticide without also having some information about when it doesn\'t occur. It might be that the ""right"" statistical approach here isn\'t complicated, and it\'s just reporting a summary table of the infanticide studies, and describing trends (e.g., 40% of documented infanticide cases occurred with weaned infants, and in species A, 80% of documented infanticide cases occurred with weaned infants) or something. Unless you believe that your cases represent a random sample of all instances of infanticide, it wouldn\'t be appropriate to run statistical tests. And that\'s totally okay for a systematic review.', 'Maybe you could do a time to event analysis / survival analysis. So for example, with a Kaplan-Meier curve youd have a chart with time on the x axis and percent remaining alive on y. And youd draw a line for males and a line for females, and you could say : among cases of infanticide, infanticide tended to occur earlier (or later if thats the case) among males, compared to females. And a log-rank test would tell you if the difference is statistically significant. \n\nIm not at all sure what such a study design would be called, I usually work with cohort data, not cases-only data. \n\nUpdate: I dont see much from a google search about case only study designs. It is a thing, but it seems to be used in studies about genetics, so Im not sure how much would translate to what youre doing.', ""I think having the dates and study design could be vital here. I would consider thinking about when observations could have been taken but weren't. It seems to me like you recorded data only when infanticide happened, if that's so then you can use the design of the survey to take a psuedo-survey. By incorporating randomness in the unrecorded non-infnticied observation, it will still allow you to give a statistical result."", ""It's late here and I can not put in the amount of thought that this requires, but you might find a solution in Bayesian modelling. Not sure SPSS does that nowadays though.\n\nGood luck at any rate!"", ""Probably best to shift to something describing major features in infanticide instead. To be honest it's the sort of thing you'd expect your supervisor to have raised before now."", 'Thank you for the quick response. Unfortunately not.  Most of the cases only one instance of infanticide on one infant occurred.', 'I think this is probably the best way to go!', ""Honestly, thank you so much for your comforting words, sometimes, academics is not for me at all and I have been fighting not to drop out . I really thought it was all thought out. I just didn't plan my review well enough, just picked a research question that I thought would work and jumped straight to analysing papers because I thought that would take the most time- dumb. When you're nervous about asking for help, it's easy to fool yourself into thinking it will somehow pull together. I did run this past my tutor, but should have met with her more to confirm things as I went along.   \n\n\nLuckily, 5 days is some time to figure *something* out.  I think you're right, all I can do is write a reporting summary of my data and hope it passes. I will make mention of the lack of literature/ data available in the area which hinders the ability to make any real conclusions and suggest further research.   \n\n\nIn terms of research questions i could look to answer and PICO/PRISMA etc framework, do you have any suggestions? Sorry to ask- definitely should have realised that my current plan didn't align with these earlier."", 'Thank you for your attempt to help and google search on my behalf. I appreciate it greatly', ""Most cases or *all* cases? A variable is only a variable if it has variation... Does yours have variation? And if so, what is the variation?\n\nAlso - you said dissertation. Are you doing a PhD? It's a little strange to have a deadline sneak up on you for a PhD dissertation."", ""It's not a PHD luckily or I would really be in trouble. It's undergraduate. It's honestly my own fault more than anything but depression + ADHD came into play.Apologies, I read what was said again, in some of the cases more than one infant in the group has been killed during a single study. Is your suggestion that I look at unweaned vs weaned infanticide and look at instances of infanticide within the same group? Eg. There is a greater risk for unweaned infants in groups where there are more unweaned infants present ( demonstrated in my data by infanticide occurring more frequently in groups with unweaned infants vs weaned infants )\n\nThank you so much for the suggestions, I feel like an idiot""]"
[Q] Interval-censoring in the context of marginal structural models and g-estimation,"The causal inference literature is full of models that handle time-to-event data and time-varying confounding, like g-estimation for time-to event data and marginal structural Cox models.

However, I have not come across any papers discussing interval-censored data in the context of time-varying confounding; such as when you don't know the exact time an event has occurred, but only know that it lies within a specific interval.

Could anyone point me in the right direction on how to handle such data for time-varying exposures?",13e4e6c,Shoddy-Barber-7885,1683755305.0,12,0.94,"[""I can't say I have any direction to point you in, but I'm also interested in any answers! Interval-censoring generally seems like a hard problem to get around, especially with non- and semi-parametric models..."", 'Interval censoring always makes me think of survival models. what does time-varying confounding mean?', 'Im not really aware of any canonical solutions to dealing with interval censoring. Most that I see of it is always in reference to very specific problems in published papers.\n\nI would say even that dealing with interval censoring is sufficiently under-explored that exploration of it via simulation in the time varying confounding domain could be worthwhile for a masters thesis, or in greater depth with theoretical contribution could probably be a substantial portion of a PhD dissertation.', ""Wow, glad to see someone else is struggling with this issue. My team has a projects we're working on with interval-censored exposures we expect to causally impact later outcomes which are also sometimes interval-censored. At the moment, all I've been able to come up with for a practical solution is using conventional survival models for interval-censored outcomes to perform multiple imputation of the exposures. I doubt this is a robust solution, but I haven't tested it in simulation yet. As u/Kroutoner notes, I suspect this would make for a good thesis or dissertation topic.\n\nSince I'm actively searching for solutions, I'll make a note of this thread and let you know if I encounter work on the topic. Would love to know if you turn anything up as well."", 'When an exposure varies over time, the confounders also change over time. So you have time-varying confounding or different confounders at different time points', 'Arab andy']"
[Q] Imputation of missing data not at random - using highest value?,"I have a dataset where one variable is ""time taken to perform a task (sit - to stand 5 times)"", such that longer it takes, the worse the performance. I have noticed 25% is missing data, and the reason is ""not able to perform"" so this not random missing.  
I get very statistically significant correlation with other variables if I impute the mean time taken. However, in reality, the imputation should be closer to ""infinity"" as it is 'unable to perform'. Someone suggested I could actually use the ""highest value and add 1 second"", which makes the correlation even more significant.  
  
Is this a legitimate strategy in this situation? What is this method called?",13e2gdk,External_Damage9925,1683751031.0,3,0.71,"['You get half-credit for trying to do something reasonable rather than blithely assuming your missing data was MCAR.\n\nBut I think you\'d do better to create a 2-stage model, with ""did the person complete the task"" and ""if so, how fast"" as two DVs.', 'You cant impute this since its an actual data entry. You either have to drop these observations in their respective time categories such that sample size becomes less and less as time goes on, or you need to treat this as some kind of survival analysis instead where p(failure) increases over time.', 'A trick I just learned from some biostatisticians on a similar problem: invert the values to a velocity.  Take 5/time to get a variable whose units are sit-to-stand per second.  Then all the missing values that are ""not able to perform"" can be set to zero.', 'Isnt this technically censored? So you might interpret the missing as actually indicating that those individual would have taken at least X-s to perform the task then you could model such right censoring. But without more details on how the assessment was performed its hard to say', 'This is pretty much a case example of p hacking \n\nImputation by itself can be dangerous, and has weak theoretical justification. Unless you know the joint distribution between all the variables a priori and diagnose the distribution of missing ness (MCAR, MAR, etc) and provide some extremely strong assumptions along with it-  youd be hard pressed to justify it because again;  its not theoretically justifiable unless youre able to really force unreasonable assumptions on the data. \n\nYoure getting significant results because youre adding more extreme values to the column youre imputing. This is how Neyman Pearson hypothesis testing works;  if you start ramming in enough extreme values into your missing ones, your mean or whatever youre testing is gonna move towards the rejection region. \n\nYou should communicate with your team why they feel imputation is good. If theyre only answer (I.e. because we have missing data) then there are literally thousands of easy to find examples on why you shouldnt impute', 'Do you have other variables you can use? Have you tried graphing the data? What do you intend to do with the data once imputed? Have you considered doing simulations and testing various imputation methods and to see the impact on your conclusions?', 'I agree with /u/ExcelsiorStatistics above, but wanted to give more context.\n\nYou have a sample selection issue. You only observe the output for those who can perform the exercise. \n\nA two step approach would work, but to be more specific you should look into Heckit models from Econometrics. Also called Heckman selection models.\n\nGood luck!', 'this one. cant do it is definitely data, and its not the same as infinite time', 'This is just making data up?', '>tribution between all the variables a priori and diagnose the distribution of missing ness (MCAR, MAR, etc) and provide some extremely\n\nTo give more detail, specifically I am looking at genotype (lets say a person can be genotype A or B) and variables, includiing this ""chair stand time"", which has by far the highest missing data rate (25%, as they were unable to do this.)\n\nChi square test shows that genotype B are more frequently unable to do perform vs being able to perform. Perhaps I could just leave it at this wiuth chi square result, as this suggests missing not at random and backs up that genotype B performs worse. If I do leave out missing data, genotype B is slower than A (p=0.014) by Mann Whitney already - but the point is that this worse performance is underestimated by the missingness in this group.  \n\nThere are other variables also looking at performance (such as performance score, distance walked...and these are also consistent with genotype B performing worse).', 'Unless their team is a dedicated stats team, simulation might fall on deaf ears. \n\nFrom the description above op and his team might be unfamiliar with some of the basics.', ""...no it's not, it's just moving the data from the time domain to the frequency domain.""]"
[Q] Need help calculating statistical probability,I'm trying to figure out what the probability would be if there's a 10% chance of something occurring and that thing doesn't occur for 100 straight instances.  I'm not sure what formula I would need to use but if you could provide the formula that needs to be applied I'd highly appreciate it!,13dznmr,Soulsearcher14,1683744638.0,0,0.33,"['You are looking for the binomial distribution formula. The chance of this event happening once is 90%, the chance of it happening n times without another outcome is (.9)^n.\nMore information and full formula here https://byjus.com/binomial-distribution-formula/', 'To compute the probability of an event (with a 10% chance of occurring) NOT occurring for 100 times, that\'s just a 90% chance a hundred times.   \n\n\nFor the ""100 times"" condition, you have to also know that if we want to know the probability of several outcomes as a result, we multiply them together (assuming they are not correlated). Our formula says that we need to take the chance of the thing not happening (which is 90% in this case), and multiply it by itself 100 times.   \n\n\nTo generalize, the probability of an event not happening over some number of instances:\n\n(1 - probability of the event occurring) \\^ number of instances\n\nIn our case, (1 - 0.1) \\^ 100 = 0.9 \\^ 100. This is approximately 4e-87, which is approximately a 0% chance of happening. That is, again, assuming that the event outcomes are completely independent of one-another. (The result of one success/fail does not influence any of the other successes/failures.)', 'Thank you so much, super helpful!']"
[Q] Biostats texts for applied mathematician,"Hello r/statistics,

I am looking for a book (or several) that I can study certain elements of biostatistics from. I'm making this post in the hope that someone can recommend something appropriate for my background: I am a PhD in mathematics and I have a masters in applied and computational mathematics. I once took a course in the foundations of (measure-theoretic) probability - mostly about understanding random variables and distributions. I also once took a course in SAS programming as part of my masters but I found that there was little to not statistical content; I mostly learned about manipulating simple datasets.

Here's the topics I am interested in:

* A short, general overview of biostatistics, especially as it is used in the pharmaceutical industry
* Regression analysis, especially function transformed (i.e. log) regression
* An overview of ANOVA, RMAN(C)OVA, etc
* An introduction to categorical data analysis, especially Mantel-Haenszel
* how to do these tests in SAS

Thanks for your time,  
/u/mattlink92",13dyh8t,Mattlink92,1683741993.0,11,0.93,"['Perhaps the Statistical Inference by Casella and Berger would be a good starting point from the mathematical statistics side.\n\nThe last chapters are about linear regression and may be useful\n\nStatistical Inference https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf', 'Ah, a kindred spirit.  I did my masters in applied math before doing a PhD in Biostats.\n\n* Jumping right into pharmaceutical stuff would probably see you using mixed effect models.  Might I recommend learning more about regression first?  I liked [this](https://www.wiley.com/en-ca/Biostatistical+Methods:+The+Assessment+of+Relative+Risks,+2nd+Edition-p-9780470508220) book as an intro to some basic statistical approachs in biostats.  It has some SAS code.\n* Frank Harrell\'s ""Regression Modelling Strategies"" and ""Biostatistics for Biomedical Researchers is also good"".\n* If you learn about regression, you don\'t need to know about ANCOVA and the variations thereof.\n* Agresti\'s ""Categorical Data Analysis"" is another seminal text in GLMs and categorical data analysis.', 'You can check the following books on Biostatistics and Clinical Trail Management\n\n1.Clinical Trial Management  an Overview \n\nISBN-13  :  978-1393386179\n\n2. Essentials of Bio-Statistics: An overview with the help of Software - R Programming \n\nISBN-13  :  978-1723712074\n\n3. Essential of Biostatistics: An Overview with the help of software- 2nd in Series: Pocket Guide - SAS\n\n[www.ijsmi.com/book.php](https://www.ijsmi.com/book.php)', 'Following!', ""Thank you! I'll check it out"", 'Thank you for the recommendations!', 'Thank you!']"
[Q] Standardization with multimodal data,"I'm attempting to apply PCA to metabolomics data, but am running into a gap in my understanding.

I need to standardize the data so that PCA will work right (ie, not prioritize variables with high variance/intensities). The most straightforward explanation of the methods I got from here: [https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02#:\~:text=turn%20to%20StandardScaler.-,StandardScaler,values%20by%20the%20standard%20deviation](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02#:~:text=turn%20to%20StandardScaler.-,StandardScaler,values%20by%20the%20standard%20deviation).

However, I am technically expecting my control set and test set to have different means and probably variances, which would make my base data multimodal. Do some standardization methods (such ""StandardScaler"" which affects the variance of the dataset) distort multimodal data by treating it as part of a single distribution? Do other people who work in bioinformatics use the StandardScaler (0 mean 0 stdev), or the RobustScaler (0 median, divide by interquartile range), which I imagine leaves the original distribution relatively intact? I understand that using the RobustScaler would probably result in higher variance features being prioritized, but is that a bad thing for metabolomics data (ie, wouldn't features affected by the treatment look like they have higher variance)?

For context, my goal for using the PCA is to discover a smaller set of features with explanatory power in the data before directly inserting class information.",13dswcm,LoopyFig,1683730120.0,1,1.0,"['Most implementations of pca standardize the features by doing the eigenvalue decomposition of the correlation matrix, rather than the covariance matrix as most people are commonly taught pca.', ""The PCA loadings can span orders of magnitude.  Different strategies for scaling probably only differ by a factor of, say, 2.  Therefore, I don't think the results will be overly sensitive to the scaling strategy.  But you might want to read a few papers doing similar work to see if there is a standard practice in this specific field."", 'Thanks this is really helpful!']"
[Question] question about confidence levels.,"Is it possible to find the confidence level while only knowing the binomial confidence interval, the margin of error, and the sample size?",13dqgn3,Effective-Ant9518,1683724816.0,2,1.0,"['Confidence levels are chosen prior to constructing the confidence interval.\n\nIf you have the binomial confidence interval constructed, then you should have chosen alpha already. Your confidence level is 1-alpha.', 'You can determine the confidence in the following way:\n\n1). Assuming you are using the normal approximation here you have a interval of the form\nHat(p)  z(alpha)*sqrt(hat(p)(1-hat(p))/n)\n\n2). As you know the interval and the margin of error you can solve for hat(n) and know n you can solve for z(alpha) \n\n3). Using z(alpha) you can plug into the inverse normal CDF to solve for alpha', 'im sorry, im new to notations, what do those mean?\n\nHat(p), Hat(n), z(alpha) all flew over my head haha', 'im sorry, im new to notations, what do those mean?\n\nHat(p), Hat(n), z(alpha) all flew over my head haha\n\nmy current question that is confusing me is given the binomial confidence interval of 0.591-0.749 and a sample size of 25 trials, what is the confidence level that they used? im so confused. its also asking me what the success rate is within the sample first which is also confusing, thanks for any help, you look like you know what youre talking about haha', ""Here are the definitions for the notation:\n\nhat(p) = # of success / # of samples (think proportion of heads with coin flips)\n\nn = the number of samples\n\nz(/2) = The 1-/2 quantile of the normal distribution.\n\nI miss-typed previously there is no hat(n).\n\nYou can read about the confidence interval here: [https://en.wikipedia.org/wiki/Binomial\\_proportion\\_confidence\\_interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval)\n\n&#x200B;\n\nRegardless, here is plan to find the alpha level in more detail.\n\n1). Find the center of the interval: (.749 + .591)/2 = .6685, This corresponds to hat(p).\n\n2). We can then find z(/2)\\*(hat(p)(1-hat(p))/25) = .749-.6685 = .0805, and solve for z(). In this case (as we can find (hat(p)(1-hat(p))/25) = .09415), we have z(/2) = .85.\n\n3). Using a Z-table we can find the coverage of this interval to be about 60% (I'll leave that part to you)"", 'thats great, thanks so much! i was completely stumped on this so thank you.\n\nbut doesnt (.591 + .749)/2 = .67? whats 0.6685?', ""Your right, so just adjust the calculations accordingly, The answer shouldn't be far off."", 'great, thanks!']"
[Q] Does Anyone have access to Statista and help out a poor college student? Their yearly rate is egregious and I need data for the research thesis I’m working on,"I cannot afford the yearly subscription and they do not have any monthly plans which I was willing to pay for but they don't exist.

Any chance someone can help please?

 [https://www.statista.com/outlook/dmo/eservices/dating-services/online-dating/united-kingdom](https://www.statista.com/outlook/dmo/eservices/dating-services/online-dating/united-kingdom)

Comment if you can help and I can DM you my email.

Thank you so much",13do8pp,yfzi,1683719343.0,0,0.25,"['Yes, dm me', ""Done\n\nYou're a lifesaver thank you"", 'Can I please email you too ?Ill pay', 'Go ahead!', 'FACE THE LEAD!', 'Just did !']"
[Q] How do you compute how many times (based on statistical probability?) four straight heads would show up in a coin toss of 100 times?,"Assuming a fair coin toss of 50% heads and tails showing up.


Five to seven straight heads is still considered one count, but eight straight heads is considered two count.


Given these information, what is the chance that four straight heads show up once, twice, 3x, 4x, and so on in 100 coin toss? Is there a formula for it?


Thank you.",13dkcok,Resident-Nerve-6141,1683707313.0,8,0.9,"['There are two ways to go about this. \n\nThe first way would be to just simulate it. Flip 100 coins (hopefully through the use of a computer) count the number of 4 in a row. Repeat many times (for some reason 10000 is a commonly chosen number of repeats) and then see the empirical pmf. \n\n&#x200B;\n\nThe second way is to just calculate the probabilities. There is some issues with this because you have defined things in an awkward way (I would probably think that 5 straight heads counts a two sets of 4, the first 4 and the last 4). Do you need the exact value?', 'Consider your group of 4 heads a unit and calculate the combinations', 'Not 100% sure on this, but scrapped together a simulation based on some of the comments here and made a plot of it. Will include the code here in case anyone else sees an error (especially considering how you group by the final cumulative distribution since not all trials will have the same lengths).\n\nBut I think the answer is 3% of the time.\n\nIn R of course. \n\n`# [Q] How do you compute how many times (based on statistical probability?) four straight heads would show up in a coin toss of 100 times?`  \n`n_simulations <- 10000`  \n`simulations <- c()`  \n`for (i in 1:n_simulations) {`  \n  `simulations[[i]] <- rbinom(n = 100,size = 1,prob = .5)`  \n  `}`  \n`# Clean Up for dataframe`  \n`sim_name_vector <- c()`  \n`for (i in 1:length(simulations)) {`  \n `sim_name_vector[[i]] <- paste(""Simulation"", i)`  \n`}`  \n`names(simulations) <- sim_name_vector`  \n`p <- lapply(simulations, rle)`   \n`t <- do.call(rbind.data.frame, p)`  \n`t$sim <- row.names(t)`  \n`t$sim <- gsub(pattern = ""\\\\.[0-9]+"",replacement = """",x = t$sim)`  \n`# Plot It`   \n`#simulation_data`   \n`simulation_data <- t |>`  \n  `tibble() |>`  \n  `arrange(values, lengths) |>`  \n  `group_by(values, lengths) |>`  \n  `count(lengths) |>`  \n  `ungroup(values, lengths) |>`  \n  `mutate(tot = sum(n),`  \n`freq = n/tot)`   \n`# Find how frequently sequences of four sets of heads happen`   \n`simulation_data |>`  \n  `filter(values == 1) |>`  \n  `filter(lengths == 4 | lengths == 8 | lengths == 12 | lengths == 16 | lengths == 20) |>`  \n  `mutate(final_sum = sum(freq)) |>`  \n  `select(final_sum) |>`  \n  `distinct()`  \n`# Plot it`   \n`reddit_plot <- simulation_data |>`  \n  `ggplot(aes(x = lengths, y = freq)) +`  \n  `geom_point(shape = 4, size = 4) +`  \n  `facet_wrap(~values) +`  \n  `theme_bw() +`  \n  `#stat_summary(fun = ""mean"", colour = ""red"", size = 1, geom = ""point"") +`  \n  `scale_y_continuous(limits = c(0,1), seq(0,1,.1),name = ""Probability"") +`  \n  `scale_x_continuous(breaks = seq(0,20,1)) +`  \n  `labs(title = ""Consecutive Coin Flips"",`   \n`subtitle = ""10,000 Simulations"",`   \n`x = ""Number of Consecutive Flips"",`   \n`y = ""Probability"")`  \n`reddit_plot`  \n`ggsave(filename = ""~/Desktop/reddit-plot.png"", height = 3, width = 6, dpi = 300)`', 'Probability that 4 heads occur consecutively is (1/2)\\^4. \n\nProbability that 4 heads occur 2 times would be\n\n((1/2)\\^4)\\^2\n\nReplace the last 2 with n for n times. \n\nFor 100 consecutive tosses, we can fit 25 -4 time tosses- inside. The probability of having all heads would be:\n\n((1/2)\\^4)\\^25', ""If I were going to try to calculate this seriously I'd probably just simulate, but I think there are multiple other avenues of attack. Either way you do need to be a little more precise - how many sequential runs of 4 are contained in a sequence of 100 heads?\n\nPending a more precise definition, two things come to mind:\n\n1. If you think about the sequence of tosses as a Markov Chain, then there are some nice results you can use. [Here's an example](https://stats.stackexchange.com/questions/264811/coin-toss-probability-of-a-run-of-certain-length-out-of-a-longer-sequence) of trying to calculate the probability of at least one run of a given length in a fixed number of tosses, which is not quite what you're asking but related. \n\n2. Another general strategy that comes to mind is brute force - you'd still need to use a computer but it seems feasible to calculate P(one run of length 4), P(two runs of length 4), .... etc.  via careful enumeration."", ""If there are 9 heads in a row, which should count as 2 lots of 4 per OP, I don't think your algorithm picks up on it."", 'Plot for those interested: \n\nhttps://imgur.com/a/7bzPZFp', 'Im no expert, but the probability asked is for out of 100 tosses and your reply is for any given toss and subsequent throws in succession. I believe using your formula for any toss as *P* in the binomial equation will the right answer.', ""Here's another similar post on Markov chains\n\nhttps://www.reddit.com/r/DreamWasTaken2/comments/kkaysw/the_chances_of_lucky_streaks/"", 'Ah, yeah, you would just have to change what the cumulative filter would be where it says \\`filter(lengths == 4 | lengths == 8 | lengths == 12 | lengths == 16 | lengths == 20) \\`. But I think those probabilities were so low it wouldn\'t really affect it too much. I think this is as far as I could go without having it be a very formal operationalization.  Didn\'t know if that is how to do it since OP also says ""Five to seven straight heads is still considered one count"".']"
[Q]What are skills you wanna master at work to make your life easier?,,13df9eb,THE_REAL_ODB,1683690069.0,1,0.67,"['File management', 'Coding to automate tasks like file organization, running forecast generation over the weekend, and scraping the web for data.', 'Convincing people to care about data quality, management, and pipelines', 'Office politics']"
[Q] plot confidence interval of a bivariate,"I have a question regarding the confidence interval of a bivariate.
I have found the confidence interval of each variable, and now I want to plot the confidence interval in dimension 2.
How is this usually done ? 
Thank you.",13db8ur,Equivalent_Good_753,1683678739.0,1,1.0,"['confidence interval for what parameters? of what distribution?\n\n>  How is this usually done ?\n\nWell, not from the marginal CIs, but jointly. Specific details will depend on stuff you left out.', 'It depends on what estimates the confidence intervals are based on. There are some times where you can do this (for example with sample means and variance) when you can have two marginal of coverage sqrt(1-alpha), which by independence will give 1-alpha coverage. You could also to a bonferroni region where you have the marginals with 1-alpha/2 coverage to give a Cartesian product of the sets 1-alpha coverage. Without more information the bonferroni correction is the way to go.']"
[Q] Applying for jobs while feeling unqualified. How was it like for you?,"Im graduating soon with my bachelors in statistics, but I still feel super unprepared for the real world. I have found some jobs online, but writing cover letters for them makes me nervous because thats basically me overhyping myself. 

My worst nightmare is getting a job and being expected to do something that I either did not learn, or I forgot how to do. Im very curious how you guys got started, how was it. Also recommendations on where to apply as an entry level data analysis/scientist would be helpful as well. 

Also lots of jobs online require masters degrees, but I still want to apply to them. Landing a job that requires such a degree would be cool, but what might be expected of me is worrying me. All my statistical analysis experience comes from my courses.",13d9wv4,lewdjojo,1683675270.0,53,0.91,"['There\'s a saying that ""If you have everything described in the job description you\'re probably overqualified"". That helped me with applying', 'Your first focus should be  get a job. You cant get experience without a job. \n\nIf you just graduated, your manager will know you need training and they also know youll be cheaper to hire than someone with a lot of experience, thats why there are entry level jobs.\n\nGood managers will prefer someone with good values and a good attitude and willingness to learn but no experience, than someone with a lot of knowledge and bad attitude.\n\nWhen you get a job, ask why they chose you, youll see that they focous on your potentiential, and please ask anything that you dont know. If you are in a good company with a good manager, theyll be glad you ask instead of hiding what you dont know.', ""I do sometimes wonder if you ever feel really qualified. \n\n>  being expected to do something that I either did not learn, or I forgot how to do\n\nThis is very likely to happen. You need to keep in mind that you'll rarely have to do something instantaneously and that you're able to *go look things up* or *ask people questions*. \n\nGood training can't teach you how to solve every problem, but it should equip you with the tools to be able find some way to do it most of the time. You have many resources, life is not an exam.\n\n> All my statistical analysis experience comes from my courses.\n\nThat's less good; you can choose to do things outside your coursework, right? But once you have a job, the job itself should help give you plenty of things to be getting experience from."", ""I would recommend doing something else during this time besides just looking for jobs. The most obvious thing is to continue learning things that the industry requires (especially technical stuff such as coding). Online courses are useful (they always have discount for students). But doing an online project from Kaggle or DataCamp is something I *strongly recommend,* since it will give you a taste of the real world and something nice to put on your resume.\n\nWorking part-time or volunteering are not bad ideas also. Expertise and skills are important, but companies also desperately need people who can work well with others. I saw some talented people who ended up nowhere because they are too stubborn and uncooperative.\n\nEither way, keep the wheels running and don't let any gap. Your statistics degree means you possess the fundamentals for the most trendy things nowadays. There are a ton of things for you to do, and all you need is to take an extra step."", 'My motto is let them decide if I am the right person. \n\nOf course if they need someone with 5 years experience and you are just starting then apparently its not for you.\n\nthe skills/requirements on the ad most of the times are a wish-list not an exact description. \n\nSo apply and let them decide. Dont close the door to your self!', 'Rather than explaining how it was for me, id rather share with you how full of shit people are at least in the US. You are likely more qualified than most of people with 5+ years experience. The biggest challenge for new grads is going to be the change of pace from school to working. If you can adjust to a company culture and mentality then you will be fine. Its more about projecting confidence than anything else.', 'Yeh I feel you :/ Im more or less in the same situation and tbh I think no matter the qualification to some degree thats very normal because well its a new start.. good luck ', 'Really? Are there data sciences jobs? MS with how many years of experience?', ""If you are U.S., www.usajobs.gov. Try searching math, statistics, operations research, etc. Get in gov't and let *them* pay tor your MS in stat."", 'Sounds like me right now. Everything i do isnt what i knew. You google what you can, ask around what google can answer, talk to your boss for things your networks cant help you. And lastly, plan how to solve what your boss cant solve. \n\nJust cuz you arent qualified dont mean you cant do the job. Qualifications vs capabilities are very different.', 'Fake it til you make it!', 'What was your first job after graduating? Or did you continue to get your masters?', ""> If you just graduated, your manager will know you need training and they also know youll be cheaper to hire than someone with a lot of experience, thats why there are entry level jobs. \n\nI'm not sure this is true everywhere. At least where I am, every entry level role gets hundreds of applicants and the ads generally have a lot of requirements for a low pay. A lot of employers want experienced candidates for entry level pay, which they're sadly able to get."", 'Well Im teaching myself sql right now since lots of jobs want to see it. But still, everything I know was from what I was taught in class and the projects I was assaigned to do, which contributes to my fear of being under qualified. \nI just hope that whatever data analysis/science job I get into, I have a mentor to guide me.', 'I went the Comp Sci developer route first before I transitioned into data eventually. My first job was full stack dev but I applied and interview with a few Business/ data analyst role also', 'Learning SQL is a good move.  Very often is a must have, and is relatively easy to learn for most of your likely use cases.']"
[Q] how would you visuallise my data?,Hi everyone! I wanted to create a clustered boxplot but encountered a problem. The sample sizes of the subgroups on the left side all are all 2 and thereby do not properly form boxplots. The subgroups on the right side all form proper boxplots. How would you visually compare this? Should i just display my individual data in a table? Thank you in advance,13d5ctr,AD-Films,1683664887.0,0,0.5,"['Compare what? What features of the data are you trying to visualize?', ""You really can't do a lot of meaningful statistics with a sample size of two. Even if you could draw a box plot it wouldn't tell you very much. You can plot the mean and the range but... It's not going to be very meaningful information.""]"
[D] Is time series analysis much better than just using common sense?,"I did some poking around about sales forecasting.

It should be possible to predict monthly or seasonal sales of an item by looking at previous sales of the item. However, you can do this just by looking at a list of sales or taking average sales per month. However, the number of items available seems to be conspicuously absent from these computations. Obviously, you can't sell more than 24 widgets if you only have 24 widgets.

Similarly, it should be possible to predict what hour of the day people go shopping or other similar data.

My plan was to just cram data into a neural network and hope for the best. There's a limit to much data you can cram into a neural network in a reasonable amount of time, and neural networks are kind of nonsense anyways.

I noticed that other people use more advanced mathematical tools for this. It looks like they have different mathematical models, and they attempt to see which model is most accurate for the data that they have. What that says to me is that the mathematical models are wrong. When they're right, it could just be noise. It looks like they're using math to connect the dots on the graph and produce some sort of function for predicting sales.

I assume this is better than having a 5-year-old draw on a graph with crayons, but is it really that effective? Shouldn't inflation data, production costs, and the competitive landscape be factored into this somehow?

There seems to be a lot of sales forecasting going on, and it sounds to me like a bunch of people are making a bunch of overhyped sales forecasting products. It's just that no one can tell because the math is complex.",13d3bib,lootsmuggler,1683660371.0,0,0.45,"['> I assume this is better than having a 5-year-old draw on a graph with crayons, but is it really that effective? Shouldn\'t inflation data, production costs, and the competitive landscape be factored into this somehow?\n\nNoisy, highly seasonal, and poor quality data is the norm at my work, so ""looks like a 5 year old drew this extrapolation curve with crayons"" is the gold standard I work towards', '""Time series forecasting"" can include a whole slew of models, of which you seem to be considering only a particular kind.\n\nIt also sounds like you have information that\'s not in your models; relevant external information should inform your models whenever possible, for all that it might make things more complicated to do so.\n\n>  Shouldn\'t inflation data, production costs, and the competitive landscape be factored into this somehow?\n\nCertainly, if they\'re relevant, they should come into the modelling of the variable, and into the forecasting.\n\nSometimes ""obvious"" factors are already fairly well covered by fairly naive models like ARIMA (because they\'re also strongly related to previous values in the series which are being used in the predictions), but sometimes not. \n\n>  it sounds to me like a bunch of people are making a bunch of overhyped sales forecasting products\n\nOh, no doubt. What people sell and what people use may both fall far short of what should reasonably be done.', 'Time series analysis can indeed be a better approach than relying solely on common sense when it comes to sales forecasting. Common sense can certainly be helpful in understanding the underlying factors that may affect sales, but it is usually limited by biases and incomplete (and potentially inaccurate) information.\r  \n\r  \nUsing past sales data to predict future sales can be a good starting point, but it is important to use appropriate statistical techniques to account for factors that may influence sales, such as changes in the competitive landscape, inflation, or production costs. Simply taking averages or looking at raw data can oversimplify the complexities of the data and result in inaccurate predictions.\r These math tools in statistics can help account for these factors and provide more accurate predictions. These tools do not necessarily produce a ""function for predicting sales"" but rather provide a statistical model that can be used to make predictions based on historical data and other relevant variables.\r  \n\r  \nNeural networks can be a useful tool for sales forecasting, but they require appropriate data preparation and parameter tuning to produce reliable results. Additionally, as you noted, there are limits to how much data can be processed in a reasonable amount of time, and neural networks can be prone to overfitting if not used correctly.\r  \n\r  \nSo, while there may be overhyped sales forecasting products out there, using appropriate statistical techniques can provide more accurate predictions and help businesses make more informed decisions.', ""My expectation is more items that there were a statistically insignificant number of or that weren't consistently in stock with no indication that they weren't in stock.\n\nI don't know what to do with it, but that presupposes that I ever actually have the data."", "">It also sounds like you have information that's not in your models; relevant external information should inform your models whenever possible, for all that it might make things more complicated to do so.\n\nIt's worse...\n\nI don't even have a model.  I shouldn't even be involved with this.  I'm not sure whether I'm technically hired or not, but I shouldn't be.\n\nI swear that someone is trying to dump this problem on the first person they could think of who had something to do with AI.  All I can do is shove whatever data they give me into a neural network and hope for the best."", 'ChatGPT has spoken.', ""It's actually a mash-up from a recent review of Springer's Applied Predictive Modeling and my experience with time series analysis. On mobile, it's easier to copy and paste the original post and reply to the concerns in-order. I'm assuming that's the ChatGPT flavoring you had noted. \n\nOr maybe I'm a ChatGPT bot pretending to be human. In that case, who knows?"", 'Whether the information was from ChatGPT or not, it was still useful information.']"
[meta] statistically determining the best way to become happy,"We have a data set with millions of data points. Each single data point represents one method to become more happy, like meditation, working out, relationships, watching youtube, etc. Alternatively, a data point can also be a combination of other data points, for example a religious teaching containing multiple methods. Each data point has a value of  -100% (leading to 100% despair) to +100% (leading to maximum happiness).

The problem is: The value of most of the data points is hidden to us and we don't have the time to check every single of these millions of data points by our own.

How do we find the data point that leads to the highest happiness? I have a candidate, but how can I be sure that candidate is the right one as there are millions of data points with hidden values? Any tips narrow down the list? 

This question might seem technical, but actually, isn't this the only game that we as humans are playing all the time? Constantly trying to find happiness? Therefore, I think it's highly important to thing strategically about the right approach.",13d07rk,EmptyImagination4,1683653595.0,0,0.33,"['Some questions cannot be answered by statistics.', 'Why did you post essentially the same post twice?', ""Latent variable models might be applicable here. A web search for that term will find some resources.\n\nYou'll have to be careful about cause and effect here. It's hard to gauge the effect of some intervention when people choose the intervention for themselves, because there's some correlation between the choice and the effect. Maybe there's some kind of proxy for a random selection -- I saw a reference to a study which compared people who had been drafted into the army with non-draftees. Dunno what else there might be.\n\nIn general, I don't see why a statistical approach to happiness couldn't work at least as well as the multitude of other approaches that have been proposed, so have at it, I would say."", ""A big issue here is that this data doesn't exist, and presupposing that it exists (as you've described it) sort of already solves the problem you've posed. \n\nI mean, a dataset in which one million human activities are already rated for how happy they make someone? Never mind how impossible getting it would be, doesn't that data itself just contain all your answers?\n\nMaybe I misunderstood. Are you asking how one might produce such a dataset?"", 'Spank the monkey', 'where should I ask then?', 'I think you might be right. I can reduce the problem into the following categories. Now I think if I know which value to assign to each category, the problem is solved, right?   \n\n\n Class 1: Materialistic Strategies.\r  \no Subclass 1.1: Buying things\r  \no Subclass 1.2: Buying services\r  \n Class 2: Spiritual Strategies.\r  \no Subclass 2.1: Religions of Faith\r  \no Subclass 2.2: Religions of experience (non-Buddhist)\r  \no Subclass 2.3: Meditation\r  \no Subclass 2.4: Personal Development books consistent with Buddhism\r  \no Subclass 2.5: Personal Development books at odds with Buddhism\r  \no Subclass 2.6: Buddhism\r  \n Class 3: intangible strategies (all strategies that are not assigned to class 1 or 2)\r  \no Subclass 3.1: Maintaining relationships (friends, family, colleagues, partners, ...)\r  \no Subclass 3.2: ""succeed:"" Have influence/power\r  \no Subclass 3.3: ""succeed"": become famous\r  \no Subclass 3.4: ""succeed"": be rich\r  \no Subclass 3.5: ""gain freedom""\r  \no Subclass 3.6: Meaning in life (activism / helping others)\r  \no Subclass 3.8: Health / Sport\r  \no Subclass 3.9: Media Consume\r  \no Subclass 3.10: (all other) recreational activities not contrary to the Dharma\r  \no Subclass 3.11: (all other) recreational activities contrary to the Dharma', 'lul', ""This is a question as old as man and honestly is the story of humans in general. I don't have an answer for you on that one but statistical studies on something like emotions are not an easy thing to do. It's hard to have a measurement system that can appropriately measure something we cannot physically process."", 'You\'re asking a philosophical question rather than a statistical one. We can\'t event agree on definitions of happiness, much less operationalize it for statistical purposes.\n\nHonestly, just ""philosophy of happiness"" on Wikipedia might be a good place to start. It\'ll at least familiarize you with all the various ways we\'ve tried to nail down happiness throughout history.', 'Yes. I think we\'re on the same page. The dataset that you\'re proposing, categories or not, seems to simply contain the answers you\'re looking for without the need for any statistics. If you had a dataset that just *told you* which activities produced varying levels happiness, then no analysis would be needed.\n\nNow that I look at these categories, it seems pretty clear that you\'re motivated to investigate some questions pertaining to buddhism. That\'s really neat. I suggest that you focus on some more pragmatic questions like ""How might you measure or infer a person\'s happiness, and how reliable/unreliable would that be"".  \n\n\nI think that I suggested in another thread investigating philosophy of happiness, but there\'s likely a sizeable body of behavioural research as well. You\'re likely not the first person to ask these sorts of questions and digging into what others have done in the past would be incredibly valuable I think.', 'hi, thanks for the answer.\n\nWell, yes I have done the research and looking at the research of subjective wellbeing I can actually give a value to most of the subcategories. \n\nThe value used in the literature is how much activity x is correlated to subjective well being. \n\nIf I can apply a value to all the subcategories, the problem should be solved, right?', 'Regardless of what you\'re measuring, your measurements are always best defined at the lowest level of aggregation that you can manage. You can always aggregate up to higher ""categories"", but you can\'t slice things up if there\'s nothing to slice. I hope that makes sense.']"
[Meta] Statistically vetting a spiritual teaching?,"I have come across a spiritual teaching that claims to lead to ultimate happiness, highest bliss.Now I need your help: Please go through the following statistical reasoning and tell me if this reasoning is true or false:

Now, there are millions of ways or let's call them strategies that claim to lead to happiness, like:  
\- Buying certain products or services (millions)  
\- Reading and following the teaching of certain books (roughly 100k-1million)  
\- working on relationships  
\- working out  
\- following certain religions (roughly 10000)  
\- etc

Therefore, statistically, the probability that the teaching I have come across is true, is one in millions. Do you agree or did I overlook something?

**EDIT 1**: I think statistically we have a distribution with millions of data points. Each data point represents one strategy and it's effectiveness in increasing happiness, on a scale from -100% (leading to 100% despair) to 100% (leading to complete happiness). That's what we know. Now, how probable is it that the spiritual teaching I have found is actually the data point with the highest effectiveness?

**EDIT 2:** The underlying assumption here is, that each strategy has the same probability. This places an arbitrary, excessive overweight on the materialistic strategies like buying products or services, simply because there are more of them available. How do we correct this overweight? ",13cyqsr,EmptyImagination4,1683650437.0,0,0.33,"['How high are you right now', 'I think this is a bit far from statistics. I will just say your assuming several things: there is one state of happiness that is the same for everyone and also that you are as likely yo get there with any possible action. The first point seems really not true in a logical way since not everyone gets happy the same way, and the second one is also linked to this and not true. \n\nI think this is not really a stats question, but I\'ll try my best to explain the second point (you can\'t always assume equal probability to all possible events) as I think this is a common way of thinking if you\'ve never seen prob distributions. Let us forget the philosophy behind it and let\'s assume there is a right ""way to happiness"", then we can\'t say anything about how likely it is to find it without knowing more. Imagine it is turning 20 yrs old for example. Then the probability of you finding it is 100%, as you will eventually age. If the way of happiness is to visit hawaii, then the probability of it happening to you is affected by your environment. If it is earning 1000000 in a casino, then it is very unlikely. I could go on, but the point is that the event changes completely the probability.\n\nThe point is, events a lot of times do not have the same probability of happening, and thus we need to asign different weights to different events.', '""the probability that the teaching I have come across is true, is one in millions"" - for that to be true, you would need to have randomly sampled that one teaching from a pool of all of the other millions of alternatives. If you heard about this teaching in a non-random way, a friend told you about it, or you found a book about it - that is not a random sample. It may be that more effective teachings are more likely to be repeated and written about.', 'Consider a scenario where people are randomly assigned to strategies for happiness, and then each group is surveyed with the question ""how happy are you, on a scale from 0-10?"". For data we would report the mean and standard deviation of each group.  Now we want to know, how do groups associated with your teaching rank among the others? \n\nIf you feel this sufficiently matches your scenario, I suggest using it instead because you\'ll get better help from us.', ""Several problems here.\n\nFirst, this is not a statistics question - it's an *a priori* probability question.  For statistics, you generally need to have (a) taken measurements / data from a population, (b) assumed or inferred a probability distribution for the population, and (c) applied a test to the distribution and data.  Your question is different.\n\nFor that matter, your question isn't meta-statistical, or a meta-level discussion.  It's just a question of determining the best prior probability for a happiness strategy.  Fundamental and abstract, and arguably interesting.  But not meta.\n\nSecond, you are assuming exclusivity, that only one strategy will lead to happiness.  But it's possible that *many or all* strategies lead to happiness.\n\nCompare: there are thousands of recipes that claim to produce a good meal.  Must only one succeed?  No, many or all can produce a good meal."", ""I have some concerns with the reasoning. The assumption that each strategy has the same probability of leading to happiness is not founded in any reasonable calculation. Certain strategies may have higher probabilities of success due to factors such as effectiveness, compatibility with an individual's personality/lifestyle, and/or the circumstances around which they are implemented.\r This reasoning does not account for the fact that happiness is a complex construct. Different strategies may contribute to happiness in different ways. It is possible that multiple strategies can be combined to achieve a higher level of happiness than any single strategy alone.\r   \n\n\nThe distribution of strategies is also not necessarily uniform. While there may be millions of data points representing different strategies, their effectiveness in increasing happiness may not be evenly distributed. Some strategies may be much more effective than others, and thus, the probability of the spiritual teaching being the most effective strategy is not necessarily one in millions.\r  \n\r  \nTo correct the overweight on materialistic strategies, it would be more appropriate to analyze each category of strategies separately, and potentially assign a different weight to each based on the available evidence for their effectiveness. In this way, you could obtain a more accurate estimate of the probability that the spiritual teaching you found is the most effective strategy for achieving happiness.\r Ultimately, it is important to remember that statistical reasoning is only one way to evaluate the effectiveness of a strategy. Personal experiences, values, and beliefs may also play a significant role in determining what works best for an individual when it comes to achieving happiness.  \n\n\nEssentially, this approach is like grabbing smoke - too many parameters that aren't nailed down will lead to hand-wavy conclusions with no room for analysis."", 'Hi - how are you.', 'Hi, thanks for the quick answer! I think we talk about different probabilities.\n\nProbability one is the probability that a certain claim or strategy actually leads to happiness.  \nprobability two is how likely you can actually execute that strategy successfully. \n\nWhat I am talking about in my post is probability one. What you are talking about is probably probability two. Which is also interesting but not what I meant. So thanks for the opportunity to clarify this.', ""Yes I think that's right. The underlying assumption is that each strategy has equal probability. \n\nThis places a synthetic, excessive overweight on the materialistic strategies like buying products or services."", 'amen!', 'ok I have ordered all strategies into categories:\n\n Class 1: Materialistic Strategies.  \no Subclass 1.1: Buying things  \no Subclass 1.2: Buying services  \n Class 2: Spiritual Strategies.  \no Subclass 2.1: Religions of Faith  \no Subclass 2.2: Religions of experience (non-Buddhist)  \no Subclass 2.3: Meditation  \no Subclass 2.4: Personal Development books consistent with Buddhism  \no Subclass 2.5: Personal Development books at odds with Buddhism  \no Subclass 2.6: Buddhism  \n Class 3: intangible strategies (all strategies that are not assigned to class 1 or 2)  \no Subclass 3.1: Maintaining relationships (friends, family, colleagues, partners, ...)  \no Subclass 3.2: ""succeed:"" Have influence/power  \no Subclass 3.3: ""succeed"": become famous  \no Subclass 3.4: ""succeed"": be rich  \no Subclass 3.5: ""gain freedom""  \no Subclass 3.6: Meaning in life (activism / helping others)  \no Subclass 3.8: Health / Sport  \no Subclass 3.9: Media Consume  \no Subclass 3.10: (all other) recreational activities not contrary to the Dharma  \no Subclass 3.11: (all other) recreational activities contrary to the Dharma', ""Ok I have edited the question because what I can say is that this is not like a 0 / 1 game. Each strategy can have varying effectivity, like on a scale from -100% to 100%. So the strategies don't necessarily exclude each other."", 'Organizing the different strategies into subclasses, this problem becomes a lot more managable. I even think I can answer on most subclasses if they increase happiness or not.', 'I\'ve read your edits, and this reply. First of all, the modelling is a bit unclear to me, but I am getting that each startegy brings you to a percentage of happiness, or close to happiness, and we assume we can measure how close that is.\n\nIf you want to know what is the probability of achieving this maximum closeness to happiness, assuming that you are equally likely to pick among all possible strategies, then just add the amount of strategies that give the maximum ""closeness"" to happiness and divide it by the total amount of strategies. Will this be small? We can\'t know without knowing all the strategies, and how effective all of them are, then also hope they are finite and that there is a maximum achieved. If we know all these, then you are set.\n\nAll in all, you could probably say that the probability is small if the assumptions are met and give a good enough amount of strategies, but my point is still that this is too general and there\'s not much to be said without assuming. If the point is to disproof that said product give you happiness, I would not go into probability arguments like that.']"
[Q] Is Akaike Information Criterion a reasonable metric to use when choosing an appropriate Time Series model?,"So, theory states that the Time Series model with the lowest AIC technically the best model. But, does this always hold true? I am building time series on the quarterly Ontario population for the past 23 years. However, when I make predictions on the past 19 quarters, the data has a significant vertical zig zag behaviour which is counter intuitive since there was no historical drop in population. Also, the correlogram values out of the lower bounds at lag 5. Is an issue perhaps is that my grid space is too small? My range for p, d, q goes from 0 to 5. Should  I perhaps increase it?",13cxm9v,PythonEntusiast,1683648129.0,16,0.91,"[""AIC and BIC are good metrics that tells you how much information you've extracted from the series. Diebold (2007) suggests to follow BIC if AIC and BIC disagree, opting for the more parsimonious model. This is good enough for modeling. \n\nBut for forecasting, I think most important is the model's root mean square error when backtested against historical data the model has not yet observed.\n\nEdit: if the residuals of your model exceed the bounds, you still have unaccounted seasonality in your residuals, which is not good. In general, a good forecast/model should have unforecastable/un-model-able residuals.\n\nHave you considered using time trends, breaks, or data transformations like taking logs? How about introducing exogenous variables into the data?"", 'I think Minimum Description Length is the superior model selection principle. There are various versions and implementations in specific models, but if you can find an appropriate one already computed for you I would use it.', '> theory states that the Time Series model with the lowest AIC technically the best model\n\nI don\'t think that\'s quite what ""theory states"".\n\nWhere are you seeing that?', 'Theory states no such thing, other than trivially.  The best model *according to AIC* is the best model according to AIC (duh!)\n\nAIC does not assume that the true probability distribution of the data is even on the list of models being selected among.   All AIC comes with is the property of selecting a model that is not too bad in terms of Kullback-Liebler information with the usual caveats of statistics: very bad models can be selected but with small probability.  When there are many models under consideration, the probability of selecting the best can be negligible.', 'If I had to guess, the vertical zig zagging is an AR term with a negative value. This could be due to overdifferencing. Make sure youre not using more than 2 orders of integration. For a population model I think a pdq(1,1,2), which is a damped linear trend model, might be appropriate.', 'Yes and no. It will be the best Frequentist model under the assumptions and given the role of chance effects, but that may differ from the generative model. If the Bayesian model varies from the Frequentist, then the Bayesian model is likely closer to the generative model. \n\nThe Bayesian likelihood is always minimally sufficient while the Frequentist may not be. So the AIC may be choosing the least lossy Frequentist model. \n\nThe best Frequentist model may be ARIMA(2,1,2) while the Bayesian model is AR(1) when the Frequentist AR(1) model is deprecated by the AIC.', 'Read points 4, 5, and 6 in [this blog post](https://robjhyndman.com/hyndsight/aic/).', 'Thanks for the reply. Will try it out.', 'So, the ""best"" way to select an appropriate model is simply loop over the possible p, d, q combinations, select top N with the lowest AIC, and see which have the best performance in terms of RMSE and Residual Analysis (QQ-Plot, Correlogram, and so on)?', 'It\'s impossible to specify the ""best"" model selection strategy without specifying what you\'re trying to use the model for, and what properties you want the model to have.', 'I tell people to stop with AIC and a bunch of best candidates.  Or go on to model averaging.  Your suggested ad hoc follow-up analysis does not have any desirable mathematical properties.', 'Sorry about that. I am trying to forecast the quarterly population volume of Ontario based on the past 23 years of data ([https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901](https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901)). Data is not stationary. There exists trend. I have utilized the SARIMAX model and was able to build the model to some degree of success (Normal residuals, qqplot values are all around the normal, random residuals, correlogram values are all within the confidence interval, and coefficients are significant). I have achieved that by looping over the range of p,d,q and trying to see what the (p,d,q)(P,D,Q,4) give me smallest AICs. However, even the model with the LOWEST AIC had terrible performance in terms of residuals plot so I had to tweak it manually till the coefficients are significant and residuals plots are appropriate.\n\nAlso, I fitted the model on the differenced data although DW test results were insigfinicant. Trying to further it now by removing the trend (removal of trend produced significant DW results). Just trying to wrap my head around how to get the forecasts in the form original data since it forecasts detrended values.\n\nThanks', 'What desirable mathematical properties are you talking about? What we generally care about is that the model has significant coefficients and that the residual analysis is within bounds (e.g., points in the correlogram are within the bounds and that the residuals are normally distributed with a random behaviour (no trend)).', ""Still insufficiently specified. Like the person you responded to said, you have to consider not just what you're trying to fit, but *why*. How will be model be used? Is it more costly to over-estimate or under-estimate? Is it more important to correctly estimate the trend, the variance, the seasonality?"", 'Correctly calculating statistical inferences (""statistically significant coefficients"") *after model sellection and correctly adjusting for the model selection* is an open research question.  I doubt you have any mathematically valid method for that.  That is, your ""significant coefficients"" are bogus, not based on math.', 'Model will be used internally at work to forecast specific volume of the population subset based on the forecasts produced by the Time Series model. Underestimation is more costly. Trend is more important (population is expected to grow).']"
[Q] Comparing two unequal groups,"I need to compare two groups of unequal group sizes to determine if the results are equivalent or comparable. The groups are two different populations that are from different manufacturing processes, and Im trying to determine if the in-process testing results for certain parameters are the same. 

Problem is, one of the groups has a very small sample size (n=8), while the other group has around 30 samples. 

I considered performing a Kruskal-Wallis test, but my understanding is that distributions of the data need to have a similar shape and spread, which they do not. The group with with the larger samples size has a much wider spread than the group with the smaller sample size. Does it really matter, or should I consider a different test?",13cxdlc,alcarcalimo1950,1683647772.0,2,1.0,"[""Hey there! So, you want to compare two groups that have different sizes, right? The two groups come from different manufacturing processes, and you want to see if their in-process testing results are pretty much the same.\n\nThe thing is, one group has only 8 samples, while the other group has about 30 samples. That's a bit tricky!\n\nYou mentioned the Kruskal-Wallis test, but you're right that it might not be the best choice here. That test works best when the two groups you're comparing have a similar shape and spread, and it sounds like that's not the case for your groups.\n\nInstead, you could try using a test called the Mann-Whitney U test. This test is useful when you have two groups with different sizes and different shapes/spreads. It helps you figure out if the two groups are likely to come from the same population, or if their differences are significant.\n\nJust remember that when working with small sample sizes, like your group with only 8 samples, the results might be less reliable. But the Mann-Whitney U test is still a good place to start for your situation. Good luck with your comparison!"", '> I considered performing a Kruskal-Wallis test, but my understanding is that distributions of the data need to have a similar shape and spread, which they do not. \n\nYou can find this claim in numerous places but it\'s not at all true.\n\nThe need to have similar shape and spread is not in the data, but in the populations, and specifically *when H0 is true*, in order to get correct significance levels, and hence p-values. \n\nThe conditions you need under H1 can be considerably weaker, unless you feel the need to impose conditions beyond those needed for the test to have the usual things people expect of a test (an actual  significance level close to that selected from the available significance levels, consistency against some set of alternatives, etc). Since a pure-equality null is almost certainly false, the data aren\'t necessarily even relevant to this consideration.\n\n> Problem is, one of the groups has a very small sample size (n=8), while the other group has around 30 samples.\n\nHuh? Why are you using *Kruskal-Wallis* if you only have two groups? \n\n>  The group with with the larger samples size has a much wider spread than the group with the smaller sample size. Does it really matter\n\nIt might or it might not. It depends on what the situation would be *if H0 were true*. For one example imagine a case where spread and mean increase together (e.g. the sort of thing you might see with a gamma distribution with fixed shape parameter). You would have that the shape and spread are the same when H0 is true (so the required exchangeability holds), but not when H1 is true, so if H1 is false and you see different spread in the data, that might be of little consequence. What matters is whether it would be reasonable to think they\'d be more alike when H0 were actually true (e.g. one example where it makes sense is where it makes sense to thing that a treatment has no effect at all, so that when H0 is true, nothing happens; what you see under H1 may well allow both shape and spread to change as you move away from H0)\n\n>  determine if the results are equivalent or comparable\n\nThis ""equivalent or comparable"" phrasing sounds more like you need an equivalence test, not a test with an equality null.\n\nWhat is your response variable?', 'For two groups, Kruskal-Wallis and Mann-Whitney are identical.', 'And neither test actually necessitates the two distributions have similar shape and spread, unless the test is being used as a test of the median.', ""You are right!\n\nSince we are comparing two groups, the Mann-Whitney U test is still the more appropriate choice for the situation. The Kruskal-Wallis test is designed for comparing three or more groups. The Mann-Whitney U test will help determine if there's a significant difference in the rankings of the two groups, regardless of their shape and spread.\n\nRight?""]"
[Q] How to calculate rate disparity using cars,"Let's assume that there are 200 cars on the road in an isolated town that comprise of 5 different makes. We'll go Tesla, Ford, Chevy, Nissan, and Maserati. 

Of the 200 cars, 130 are Tesla. 26 are Ford. 26 are Chevy. 11 are Nissan, and 7 are Maserati. 

There's only one mechanic in town who's constantly busy. He's always working on a car, but he notices a significant difference in the rate of the make of cars coming in compared to how many there are on the road in town. 

Because there are so many Tesla's (65% of the total cars) you would think that he would be working on those the most, but he's not. In fact, just slightly more than half of the cars that come in are Chevys.

Trying to solve for the disparity, the mech sees that there's no noticeable difference in the roads the Chevys drive on compared to the others. Mileage of each car hovers near average. If anything, the Chevy dealership in town offers the Chevy owners free tire rotations and oil changes, while other makes do not have such luxury. He just can't put his finger on it.

Given the Chevys only account for 13% of the cars, but account for 52% of all trips to the shop, what is their likelihood of visiting the mechanic more often than say, the Tesla, who represents 65% of the cars, but only 40% of the shop visits.",13cua9o,a_coupleofwavylines,1683644726.0,2,1.0,"['Quick thoughts:\n\nThis sounds like set-up for a *chi-square goodness-of-fit test*.  Looking at examples may give you some insight.\n\nYou might be interested in the *odds ratio*.  Perhaps starting by just thinking about Chevys and Teslas.', ""Thanks Sal. I'll see if I can find a tutorial for that""]"
[Q] Textbook recommendations - comprehensive exams,"I'm at the end of my first year as a PhD student in Biostatistics and will be writing the comprehensive exams for my program at the end of the summer. One of the exams will be purely math stats, and one applied stats. I'm planning to use Casella & Berger mostly, also have read most of Keith Knight's book, Mathematical Statistics,  and All of Statistics by Larry Wasserman so will be using those ones as well. Are there any other good texts that this group would recommend? Does not need to be an intro text.

Sorry if this has already been asked, used the search bar but results didn't match what Im looking for.",13cs64u,Significantpvalue,1683642974.0,2,0.75,"[""Will the pure math stats exam use decision theory? I haven't heard of the Knight book but if you will be tested on it. The two Lehmann books (TPE, TSH) are classics in that regard. Although It is worth mentioning they are a level above Casella and Berger IMO. For the applied side there are nearly an infinite number of books. Does your department focus on experimental design, causal inference, bayesian methods, etc. Knowing this will help me to recommend some more.""]"
Are F- and t-tests for regression coefficients always meaningful? [Q],"For example we might predict body fat using abdominal circumference as our predictor. 

y = b\_0 + b\_1(circumference)

It doesn't make sense to have a person with zero abdominal circumference so do we care if its coefficient is statistically significantly different than zero?",13cn2he,DerekTheTerrible,1683632089.0,7,1.0,"[""The coefficient b\\_1 is about the *effect* of circumference on body fat. The coefficient being 0 wouldn't mean that there's somebody with 0 circumference, but rather that circumference has 0 effect on bodyfat. That would be an interesting conclusion.\n\nFor the intercept term b\\_0 it's true that it's often not interesting whether it's different from 0. Note that in regression we only claim that the model is correct within the range of observed data. We're not trying to extrapolate outside of that range and predict that the bodyfat of someone with 0 circumference would be b\\_0, that would be bad statistics. The intercept is simply positioned so that the model properly fits the observed data."", 'Are they always significant? Yes, if the test statistic exceeds the critical value.\n\nAre they always meaningful? No. And you just gave one of 1,000,000,000,000+ reasons why not.', 'thank you! This is where I was confused.  :)', ""While it is often ignored, the intercept can still give some insights about the data and/or the correctness of the model.\n\n\nIf you Center the predictor, which is often a good idea, the intercept will give you the average of the predicted value after correcting for the effects of the predictors.\n\n\nIf you don't center it, a significant intercept could also be a hint that your model is either missing some additional predictors (e. G. Non-linear terms) or that you would need a transformation on your dependent variable or a different error distribution etc. You might also want to force the intercept to be zero in some cases. However, most often the intercept is indeed ignored, as the model might still make sense in most cases and the errors only lead to minor improvements but require complicated methods (e. G. Cross validation). It could also tell you the model is only applicable to certain cases. \n\n\nYou would need an expert in the domain to tell if the model makes sense. In the case of body fat predicted by circumference, I would not worry about a negative intercept, as I intuitively expect the body to almost use up all available fat at some (unhealthy) point. Those people would still have a circumference but close to zero body fat. If I get a significantly positive intercept, I would either expect the model to be applicable to normal or obese people but not to anorexic people."", '>Are they always significant? Yes, if the test statistic exceeds the critical value.\n\nWell, assuming there *is* a critical value to compare against. Preferably one that was determined before you saw the data, because of forking paths et cetera.\n\nAnyway this is true to the extent that ""[statistically] significant"" is a term of art. But it can\'t be overemphasized that the word ""significant"" has other meanings, both plain and technical, which can make it very misleading when used carelessly. For example, the first paragraph of your post would be a little sketchy if taken out of context: the second paragraph is critical to show that we are using ""significant"" in a limited sense. Undoubtedly you know this (that\'s why you wrote the second paragraph!) but I wanted to make it explicit for anyone who might remember this advice in bits and pieces.', 'What kind of response is this?\n\nObviously my whole post was meant to be taken together.', 'It\'s the kind of response that basically agrees with your post and thinks it is worth elaborating upon? If the elaboration is unnecessary then no harm done. \n\nI suspect some readers could overlook the subtext of ""I\'m using the word \'significant\' carefully, and you probably should too,"" when the plain text says ""We can answer this type of question with a simple yes or no. [And I will also answer a different question as a bonus.]"" But this is just a guess. I think you and I are both aware of the statistical discourse around the word ""significant,"" which makes the subtext pretty obvious to us, but would it be obvious to everybody? I can only try to imagine.', 'That wasn\'t my interpretation. It reads to me like a very nitpicking ""response for the sake of having a response"" sort of comment.']"
[Q] Which evaluation procedure for nine variables?,"Hello everyone!

&#x200B;

As part of my master's thesis, I have conducted a quantitative survey. In total, the dataset contains 9 different variables that are to be compared. The variables break down as follows:

&#x200B;

Socialization Tactics:

1. collective vs. individual

2. formal vs. informal

3. sequential vs. random

4. fixed vs. variable

5. serial vs. disjunctive

6. constructive vs. destructive

&#x200B;

Newcomer Adjustment:

7. role clarity

8. social acceptance

9. self-efficacy

&#x200B;

All of the above variables were collected within the questionnaire using a 5-point Likert scale and were rated by the subjects.

&#x200B;

Since I unfortunately have little contact with statistical procedures and their evaluation during my entire studies, I now turn to you. The question now arises for me, which statistical evaluation procedures I have to use. The descriptive procedure is clear to me so far. Current problems I see with the inferential statistics.

&#x200B;

Which procedure(s) do I have to use within the inferential statistics in order to be able to make statements regarding the interaction of the variables?",13cm4bb,Minispider95,1683629422.0,1,1.0,['Have you looked at Item Response Theory (IRT) yet? Have you analysed the validity of questions using Cronbach alpha?']
"[Q] When you toss a coin 100 times, are each toss considered an individual event? So would chance of 3 straight heads be 0.5 x 0.5 x 0.5 ?","So if I toss a coin a million times, would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss? Because I am thinking each toss does not affect future toss. Or am I mistaken?


(Let us assume both heads and tails have equal 50% chance to appear)",13ckogr,Resident-Nerve-6141,1683624676.0,15,0.79,"['The answer to your question in the title is, yes.\n\nThe question posed in the body of your text is a bit confusing. I think you might mean to ask ""what is the probability that my next toss will be heads, given that the previous two were also heads"". As these events are independent, the answer is 0.5. I\'m not sure what tossing the coin a million times has to do with it .', '\\*Assuming a fair coin.\n\nThe chance of throwing 3 straight heads is 0.5x0.5x0.5 before you toss the first coin.\n\nThe chance of throwing an H after already throwing 2 straight heads is 0.5.\n\nThe chance of throwing an H after throwing 1,000,000 straight heads is 0.5. But if you threw 1,000,000 straight heads you would probably not have a fair coin.', 'They are all independent events. However, if you toss the coin 100 times, the probability that a streak of 3 heads will appear *somewhere* in that series is quite high. Much higher than 0.125.', '> are each toss considered an individual event?  \n\nThat\'s for you to say. One could imagine a model in which they are not independent. For example let\'s say your coin is a magnet, and everytime the coin falls on the surface the polarity of the surface changes a little bit depending on the face the coin went up. In that case the throws are not independent anymore, and therefore you can\'t compute the probabilities as if they were independent.  \n\n> Because I am thinking each toss does not affect future toss.   \n\n\nIn that case yes, they are independent.  \n\n>  would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss?   \n\n\nNo. We stated above that the throws are independent. One way of writing independence between the event A and the event B is: P(A|B) = P(A), which reads *the probability of A happening given B occurred is equal to the probability of A happening*. Here, A is the event that the third toss is H, and B is the event that the last two tosses were HH. Therefore it becomes:  \nP(get H | got HH) = P(get H) = 1/2   \n\nThe probability you gave is correct for the event D = get HHH.  \nP(get HHH) = **P(get HH | got H)** * P(get H)  \n= **P(get HH)** * P(get H) *because throws are independent)*  \n= **P(get H | got H) * P(get H)** * P(get H)  \n= P(get H) * P(get H) * P(get H) = (1/2)^3 = 1/8 indeed   \n\nThis is using the decomposition for the intersection of events. P(get HH) means *probability to get H on first and H on second* and is the intersection of the events *get H on first* and *get H on second*. The formula is P(A inter B) = P(A|B) * P(B) ( = P(B|A) * P(A) )  \n  \nHowever, this event is very different from the one you were asking:  \n- what\'s the probability that my next 3 throws are H   \n- I already threw HH, what\'s the probability that my next throw is H?   \n\n> So if I toss a coin a million times, would chance ...    \n\nThis piece of information doesn\'t matter for your question: same here  \nP((A|B) | already threw 1M coins) = P(A|B) = P(A) = 1/2   \n(*note: the notation P((A|B) | C) isn\'t really great, I only wrote it this way for the sake of clarity. It\'s generally preferred to write it P(A | B,C)*)  \n\nBasically, independence implies that each throw is not affected by any other previous outcome of a/the coin\n\nIn general in probabilities, the ""coin toss"" example is given as an analogy for independent events.', '> would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss? \n\nThose first two tosses have already happened, so they are not probabilistic events anymore. \n\nConsider it like this, if you throw three coins in the air at the same time, the probability of all 3 getting heads is .5 x .5 x .5 . Alternatively, if on your first  or second throw you had gotten tails would you have stopped? That would obviously make them *dependent* instead of independent.', '>would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss?\n\nIf you have already tossed two heads the chance of a third is 0.5. The reason the chance of three straight heads is only 0.125 before you tossed the first two is that the chance of tossing the first two heads is 0.25.', 'This is intro probability, not statistics.', 'They are independent. No way for the previous coin toss to affect the next one. Hence, independent events. The probability for a fair coin will always be 0.5 for each toss. \n\nI think you might be trying to bring in the law of large numbers though with the million tosses comment. The process will converge to the 0.5 outcomes given enough rounds.', 'The answer to your question is yes. If you want to get in depth about this, coin flipping follows something called a binomial distribution.', '>So if I toss a coin a million times, would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss? Because I am thinking each toss does not affect future toss. Or am I mistaken?\n\nI used to (and still sometimes) have trouble accepting this fact too. Reality is though is the probability of rolling three straight heads when you ""already got 2"" is 0.5 because really you are just flipping one more coin in a compound event. The events are independent so they don\'t affect the next flip. If your condition is 3 heads in a row and you flipped 2 with success, then your next flip has a .5 probability of meeting your condition.', 'As others are saying, every toss is independent, so the odds of any three tosses all coming up heads are 0.5^(3).\n\nIf you\'ve already tossed two heads, the odds of the third one being heads is now 0.5 -- that\'s the definition of independence. You can also refer to this as the conditional probability of throwing three heads in a row *given that you\'ve already thrown two*.\n\nIf you flip 100 coins, the odds that you\'ll see three heads in a row sometime during those 100 flips is trickier to compute, but it basically comes down to ""it could be the first three flips, or it could be flips 2-4, or it could be flips 3-5, ..."" all the way up to flips 98-100. If any one of those sequence of three comes up heads, then you ""win"", so to speak. What\'s the probability of that happening?\n\nIn probability, ""or"" is usually harder to deal with than ""and"". If you need the probability that two independent events both occur (the ""and""), you just multiply the probabilities. So we can turn this around and say, ""what\'s the probability that flips 1-3 are not all heads, **and** flips 2-4 are not all heads, **and** flips 3-5 are not all heads, ..."".\n\nWe already know that the odds of getting three heads in three flips is 0.5^(3). So the probability of that not happening is 1 - 0.5^(3) = 0.875. So 87.5% of the time, flipping three coins won\'t give you all heads. You need all 98 possible three coin sequences in your flip of 100 coins to also come up negative, and the chances of that are 0.875^(98) = 0.000002 or 0.0002%. That means 1 - 0.000002 = 0.999998 or 99.98% of the time, flipping 100 coins would turn up at least one sequence of three heads.', 'Its not obvious at first, but asking the probability of a single coin toss is different from asking the probability of 3 in a row.  Each coin toss are separate events with probability 0.5. The probability of a single third toss after 2 tosses is still 0.5 because the previous tosses dont affect the next one.  The probability of 3 tosses in a row coming up a certain way is indeed 0.5\\*0.5\\*0.5.  When you already know the results of the first two, there is no longer a chance of what they could be (you already know what they are) and the equation changes to 1\\*1\\*0.5.  So the next toss is always 0.5 chance.  \n\nGenerally speaking though, we dont rewrite the equation with ones, we just dont include them because theyve already happened, and you only consider the 0.5 chance of the toss that hasnt happened yet.', 'Well, we should be careful about models versus reality here. **If** you make certain assumptions, the results are relatively neat, and it comes out just the way you said. **If** you assume tosses are independent, then one concludes that chance of 3 heads in a row is the same wherever you are in long sequence of tosses. \n\nHowever, you have to be careful about what\'s the real, physical system, and what\'s a model of it. When you say, ""each toss does not affect future toss"", that is a modeling assumption; maybe it is a useful assumption and maybe it isn\'t. See the work of Persi Diaconis about the ways in which coin tosses are not really random.', 'To be precise, if you are at position n then the chances of observing HHH in n+1, n+2, n+3 position is 2^(-3). This is true for any n. \n\nHowever, if you have observed two HH in the last two trials, the chances of observing a third head is only 0.5 and not 0.5^3. The reason is that probability of (history) what has already happened is always 1.', 'Another thing that might be of interest is that if you tossed a coin million times, youre almost 100% guaranteed to get a sequence of three straight heads followed by two tails. Thats a different more complicated question because that sequence of events could happen many different ways.', 'No matter what your previous coin tosses were, the chance that your next three will be 3 heads is always 0.5 x 0.5 x 0.5.\n\nSo yes, after having two heads already, you have a chance of 12,5% to get an overall streak of 5 heads from this.', 'Yeah what\'s the name of that one fallacy to do with probability where in roulette it\'s a fallacy to think that the ball is ""due"" to land on black bc of a long string of previous reds, even thought the events are actually independent.  \n  \nEdited', 'Although things can get a bit tricky. If we find that we toss the coin many times and it does not tend towards 0.5, then it may be that the coin is not truly fair or there is some other issue with the way the coin is tossed, such as always starting heads and the flipper having a precise flip leading to a similar number of rotations each time. While the events are still independent, we may be able to infer new information from past flips.', 'If it\'s a million heads in a row, the usage of the word ""coin"" is called in to question!', ""If the coin was thrown like 10\\^300000 times, he'd potentially get a million in a row..."", 'I dont think this is what theyre asking, they use an arbitrary number of coin flips but put a qualifier each time to be that they only care about the next three tosses. Not the occurrence of three straight heads in 1 million tosses or 100 tosses. Because yes, the likelihood of three straight heads at least once in 100 tosses or 1 million tosses is substantially higher than 0.125, but the way the question is stated it is still 0.125.', 'This somewhere probability could be modeled as follows (but not exact):\n\nA binomial with 33 trials and a success probability of .125 and asking the probability of at least one success. The reason its not exact is because youd have to do 33.33333 trials but thats not well defined when you do the binomial cdf.', 'Yeah theres a difference in answering the question how confident am I this is a fair coin 50/50 chance of heads or tails vs assuming its a fair coin what is the probability the next three tosses will each be heads.\n\nBoth are valuable questions to answer but the first is a bit more advanced.', ""Hi, why is the 3rd toss suddenly 0.5 (if you already have two heads in the last 2 toss) when the previous 2 tosses has no effect in the 3rd toss' outcome? Shouldnt it still be 0.5x0.5x0.5 = 0.125?\n\n\nOr am I viewing things wrongly?\n\n\nDoes it mean that before we toss a coin it is 12.5% chance of 3 straight heads, and as we get 1 head then another head, the probability increases from 12.5% to 50%?\n\n\n* while typing this, i think Im understanding it more clearly now. I am confusing two statements. 12.5% is the chance of getting 3 heads in a row before tossing the coin assuming I will toss 3 times. when I get 1 head it becomes 25% chance of getting 3 heads, and when I get 2 heads it becomes 50%. So the odds increases because the data changes. Did I underatand this correctly?"", '>If you need the probability that two **independent events** both occur (the ""and""), you just multiply the probabilities. So we can turn this around and say, ""what\'s the probability that flips 1-3 are not all heads, and flips 2-4 are not all heads, and flips 3-5 are not all heads, ...""\n\nThat doesn\'t quite work because those aren\'t independent. If flips 2-4 aren\'t all heads then either flips 1-3 aren\'t all heads or flips 3-5 aren\'t all heads (or both)', ""> Yeah what's the name of that one fallacy\n\nIt's called the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler's_fallacy)."", 'it could still be a coin, just a coin with a picture of a head on both sides', 'Yeah that makes sense, just thought I could add some color to the conversation.', ""The bigger problem with that is the assumption of 33.3 independent trials, whereas in reality if the first triplet of flips ends in heads it doubles the probability that the next triplet finishes the streak. The second triplet would therefore actually have probability 4/7\\*1/8+2/7\\*1/4+1/7\\*1/2 (since the case where the first group is HHH doesn't count)0.214 of completing the streak. \n\nBut the third triplet would have a different probability, since the dependence between the first two triplets also affects which second triplets count (i.e. we'll need to exclude xxH HHT and xHH Hxx on top of xxx HHH), and so on.\n\nA better way to calculate this is to look at \n    \nf(n)=the probability that the first streak of three heads ends at the nth flip    \n\nand sum to get     \n\nF(n)=the probability there's a streak of three heads in the first n flips (where the specification of *first* streak prevents overcounting)\n\nThen     \nf(1)=f(2)=0    \nf(3)=1/8     \nf(n)=1/16\\*(1-F(n-4)) for 4n100 (1/16 because flip n-3 must be tails or else flip n-1 would end the first streak of three instead)\n\nThen for 7n100,     \nF(n)=F(n-4)+f(n-3)+f(n-2)+f(n-1)+f(n)    \n       =F(n-4)+1/16(4-[F(n-7)+F(n-6)+F(n-5)+F(n-4)])    \n       >F(n-4)+1/16[4-4F(n-4)] by monotonicity of F(n)    \n       =3/4F(n-4)+1/4\n\nSolving the recurrence relation then gives F(n)1-(3/4)^(n/4) for n6. So over the long run run we'll average a probability per triplet of at least 1-(3/4)^(3/4)0.194 instead of 0.125 and F(100) works out to >99.92% instead of 98.83%"", 'Hot hand fallacy is another name for it.', 'lol good thinking', 'What color did you add? Personally a nice seafoam green is always welcome.', 'They\'re not quite the same thing. They sort of complements of each other. Both are dealing with a person observing an outcome happening ""too much"", but then the person is making a different conclusion. With the Gambler\'s Fallacy is the conclusion that the process will switch to the opposite outcome as a ""correction."" With the Hot Hand Fallacy is that the process is on a ""streak"" and will continue getting the same outcome.', 'He makes his own luck.', 'A nice deep evergreen is more statistical to me', 'Good point. Slight difference interesting that no matter which direction you go its a fallacy (which is correct bc the odds dont change in either direction). \n\nThanks!', 'Taking random forest literally.']"
[Q] Can anyone help me understand the stats in this study?,"I keep seeing references to this paper to the effect ""dancing reduces risk of developing dementia by 76%"".  I can't figure out how they got that conclusion from the 'results' tables included in the original study. Can anyone help me understand the stats? Thanks!

Example reference of the 76% figure: https://socialdance.stanford.edu/syllabi/smarter.htm

Original study: https://www.nejm.org/doi/full/10.1056/NEJMoa022252",13cko2b,BillTribble,1683624645.0,2,0.75,"['The pertinent table is [table 2](https://www.nejm.org/doi/full/10.1056/NEJMoa022252), and looks like folks that are statistically naive pulled the 76% as a misinterpretation of the 0.24 hazard ratio. Unfortunately hazard ratios are notoriously misinterpreted like this. First, the study only looks at relationships between behaviours and dementia, and VERY importantly, this study cant assert causality from any relationship it observes; there are many stories consistent with the existence of a relationship, and dancing reducing risk is only one such story. Second, when verbally describing association-with-risk data like this, its always best to describe things in multiple ways. Hazard ratio is more useful for folks that are very familiar with statistical models and the base rate of the particular risk of interest. For most people, its better to talk about the rates of the identified groups, so here it would be that dementia occurs in about 30% (99/339) of people generally, but in only 20% (25/130) of people who dance frequently. One way of describing those numbers together is to say the % has dropped by a third, but I tend to always prefer fully showing the two rates. The 76% is an outright misinterpretation; presumably the misinterpreter thought one could subtract then hazard ratio from one to get a meaningful number, which you cant.\n\nNote also: the data from that study are such that while the one-off best-estimate it can achieve for the association between dancing and dementia risk reflects the reduction mentioned above, given the number of people observed, that reduction is not particularly inconsistent with a reality where theres no true association at all, and our observation of a best-estimate reflecting a reduction is merely a product of random sampling. A Bayesian analysis would provide a proper description of the relative credibility of proposals for what the true association is, and Ill post a result from one when Im at my computer next. But from the frequentist confidence interval shown in the table, the confidence interval for the hazard ratio barely excludes 1. And such barely-excluding-one intervals should be considered particularly non-diagnostic in the context of an exploration of many different associations, as present in this paper. (When you look at many things, you should expect to see at least some that appear to have non-zero associations, even when reality is such that theyre all zero)', 'They get the number from the hazard ratio of 0.24 that was found for dancing. Thats a measure of how often dancers will get dementia compared to non dancers over time. \nHope that helps!', 'Brilliant, thanks so much for this, makes a lot of sense!']"
[Q] How can there be only one log-likelihood value for the OLS.summary() output?,"Imagine that we are doing a linear regression on multiple variables and I use the statsmodels OLS function to do so. When I print the OLS.summary() output, there will only be one log-likelihood value which doesn't make sense to me. This implies that there is only one parameter in the parameter for which we are finding the log likelihood value. If so, what is this parameter?

I ask this because what if we are finding the log likelihood of a normal distribution, then we can get two different values - one for \\mu and one for \\sigma\^2.

Am I missing something here?",13c93ei,travybel,1683589836.0,1,0.67,"[""> This implies that there is only one parameter in the parameter for which we are finding the log likelihood value\n\nIt's is the value of the (log) likelihood function at the estimated coefficients. The likelihood functions accepts (all of) the parameters as inputs, and outputs a single real number.\n\n> I ask this because what if we are finding the log likelihood of a normal distribution, then we can get two different values - one for \\mu and one for \\sigma^2.\n\nNo, the likelihood accepts mu and sigma as arguments, and outputs a single value, which is the value of the joint density at the observed data when the parameters are set to mu and sigma."", ""It's the log-likelihood for the model. Indeed the documentation says as much. \n\n> Am I missing something here?\n\nPresumably the log-likelihood value it offers will be the log-likelihood for both parameters at their ML estimates, rather than some marginal/profile likelihood or conditional likelihood."", ""Ah wait I'm stupid. I'm sorry for asking such a silly question. Thinking of deleting this post but it may help some other people so I'll leave it up.\n\nI confused MLE and Log-Likelihood and thought that we would would want to take the derivative w.r.t each parameter and this would give us a log-likelihood.\n\nThanks!"", 'Youre not stupid. Youre learning!']"
[Q] How to generate synthetic dataset for anomaly detection?,"Hi, I'm implementing an anomaly detection algorithm and I need to generate some synthetic data to test it. My current setup for generation is as follows:


* define a multivariate normal distribution from a random mean  and random covariance matrix
* sample N points from this distribution
* generate anomalies by picking a percentage of the N points and adding k* to these points
* varying k to see how my algorithm behaves for ""bigger"" anomalies

I am not very knowledgeable in statistics so I don't really know if this method of generating anomalies is correct. Do you suggest any improvements on it? Thanks!",13c32iy,Meloon01,1683576254.0,1,1.0,"['What could be correct would depend on what you want power to detect...', ""Which data are you using to train your model? I'm trying to understand why you need synthetic data to test it..."", 'Maybe a T1 distribution for one or more of the marginals?\n\nContaminated normal?\n\nJust my $0.02.', ""I will be using MNIST images with corrupted images as anomalies. It's for a final project and our assignment requires to first test our algorithms on synthetic data."", ""Maybe you can use a synthetic data generator and use your current dataset as input? I believe there are a lot of GAN-based models for this purpose out there.  \nThe ones listed on [https://github.com/Data-Centric-AI-Community/awesome-data-centric-ai](https://github.com/Data-Centric-AI-Community/awesome-data-centric-ai) are mostly focused on structured data, but I'm sure there are similar packages for images."", 'What algorithm are you using exactly, I assume it is a binary prediction with 1 = corrupted, 0 = uncorrupted?']"
"[Q] If I built the Time Series model on the detrended data and forecasts are detrended values themselves, it is possible to get the actual value forecasts (detrend + some previous data equals actual population)?","So, detrended data can be explained as the one that has the moving average removed. For example:

\- Original data (od) = \[y\_1, y\_2, ..., y\_n\]

\- Detrended data = (od - 4 quarted moving average of od) / 4 quarter moving standard deviation of od

So, just trying to understand how to reconcile the forecasted data to get the original data. 

Thanks",13c2v3v,PythonEntusiast,1683575818.0,7,1.0,"['data = detrended-data + fitted-trend    ->   \nforecast = detrended-data-forecast + trend-forecast', 'Would OD = DD * SD + MA get you there?\n\nI dont work with time series often so Im probably missing something. But if thats the formula used couldnt you just solve for OD and plug the observations to get the original data?']"
Where to start with Causal Inference [Q]?,"
Hello, i am an incoming MS student in Statistics. I am looking to self learn causal inference to the point where I can be comfortable using the methodology in practice on real world datasets. My background is a BS in statistics with a mathematics minor up to real analysis, and Ive taken inference, regression and a lot of the fundamental statistical inference courses. I was wondering what some good papers/books would be good starting points to get me up to speed in causal inference and the various methodologies used in it. Thanks.",13c0xv4,Direct-Touch469,1683571714.0,66,0.97,"[""I like this one (free online): https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/\n\nPearl's is good too, as i rubin and imbens book."", ""My favourite book on the subject is The Effect by Huntington-Klein.  Very accessible, great for practitioners, stats undergrads, or maybe graduate students in something like a psychology masters.  If you're looking for a deep dive into causal inference then maybe not the best choice, but if you're looking for a better intuitive understanding, or wondering how to answer some research question you have, then this is a great choice"", 'I am using a combination of the Mixtape book by Scott Cunningham, and The Effect by Nick Huntington-Klein. The former is quite technical and deep, while the latter is good for intuitive understanding.', '[Imbens and Rubin](https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB)\n\n[Morgan and Winship](https://www.cambridge.org/core/books/counterfactuals-and-causal-inference/5CC81E6DF63C5E5A8B88F79D45E1D1B7)\n\n[Angrist and Pischke](https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics)\n\nAll very solid books which should be very accessible given your background.', 'I would be sure to read this 40-year-old article cautioning about errors that can be made using structural equation modeling (applies to  most causal models). Then see how well the modern treatments avoid the pitfalls. \n\nSOME CAUTIONS CONCERNING THE APPLICATION OF CAUSAL MODELING METHODS\nNORMAN CLIFF University of Southern California \nMultivariate Behavioral Research, 1983, 18, 115-126', 'Hijacking the thread: would anyone here recommend the Elements of Causal Inference by scholkopf?', 'Interesting timing. I just bought two books (arrived today) by Pearl.\n\nCausal Inference in Statistics. A primer and The Book Of Why. I have 2 MScs, one of them in stats.\n\nGood luck on your journey.', '[Causal inference for the true and brave](https://matheusfacure.github.io/python-causality-handbook/landing-page.html) is a nice online book with good examples and code.', 'Hansen is the best econometrics book imo', ""It might be useful to know that there's a war going on in the field of causal inference. On one side is Rubin and his potential outcomes framework, and on the other is Pearl and his causal graphs. Rubin and Pearl seem to hate each other, and refuse to acknowledge each other's work.  I think it's important to be aware of this, otherwise you'll be confused by how disconnected these two approaches are.\n\nThere are some attempts to bridge the gap, and I think there are equivalence results showing that the two frameworks contain the same results. I believe both are useful, and that it's good to be familiar with both. \n\nWhich you choose to focus on should probably depend on which field you're in. Economists mainly work in the Rubin framework. I'm not sure which is dominant in other fields, but it should be easy to find out."", 'Following!', 'I like Brady Neals book here https://www.bradyneal.com/causal-inference-course\n\nWhat if by Hernan and the Causal Inf in statistics primer by Pearl is good\n\nHernan What If book is good\n\nFor a very quick and short intro to stuff like G computation though I would recommend the causal inference chapter in the Probabilistic ML by Murphy series, I think its in Part 3.', ""I tend to favor the newer books, such as NHK's The Effect and Casual Inference the Mixtape that have code and examples you can work through. I think it depends on what type of skills you want to acquire and research you might to do, but there has been many large changes to the canonical research designs for casual inference in the last couple of yeara. New and better ways to estimate time varying treatment effects for Difference in Differences (DID), huge problems with statistical power for Regression Discontinuity Designs, and doing away with using propensity scores for matching in some disciplines. From a practical perspective, this means I would favor books that are online, such as The Effect and Casual Inference the Mixtape, that can be updated, are more recent, and have code examples. The amount of change in even the last couple of years is simply extrodinary. If you want a more theoretical perspective, the older books are probably still very useful."", ""The Book of Why by Judea Pearl and Dana Mackenzie is a solid book for building intuition around what causal inference is, how it differs from and incorporates statistics, and explaining statistical ideas you're familiar with in causal terms (e.g. Simpson's paradox).\n\nI'll give you a warning that the authors are particularly brutal towards statisticians for their tendency to avoid speaking in causal terms, and tend to promote their approaches in a way you might find dogmatic.  (If you follow Pearl on social media this will be entirely unsurprising).  But I still think it's a solid introductory book."", 'Start here OP. As a causal inferenceologist I highly recommend this book. Maybe skim Imbens & Rubin as well. Also worth picking up Mostly Harmless Econometrics if you have any inclination at all towards metrics or econ kinda work. Everything else will just be confusing as shit at this stage in your learning.', 'Im going to have to pretty strongly disagree with this recommendation. I dont actually like this book much at all. Its simultaneously extremely verbose, with long winding wordy explanations of everything, but also somehow lacking in necessary technical detail in many areas, where basic mathematical details and derivations of things are left out or unjustified.', 'Gotcha. Yeah I mainly need it to be able to use the methodology. I dont mind some theory, to understand whats going on under the hood, but Im looking for a book which is going to allow me to use it in practice. So Ill check it out. Thanks!', 'The Effect seems to be expensive to buy. Any idea where I can get my hands on a pdf :"") ?', 'Thanks.', ""You can also read Pearl's comments on structural equation modeling.\n\n[https://ftp.cs.ucla.edu/pub/stat\\_ser/r395.pdf](https://ftp.cs.ucla.edu/pub/stat_ser/r395.pdf)"", 'Ill check it out', 'Thanks!', 'Really enjoyed this book, glad to see it recommended.', 'Interesting. I didnt even know that there were two ways to do this. Is causal inference a way to do modeling?', ""FYI Pearl has always engaged with the potential outcomes crowd---communication in the opposite direction has been sparse. For example see [Pearl's blog post from 2012](http://causality.cs.ucla.edu/blog/index.php/2012/12/03/judea-pearl-on-potential-outcomes/)."", 'Kind of depends too if you are doing social science research versus ML / DS.', 'Thanks. So is causal inference mainly an econometrician driven field? Im confused by what you meant by how they are brutal towards statisticians.', 'Hey, thanks for the recommendation. Since you mentioned econometrics, How much of causal inference do you think could fit into the framework of quantitative finance? Im interested in exploring the use of these methods in the space of financial econometrics.', ""There is a web version of it, which is the original before Nick started to work with a publisher. The web is still free, although I don't know if there is any PDF available. Link to the website: https://theeffectbook.net/"", 'Morgan and winship is very good', "">Is causal inference a way to do modeling?\n\nEssentially, yes. It refers to ways of identifying a particular causal effect, i.e., putting restrictions on your model so that you're able to estimate the causal effect of something on something else. The simplest such restriction comes from and randomized experiment, where the causal effect is directly identified by comparing outcomes between a treated and a control group. Other approaches include controlling for all confounding variables, or using policy variation."", ""Imbens and Rubin [commented](https://academic.oup.com/biomet/article-abstract/82/4/694/251822?redirectedFrom=fulltext) on Pearl's 1995 paper. More recently, Imbens has discussed [causal graphs from an econometrics perspective](https://www.aeaweb.org/articles?id=10.1257/jel.20191597).\n\nI also think Imbens commented on Pearl's blog at some point. \n\nFrom what I've seen, both Pearl and Rubin are extremely arrogant and very condescending towards each other's work. Imbens seems more balanced."", 'Agree completely. Im coming from a social science research perspective, so my advice (which is certainly subjective) would probably not apply for ML/DS.', '""Causal inference"" is driven by a mix of computer scientists, economists, philosophers, and some statisticians.  AFAIK there aren\'t any causal inference departments in the same way that there are tenured professors in the fields that have worked on causal inference problems.  Economists have done a lot of important work developing techniques because the field has a ton of major research questions that are impossible to address using a randomized controlled experiment.\n\nWhen I say ""brutal towards statisticians"", I mean how statisticians have historically talked in terms of correlation and not being able to draw causal conclusions even though drawing a causal conclusion is the point of the research they contribute to.  For example, statisticians talking in terms of association between smoking and lung cancer while being very hesitant to say ""smoking causes lung cancer.""  Pearl also insists that statisticians should be using DAGs to explore causal relationships and that a lot of important causal findings (e.g. smoking and lung cancer) would have been realized much sooner had they been used.', ""You're welcome. And to be honest, the resources recommended here so far won't come in helpful at all for quantitative finance work. You need a completely different framework for that. You might want to read up on Granger causality and maybe to bias your reading towards the more econometric-focused books. So start with Mostly Harmless Econometrics by Angrist & Pischke, then move on to more rigorous econometrics books and more specialized fin econometrics books."", "">How much of causal inference do you think could fit into the framework of quantitative finance?\n\nAs I'm sure you know, prediction does not require modeling casuality. Yes, you might find ways of using causal modeling profitability (for example, you use a macroeconomic model to run what-if scenarios of different policies), but you can predict asset returns without it."", 'Thank you ', 'So then which is the best way? How should I approach learning it? Learn both? I guess thats the best way right?', 'Ah I see. Yeah I asked this because I see causal inference a lot in biostats departments.', 'Agreed. I actually wanted to look at applications to quant finance specifically since Ive heard lately firms have been trying to incorporate causality in determine trading signals. This is something new, and an approach firms have started taking, and since I want to get into that space Ive been trying to understand the methodology.', ""Given that you're in statistics, I'd probably focus on the Rubin framework to begin with - Rubin is a statistician, while Pearl is from computer science, so I think the Rubin model is a bit more popular in statistics. But it's certainly a good idea to be aware of the Pearl model too.\n\nPersonally, I find the Rubin model more useful, but the Pearl model is perhaps more pedagogical."", ""I'd look at the problem a bit more generally. Given that asset prices are extremely noisy, prior knowledge from theory or your own intuition can be helpful if imposed on your predictions."", 'I found the Pearl model a lot easier since its more algorithmic and my background is biostats. \n\nThe Hernan and Robins What If book does use DAGs too though.', ""I was taught the Rubin model, so I still find it easier to work with. Some research designs also seem to fit better into the Rubin model.\n\nBut DAGs are great too! It's really unfortunate that these approaches got caught up in a fight between two big egos, but there's a lot of promising work on building bridges by the younger generation.""]"
"[Q] What test to use? Ordinal DV, multiple IVs","Hello!

I need to find out which of my IVs (if any) have an effect on the dependent variable. The dependent is ordinal, it's measured on a 10-point rating scale, and doesn't follow normal distribution. My three independent variables are ordinal and categorical (binary). At first I though I should use ordered logit regression but since it requires normal distribution it doesn't seem to fit with my needs.

What test could do what I need it to do? Or should I choose a test that only allows for one IV, test all my three IVs separately and see if any of the results are significant? I'm using SPSS, if that matters.

&#x200B;

Edit. thanks for the responses! I think I read too much about different tests and their requirements and ended mixing things up in my head lol",13bxy9p,pilvihaituva,1683565264.0,2,0.67,"[""Ordered Logit (aka Proportional Odds) seem like a good fit here. And no, it does not require your data to be normally distributed (even conditionally). Maybe you've read something about a latent normal distribution ? But that's not an assumption per se, more like a modeling choice. You're simply assuming that your likert scale captures discrete values from an underlying (latent, unobserved) continuous opinion/ability/etc distribution accross your population. Take a look at [this paper](https://psyarxiv.com/x8swp/) for a good explanation of the various types of ordinal models and how they model that latent scale from the observed ordinal data.\n\nIts hardest assumption to meet is its namesake, aka the proportional odds assumption."", 'Im not exactly sure what you mean by ordered logistic regression, but unless its something weird, I dont see why there would be a normality assumption.', ""> The dependent is ordinal, it's measured on a 10-point rating scale, and doesn't follow normal distribution\n\nIf it's ordinal, it cannot be normal (which is continuous,  necessarily numeric/interval). So the first 4 words cover it.\n\n> What test to use? Ordinal DV, multiple IVs\n\nStart with what model:\n\nOrdinal logit models, ordinal probit models, etc...\n\nAs for what test, it depends what you're testing, exactly, but the usual tests for glms apply.\n\n> At first I though I should use ordered logit regression but since it requires normal distribution\n\nNo, that's not the case. \n\nWhat exactly led you to think that? Did you read it in some book or paper? If so, which one, and what did it say, exactly?\n\nThere's some assumptions about how the levels of the ordinal variable relate to each other, depending on which ordered model you use, but it's not critical that those hold exactly."", 'The key question is how much the residuals deviate from normality. Dont do significance tests to test whether they are exactly normal. You basically have an ANOVA design, and ANOVA is robust to non-normality in many situations. There are other issues with ordinal data but in many practical applications they are not serious.']"
Statistical Learning as a Research Area [Q],"I have seen this topic, statistical learning, show up as a research area across a broad range of departments, even outside of statistics. For example, I see the standard statistics departments with a few faculty doing work in statistical learning (my dept has folks working on Bayesian regression trees). However, Ive noticed that this field also shows up in CS departments (statistical learning theory) as a research area. My question for anyone here who is a statistician who researches statistical learning, how do both of these departments approach research within the area of statistical learning? 

For example, in a statistics departments, is statistical learning research essentially a spin-off of nonparametrics? Where they work on developing new statistical learning methods with theoretical properties of these existing estimators? Ie. The advancements of ensemble learning, do people just try and find new ensemble methods? 

Are CS departments using statistical learning as a computational problem? Ie. How can we make x method faster, a more efficient algorithm for y etc. how can we make ensemble methods faster?


This would help me decide if I ever did a phd which departments statistical learning is worth researching in. A statistics department or a CS department. If anyone can give some example of questions people may pose in statistical learning that would give me some examples between how the two departments think.",13bvari,AdFew4357,1683559481.0,6,0.8,"[""Don't base your PhD decision on a Reddit post. Talk to people in the department."", ""In adjacent fields like CS, OR, IS, I've seen both developing new statistical learning methods, and attempted applications to existing methods in industry domains, and often a blend of both. For a more specific look at pushing statistical learning boundaries, one place to start might be Mike Jordan, who is joint faculty between CS and Stats, his publication history is heavily skewed to nonparametrics. [https://people.eecs.berkeley.edu/\\~jordan/publications.html](https://people.eecs.berkeley.edu/~jordan/publications.html) ; he also has a great AMA from a while back that gives insight into how he approaches stats in CS: [https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama\\_michael\\_i\\_jordan/](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)"", 'Thats true. I guess what I really wanted to know is what type of research is in statistical learning, which I could ask profs about too', 'Ah okay. Ill check this out, thanks! I really havent ever been able to pinpoint what stat learning research really is. Its very cool, and introduction to stat learning made me want to learn more. I am reading elements right now, and was always interested in knowing what exactly research within this space is. Ill read some of these. Thanks']"
[Q] Stupid question about M&Ms.,"This genuinely happened, and I want to know what are the odds that it happened because it struck me but I don't know anything about probabilty beyond, maybe, one die. Two dice? I'm putting my money on boxcars at the casino because it pays better. That's my level of understanding, so Dunning Kruger is at the wheel. Right?

This is what happened: My wife bought a four pound jar of pastel M&M's on sale after Easter. 5 colors, let's assume equal amounts, random distribution in the jar.

I reach in, grab a handful, and put them on my desk to eat while I work. As I did, I noticed I had taken 10 M&Ms, and exactly two of each color. Weird.

I could have grabbed more or less, but let's say less than seven and I would have taken more, and more than 13, I would have dropped some back. Random chance, between seven and 13. I picked 10 and I got two of each color, there being five colors in the jar. (If it affects the math, the jar was half full, so about 2 lbs, which is about 800 M&Ms. If that even matters.)

What are the odds against me doing that again with another random handful? Did I hit a lottery-level freak occurence, or was it like rolling boxcars in a craps game - unusual but not exceptional?",13bva41,WriterofWords2021,1683559436.0,9,0.8,"['Since things happen all the time, unusual things happen all the time. Trouble is we only pay attention to them if they are meaningful to us. So ""after timing"" probabilities like this and marveling at the tiny numbers doesn\'t really take this into account.', 'Let\'s call the types (colors) A B C D E and assume the glass has 20% each, and it\'s so large that drawing 10 doesn\'t change the contents notably. We draw 10. The chance that one M&M is ""A"" or ""B"" is 2/5, the chance that all 10 are is (2/5)^(10). We don\'t need A and B in particular, however, any combination of two colors will do, and there are 10 of them. If we multiply 10\\*(2/5)^(10) we over-count cases with only a single color, however: If all are ""A"" then it\'s in ""A and B"", ""A and C"" and so on, i.e. we count it 4 times. We need to subtract 3 times the chance to get only a single color, or 3\\*5\\*(1/5)^(10), which means our total chance is 10\\*(2/5)^(10) - 3\\*5\\*(1/5)^(10) = 0.0010 = 0.1% or 1 in 1000.\n\nDrawing 7 increases the chance to 1.6% or about 1 in 60, with 8 it\'s still 0.65%.\n\nIf you first randomly select a number of M&Ms to draw then we should average all the cases (7, 8, ...), which leads to an overall probability of ~0.4% or 1 in 250.\n\nNot that common, but also nothing super unlikely, and you only noticed it because it happened. Our daily life is full of things that can show in 1000 coincidences - we notice these, but not the 999 times more common occurrences where they don\'t happen.', 'The probabilities in this case are given by the multinomial distribution.', ""I'm not great at statistics, but my job sucks and I'd rather do this.\n\nI'll just assume you are grabbing 10 M&Ms\n\nTo make things easier, let's first figure out the probability of choosing 1 M&M and it being red or green. This would be:\n\n(# of M&Ms that are red or green) \n(# of M&Ms that are in the jar)\n\nBut for simplicity, lets just assume there are an equal amount of each color in the jar. Notice as well that everytime you choose an M&M, the probability would change. However, since there are so many, the probably won't change much, so I'll assume it doesn't change. With these assumptions, the probability of choosing an M&M that is red or green is 2/5.\n\nThat means the probability of choosing an M&M that is red or green 10 times will be (2/5)^10 .\n\nCoincidentally, there are 10 different ways to choose 2 colors from a little of 5 colors. So to generalize from thinking about red and green to any two arbitrary colors, we just multiply the probability by 10:\n\n 10 * (2/5)^10 = 0.001 or about .1%\n\nSo not terribly unlucky, definitely not as unlucky as the lottery. To be honest if the ratios of the amount of each color is off, the probability could be much higher."", ""For a given number of M&Ms drawn, the colors follow the multivariate hypergeometric distribution (the multinomial distribution mentioned elsewhere in the thread is the approximation for an infinite jar), so when you grab 10 the odds of getting 2 of each is (160 choose 2)^5 /(800 choose 10)1.2%. Assuming a 1 in 7 chance that a handful has 10, the overall probability of getting 2 of each is then 1 in 588.\n\nSince you're about 80 handfuls in there's about a 1 in 8 chance this would've happened by now"", 'High school AP stats courses do a simulation similar to this. The process, though, is dependent on an uneven distribution of colors. Theres some research about regular M&Ms *not* being evenly distributed. (So if there are 1000 candies they dont load with (or even produce) 20% or each of five colors.)  \n\nSo doing the distribution of whats the chance of getting 2 of each in 10 dependent draws is harder than if they were equal. \n\nIFF they were equal there are 160 candies of each color. To get one of one color is 160/800 then another would be 159/799. Then to get another color would be 160/798 and then 159/797. The denominator keeps shrinking but the tops are similar. So you have five colors and two of each. When you have these types of problems the values for each option you multiply each value. \n\nNumerator is (160^5 x 159^5)\nDenominator is 800 x 799 x 798 x  x 791). \n\nMy calculator says p=0.00000010500549 or 1.05 x 10^(-7). \n\nIts rare under these conditions. And this doesnt account for whats the chance I got 10 in my handful. This is just *given* 10 in the handful. If you have a randomly-equal chance to get anywhere from 7-13 M&Ms in a draw, one might argue that getting 10 is an addition 1/7 chance. So thats makes it 1.5 x 10^(-8). \n\nBut the other guy said it doesnt matter. Itll happen eventually. And there are so many assumptions that my math doesnt matter', 'My novice statistical knowledge would say that if the bag has equal numbers of each color, central limit theorem says this is going to be the most common outcome over time']"
"[Q] Paired T test, data shows difference in mean but p value says it is not significant.","Hello everyone sorry if this is a beginner question but I keep confusing myself. So my data is as follows

||Muscle Recovery Time (mins)|Muscle Recovery Time (mins)||
|:-|:-|:-|:-|
|Participants |Weight|Without Weight|% Change|
|1|6|3|0.50|
|2|4|3|0.25|
|3|4|3|0.25|

Running a paired t test of weight and without weight I get a p value of 0.13 which is > 0.05 and signifies that the mean difference is not significantly more than 0. The average % change from without weight to weight is 36%. I feel as if with the % change the p value should support the change. Could it be because of the small sample size or am I doing something wrong? Any help or clarification would be greatly appreciated.",13bgqvv,MidC1,1683528857.0,19,0.83,"['Consider the following scenario:  \n\n\nYou have a class of students with an equal number of boys and girls. You want to find out whether generally speaking, boys are taller than girls in the population (remember, when doing inferential statistics, we are always interested in population-level differences, based on the sample differences). You take a single boy and a single girl, and measure their height. The boy measures 180 centimetres, and the girl measures 150. There is a 30 centimetre difference. Can you conclude that boys are taller in the population? No, because you only have a single subject from both groups, and therefore a lot of uncertainty. It is perfectly possible that you just happened to randomly pick a taller-than-average boy and a shorter-than-average girl, and in fact, if you were to measure all boys and girls, girls would be taller.   \n\n\nThis is reflected in the formula for the paired-sample t-test: The standard error in the denominator contains the square root of N in its own denominator. The larger N (sample size) is, the smaller the standard error, and consequently, the larger the t-statistic. Your result simply indicates that based on the available evidence, you cannot say that in the population, there is a difference between the two means.', ""This is literally why you do this test so that you don't fall into the trap of considering nominal differences meaningful when they are outweighed by the uncertainty.\n\nNow you come to us and ask for ways to yet fall into the trap.\n\nYour differences are large but your number of participants tiny. I'm not sure if I should interpret 3 or 6 total, in any case a tiny amount, to the point that you probably cannot even use a t-test there."", 'You could also calculate effect size to report how large (or small) the difference is.', 'Why is a p value < 0.05 significant to you specifically?\n\nWhy an alpha of 95% instead of 99 or 85?\n\nOther commentators have already brought up the the importance of considering sample size and effect size. It would also be worth looking into p-values and the how/why for selecting your cutoff point. \n\nThe fact that we use 95%  for almost everything is something Fisher is probably rolling in his grave about. Especially since he pulled 95% out his a** and specially asked future statisticians to select their own numbers and not just blindly follow that example.', ""3 pairs is so tiny you need a *huge* effect to pick up a difference - that is to get something big enough that chance variation from what you'd expect under H0 could not explain it.\n\nConsider a coin tossing experiment, where you're testing a null of a fair coin against a general unfair alternative.\n If I  toss the coin  three times and get 100% tails, that might sound impressive but it's not. I could not reasonably reject H0. It could easily happen that I get 100% heads or 100% tails with only 3 tosses. But with 100 tosses, even 61% tails would be a reasonable indication that the coin was not fair. At 300 tosses, 57% tails would do (for a 5% significance level). \n\nTiny samples make it really hard to find anything"", 'This is a good explanation.  The only way hypothesis tests make sense is to remember that they are tests on the population given the data that\'s been sampled from the population.\n\nIt\'s unfortunate that sometimes we say in shorthand, ""real difference"" or similar things.', 'Thank you for the clarification it helped a lot', 'I think you mean an unpaired t test.', 'Also I\'m not convinced this recordkeeping is granular enough.\n\nIf the difference is like 1-2 minutes apart, probably we should have measurements more accurate than ""how many minutes does it take to recover.""', 'I know this is meant to be colloquial, but describing the results of a hypothesis test as identifying a ""meaningful"" difference, as opposed to a ""nominal"" difference, probably just adds to the confusion.', 'Thank you for the help', '> you probably cannot even use a t-test there.\n\nThere is no minimum sample size for t-tests?', ""This is good advice. The point estimate for Cohen's *d* for the shared data is greater than 1.  (Probably c. 1.4). This suggests a relatively large effect.  (The mean difference is 1.4 times the standard deviation of the differences.)  However, with only three observations, there's not good evidence that this measure in the population is greater than 0.  It's helpful to report the confidence interval for the effect size statistic."", 'Thank you for the help', 'They already did!\n\n>""The average % change from without weight to weight is 36%""\n\nI\'m assuming though you mean a standardised effect size that takes the distribution into consideration e.g. Cohen\'s d.', 'My example indeed shows an independent samples situation, because my experience is that it is easier to build intuition with, but the formula works out the same way with n in the denominator.', ""I wanted to do one thing after another there.\n\nIt can be statistically significant without being meaningful in size, but you cannot be sure it is meaningful in size if you don't even know with reasonable confidence that it exists at all."", 'You need at least 2 measurements per group or a min N = 4. Any less and you cannot calculate a standard deviation for each group, and therefore cannot calculate a meaningful t value. Clearly though this number is still not sufficient - realistically a minimum of 3/group is needed, and even then will be insufficient for the most part unless there is almost no variance amongst them.', 'If the data is normal, then no, but you practically never know that your data is normal', ""I agree. I didn't want the OP to think his case was a paired situation.""]"
[Q] MaxDiff Analysis Packages in R?,"I'm trying to run a max diff analysis in RStudio for an assignment but I have never done this before so I'm struggling. Does anyone know of any packages I can use (that have detailed documentation preferably)? I was trying to use flipMaxDiff but it seems like it's no longer public. Or alternatively, is there a way I can do it without a package? Thanks for any help!",13b5n93,Confident-Narwhal557,1683499734.0,13,0.85,"[""I think you don't need to use a package as per https://docs.displayr.com/wiki/MaxDiff_Analysis_Case_Study_Using_R"", ""I attempted to use it but I get an error that flipMaxDiff is not installed, so it's still dependent on a package. Apparently it used to be available in R but is not anymore? So now I'm trying to install it from here ([https://rdrr.io/github/erikerhardt/flipMaxDiff/src/R/max-diff.R](https://rdrr.io/github/erikerhardt/flipMaxDiff/src/R/max-diff.R)) but I'm messing up setting up my git PAT because I keep getting the error that I have exceeded my rate limit. I'll probably just try again tomorrow.""]"
[Q] Perfect Collinearity,"Hi everyone,

I am coming across some difficulties for a regression model. I am deriving a variable from income by making a few manipulations to it. However, when I run a multiple linear regression and include income as a control, I see that there is perfect collinearity between income and the derived variable.

Now I understand that I can drop one of the two variables from the model. But the issue is that I see other papers including income as a control, and I dont understand how other researchers are not coming across this same issue.

Can anyone tell me if it is possible to include a variable derived from income and then include income as a control in a model?

Thank you",13b42a0,conu905,1683496196.0,10,0.92,"['> I see that there is perfect collinearity between income and the derived variable.\n\nIs this other variable just a linear transformation of income? If so, why include it?', 'The important information about perfect collinearity is that all the information in one is fully in the other. The transformation you performed was non-informative.  You have to drop one. \n\nUnless there is a strong theoretically sound reason for the transformation, I would keep the raw data. \n\nThe transformation you performed will impact the location of the intercept but nothing else.', 'Stepping back from the algebra, consider that the model you are trying to fit is DOA, conceptually; it could only generate nonsense even if it were possible to fit it to data.\n\nA model that ""controls for income"" aims to understand the implications of varying the income-derived quantity while holding income itself fixed - i.e. to compare the expected outcome for two hypothetical individuals who differ on the derived variable but have the same income.\n\nBut notice: no such pair of individuals can exist, even in theory. If they differ on the derived variable, they must differ on income.\n\nIn short, it is nonsense to speak of the \'effect\' of the income-derived variable \'controlling for income.\'\n\nHow is it possible, then, that others have successfully fit the sort of model you are trying to construct? What may be happening is that they are using income in two different ways in concept, and generating non-collinear variables that match that concept.\n\nThey may, for example, generate the derived variable, and then separately generate a discrete analogue of income (e.g. classifying individuals into categories such as \'low income,\' \'middle income,\' and so on). They could then use these categories as proxies for socioeconomic status or social mobility in general, but call it \'income,\' and talk about how their models \'control for income.\'\n\nIt would then be possible to fit a multiple linear regression model with these two different versions of income as independent variables. But how to interpret the associated regression parameters, and whether the model that includes both even makes sense, would still be important considerations.', 'Seems like you miscalculated the variable or did something wrong, Id be very careful. If you summed up some differences with logical rules, this seems nonlinear, ie, not a linear transformation of income, and i would be very suspicious of perfect multicollinearity', 'can you describe what you did to the variable?', 'The other variable is the sum of difference between a persons income and all those with a higher income than that person. \n\nThe reason to include it is because this derived variable is shown to have more of an impact on the outcome than just income itself. But due to such a high correlation between the two variables, it seems that I am measuring the same phenomenon and there is no difference between them.', 'Thank you that makes sense!', '>The other variable is the sum of difference between a persons income and all those with a higher income than that person.  \n>  \n>The reason to include it is because this derived variable is shown to have more of an impact on the outcome than just income itself. But due to such a high correlation between the two variables, it seems that I am measuring the same phenomenon and there is no difference between them.\n\nAbove is the same response that I provided, which describes what I did.', ""Choose one or the other. The derived variable didn't add any information. Tomato tomato"", ""That shouldn't be collinear with income. When you increase income by one that should increase by the number of people with greater income, so the slope should decrease from N to 0 as you pass people""]"
[Q] Surveying customers. Never get to n = <30. Can statistical relevance still be found?,"So, lets say I have 100 customers come to an event and I send a survey asking them a series of questions, but only 20 respond. Can I still infer statistically sound meaning from the responses? For example, 18/20 respondents rank experience A 10/10 but only 13/20 of the same respondents rank experience B 10/10. I would think the deviation between the two is meaningful and that experience B should be evaluated and changes made. Is that true? And what would be the language I would use to explain the significance?",13anj5y,dronesandwhisky,1683465623.0,0,0.43,"['With a response rate of only 20%, I would worry about non-response bias more than a small sample size.', ""You can only use what you have to use.\n\nAs, mentioned, just be cautious that you may have response bias.  In the example you mention, it doesn't sound like there would be an obvious response bias, but these things can be tricky.  For example, in American politics, you might suspect that an imbalanced sample for age or gender might bias the party preference results, but you might not think that there would be difference in political preference for those who would or would not answer the phone in the first place.\n\nBut also in a lot of situations, a 20% response rate isn't bad.  \n\nBTW, the number 30 doesn't have any special meaning."", 'Suppose your question is ""do you ignore online surveys?"". \n\nYou send it to 100 people. 20 (or 30, 50, whtever) return it awnsering \'No\'. \n\nYou conclude that 100% awnser online surveys.\n\nDo you see the problem?']"
[Q] robust TOST in jamovi,"Hi there!
I am trying to calculate a robust TOST (Wilcoxon TOST) in jamovi, however I cannot find a way to do so. It is supposed to be included in the TOSTER add-on (as wilcox_TOST function), but I cannot find it there - I already clicked trough all options. Am I maybe just to stupid to find it?",13anhrl,slania705,1683465525.0,2,0.75,[]
[Q] What test to use for a single participant study?,Say you wanted to determine whether giving someone alcohol has a causation of making them dance with a lampshade on their head (the qualitative data would be how many times they dance vs how many times they don't). What variables would you need to consider and how would you analyze the sample (or would it count as a population)? Would I include data from times they're not drunk?,13aibp6,nanistani,1683450607.0,3,0.72,"[""Sounds like a time series to me, and of course you would have to use data from when they're not drunk, other wise you have no variation in your independent variable."", 'With a single participant, your inference would be to that individual. sampling that individual in any valid way would be very challenging', 'You can consider two groups of people (group 1: drank alcohol vs group 2: did not drink alcohol). The alcohol brand and content must be the same for everyone in group 1. Expose both groups to some music for a fixed people of time and observe whether they dance or not (or number of times that dance). Use chi-square test if you look at binary dance or did not dance, or use wilcoxon rank sum test if you look at the no of times they danced.', 'You test that individual on multiple days, randomly making them drunk or not.', 'OP said one participant', 'What does single participant mean?', 'It means there is only 1 group of people with 1 person in it', ""I see. Then OP wouldn't need any sort of inferential statistics. Just a descriptive one would do ie counts.""]"
"[Q] What is better? To do 2 studies of N = 1000, or 1 study of N = 2000?","I recently read a study where scientists looked at 1000 people's brains and found some effect (brain activity locations predicted psychopathological symptoms). They then ""replicated"" the findings by looking at another 1000 people. This was not their data, but publically available brain data. 

Outside of the issues with fMRI, I'm wondering if there is a reason why it would be better to do one study of 1000 brains, then replicate it with 1000 other brains, vs just finding the effect with the first 1000 sample, and then adding the second sample to the first and re-running the analysis with all 2000 people. 

Thanks.",13abpg3,bennettsaucyman,1683430867.0,58,1.0,"[""If they're both from the same population, then you have exactly the same amount of information in both cases."", 'Did they use the first 1000 brains to explore the data and identify possible effects, and the second 1000 brains to test whether the effects replicate on the new sample? If so, this is good science. The effects found in the first study are not very convincing since the researchers were effectively predicting the past (not difficult). The second study gives them a chance to try to predict the future (very difficult). This is much more convincing and suggests that they really know something about the brain.', 'There was a debate about this on the gelman Blog a while ago: https://statmodeling.stat.columbia.edu/2017/04/23/prefer-three-n300-studies-one-n900-study/', 'In this context, it would make sense if the first 1000 were used for *exploration* and the second 1000 were used for *confirmation*.  \n\nWhen it comes to fMRI, there are a variety of variables that they may have tweaked during the first study to ""find an effect"", e.g. to find p-values that survive some correction for multiple comparisons.  \n\n---\n\nIf you are asking about future studies where you have *a priori* hypotheses, in that context, it would make more sense to do the larger sample because you\'d have more accurate estimates since you have more people.\n\nHowever, it might make even more sense (insofar as the results would be more compelling) to run multiple studies with slight variations to address the research question from slightly different approaches in order to ""triangulate"" the answers. The idea would be a series of several studies that converge, which is called [consilience](https://en.wikipedia.org/wiki/Consilience).', 'I\'m no statistician but my first thought is (assuming the 2 studies were done on the dame type of people , which you probably can never guarantee) how far apart  in time the studies wee done /how old anf far apart is the data? If the samples are super close proximity-wise in time  (and assuming the population composition is the same) then I\'d say ""whats the difference?""', 'This exact question has already been studied in the clinical literature. See the [link](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2015).\n\nIf everything is equal, and the two studies are randomly drawn from the same population, then one larger study provides more power (more precise estimates) over two smaller studies. You don\'t get more information per se, because you have exactly as many total subjects, but you have a more efficient use of that information in the larger study. If however there are two populations, then it is invalid to use the ""one larger study"" approach.', 'Looking at these samples independently controls for confounding variables in some ways. Contrasting the first dataset with the second could confirm the existence of an effect despite the fact that the data was generated by researchers who perceive the same hypothesis in different ways, for example. Theres also value in a researcher pointing out that, hey, they looked at the same thing we are from a different angle and came to the same conclusion.', 'Two studies where one is a ""replica"" of the other. I say ""replica"" because the idea would be that the results should match / be self-consistent, and if not then there would be some confounding factor(s) to consider which can lead to a further investigation.', 'Replications add to our surety that the effect you hypothesise happens in nature under similar but not exact circumstances.\n\nLarge N studies on the other hand are troublesome because the law of large numbers means that the larger your sample size, the *easier* it will be to find that effect just by the nature of there being a large sample. I think thats why power+effect size and ideal N calculation is done before a study because they are looking for the *minimum required N*. But there is not a lot of laymans talk about the *maximum*. To me, that indicates that replications should be the preferred method of scientific surety (which can be bolstered by meta-analyses later on)', 'Two studies where one is a replication of the other.', '>Outside of the issues with fMRI\n\nThis is probably the key part of the study methods - to attempt to detect whether the effect is replicable given the site-specific effects of fMRI. There are so many choices to be made in experimental setup, data preprocessing, stats, etc. If the effect can be detected in a totally external dataset, that gives much better credence that it is a general finding about the brain itself, rather than the brain *in their specific environment*. Usually the last few lines of almost any neuroimaging paper are ""further studies are required...""', 'If the data are sampled differently then you reduce the chances that your findings may be influenced by sampling bias. That seems to be the case in fMRI example. Otherwise two studies dont add much (imagine collecting 2000 observations and splitting the data in two: we wouldnt consider this a replication)', ""I think from entirely practical / human-element reasons I'd prefer two studies. It's like adding robustness to the results, and averaging the experimental biases. Every experimental design will make some tradeoffs, make mistakes, implement things differently in practice. I think it's valuable to have multiple attempts at answering the same question."", 'Its essentially the same, but being able to break down a large study into subsamples and have them pass would help you to validate those claims arnt skewed due to a large section of a special case.', ""Interesting study! Replication is an essential part of scientific research, and I'm glad to see that the scientists in this study took the time to replicate their findings. As for your question, there are a few potential reasons why the researchers chose to conduct two separate studies with different samples rather than combining the samples into one larger study. One possibility is that the researchers wanted to ensure that the effect they observed was not specific to the first sample they studied. By replicating the findings in a completely separate sample, they could confirm that the effect was robust and not simply a fluke of the first sample. Additionally, by using publicly available data for the second sample, the researchers were able to avoid the potential bias that can arise from collecting their own data. Overall, I think the decision to conduct two separate studies was likely a deliberate one, aimed at ensuring the validity and generalizability of the findings."", 'From a pure stats perspective they are the same in terms of statistical information.\n\nBut from a logistics perspective, a followup experiment gives an opportunity to polish up the mistakes from the first trial and give better controls or instructions. There is also a new parameter (time and possibly location) that separates the sampling populations. Even better would be a entirely second group of researchers rerunning the original study.', ""depends on your goal? you can have a training vs test cohort then you'd need 2 cohorts."", 'Well, hypothetically you get *more info* as you can reproduce \n\nThis assumes that the n=1000 gives proper power, the sample is constructed well etc.', 'No, this ignores the Garden of Forking Paths/Experimenter Degrees of Freedom. Choices have to be made by the researcher in data processing and analysis. These are often (unwittingly) conditioned on the results they generate, which undermines the severity of the hypothesis test(s). If the authors developed their processing and analysis workflow and its code implementation on the first dataset, and pre registered it and applied it to the second, the second confirmatory analysis has higher evidential weight. Lets not pretend that the replication crisis didnt happen or that there are not things to be learned from it. \n\nThere is a strong analogy with training and testing datasets in machine learning. No one would take you seriously if you argued that results found purely in training datasets were valid, you always need strictly constrained confirmatory analyses/out of sample predictions.', 'Non replication doesnt have to be due to confounds or moderators, it can simply be error variance, ie false positives and false negatives do occur (ie alpha and power).', ""If they're both from the same population and they're both random samples, then the split between them is arbitrary. You don't gain any additional information by splitting your 2k sample into two 1k samples.\n\nMy hunch is that the two studies' data are not from the same population due to either instrumentation or sample selection differences. Then maybe it simply wasn't possible to combine datasets. I've seen things like that happen before.\n\nOh! I also just realized: if they can't guarantee independence of the two datasets (eg one patient is in both) then they should analyze them separately."", '> Well, hypothetically you get more info\n\nI think hypothetically you get the exact same amount of information since the second derivative of the log-likelihood, i.e., the Fisher information, is additive.\n\nMaybe in practice you get more info, but not hypothetically.', 'Statistics do change when doing two separate studies. Even if they use the same template/baseline.\n\nEdit: I prefer many separate groups on different days, etc...', 'Surprisingly, this isnt the case (see my post down in this thread). Lets assume that either two smaller studies or one larger study are designed the same way, analyzed the same way, have the same analysis plan and randomly sample from the same population. Lets not consider scenarios where experimenter degrees of freedom or sampling the same individuals in both studies, or other deviations from the ideal scenario are present. \n\nYou will find that, in the presence of a true treatment effect, the larger study yields more power than pooling the two smaller studies. You may easily confirm this, as I have, with a simple simulation. \n\nYou claim that knowledge of replication is important, but I think its value is overstated here. If you find a precise estimate from a large study, then you can be fairly confident that you found a true effect. Did the results conflict because the relative variance was too large, or did you simply have one false negative? The conclusion is very different depending on the true data generating mechanism. Do you then run a third trial and potentially waste more resources? Its better to have a single, larger study that can provide a more precise estimate.', ""True. I just gave an answer from the top of my head. I normally don't engage with statistical tests, but rather statistical modeling so I sometimes miss these things."", 'Im just speaking from a pure replication standpoint in general, should preface  that. \n\nIf we have reason to believe that n =1000 or n= 2000 is sufficient and we can create two n= 1000 iid datasets and do our due diligence with respect to experimental design, then we gain more then we would with respect to a single study.  \n\nTheres a lot of prior work here. And Ill admit Im trying to answer to the title question in a very general sense. Im also assuming were following some frequentist or NP paradigm here.', 'No, not if the experimenter degrees of freedom were constrained between the original and replication. One large p hacked sample is not as informative as one smaller confirmatory preregistered analysis.', 'Where does the additional information about the data generating process come from when you split your iid sample in two?\n\n(P.S. you could preregister either study...)', 'I think I may have led us down a path away from the ops question. After re Reading it, im certain i did. \n\nI was trying to say that if were design an experiment and a priori we know that two independent replications can be done by dividing a candidate population in half, we can gain a lot from it. As you hinted at, we can corroborate things like effect sizes, or just suss out issues with design if we see some really interesting contrasts between the standard errors of both models. \n\nBut again, wed probably need some pilot studies to do something like this.', 'As I said the out of sample prediction helps you not fool yourself, it helps you not overfit. By OPs description, the second study attempts to replicate the effect found in the first one. We can reasonably assume the first analysis was more exploratory, the second more confirmatory, and therefore the first was difficult to preregister. The extra information about the data generating signal comes from the assessment of the replicability/generalisability of the effect. Overfitting obscures the data generating signal.', ""This is only partially correct. The researchers still could have used any partition of their data to assess out of sample predictive accuracy. It's not as though the second dataset wasn't available to them from the beginning."", 'How is it incorrect? My point is agnostic to their specific split, its the fact that they split. You started by asking how the split tells you more about the data generating signal, and the answer is it helps you not overfit.', 'But that split can be made anywhere in the data - there\'s no benefit to using a predetermined split over combining the data a priori and then splitting. If your design requires a train/test split (and they don\'t all require it) then do it. But splitting based on your sample versus a third party sample gives you no benefit over combining samples and splitting *anywhere*, as long as your other assumptions of random and iid samples aren\'t violated.\n\nAnd my understanding is that was the original question: given a research design, is there a benefit to doing two half analyses or a single large analysis? Well your decision to have an out of sample predictive validation step is _part_ of your research design and can be done in either approach. Think like a Bayesian. You have a prior. Every data point updates your prior. Doing that all at once or in two stages where posterior 1 becomes the prior for posterior 2 makes no difference. You\'re conditioning on the exact same information.\n\nEdit to elaborate. You have two and only two sources of information about your question: your prior and your data. As long as your data are iid (and obviously they\'re not always) then it makes no difference the order in which you ""use"" them. That\'s literally the whole point of iid. Furthermore, you can\'t magically squeeze more information out of one partition of the data than the other. All of the data points are identical distributed and therefore carry equal information (weight) about the DGP. _How_ you choose to use your data to update your prior is up to you, but that\'s a research design question, and does not change the amount of information in the data.', 'No one has suggested the thing youre arguing against.']"
[Q] What kind of logistic regression analysis is most suitable?,"Hi all,

What kinds of logistic regression are (most) suitable for a model with 1 within-subjects categorical factor, 1 categorical between-subjects factor and a binary nominal dependent variable? 

Thanks a lot",13a90ls,Canecia07,1683423897.0,3,1.0,"['What\'s the outcome like? Nominal as in no natural ordering e.g. ""male/female"" or nominal as in ""0 (didn\'t) or 1 (did) happen?"" The first would be more multinomial logit (that\'s the most common I think).', ""The dependent variable is a prediction on the outcome of a casino wheel (red/black). So multinomial then? I asked mainly because I'm not sure which types are inappropriate due to the mixed model with between/within factors, I'm not very familiar with logistic regression :)"", ""You can do binomial GLMM (generalized linear mixed modeling), just as you might a linear mixed model. Clay Ford has a [good tutorial](https://data.library.virginia.edu/getting-started-with-binomial-generalized-linear-mixed-models/) in R on it. With just two levels to your outcome you can convert them to 0/1 and use regular ole logistic regression (the binomial GLMM in this case). Sorry for not being clear/reading your OP closer; multinomial's more for 3+ outcome levels. At 2 levels multinomial's just binomial LR."", 'Awesome, thanks for your help!']"
[Q] Help with interpreting regression line,"Hi, absolute begginer here. 

Just a quick question. Can we say that XOM and VDE daily returns are highly  correlated ? 

[https://imgur.com/iiYvS2U](https://imgur.com/iiYvS2U)

Is there more to be said when explaining this graph ?

Thank you",139vuo1,murdafeelin,1683393009.0,7,1.0,"['Yes, but an assumption is your model is correct. A model that also includes some other factor, say average blue-chip daily returns, could be more believable. After statistically controlling the correlation could be eliminate (or stronger, reversed, weaker, ...).', 'I don\'t know if this is for an assignment or personal use, but it looks to me like you\'re doing a Capital Asset Pricing Model analysis of XOM. In that model, the slope coefficient is called ""Beta"" and it measures the volatility of one stock to the wider market. \n\nPositive betas imply that the individual stock moves in the same direction as the wider market (pro-cyclical); Negative betas imply that the individual stock moves in the opposite direction as the wider market (counter-cyclical). Betas greater than 1 or -1 mean the individual stock is more volatile than the overall market.\n\nSo, if you have a beta of 1.5, what that means is that, on average, the individual stock moves in the same direction as the wider market but moves 1.5x as much. So if the market increases by 10%, the stock increases by 15%.', 'I will be very careful before coming to any conclusion with this chart and regression line.\n\nThe suitable kind of regression for your data is time series regression, i.e., the relationship of the daily returns of XOM and VDE should be considered in the chronological order. A vector auto regression (VAR) model is normally used in this case.\n\nHowever, what you are doing in here is regression on the cross sectional data. You put every available values of XOM and VDE together and run a regression, regardless of their order. It is easy to have a high correlation coefficient in that way, since daily returns of stocks are usually have similar distribution and centered around 0 (try to use histogram for both and you will see that).\n\nTo sum up, in this case, you need to read more about time series regression, because what you want to investigate in stocks data is how it changes over time.', ""Good thoughts already from Doc.\n\nThe residuals of your model seem to be dependent on your independent variable (that is, at low XOM, all points are below the regression line, and at high XOM, all points are above)\n\nMy guess is that you don't have an intercept, and including one would lead to a much better fit (as long as it makes sense for VDE to be zero when XOM is not, and I have no idea what they mean so that'd be up to you to consider)"", 'Thats a line guy', 'Besides everything that people have said regarding the stats, the other thing worth pointing out is that VDE is set up to track the MSCI US Investable Market Energy 25/50 Index, whose largest constituent at 24% *is* Exxon Mobil, so youd be shocked not to see high correlation.', 'I think the purpose of what OPs doing isnt predictive. It seems like its just a Capital Asset Pricing Model application. In that case, normal regression should be fine and using daily returns rather than stock price should make the day stationary (i.e. remove most autocorrelation).', ""Thank you for the answers u/EvidenceEarly3893 u/sickday0729 u/DocAvidd u/JimmyTheCrossEyedDog.\n\nI have a project that I need to do for school, I need to make univariate and multivariate time series models for forecast, and this is just starting out, I'm exploring data etc. I wanted XOM to be my target variable and these other tickers like VDE (Vanguard Energy Index Fund ETF), XLE (Energy Select Sector SPDR Fund) and CVX(Chevron) explanatory variables. So, yeah, that's about it :)\n\nThank you again"", 'Sickday you are right. I didnt think of the CAPM approach.\n\nHowever, I might disagree with the idea of using daily stock return for CAPM model. Daily return is a very noisy data, since the volatility of stock during one day is mostly random and doesn not contain much information. I think weekly return is better for estimating CAPM Beta coefficient (monthly could also be used for long period like 40 50 years)', 'Then EvidenceEarly was right about the models you should be using. ARIMA, VAR, stuff like that.']"
[Q] Getting LOR from stats processor,Going to apply for masters programs this fall which will be after Ive graduated. Im getting a LOR from my research professor and one from my job. But for some programs Ill need 3 letters and I think it would help if one is from a professor whos class Ive talent. Problem is idk how well the professor should know you before you ask them? I dont want to be rude and demand a huge favor from someone who doesnt know me.,139u4bx,Jorjuslero,1683389267.0,2,0.75,"[""I'm not in statistics, but I'm a professor. I think that asking for a favor is ok, but of course the answer can be no or something else negative. It's the professor's choice to write something or not, write something good or not.\n\nHow much I know the student will affect what I can write.\n\nIf I barely know the student, then I barely have anything to write. So, I'll probably refuse writing the letter. I would expect the student to first have a stronger connection with me. \n\nIf I know the student, but not much, I may write a letter but it won't be a strong letter. I can't strongly recommend someone that I don't know well. So, the letter would be one of the three letters required, but probably would not make much of a difference otherwise. \n\nIn your situation, my recommendation would be to talk to the professor first. Explain what you're doing, your plans, your goals. Ask for advice. And ask for a letter only after you had a good talk, so the professor would feel better about you, and about writing a letter of recommendation for you. You want a huge favor, put some effort into it.\n\nThe worst case I had was a student who didn't even ask. I got an email from a university saying I had been listed as one of the recommenders, but the student didn't even talk to me about that. Then I get an email from the student the day before the deadline, not even giving me time to write a letter if I decided to write one. Students sometimes feel they are entitled to a letter of recommendation (a positive and strong one), and that's the main thing to avoid. Be respectful, communicate, do your part before you ask for a favor like that.""]"
[D] The probability of Two raindrops hiting the ground at the same time is zero.,"The motivation for this idea comes from continious Random variables. The probability to observe any given value of a continious variable is zero. We can only assign non zero probabilities to Intervalls. Right?

So, time is mostly modeled as a continious variable, but is it really ? Would you then agree with the Statement above? 

And is there even a thing such as continuity or is it just our approximation to a discrete prozess with extremely short periods ?",139pc7o,peppe95ggez,1683381428.0,38,0.72,"[""Couldn't you generalise it to the probability of any event happening at the same time as another is zero?"", 'Bring measurement into your thinking. We only measure to some level of accuracy. So specifying a time is always plus or minus some amount. Then probability, as you say, is assigned only to intervals, but every measurement of time is actually an interval.', '> *Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.*  \n-- George Box\n\n\n\nThere\'s a difference between models and the thing they\'re modelling. This only becomes an issue with any conceptual difficulty if you think a probability model is in any sense \'real\' rather than a *model* -- an abstraction and a simplification. The most that we can practically hope for is that it may be useful.\n\nThe Poisson process is a model. It may well be that *no process* is ever exactly a Poisson process. This is not particularly material to the purpose of the model, which it to enable probability calculations where the process is a good approximation -- and in many cases it is an excellent one.\n\nIndeed, for such probability models, the question of whether any physical process is ""actually"" continuous is almost in \'how many angels can dance on the head of a pin\' territory; there are more realistic aspects of the discrete/continuous dichotomy that come up at much less precise level of data.\n\nConsider the difference between any *recorded* time and some actual specific instantaneous *instant*, were it possible to define the event precisely enough to associate an instant with the event. As a practical matter, we deal with the first thing (the data we\'re modelling), rather than the second thing.\n\nIndeed, arguably any sense of continuity in either time or space may break down if you go fine enough, but this is not normally of any consequence for us. Practical ability to measure and record differences many, many orders of magnitude larger is already nonexistent.\n\nIt\'s not unusual to find that there\'s several different levels of abstraction where different stages of model flip between continuous and discrete -- at this level it\'s continuous, at that one discrete, and at another, again continuous, and so forth. Whether the lowest level is \'really\' discrete or continuous (indeed even  and the need to consider whether there is \'really\' a lowest level at all) is moot. \n\nWorry about the usefulness of your probability models, don\'t expect them to exactly represent reality, in practice they don\'t. Leave considerations of what\'s ""really"" happening at some sufficiently fine level (and whether that question of some lowest-level fundamental \'reality\' is even particularly meaningful) to the physicists and philosophers.', 'This pretty obvious but completely depends on your definition of exact same time. By the second, obviously false, by the 10 billionth of a second, probably.', 'That\'s just a feature of continuous distributions.\n\nIf we assume continuous distribution, then it isn\'t 0 per say it is ""almost never"" which has a different nuance than never or impossible.\n\nThe probability of pulling exactly any number from a uniform distribution is ""zero"", but clearly possible since if you sample a random number it was drawn and also had a probability of ""zero"".\n\nThere\'s a difference between zero in the limit and zero as in impossible.  \n\nOf course if we want to get into the details on a physics level.  Continuous point distributions are useful tools for modeling, but they are simplified tools. underlying physics says that because droplets have a non-zero volume you can actually come up with alternate modeling which the probability would not be 0.  \n\nIf you define a droplet of non-zero volume and figure out the probability of a two tangent lines hitting it will have a small, but existing probability.', ""Time does have small steps known as Planck time. (I don't work in the field or know enough to know if this is an actual time step or if this is just where our ability to measure time breaks down). It's so incredibly small that it's practically continuous. 5.4E-44 seconds is a Planck time.\n\nSo yeah, without doing the math, my gut says that no two drops have hit during the same Planck time."", 'Hitting the ground isnt something that happens instantaneously though. Itd take a few milliseconds.\n\nIts probably somewhat simple to define the start of the hit the ground process (surface tension of the water breaks, maybe). But how do you define the end of the hit?', 'Thats assuming that a raindrop hitting the ground is a point occurrence with duration = 0. Now if each raindrop splat lasts a few milliseconds, the probability of two splats overlapping would be nonzero', 'Theres an infinite amount of points in time in any given arbitrarily small window of time, which is one of the main intricacies of continuity.', 'The probability of a raindrop hitting me in the eye is zero and yet it happened to me :(', 'Following!', 'If you define the time you consider exact = exact (for instance 1 Planck time as above) then you set up the scenario for it to be inevitable', 'Isnt the number of raindrops hitting the ground during a certain time interval a Poisson variable? Therefore a discrete and not a continuous casual variable?', ""That is assuming time is continuous and not discrete.  Which is the most common assumption, but it's never really been proven either way."", 'Time may actually be quantized. That is, that it is discrete and not actually continuous. So the probability would not be 0.', 'The ground is fractal, so it depends on the length of the ruler (the coast of England problem)', 'Does your model adjust for the perspective of how we see those two droplets hit and adjust for the delay in time between when the two lights of those droplet hits? From that perspective, would it not depend on where those droplets hit and thus expand your model data significantly and the chances of both of them hitting at the same time increases significantly?', ""...what? You're telling me not a single drop of rain falls at the same time as another from the same cloud? How, why?"", 'We can only assign non zero probability to intervals \n\nwhy would that be the case? Why would it not be an infinitely small probability? You can argue that an infinitely small number is 0 but then you are deliberately ignoring that the range between 0 and 1 is also continuous, as continous as your mesrument/perseption of time. \n\nAn other way of thinking at this is why would time be split on infinitely smaller spaces than the smallest probability you can make up?.\n\nWhy would t (duration of rainfall) could be divided in smaller steps than 0 to 1 (probability)? Especially when you can (conveniently) measure t in such a way that goes from 0 to 1? (Where 0 is rain starts and 1 is rain stops, not any known measure of time but time is measured between events, so you could say 0 at rain start and 1 at rain stops). \n\n\nhttps://en.m.wikipedia.org/wiki/Zeno%27s_paradoxes', 'Raindrops take up discrete chunks of volumetric space, rather than truly continuous', 'The probability of any single event for a true continuous variable is always zero. However, thats just a limitation of the mathematical model we use to explain the real world. It is obvious that I am a certain height, however there is zero probability of me being that height.', 'Actually you define probability of continuous random variables on intervals. A point is a closed interval of 0 measure, thus the probability is 0. Of course, this is only on theory, since in practice you cannot measure since any real world measurement implies an approximation, and a non-zero measure interval as a consequence', 'If you use a quanta of time equal to the Planck Time, you wont get zero. Planck time is 5.391247  10^-44 seconds and is the time to takes a photon to travel the Planck Distance.', 'If by time you mean an exact value of t then it is 0. But as soon as by time you mean a span of time greater than 0 then the probability gets higher. Plus remember, probability equal to zero doesnt mean impossible', 'Sure. This is just the occasion which sparked this idea in my head.', ""I'm confused why this is here. It's like, one of the most basic things from probability theory."", 'Good answer! That is true. Maybe i am just overthinking this stuff... Can we even understand what a ""real"" instant is.', 'Exact same time -> Exact same time. No difference in time. Irrespective of how precise you measure.\n\nIt is obvious but at the same time feels counterintuitive.', '>That\'s just a feature of continuous distributions.\n\nI\'d go even further and say this is just a feature of integrals in general. It\'s kind of a nebulous task to define an integral *at* a point rather than between unique two points. Since the integral is just the limit of a riemann sum, you similarly couldn\'t meaningfully ask ""what is the area of a rectangle with known height and arbitrarily small width?"" Since the width is defined by an infinite limit, you can\'t pick ""at infinity"" as a measurement point and if you pick any finite limit then you\'re just approximating.', ""I think i don't fully understand this principle then.\n\nI know that probability of zero doesn't mean it is impossible but what does a probability of zero mean then ?"", ""Plank time isn't guaranteed to be the smallest unit.  It's just currently the unit where we can't seem to get any smaller."", ""Interesting maybe i should read on that. I've heard of the planck length but it feels weird to think of time being a discrete thing."", 'There is plank length as well if I recall correctly. And that basically means the difference between hit and not hit in a frame by frame reply.\n\nProbably related to the plank time and speed of light in vaccum too. IDK...\n\nI feel that probability of two drops being away at distance zero from the surface at the same time has to be non zero.', ""We cant know, but almost all physicists don't believe time or space is discretized, because our most fundamental and well tested theories only work in continuous space time. Notably, any discrete space time cannot be Lorentz invariant on some level. That is, when changing reference frames in special relativity, things stretch and transform in non discrete ways. A discrete space time would result in discrete lorenzt transforms, and measurements of photons coming from far away are inconsistent with spacetime being discrete on the plank scale. \n\nSo basically  we don't know, but all the theory and evidence is inconsistent with plank scale discrete space time/consistent with continuous space time."", 'Depends on what you define as ""hitting the ground"".', 'Im sorry for you :(\n\nHaha :D', ""That is true, it's basic. But in my opinion the implications for the real world are interesting."", ""Isn't technically 'real' time just a series of snapshots at the amount of time light needs to cross Max Planck distance?\nhttps://en.wikipedia.org/wiki/Planck_units\n\nAlso to expand on interval, I would posit that a drop of rain takes certain amount of time to merge with surface. During this interval many other drops will be doing the same.\n\nOne has to be very pedantic to go into 'actshualy, if we look at planck units and first atom of water droplet -touching- surface, then maybe statistically no two droplets meet surface at exact time'.\n\nNote: to properly calculate this, we would have to calculate surface area, and amount of droplets per surface area per time, and the size of the droplet. Considering that droplets often merge during rainfall, I would expect that proper answer is that tons of water droplets hit the ground at the same time."", 'If you\'re getting this deep into the physics of it then two things happening ""in the same instant"" depends on where you observe those things from, anyway. If you observe two raindrops impacting the ground at the same instant (somehow), you would have seen something different if you were standing somewhere else. Simultaneity is relative to the reference frame.', '> Irrespective of how precise you measure.\n\nif I measure exact time in precision down to a second, I would have tons of raindrops share same second. So definitely we are talking about some precision here.', '>Exact same time -> Exact same time. No difference in time. Irrespective of how precise you measure.\n>\n>It is obvious but at the same time feels counterintuitive.\n\nBut it can\'t be both obvious and counterintuitive ""at the same time""', 'I think your confusion comes from thinking of the continuity of time differently from the continuity between 0 to 1. \n\nAssume you follow a drop from when it falls to when it hits the ground. Lets say that happens in less than a minute. Somewhere between 0 minutes and 1 minutes we have the time of impact. You can find a number between 0 and 1 that will fit the exact time of impact as a fraction of a minute. Always, simply because there are infinite (continuous) numbers between 0 and 1. \n0 and 1 also used for probabilities, so the probability of a drop hitting the ground at any time between 0 and 1 minutes is the delta between the time before the impact and the time right after the impact. The delta can be infinitely small without being zero, the same way that the exact same time exists. \n\nhttps://math.stackexchange.com/questions/1327444/more-numbers-between-0-1-or-1-infty', 'Sorry my flight was landing and couldn\'t respond immediately. \n\nIt\'s a quirk that happens when you introduce infinity anywhere into a probability distribution.  It catches a lot of people when they first try to look at continuous probability, but it does have a pretty significant difference in application.\n\n""Almost never"" which is when a probability appears 0 or ""Almost always"" when it appears 1 implies that it\'s 0 in some kind of infinite/infinitesimal limit \n\nSo first let me give you an example of truly impossible event.  Pull two numbers from the distribution between 0 and 1 and add their result.  What\'s the probability of obtaining a sum of 1.9 if the first number was 0.4?  The answer would of course be 0. Because there\'s no number between 0 and 1 that sums to 1.9 from 0.4.  That\'s an example of truly impossible implying that there are zero states that could give you that outcome. \n\nNow let\'s look at another one.  What\'s the probability of getting 1.9 if the first number was 1?  Well zero, but it\'s not the same as the zero before.  The number you need does exist on the distribution.  0.9 is between 0 and 1. So it is still possible to get that number, just infinitely unlikely. \n\nThis may seem insignificant at first, but the key is often to work with continuous distributions you eventually need to add some level of discreteness back into it.   An example is integrating over an interval to give a finite volume.  In the first case when you add finite elements back in, the probability is still 0.  Because the issue wasn\'t that you had a quirk of continuous variables there, the problem was there\'s just simply no solutions.   But in the second case when you add it back in, the probability is no longer 0 and instead has a well defined chance of happening.\n\nThe reason is because there\'s two different things causing the number to approach 0.  Look at an elementary definition of probability.\n\n    P(x) = (Number of outcomes containing x)/(Number of total outcomes)\n\nIn the first case very often the Number of outcomes with X is what goes to 0.   Meaning if the bottom is non-zero the probability is still 0.  In the second case what\'s going to zero is the number of total outcomes. In a continuous distribution you have an infinite number of points on an interval, meaning there\'s infinitely many points you can pick from.  They will skew according to their distribution, but so long as there is density over any given interval the probability of a single number it will always go to 0.   But clearly you have to be able to pick a point from a distribution to sample it.  It\'s not that it\'s impossible, it\'s that there\'s infinite points and infinity causes the number of total outcomes to always drop to 0. \n\nWhy this matters is usually when it comes to things like physics when you start to account for things such as the size of the droplet, the ground having mass, etc.  what you find is you remove infinity from the equation and replace it with a finite volume. When you do that, the probability then becomes non-zero.  But in the case of truly impossible, it will always be 0 even if you attempt that.\n\nAs such, when you\'re dealing with continuous distributions, you do actually need to ask which zero you\'re dealing with.  Because they can result in very different interpretations of the probability.', ""I think it's making the distinction of something that is so exceedingly rare that it's practically zero vs. Something that is a hard zero. And that when you transition from the pure math world to the real world, just by the very nature of the world being made of things being roughly measured by us, that a lot of those perfect hard lines get rather blurred. \n\nBut do correct me if I read into that wrong."", 'It just means that under perfect precision on the continuous range of possibilities, the probability that it is within that range of precision is effectively 0.\n\nThe probability of you guessing where a raindrop will fall will always be 0, because you can always zoom in further and show that your guess is slightly off. Here 0 is more of a limit towards 0 rather than the empty set 0.', ""I guess if you want to be pedantic, yeah, our understanding of the universe can always expand. \n\nBut as it's a derived value from 2 heavy hitters in the physics world and that it's been in use for something like 120 years, I don't see that changing anytime soon."", 'It is kinda strange to think about. Think of ""Planck"" like the real world version of ""Quantum"" in the Marvel movies. Anytime it gets thrown in front of words, that means the normal rules are out the window, and things are about to get real brain-fuckky.', 'The concept of drop is not meaningful on Plank scales.', ""Yeah, a Planck length it the distance a photon travels in a vacuum in 1 Planck time. My gut says that the probability is some nonzero number, but as these measurements are so ridiculously small, so would too be the probability. And practically speaking, I'd say that it's small enough to be zero."", ""> Isn't technically 'real' time just a series of snapshots at the amount of time light needs to cross Max Planck distance?\n\nThe fact is, we don't really know.  As far as humanity knows, it could be either way, or something weirder.\n\nSee, e.g., \n\n* https://physics.stackexchange.com/a/33289\n* https://physics.stackexchange.com/a/35676"", 'My theory is the universe is a simulation and the clock cycle is equivalent to the time it takes light to travel a plank length', 'This is an interesting perspective. For any two space-like raindrop events (space-like means outside of each others light cones, I think) there exists a reference frame in which their dropping is simultaneous.\n\nI think this holds for any three space-like events as well, but not any 4 events (three events form a plane in space-time which should correspond to a reference frame). Maybe someone who studied this more recently can confirm?', ""And what if in this instant there would spawn a Unicorn in your room, the existence of Unicorns would be obvious yet to most people counterintuitive i'd say."", 'Thank you for explaining in depth! It actually helped me understand better why the elementary result of a continious RV has Probability 0.', '> I guess if you want to be pedantic\n\nWell, this *is* a mathematics subreddit...', ""It's far from being pedantic, its very much what many of the heavy hitters in the field will warn you about reading too much into plank length. Because it may be a singularity where that's just where are measurements and models break down rather than being a hard and fast rule."", 'But drop boundry is...', 'But measurement accuracy brings the distance to a pretty large number, same with the time.\n\nSo in simple tv terms, your pixel just got bigger. The difference between minimum measurable qty and plank values is huge enough that we can say that itd probably happen a few times every tornado or something...\n\nEdit:\n\n*distance = minimum measurable distance\n*Time= minimum measurable time interval', 'I think you might be heavily over thinking this.', ""True, rereading it, I can see coming off as more mean than I was intending.\n\nI meant it more as being exacting to the technical definition, which in a maths subreddit I'd expect nothing less, but that as it's reliant on other values, those values would need to change for it's value to change."", ""Sorry, it was poor word choice on my part, and it came off kinda mean. I was just meaning that, though there is nothing stopping a smaller measurement of time from being used or found, that as it's derived from two well-used values, the big hitters I was talking about, that for it to change, we would have to change the speed of light or the gravitational constant."", 'https://www.youtube.com/watch?v=36GT2zI8lVA\n\n\nYup. This discussion by Feynman goes into how if you just keep asking why you will hit the edge of known human knowledge', 'are you sure? You have water molecules evaporating but also being re-absorbed. At this scale its not a smooth surface either.  How do you define if a molecule is part of a droplet water or evaporated from same drop?\n\nthis while ignoring Heisenberg uncertainty, and we may not even exactly know which H atoms are part of a H2O molecule, as there are some free H^+ which might join the party or a H^+ may abandon a molecule resulting in free OH^-', 'Yeah, in any measurement that is actually meaningful for this scale, they probably happen often enough.', ""The question isn't about the value itself though, but rather, our interpretation of it. \n\nThe Plank Time is often described as the shortest possible unit of time (as if the Universe was fundamentally discrete), but it's better understood as being the shortest possible unit of time our existing theories can make coherent predictions about. It's a feature of our *model* of the Universe, not the Universe itself. \n\nIt's possible another model of physics will emerge that can resolve shorter intervals. That doesn't mean that the value of the Plank Interval will change (the number could stay exactly the same)."", ""All good.  The thing I usually hear is that because we don't have a unifying theory of physics just yet, there's still tons of room for a major disruption."", ""You're being a little too pedantic\n\n>At this scale its not a smooth surface either.\n\nIt can be in a thought exercise.\n\n>How do you define if a molecule is part of a droplet water or evaporated from same drop?\n\nYou don't. Thought exercise.\n\n>this while ignoring Heisenberg uncertainty\n\nYep!\n\n>we may not even exactly know...\n\nDon't care."", ""Not working in the field, I wasn't sure if they believed that the universe was continuous with discreet measurement vs. discrete with discrete measurements. I always leaned towards the first personally and assumed that it was just an artifact that popped up in the maths to make it workable, and it sounds like that's the general consensus. \n\nI see where you are coming from and what I got caught up on. I was thinking along the line that as it was worked back from almost fundamental values in physics and math, that for use to resolve smaller times or lengths or what have you would have to inherently invalidate those values. Changes to the numerical value would require a massive rewrite of everything, but finding new maths or models that can resolve smaller wouldn't necessarily require that.""]"
[Q] Choosing between a count model and spatial error model,"Hi everyone, I hope this doesn't fall into the homework category: 

I working with a dataset consisting of spatial grid cells. After performing some spatial diagnostics (Moran's I, LaGrange multiplier tests), it seems that there is spatial dependency in the form of spatial error, so a spatial error model seemed fitting. However, my dependent variable is a count variable and is quite skewed (the standard deviation is much higher than the mean). From what I have gathered this violates the assumptions of the spatial error model (which assumes more continuous data) and also seems to lead to heteroskedasticity, biasing the results.

An alternative could be to run a negative binomial model, which seems to fit the data better but leaves out the spatial dependencies, also biasing the results. I know there are some ways to perform spatial count models, but these seem to complex to grasp for me at this stage (or to difficult to implement in R).

My question is: how do I choose between these two evils; unfortunately the results are quite different. Is there a good way to compare goodness-of-fit? The AIC and Log Likelihood are much better for the negative binomial model whereas the RMSE of the spatial model is better but I am not sure if I can compare either those metrics across the models.",138xb6m,JWeenink,1683311674.0,13,0.85,"[""Wouldn't it make more sense to model both aspects  together rather than choose?"", ""You should be able to use mgcv in R. Something like `gam(target_var ~ te(latitude, longitude), data = data, family=quasipoisson())`. Depending on where you're at in your spatial work or with ecology or whatever, you can check out Simon Wood's book on GAMs with R. \n\nYou should be able to compare models in R with `AIC(model1) < AIC(model2)` and you can also just roll with `anova(model1, model2)`.\n\n If you've got things to a train/test split to predict then you should just be able to use RMSE - something like `sqrt(mean((data$actual - model1$predicted)^2)) < sqrt(mean((data$actual - model2$predicted)^2))` \n\nOne package that deals with varying scales is gjam, so if you're just working with lm then that might be a non-spatial option that fits better. You could also try a transformation (boxcox is probably fastest, but something like log-log is more straightforward in its interpretation than dealing with the boxcox lambda).""]"
[Q] Studying regression and correlation - which resources to get a deep understanding?,"Years ago at university, I met all many of the usual statistics elements introduced to students. I was interested in the topic and had a good intuitive understanding, but I was also frustrated when we were, for instance, taught that the standard deviation is ""the mean deviation"". I could clearly see from the definition that it was not - and later saw Nassim Taleb talk about this where he also defines it rather as the "" root mean square deviation "".

I ended up dropping my statistics studies but want to get into it again. I'd like to understand things at a deep level and not just use an equation without knowing its logic. And in particular, I'd like to move on to multiple regression and be able to use data to forecast what values can be expected. As a bonus, I'm also very interested in the use of absolute deviations versus squared deviations.

Any great resources you want to recommend?",138wyey,supertexter,1683310887.0,3,1.0,"[""What area are you in? What mathematics have you covered?\n\n>  I'd like to understand things at a deep level and not just use an equation without knowing its logic.\n\nThen some basic mathematical statistics - enough to learn the theory in regression - is where I'd start.\n\nFor that you'll need some basic linear algebra and some calculus, then some probability. That will give you the background to pursue some standard mathematical statistics -  point estimation, interval estimation, hypothesis testing ...\n\nThen something with a decent coverage of regression. From there you can move out to whatever additional topics you need."", ""I have studied psychology, but done a but of linear algebra and such on my own. \nThis reminds me that I've often thought it would be effective with some 'placement test' to figure out which topics one hasn't covered.\n\nIt sounds very good, but do you also have a suggestion for a good ressource? Maybe starting from there I'll automatically find out what prerequisites I need"", ""I have seen the book *Applied Linear Statistical Models* by Neter recommended. But I think that uses least square regression. I don't know if the way to go is to learn least square first and then look at absolute deviation regression later.""]"
[Q] Benjamini-Hochberg Correction,"Hi everyone, I am trying to run a Benjamini-hochberg correction on the results of spearman correlation analysis to correct for multiple tests. I am a bit confused on how to apply the correction. I have four separate treatment groups that I ran the correlation analysis on separately. Do I run the correction four times as well? Or do I pool the p values from all the groups and perform one correction? Thank you!",138tje6,Ill_Recipe_3136,1683303487.0,3,1.0,"['If you want to consider the tests a ""family"" of tests, you pool the *p*\\-values, and apply the correction to them as a set.\n\nThis is implemented in software packages, often with the *p*-values themselves being reported as ""corrected"".  But if you are doing it by hand, you are setting a threshold to reject / not reject for each *p*-value.']"
[Q][E] Lottery with multiple ball sets,"Supposing it is possible for a lottery to use different sets of balls, such that each set is biased (by weight presumably), such that the average frequency of numbers/balls drawn is nearly equal over time. How would one analyze the result for the past n drawings to discover the sets?

My thinking is that in an elementary case, where in 1-50 lottery, if there are two sets of balls; set A the balls from 1-25 are 1 gram less than the balls 26-50. In set B the opposite. One could simply sum the 5 numbers drawn, per drawing, and see the sets. Or at least have a good idea where the bias is.

When considering a more complex case of gaps of interleaved weightings is where it gets complicated (obviously). So, how would one analyze this situation? Thanks!",138t6gn,markdacoda,1683302715.0,2,1.0,"[""This is an example of [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis). Balls with the same deviation in a set will be drawn together more often than we would expect from random chance. If you assign a distance to numbers based on how often they show up together you can look for clusters of numbers with a small distance. You'll probably need a large number of drawings before this produces reliable results.""]"
"[Q][D] Predicting a binary result or regression using an ""uneven"" amount of data (potentially time-series data)","Hi all,

I'm working on building/selecting a model to predict the result of a sales lead: whether it's ""SOLD"" or ""NOT SOLD"".  

My dataset consists of past leads with the following data:

- sales rep
- product pitched
- lead source
- date of lead
- zipcode
- result

The issue I'm running into is that I have a various amount of leads per day per rep. Here is some sample data:

| ID | salesrep | product | leadsource | date | zipcode | result |
| -- | -------- | ------- | ---------- | ---- | ------- | ------ |
| 1 | Bob | A | Website | 5-1-2023 | 12345 | SOLD
| 2 | Bob | A | Call In | 5-1-2023 | 12344 | NOT SOLD
| 3 | Alice | A | Website | 5-1-2023 | 12343 | NOT SOLD
| 4 | Bob | A | Referral | 5-1-2023 | 12346 | NOT SOLD
| 5 | Alice | A | Call In | 5-2-2023 | 12345 | SOLD
| 6 | Bob | B | Referral | 5-2-2023 | 12344 | SOLD
| 7 | Alice | B | Website | 5-2-2023 | 12342 | NOT SOLD
| 8 | Alice | A | Call In | 5-3-2023 | 12333 | SOLD

In this example, I have three dates of data. 

- Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. 
- Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. 
- Day 3 (5-3-2023) has just Alice with 1 lead.

Now let's say I have information for a lead for day 4:

| ID | salesrep | product | leadsource | date | zipcode | result |
| -- | -------- | ------- | ---------- | ---- | ------- | ------ |
| 9 | Bob| A | Website | 5-4-2023 | 12345 | ???

I want to predict the result - specifically, the probability of the result being ""SOLD"".

Initially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a ""busy season"" during the spring and summer months, and I'd also like to account for sales reps that are on ""hot streaks"".

For this I was thinking about making use of an LSTM RNN model, but I'm not sure if my data's ""shape"" is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).

Thoughts on this?  Which model would you use in this scenario?

And not to throw a wrench into it, but assuming that I'm able to somewhat accurately predict the result of the lead, I'd then start to look at trying to predict a range for the sale volume as well (using past data of course).

Appreciate anyone who takes the time to respond to this.",138rf6t,SuperMandrew7,1683298985.0,7,1.0,"['What is your intended final use for this model for your business? I ask since the data you have seems to be mostly internal data (such as who made the sale + the product). Is the plan to funnel more business to the higher performing sales reps? \n\nI would start simply. It sounds like there is legitimate reason to believe that sales differ for your busy season relative to the rest of the year, so you can create a new indicator variable for your busy season (1 for busy and 0 for not) and use that in your model instead of the daily date column. You can model this with a logistic regression to get your probabilities.', 'Thanks for the response!\n\nSo the main idea is that every day we have a list of leads that we are set to run.  I\'d like to take that list of leads and optimize it so that we\'re sending out the best rep available for that specific lead (based on the limited amount of data we have available).\n\nSo if our best rep only closes on product A, if the customer is looking to buy product B, I\'d run my model and evaluate the probability of ""SOLD"" for each rep, and then provide a ranked order in return.  I\'m hoping for my model to pick up on some underlying trends that maybe most people wouldn\'t know to look for (Bob never closes on Mondays, or Bob has gone 0 for 100 in this specific zip code, etc).\n\nHopefully that makes sense!\n\nEDIT:  Sorry, one last thing!  I\'m also trying to take into account how well a rep has been performing recently moreso than their past performance.  A rep may have been doing well 5 months ago, but really the last 4 weeks are more indicative of their current performance.  And I\'m trying to capture that performance and hopefully combine it with the rest of the data (how well a rep does with a specific product, or with a specific lead source, etc)', 'One thing to think about with zip codes is they are so granular they are likely to be very unbalanced or very dispersed i.e. 90% of your sales might come from 4-5 zip codes and then you have 100 zip codes with only 1 or 2 sales or you may find that you have hundreds of zip codes with a few purchases in each giving you a lot of data to process and not a lot of return. \n\nI would again look at this through the lens of your business expertise - is there a reason why someone from one zip code is more likely to purchase from another? Do you focus advertising on a specific geographic area? Do you only serve customer base from a narrow zone. If your business is national, do you actually see a concentration within certain zip codes? If so, why? If not, it will likely not be useful for your model. \n\nMy personal strategy (which sometimes does not work) is to start at a more aggregated level and then start drilling into more detailed analyses in future models after some robust exploration. A simple model may give you exactly what you need without extraneous details.', '>So if our best rep only closes on product A, if the customer is looking to buy product B, I\'d run my model and evaluate the probability of ""SOLD"" for each rep, and then provide a ranked order in return.\n\nFor this sort of thing wouldn\'t a simple proportion do to rank salespeople?\n\nAlso keep in mind that once you\'re using this kind of model to direct salespersons, you will force specialisation.\n\nIf Bob is great at product X, he will always sell product X and forget about product Y.', 'Is this your first data mining project? You have a lot of hopes and dreams and most of it sounds like a nightmare to implement.', 'Great points.  I actually already attempted evaluating my data using an XG Boost classifier, and when evaluating different encoding methods, I would try all categorical variables including zip code + mailing city, and then try them again with that information left out.\n\nFor zip code, one technique I found was good for encoding was using a weighted mean target encoding - essentially if I have ""enough"" data points for that zip, then encode it using target encoding, but if I don\'t have enough data points, then use the weighted mean across all zip codes instead.', ""This is a great point and definitely something I'll have to keep in mind.  We have our sales manager who checks on everyone's numbers to make sure we're not missing on stuff like that (and if someone is struggling with a specific product, they get re-trained on it).\n\nThe idea here is to provide a suggested list per day for the leads, and let the sales manager choose to override that suggestion if he notices something like that."", ""I'm really not looking to find what those patterns are, I just want the model to be relatively accurate at predicting the binary result.  I think a neural network or other classifier may pick up on these things given enough training, but I'm not trying to pull out that information.  Like you said, it'd be a nightmare to extract whatever that pattern is from the model.""]"
[E] My solution to the Sleeping Beauty problem.,"[https://www.youtube.com/watch?v=XeSu9fBJ2sI](https://www.youtube.com/watch?v=XeSu9fBJ2sI)  


Obviously, the probabilities for the Sleeping Beauty problem are just = (0.5 for heads)\*(1 of token A) + (0.5 for tails)\*(2 of token B). The chance of the coin is 50-50, from Bayesian testing. The number of tokens is equivalent to the number of ""links"" on the decision chain. Sleeping Beauty's chance of correctly guessing a specific token is 1-3. There is more information in the question than expected.

The mistake people make, is assuming the payout probability is the same as the win probability.

Essentially it is an overpaid bet - this is how casinos make money. The roulette wheel has 33 slots, but one slot pays 2-1 to the casino. The calculation for the roulette wheel is = (32/33)\*(casino pays out 1 token)+(1/33)\*(casino wins 2 tokens). So the casino in the worst case does not lose money, and the casino in the good case wins 2 tokens (essentially the green slot is linked with a red or black slot).

After every slot has been hit, the casino always walks away with 1 extra token.

Halfers think the calculation is = (0.5 for heads)\*(1 token)+(0.5 for tails)\*(1 token). This is wrong because the links are not counted correctly. These people lose money by overpaying for bets at casinos.

Thirders think the calculation is = (1/3)\*(1 token \~ effectively for heads)+(2/3)\*(1 token \~ effectively for tails). This is practically more correct but backwards, but it misses key information.

The multiverse state is unknown because the bayesian probability is unknown and the number of links is not known, like a coin is tested to be.

This solution is obviously correct, once somebody points it out.",138g9ez,Frankenmoney,1683278636.0,1,0.52,"['Why would you write this (and tag it education) when you seem to have no idea how roulette works!?\n\n> The roulette wheel has 33 slots, \n\nRoulette wheels have [37 or 38 pockets](https://en.wikipedia.org/wiki/Roulette#Roulette_table_layout) not 33 and you omit that that in American (but not European) roulette there are *two* [green pockets  ""0"" and ""00""](https://www.eyeonannapolis.net/wp-content/uploads/2022/02/image1-2.jpg) that ""pay"" the casino. \n\n> but one slot pays 2-1 to the casino.\n\nA 0 or 00 pocket can\'t be said to pay 2-1 to the casino - you place your bet and it\'s either taken by the house, or you get paid out according to the odds. In American roulette you can bet on 0, on 00 or a split bet on either in which case the ball stopping on a zero would not pay the house.\n\nThe casino makes money any time someone places a bet that isn\'t paid out. Their edge comes from the difference between the returns they offer for a bet type and the long run average probability of the event in that bet type: https://en.wikipedia.org/wiki/Roulette#Bet_odds_table\n\n\n>  The calculation for the roulette wheel is = (32/33)*(casino pays out 1 token)+(1/33)*(casino wins 2 tokens).\n\nThe casino pays 35x your stake if you bet ""straight up"" on a single number (probability 1/38) on roulette. I don\'t think there would be many takers for roulette if you had to wager $10 on a 1/38 shot to win $10.', 'I am not quite sure what your resolution is here (thirders are practically more correct but backwards? What does this mean?).\n\nFor anyone interested in this problem, I recommend this CV thread: https://stats.stackexchange.com/questions/41208/the-sleeping-beauty-paradox', '>My solution\n\nYou mean your failed attempt...', '[Whuber](https://stats.stackexchange.com/users/919/whuber) is the GOAT.']"
[Q] What is probability of 4 losing streak if I have a betting system with historical win-rate of 70%?,"Is it correct that since my chance of losing is 30%, then I should multiply 0.30 by itself 4 times to get 0.81% chance of getting 4 losses in a row?


Or am I doing probabilities wrong?",1387i9m,Resident-Nerve-6141,1683252720.0,18,0.82,"[""0.81% is the chance to lose 4 specific games. If you play 1000 games in total then you have 997 possible places where such a losing streak can be. If all these places were independent then your chance to not have a losing streak would be (1-0.0081)^997 = 0.0003 or 0.03%. That means a 99.97% chance to have at least one losing streak in it.\n\nThe 997 places are not independent, however - if you won the first 4 games then there is no way to have a losing streak with games 2 to 5, for example. You can calculate the chances in a spreadsheet ([longer explanation here](https://www.reddit.com/r/DreamWasTaken2/comments/kkaysw/the_chances_of_lucky_streaks/)), and the result is a 99.7% chance to have at least one losing streak of length 4 if you play 1000 times.\n\nNote: This assumes each bet is independent. If that's not the case, then the chance can be different."", ""That's for a single instance of 4 games.  But realistically, I imagine that you're computing the probability of a 4 game losing streak in a long run of games, which makes the probability much higher."", 'It depends on why it won 70% of the time in the past. Imagine you were betting on the tulip market in the 1600 when the mania ended. Your model may do worse or even much much worse than a coin toss. \n\nDepends on the game and how close it is to the generative model. It also depends on independence of observations.', ""I'm a bit rusty but to add to what was said:\n\nEdit: This is wrong, the formula is the probability of getting exact 4 not 4 in sequence.  \n\nThis is a binomial distribution.  Imagine that 'success' is losing with prob of .3.  Let losses be 'success', to get 4 losses in a row out of n number, the probability is \n\n0.3^{4} * (1-0.3)^{n-4}.  This is the probability of succeeding 4 times in a row, multiplied by failing n-4 times.  You can fail fail, then succeed 4 times, then fail for the rest, but it always adds up to this because of how exponential works.\n\nThen you need to calculate how many combinations of 4 successes in a row you can have in total, and then multiply it everything together:\n\nnCr4 * 0.3^{4} * (1-0.3)^{n-4}.  where nCr4 is n choose 4 and is 4!/(n!(n-4))!"", ""So I just went down a deep rabbit hole of Markov chain and I want check with you mine understanding of it if it's alright.\n\nI created a 5 by 5 transition matrix with. first row 0.3,0.7,0,0,0, second row 0.3,0,0.7,0,0, etc.  and If I take it to the 1000th power (matrix multiply itself 1000 times), the probability of getting  at least 4 consecutive loss in a row is near 1, not to mention exactly 4.\n\nCan you give me some pointers as to what I did wrong?"", 'Hi thank you for this. How should we modify the formula if we want to know probability of the four continuous losing bets to occur two times in 1000 bets? (ie, they dont have to happen right after the other so it could be 4 losing bets, then 1 win, then another 4 losing bets)\n\n\nthank you so much so far', 'Youve calculated the probability of exactly 4 successes in n tries, not 4 in a row.', 'hi! just to clarify, if I were to bet 1000 times, I just replace n in your formula with 1000?', 'This is wrong. You are using a geometric distribution not a binomial one. You are even using the density function of one', 'Looks right. Check if you can reproduce the 99.7% (99.6988% to have more digits to compare).', ""Actualy, it's right. If you make a million of bets the probability of finding at least one 4 losing streak is practicaly 1"", ""That's more complicated, but it's still extremely likely (spreadsheet calculation: 97.9%). The chance to have at least three of these streaks is 92.7%. This is assuming disjoint streaks, so losing 5 in a row would not count as two losing streaks. 8 losses in a row would.\n\nWe have an average of 0.0081 * 997 = 8 of these streaks (with the caveat that a streak of 5 is counted as 2 streaks of 4 for this average)."", ""Good point, I'm more rusty than I thought,  I'll give it a bit more thought and edit in the answer later."", 'I can almost guarantee youll get a streak of 4 losses at some point in 1000 bets.\n\nThe exact probability is difficult to calculate, lots and lots of inclusion-exclusion. But via simulation, I believe the probability is something in the neighborhood of 0.996.', ""Well, that's the thing, I can't.  The answer I get is so indistinguishable from 1 that it shows up as 1 in R.  It gets beyond 99.7% at the 48th or so power"", 'I see. Thank you. Would it be possible to get a copy of your spreadsheet so we could look at how it is calculated and learn from it?', '0.996 is less than 1% though so there is a higher chance of 4 losses not happening than it happening?', 'Oh, I think you used 0.7 for the chance to advance. OP has 70% winning chance but is looking for losing streaks, so you should swap 0.3 and 0.7.', 'The link above has a sheet linked for a streak of 20 coin flips in 100 attempts, different numbers but the approach is the same.', '0.996 is 99.6% :(', ""ok thank you again! you're the best!"", 'oops i thought it was in percent. Im sorry :(\n\n\nWould you know how to compute the probability of four consecutive losses occuring twice in 1000 bets? (ie, they dont have to happen right after the other, so you could have 4 straight losses, 1 win, then 4 losses again).', ""I don't know, sorry\n\nperhaps you can simulate with an algorythm to get a good aproximation""]"
"[Q] Is the bottom-middle part of this flowchart accurate? If so, how can you do a binomial or hypergeometric test of a hypothesis about a sample mean?","[Link to the flowchart is here](https://ibb.co/Mpqq2mt)

I could see using binary quantification, but that assumes symmetry, and that isn't mentioned in the flowchart. Is there some method here I'm unaware of?

This is just from a tutoring website, so it's nothing too official. I was thinking of using it in my class but that part of the chart looks a little iffy, so I thought I'd check here first before handing it out to my students.",137xspb,mjk1093,1683229638.0,6,1.0,"['The chart goes wrong at the first split.\n\n(When I saw the mu on the first arm I was encouraged because I thought ""oh they\'re going to address hypotheses in the chart"" but they don\'t; merely mentioning some parameters. But that sense of encouragement was a rather brief moment.)\n\nThe ""question"" there at the top is *type of data*, but you can see that it has *population parameters* on the arms, which is not about data but about populations. (what it should probably start with is making it clear whether we\'re looking at one or two samples, but given where we end up in the part you\'re asking about, it looks like it must be one-sample)\n\nIt commits the usual error of ignoring the specific hypothesis; it sort of nearly gets there by mentioning parameters but not how they relate to any hypothesis (nor even how many there are), but then the mention of *data* is just nonsense. Whatever they meant to do here, they didn\'t do it right, but in any case there would be several changes to fix just the first split\n\nBut lets take it at its word and assume that the whole left arm is, as labelled, about *means*.\n\nThen putting the n>=30 split next .. that\'s just dumb organizationally in terms of the flowchart, which is why it ends up pointlessly duplicating the t/z test bit. It\'s also just bad statistically (n vs 30? That\'s just demonstrably wrong), but let\'s follow it along and assume n<30\n\nThen if we don\'t assume we have normality, we head right again.\n\nWhat\'s this, proportions tests? WTF?\n\nLook back up the top, at the first arm on the left. We\'re in an arm for *testing means*.  Neither of these tests are for testing means. They\'re presumably thinking about doing a *one sample* test for a *median*.  I can\'t really see how it makes sense otherwise, at least not without a lot of handwaving.\n\nBut to do that (go to a test of medians), they just changed the hypothesis, which was about means not medians. Sadly the flowchart doesn\'t explain the reasoning so we\'re left to guess what it\'s thinking.\n\nA flowchart does not get to tell me what my hypothesis is; while we didn\'t get to say what it was, we already established that the hypothesis was at least about means\n\nSuitable tests for means in this non-normal arm (if the previous splits had made sense) would be \n\n1. Choose some other distributional assumption and test means with that. e.g. via a generalized linear model\n\n2. Don\'t choose some other distributional assumption and test means nonparametrically, perhaps via a permutation test or maybe a bootstrap test (though if n<30 perhaps the sample size would be too small to get accurate alpha with a bootstrap).\n\n> that assumes symmetry\n\nDoes it? A binomial test for a median? Oh, unless you mean ""in order for a test of medians to be a test of means"" I guess.', 'Yuck. Teaching/learning stats is so much easier when one gets out of the shackles of compute-limited tradition that leads to flowcharts like this and instead embrace modern generative inference.', ""I'd say no. A binomial distribution would still be used if the samples were being replaced."", 'I don\'t understand *all* of what you wrote, but I am going to address these two points:\n\n> Don\'t choose some other distributional assumption and test means nonparametrically, perhaps via a permutation test or maybe a bootstrap test (though if n<30 perhaps the sample size would be too small to get accurate alpha with a bootstrap).\n\nYeah, that is what I currently have in my recommendations chart, looks like I will be sticking with that.\n\n>>that assumes symmetry\n> \n> Does it? A binomial test for a median? Oh, unless you mean ""in order for a test of medians to be a test of means"" I guess.\n\nYes, that\'s what I meant.', ""Can you expand a little more on what you mean? I looked up generative inference and it doesn't seem like something you can easily apply in a classroom setting. Can you give an example or two?"", '> I don\'t understand all of what you wrote\n\nMy apologies. By all means feel free to ask me to clarify anything that\'s insufficiently clear.\n\nBesides that issue of importance of being specific about hypotheses, I\'m especially concerned that so many of these charts fail to consider other parametric assumptions than normality\\*, especially when in many cases a reasonable parametric choice may be fairly natural (e.g. models for counts, durations, etc) and so many of them are readily accessible; not just what you get in a typical implementation of generalized linear models, either -- with hardly any effort parametric survival models should offer a host of additional non-exponential-family models, and beta regression\'s not hard to find either, all of which should allow for typical tests of equality of of one population mean to a hypothesized value or equality of two, three or more population means.\n\nHowever, I wouldn\'t just reserve such things for small samples, since it\'s not just significance level correctness that people need to worry about, but also power; if your sample size is not-tiny because you want power against small effects, relying on large sample approximate normality of sample means to take care of the significance level doesn\'t help with that; you really want the option of other parametric tests - or nonparametric tests - at any sample size.\n\n> Yeah, that is what I currently have in my recommendations chart, looks like I will be sticking with that.\n\nIf you have resampling tests (like permutation test or bootstrap test) of means in yours then it sounds like yours is considerably better.\n\nIf you get to being clear about what hypotheses you might want to test, so much the better. \n\n> >  ""in order for a test of medians to be a test of means"" I guess.\n\n> Yes, that\'s what I meant.\n\nOkay, then yes, symmetry\\*\\* would be a sufficient (but not necessary\\*\\*\\*) condition for a one sample test of medians to be a one sample test of means (as long as means are finite, at least). If they meant that, they really would have to state that assumption/restriction in the chart, because a naive user of it isn\'t going to be able to infer that from the chart.\n\n---\n\n\\* and fail to mention other nonparametric tests than rank tests, and typically fail to consider anything but location-tests at all\n\n\\*\\*  though for correctness of significance level you really only need it  under H0, as with most such assumptions; the constraints under the alternative could be milder and still preserve the same ordering under H1 (you\'d just need that the population mean-median difference grows substantially more slowly than the median grows from the hypothesized value to guarantee you were on the same side of the alternative)\n\n\n\\*\\*\\* it\'s possible for (a) population mean and median to be equal under asymmetry and (b) be in a situation where population mean and median *differ* but a change in one implies a proportional change in the other. Consider for example, an exponential distribution. The median is 69.3% of the mean, so if I assume an exponential I *could* convert my mean-hypothesis to a comparison of the median with 0.693 .  In that specific (exponential) case you wouldn\'t want to do it that way -- you\'d be much better off with a chi-squared test based on the test statistic n/ (with n d.f.) -- or you could use a generalized linear model to achieve the same end, but it serves to illustrate that there are some simple exceptions to the requirement of symmetry if you have some suitable model.', 'See recordings of my lectures [here](https://youtube.com/playlist?list=PLu77iLvsj_GNmWqDdX-26kZ56dCZqkzBO) (particularly 8 onwards). Also a few good textbooks: McElreaths Statistical Rethinking (also has YouTube lectures), & Kruschkes Doing Bayesian Data Analysis. \n\nIn my experience, students grasp it much easier. No complicated estimator formulae to memorize nor tables to look up critical values. You simply work out an algorithm to generate fake data, using a handful of shapes (distributions) to reflect variability structures and variables whose precise value can reflect your core research questions. For most scenarios the most complicated math needed is mere arithmetic. After some checking that your algorithm indeed generates real-looking fake data, you place it plus real data into the magical black box that is MCMC, and out pops quantities reflecting the range of credible values for your variables of interest.', 'Thank you for the very detailed reply. I am looking to modify a chart for an introductory course, so a lot of what you mentioned is going to be way beyond anything we would be doing in class, but it certainly gives me some avenues for continued study of my own.\n\nI especially liked your last footnote, I had not considered that possibility!', ""Interesting, I will check them out. I am unfortunately constrained by what is in the textbook for most of what I teach. I can't replace parametric tests in the curriculum, but I also want to give my students a sense of what is out there beyond the standard textbook approach. Right now I only mention bootstrapping and non-parametric tests but it's always good to know more.""]"
[Q] How to conduct a post-hoc power analysis for an (partial-)proprotional odds model?,"Hello!

I have a question about conducting a post-hoc power analysis for a (partial-)proportional odds model. I have some experience with G\*Power, but I was not able to conduct a post-hoc power analysis there. My Models each consist of one ordinal response (4-point scale), one predictor numeric and one predictor ordinal (4-point scale) variable, making it difficult to use logistic regression which is the only form that seems to match in G\*Power.

I have already conducted the necessary analyses, have the odds ratios and calculated the McFadden Pseudo R2.

Is there a way to conduct a post-hoc power analysis for my (P)PO models? (e.g. alternative software, a r-command)

Alternatively, would it be possible to use logistic regression in G\*Power instead? If so, how would I go about doing that?

Any help or advice would be greatly appreciated! Thank you very much in advance! (If you need any more information about the analysis, I'll gladly provide it.)  


*Edit: Missing Letter.*",137xma9,NightmareAkin,1683229245.0,3,1.0,"['Post hoc power analysis is not a conceptually valid thing. See [this](https://gpsych.bmj.com/content/32/4/e100069) or [this](https://data.library.virginia.edu/post-hoc-power-calculations-are-not-useful/).', 'Thanks for the clarification, it helped me a lot!']"
[Q] How can forecasts be increasing if the coefficient is less than 1 in an AR1 model?,"In order to compare the forecasts of another model, I estimated a simple AR(1) model of log commodity prices with daily observations using *statsmodels* in Python. For crude oil I got an estimate of 0.9983339 for the lag and 4.21 for the constant.

 I wanted to calculate the forecasts generated by the simple AR(1) model, I do that with the code

    ar1_model = sm.tsa.ARIMA(endog=log_data, order=(1, 0, 0), trend='c') ar1_results = ar1_model.fit() forecasts = ar1_results.get_forecast(steps='2022-12-30') forecast_mean = forecasts.predicted_mean 

My problem is that the forecasts generated are an increasing function of time. I don't see why that is the case, as the autoregressive coefficient is less than 1, so shouldn't that mean that the forecasts ought to be decreasing?

I suspect that the confusion comes from the constant term and the inclusion of a trend, but I figured that this constant term here cannot be equal to the trend as that would mean that for each timestep logprices are estimated to be increasing by 4.21, which cannot be the case. However, if there is a trend, I don't see why its estimate wouldn't be given by the estimation results.

I would appreciate any clarification regarding how this works.",137wzmp,horux123,1683227845.0,3,1.0,"[""Each forecasted period is a simple linear addition of the previous day and a constant.\n\nForecast day 1 uses the last complete day from the AR(1). You basically multiply the coefficient (based off of history) of that with the result of that day, and add a constant based off of your trend.\n\nI would expect to see a different model if you just use auto ARIMA (it's the `fable` package, correct?) and I also suspect that your portmanteau test (Ljung-Box) shows an underfit"", ""What does the help say about 'trend=c'?"", 'The documentation basically just says that we can choose ""c"", ""n"", ""t"" to choose what type of trend term we want. However, I also tried estimating the model with the *SARIMAX* method (of which *ARIMA* should be a special case) and there for the results, instead of an estimate for the constant, I get the actual intercept, which does seem to work as the trend in prices.\n\nThe problem was probably just it is never really explained what the estimation results for ""constant"" mean and that the actual intercept estimate isn\'t given with the *ARIMA* method.']"
"[Q] When using the SARIMAX model in Python, can I recreate the Confidence Interval for each of the forecasted value using the simulation?","I am trying to forecast quarterly population volume on the differenced population data. The issue is that the model.get\_prediction(start = start, end = end, dynamic = False).summary\_frame() calculates the Confidence Interval values on the differenced data. Therefore, I cannot reconcile the actual quarterly population forecasts with the model's calculated Confidence Interval. Was just wondering if I could use the simulation to circumvent that since I need to take cumulative sum to derive the actual forecasts.

Thanks!",137o73o,PythonEntusiast,1683211858.0,7,0.82,"['Have a look at the forecasting packages  written in R by Hyndman et al. You can find awesome books (either fpp2 or fpp3) on otext.com which accompany these R packages and explain in great detail how all the maths work. I would recommend you check that book out and have a go at this R package combined with your data so you can have something to validate your own Python script with', 'Alright, boys and girls, buckle up. Here is what you need to do.\n\n1) Fit your model on your data.\n\n2) Run diagnostics on it to make sure it makes sense.\n\n3) Create n samples over the next t periods using the simulate() method of TS model of your choice. \n\n4) Add the last observation from your original data to the first observations of the new data and then find the cumulative sum over then t periods for your simulated data.\n\n5) Then simply calculate the mean, upper bound, and lower bound for the 95% confidence interval for your cumulatively summed data over the t time periods.\n\nEasy peasy lemon squeezy.']"
Are there any rules of thumb for sample size for hierarchical linear regression? [Q],"Hey all, 

Im wondering if there are any literature-supported rules of thumb for sample sizes that are sufficient for hierarchical linear regression. I know that Green (1991) recommends N = 50 + 8m for multiple linear regression, but Im not sure if that can be adapted for hierarchical linear regression with 2 blocks (2 predictors for the first block, and 1 predictor for the second block). Note that I cannot do an a-priori power analysis, as the sample has already been recruited (that was an oversight by me as this was the first study Ive ever done).

If you know if any papers that offer some simple sample size recommendations/rules of thumb for hierarchical linear regression that would be amazing. Or if you can tell me whether I can just use greens formula, that would also be helpful. Thank you!",137n7yd,TheEudaimonicSelf,1683210986.0,1,1.0,"[""If the sample is essentially set at this point, I don't think I would bother with this power calculation. Assuming you do this and find that your sample is too small, what are you going to do? Can you recruit more participants?"", ""This is tough to estimate, in my experience. I don't know of any rules of thumb. The recommendations I see call for simulations to make the power estimates. Jake Westfall has some webapps on his home page that might help with this.\n\n[https://jakewestfall.org/](https://jakewestfall.org/)\n\nEdit: For some reason, his webapp links don't appear on his home page. But they are on his resume page: http://jakewestfall.org/resume/"", 'Well, we used Greens rule of thumb prior to recruitment because we didnt have an a priori idea of what effect size we were expecting given that our construct of interest is a relatively new construct with very minimal research (like a few papers, with no regression analyses). Therefore we werent able to do an a priori power analysis, which requires expected effect sizes from what I understand. Greens rule of thumb is good for medium effect sizes, but in our analyses we detected significant medium to small effect sizes. So thats basically the situation.', 'Ill not familiar with Greens rule of thumb unfortunately, but I think that as long as the confidence intervals look reasonable, theres no reason to do a post-hoc analysis. Sounds like you may have overestimated the variability initially and got better results than expected, which is a nice problem to have! Normally people do these post-hoc power analyses because they dont get significant results and want to blame it on being underpowered (i.e. saying this wasnt statistically significant but it would have been with a larger sample). \n\nBased on what youve said, I think youre doing things correctly!']"
[Q] Change over time with (POSSIBLY) unpaired sample,"Currently working on a project. Wave 1 had a sample size of n=2000. We are currently collecting wave 2 sample n=2000. We did online data collection and will try to reconnect with the same sample as wave 1 but there's a good chance that Wave 2 will have some new people ranging anywhere from 1 new person to 2000 new people. not sure. 

My Question is:

1. at what sample size will a sample go from paired to unpaired? i.e. can we consider the sample paired if we have 1999 people from wave 1 and 1 new person, how about if we have 1000 new people and 1000 people from wave 1?  
2.  is it possible to look at change over time with a (potentially) unpaired sample?
   1. if so what stats test will that be?",137n15z,Slayer_of_Truth,1683210815.0,1,1.0,"['> at what sample size will a sample go from paired to unpaired\n\nIt\'s no longer strictly paired as soon as you have some unpaired people, and it\'s not longer strictly unpaired as soon as you have some paired people. In between you have *overlapping samples*.\n\n> i.e. can we consider the sample paired\n\nNo, this is *not* a matter of *id est* (""that is""); it\'s a different question with a distinct answer. The previous question is about what *is*, this one is about what you *can do*.\n\nClearly if you have a single unpaired person, there\'s little information loss in omitting them\\*. But if you had 1990 people unpaired in each sample and only 10 pairs, leaving out the unpaired people would be consequential. On the other hand, ignoring the pairing or even leaving out those 10 altogether would probably not be. (How consequential depends on how correlated the paired values are. Very high correlation means that the pairs have a lot of information, but near-zero correlation means there\'s little loss in treating them as unpaired.)\n\nNote that in between the extremes, there\'s methods for overlapping samples. If I had say 40% paired data and 60% unpaired values in each wave, unless the pair-correlation was unusually high or unusually low I\'d very much want to try to avoid the loss in either dropping the unpaired or ignoring the pairing.\n\n> is it possible to look at change over time with a (potentially) unpaired sample? \n\nPossible, yes, but the difficulty is that if there are no paired values (indeed, in many cases, even if there are), you have to find a way of attributing differences to the thing you\'re interested in rather than the alternate explanation that it\'s due only to other factors than what you want to look at, whether events that ""intervened"" or drivers of the response that changed over time. You would have to worry about issues like omitted-variable bias, taking steps to deal with such issues, or your conclusions would be suspect. \n\nThere\'s not enough information to say a lot, though (and I might not be the best person to offer a lot more detail if there were).\n\n(When you say \'online data collection\', is your sample self-selected?)\n\n---\n\n\\* however, the reason *why* they\'re missing can certainly impact *bias*: you can\'t just blithely assume there\'s no consequence to either omitting *or including* people only present in the first sample -- e.g. if they\'re missing in a way that relates to their observation, you\'ve got problems. https://en.wikipedia.org/wiki/Missing_data#Types', ""If by self-selected you mean participants choose to participate then yes. and thats the problem. we will TRY and get the same people as wave 1. BUT we have no way to guarantee that they are going to participate in wave 2.\n\nThe problem is we NEED 2000 people to participate in Wave 2. so its not a matter of excluding people who didn't participate in Wave 1 and just analyzing the people who did (i.e. only using the 1900 who participated in wave 1 AND wave 2). So it sounds like you are saying is that the sample is not going to be paired if we can't guarantee the same 2000 people as wave 1 (and we are not going to be). since the goal isn't to get as many of the same people who participated in wave 1 but to get 2000 people it sounds like we will be working with overlapping sample.\n\nWhat are my options then if we have overlapping sample and we want to look change over time. Am I just limited to a unpaired t-test and look at group differences. or is there a fancy way to look at change over time with overlapping sample?""]"
[Q] Navigating Differences in Scale: A multivariate approach with standardization or separate component analysis?," I am conducting a research study on sex differences in the physical limitations of patients with ankylosing spondylitis (AS), using the **Bath Ankylosing Spondylitis Metrology Index (BASMI)**. The [BASMI](https://www.mdapp.co/basmi-score-bath-ankylosing-spondylitis-metrology-index-calculator-616/) is a composite score that consists of five components, including cervical rotation (measured in degrees), tragus-to-wall distance, intermalleolar distance, modified Schober's test, and lumbar side flexion. Each component is scored on a different scale, with most of them being distances measured in centimeters, but each having a distinct range. For example, intermalleolar distance can be up to 120 cm, whereas tragus-to-wall distance is usually between 5-15 cm.

 To account for the differences in scale, I am considering two approaches for my analysis: (1) standardizing each component by converting them to z-scores and performing a multivariate linear mixed model analysis, and (2) performing separate analyses for each component (i.e, a univariate analysis). My main research question is focused on analyzing sex differences in the physical limitations of the patients, and how these differences vary across the individual BASMI components over time after treatment.

However, I am concerned about how to interpret the regression coefficients if I choose to standardize the components, as well as how to explain the implications of this approach in my research report or paper. On the other hand, performing separate analyses for each component may result in a loss of statistical power due to smaller sample sizes for each component, and may not account for the correlations between the components.

I would appreciate any advice or guidance on the advantages and disadvantages of these approaches, and how to choose the most appropriate approach for my specific research question and data.",137my9l,pashtun92,1683210745.0,2,1.0,[]
[Q] HELP ! what test should I use ? Comparing test times before and after having caffine or decaf,"Desperate! , I was thinking independent samples t-test but I have 2 metric and 2 categorical variables so its confusing. 

1. My participants are either allocated Into caffeine group , or decaf. 
2. They complete there first test with without drinks. 
3. After completing, I record the times it took for each person to finish the test 
4. I give them their allocated drink , 
5. wait 30 mins 
6. they complete a second test. 
7. compare the test times of before and after - to see if caffeine will make them finish quicker",137mouz,inquisitive-chick,1683210523.0,1,0.67,"['You have a mixed design on your hand! A time predictor and a treatment predictor.\n\nYou can do this with a 2x2 repeated measures ANOVA. If you want to evaluate score and time on the tests, you could use a multivariate procedure (MANOVA), however I suspect that this would be something you need to spend time learning. So, I would suggest just doing a 2x2 repeated measures ANOVA for each outcome.\n\nThe 2x2 refers to having factor one with two groups and factor two with two groups (time & treatment)', ""One possibility would be to look at a regression model, where you have a model for the after times as the response (DV) with the before times is a predictor (IV), where you'd then fit treatment and look at interaction of before-time with treatment.  after = before + treatment + before x treatment + error\n\nThere's issues with times, however. They tend to be right skew and more variable as the mean increases (heteroskedastic). I'd probably want a generalized linear model rather than a regression (a gamma glm would be the most obvious way to deal with all those issues at once).\n\nYou'd probably want to do mixed models as well, to deal with subject effects as a random effect, so that would be glmm, but I expect we're getting some way beyond what you're able to deal with."", 'Thankyou so much ! You saved my life . Ill will get on it asap', 'Repeated measures ANOVA', 'So it would be the test is the factor and the caf/decaf is the groups', 'So it would be the test is the factor and the caf/decaf is the groups ? And then I do it for test 1 and 2', 'Are u refering to two way repeated measures ANOVA?', 'Test 1 and test 2 are your time variables. Think about it, the main thing youre interested in is pre-treatment and post-treatment', 'Im not sure what to do , I also have to figure it out on SPSS', 'Sorry Im not good at any of this stuff. Ive only ever done t-test for very basic foundations of statistics \n\nSo Im guessing \nTest 1 and test 2 are the groups \nand the factor is coffee/decaf ? \nIm so sorry', 'Doing a 2x2 repeated measures ANOVA or two-way repeated measures ANOVA would be an option if your participants got both decaf and coffee, and if they didnt you cant do the above', 'So in SPSS, for a mixed ANOVA, and not two-way repeated measures ANOVA, select treatment as your between-subject factor and time (pre post) as your within-subject factor, and test time as your DV/measure name. (Analyze > Generalized linear model > Repeated measures)']"
[Q] Linear mixed model with a variable that repeats itself as IV,"Hi all,

I'm running a LMM with the formula : `DV ~ audioStim + visualStim + x_Audio + (1 | participant)`

My dataset includes multiple measurements from each participant (DVs), audioStim has 3 levels (representing 3 audio stimuli) while visualStim has also 3 conditions (3 visual stimuli). x\_Audio, however, is continuous variable that measures the perception of the participant for each audio stimulus.

I am wondering whether there are any issues with including x\_Audio as a repeated variable in the LMM. Can you provide guidance on whether this approach is appropriate?

This is an example of my dataset:

|participant|audio\_stim|visual\_stim|x\_AudioStim|DV|
|:-|:-|:-|:-|:-|
|1|A|X|3|0.35|
|1|A|Y|3|\-0.14|
|1|A|Z|3|0.5|
|1|B|X|2|0.1|
|1|B|Y|2|\-0.05|

&#x200B;

Thanks in advance!",137igbi,nirvana5b,1683201532.0,15,0.94,"['There should in principle not be a problem. \n\nHowever, if it is a continuous variable, how can it be identical across multiple measurements? That sounds to me like either it is not a continuous variable, or there is additional nesting of observations which you did not mention.', 'What is the question are you trying to answer, and how does the model syntax answer it?', ""As long as the data are coded to reflect the various levels (e.g., dummy coding) it shouldn't be a problem."", 'In a linear mixed-effects model (LMM), it is possible to include continuous variables as both fixed and random effects. However, in your case, x\\_AudioStim is not a repeated variable, but a covariate, as it is not varying across the measurements of each participant. Therefore, including it as a random effect would not be appropriate. Instead, you can include it as a fixed effect in your model.  \nRegarding the structure of your LMM, it seems reasonable to include a random intercept for participant to account for the repeated measures within each participant. However, you might also consider including a random slope for audioStim if the effect of audioStim varies across participants. You can check whether this is necessary by comparing the model fit with and without a random slope using a likelihood ratio test or an information criterion such as AIC or BIC.', ""In fact, it's an evaluation that the participant does about the audio stimuli on a 5-point likert scale, which I'm assuming it's continuous hehe\n\nParticipant evaluates it one time alone."", 'My main question is whether or not audio\\_stim or visual\\_stim or both (interaction) modulate DV (which is a physiological measure). But I have to control for x\\_AudioStim.', 'You dont need to assume anything about it, because you defined it ;)\n\nAnd its not continuous if it has discrete points. So you should model it as an ordinal variable.', '> 5-point\n\n&#x200B;\n\n>continuous', 'Since x_Audio seems to be a measure of individual difference and you are not interested in it as part of your research question, you may consider coding it as a random effect. If you believe the relationship between x_Audio and your DV varies across participants, treating x_Audio as a random effect allows the effect to vary across participants. This provides a more accurate estimate of the effect(s) you are interested in per your answer. \n \nLikewise, this would also allow you to estimate the variability of x_Audios effect across the hierarchical levels in your dataset. \n \nJust my two cents. Be sure to ask your supervisor/prof/university stats consulting service (if available).', '>And its not continuous if it has discrete points. So you should model it as an ordinal variable.\n\nYeah, the scientific literature has more flexibility about this.\n\nThis is an example: https://www.frontiersin.org/articles/10.3389/feduc.2020.589965/full', 'Maybe they can be treated as such, which is fine. But it doesnt mean its a continuous variable. In many cases, the distinction does matter. For all I know it wasnt an ordinal variable but a categorical one, in which case you really cant treat it as a continuous one. \n\nIf your software has the option to encode it as ordinal, why not use that?', 'Because I have a problem with sample size , if I treat it as continuous the model has to estimate one parameter alone, instead of 4 if I treat as ordinal.', 'Why would the number of parameters change depending on whether its an ordinal or continuous variable?', ""If I include a 5 point Likert variable, the model would have to estimate 4 parameters for that variable, as it's dummy coded."", 'Ah yes, thats true indeed. Maybe your best choice is indeed to treat it as continuous. Im personally not familiar enough with the theory to know when thats appropriate. My intuition tells me that estimating a coefficient on a continuous variable is a lot harder than estimating coefficients for a few binary variables. And I personally would not know how to explain the outcome of the model. A coefficient on a continuous variable tells you how to estimate the DV for an infinite number of unobserved values of the IV. But there are no unobserved values, because its an ordinal variable so what does the coefficient mean then?\n\nIn any case, when in doubt you must choose the more conservative option, which in this case I think would be to dummy encode it. But, that Im not sure about.']"
[Q] Chi square test or Mc Nemar?,"My data looks like this (contingency table). Two measurement approaches that should measure the same underlying behavior. 

I need a signifiance test in order to report coefficients that both measurements differ significantly. The behavior measured is quite complex and there is no ""true"" or validated measurement.  Measurement 2 was after Measurement 1. Which statistical procedure would be the most appropriate?  


&#x200B;

|||Measurement 2||
|:-|:-|:-|:-|
||| Behavior not identified | Behavior identified |
|Measurement 1| Behavior not identified | 349 | 116 |
|| Behavior identified | 60 | 126|",137dudx,skippydi34,1683187266.0,3,0.81,"['*Perhaps* the way you\'ve described your hypothesis, this would be McNemar\'s test.  The test will compare the 116 count to the 60 count, concluding that more individuals switched to ""identified"" from ""unidentified"" than vice versa.\n\nYou might be asking a different hypothesis.', 'Thanks! I think you got my idea right.\n\nWe actually invented a measurement (measurement 1) to capture the behavior. We then directly asked them if they think that show the behavior (measurement 2). The behavior is quite seldom and a little bit complex and technical but there is a possibility that people think they may do it although they don\'t. So it\'s not easy to assess.  Thus, there is no ""real"" measurement, just two approaches compared and the results have to be discussed.\n\nChi square does not fit because the groups are not paired?', 'A chi-square test of association tests something different.  If *Measure 1* and *Measure 2* are correlated.', 'Ah okay.\n\nAnd for McNear: Is it a problem that the kind of measurement differs? As I described later, we had two measurement approaches. Further, I do not think that the two measurements are independent.', 'If you can arrange the table meaningfully as you did, then the test is appropriate.  (Assuming other assumptions and stuff).']"
[Q] [C] Assessing exponential decay model against data with both random and systematic errors in the dependent variable data points.,"
I have a data set that I know would strictly follow an exponential decay function if it were free of error. I even know the decay constant with certainty. I do not know the initial value and there is a systematic error that will grow with time. I want to fit initial values to the data points on a rolling basis and then quantify how well the data fits the decay model with the know decay constant and the fitted initial values. This should allow me see when the systematic error becomes too high and truncate the data points past that so that I can make a good estimate of the initial value and project forward using the model without the systematic error. 

I have used the standard transformation to a linear regression and have calculated a rolling set of initial values using the data points and the known decay constant. I tried calculating R2 = 1 - RSS/TSS with the transformed data points ( z = lny = a+b*lamnda). None of them are very close to 1 and they eventually go negative as the systematic error gets out of hand. It is possible that there is no interval of independent variable for which a credible exponential model can be fitted. I am also concerned that R2 might not be an appropriate metric as it's assumptions only hold for linear regression. I do not know it can used for a nonlinear exponential model that has been transformed into a linear one. R2 might even be valid for the transformed model, but can't be back converted to the exponential one.

I am here to listen.",1376rtt,GreenNukE,1683166023.0,3,1.0,"[""Can you describe the sort of variable you're observing? e.g. is it a concentration (say, one substance in another), a count, a proportion, an angle, a length, ...\n\n> there is a systematic error that will grow with time\n\nCan you describe how this systematic error operates? \n\nCan you also describe how the noise (unsystematic error) would operate?"", ""Variable is a rolling average of the count rate of a radioactive source. Systematic error is knuckleheads recording values based off of what the source has 'always' read rather than as independent measures that would reflect natural decay. Random error should be gaussian. \n\nThese details are less important than determining a valid metric for comparing the estimated initial values to see if there was an interval early in the data set in which the systematic error was not obscuring the natural decay rate."", ""> Random error should be gaussian. \n\nNot if it's averages of counts, no. That might be an adequate approximation, but obviously not the actual distribution \n\n> knuckleheads recording values based off of what the source has 'always' read\n\nSo a constant value for a given source?\n\n> These details are less important than determining a valid metric \n\nThe details are *critical* to a serviceable model, and thereby to a good way to approach the question.\n\n> I am here to listen.\n\nI detect a slight contradiction,"", ""The random error distribution may be worth poking at, but the systematic is what is giving me grief. \n\nThe Sr/Y-90 source is used in the field to check that a given instrument is reading normally with a 20% tolerance. The source check readings made recipt from the calibration facility are used to calculate the rolling average, which should be decreasing per the 28.79 yr half-life. Knuckleheads have been inadvertently using the variability of the needle movement to record readings closer to what they are used to and retarding the decay of the rolling average that was supposed to trend down to follow the source decay. After enough time passed, the rolling average had diverged so far from the true source response value that even knuckleheads couldn't fool themselves into recording readings that were within 20% of the corrupted rolling average. I am trying to tease out a defensible estimate of the initial true instrument response to the source from early data points for which the systematic error should be relatively small. I could then decay that value, trusting physics, and give them a good value to perform the field source checks against.\n\nWe have a plan B of manually checking all the source and instrument type combinations, but that would entail a lot of leg work. I want to make an objective determination as to if it is possible to straighten this out with math or be able state that we really do need to do it the hard way. Thus, I need a valid metric to assess how well my exponential regression model fits subsets of data points.  If I can get a good fit, I can say this subset is not excessively corrupted by systematic error and the fitted initial value from it can be used with confidence to seed the new decay correcting source check value that will replace the rolling average.""]"
[Q] How to find the Confidence Interval values for each of the forecasted value if I trained the SARIMAX time series model on the differenced data?,"I have trained my model on the first differenced population volume for the last 93 quarters (Data is [https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901](https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901)). I have then trained the model on the differenced data. The issue is  that model forecasts the differences. The easy part is simply adding the last observation from the original data to the forecasted differences to get the actual forecasts. The issue is that the model.get\_prediction(start=start, end=end).summary\_frame() returns to the Standard Error and Confidence Interval bands for the differenced data. Therefore, how can I calculate the confidence interval values for each of the forecasted observations that were cumulatively added?",1371czs,PythonEntusiast,1683152111.0,5,1.0,"['I tried asking ChatGPT, but it does not understand.', ""There's always bootstrapping""]"
[E] Which courses should I select? (MS degree) or other recommendations.,"I have to choose two of the following classes for my degree: 

Machine Learning (svm, neural nets, regression trees, bagging, boosting, etc, not sure what language will be used, focused on classification)

Advanced Data Analysis (covers time series, nlp, done in MATLAB (?)) 

Time Series (box-jenkins, seasonal and non-seasonal, done in SAS) 

 I honestly don't know what would be the most useful. I'm also allowed to take classes in other subjects as a grad student. Any recommendations would be greatly appreciated. Thank you. I think I'd mostly like to come out with some good projects that I can add to my portfolio. I am interested in becoming something of an analyst in the future.",137165o,ApprehensiveWar2252,1683151669.0,3,1.0,"[""Time Series is nice, my own class on that also used SAS. I wish we'd used R or Python, but it is what it is. The machine learning class would be good too, hopefully it uses R or Python. I'd probably not bother with the advanced data analysis class since you'll probably be able to pick up that stuff on your own after taking more ML and stats classes. Not very informative, but it's my two cents.  \n\nI don't know if you've had a class on Stochastic Processes or on Bayesian Data Analysis but those two courses are highly recommended for a MS degree too."", 'Dont take any classes that teach stats/data science with matlab. Waste of time. Ive never, ever heard of nlp being done in matlab. Wtf? \n\nJobs will want you to know R and/or python. SAS is mostly used in pharma industry (although my knowledge this fact may be out of date, so double check this)']"
[Q] ANOVA versus MANOVA for answering research question? (GraphPad PRISM),"Hi everyone! I'm a researcher, but by no means a statistician, and I've run into a little statistical snag that neither my advisor nor committee member can help with. Thank you in advance to anyone who reads and responds!

In brief, I'm looking at whether Status and Context have an effect on Test Scores (measured in 5 domains: Communication, Gross Motor, Fine Motor, Problem Solving, and Personal Social). 

Originally, I analyzed the data from each domain using a 3-way ANOVA with sex, context, and status as the between-group variables. When a significant F statistic was identified, I ran post-hoc tests for planned comparisons using Holm-idk's multiple comparisons test.

However, my committee member is asking why I didn't run a MANOVA instead of the ANOVAs for each domain (as this inflates my risk of Type 1 error). 

From my understanding, there are a few things I am wondering:

1.  A MANOVA would allow me toassess the overall relationship between the independent variables (context and status) and the dependent variables (test domains) while controlling for the intercorrelations among the dependent variables. However, with regards to answering the research question, we were not looking for any intercorrelations among the dependent variables (e.g., whether the scores in one domain are related to the scores in another domain). Does it still make sense to run a MANOVA, then, if it will conduct more comparisons than we need? Doesn't this pose issues with inflating the Type 1 error as well?  (Again, not a statistician, so I apologize if this is a pretty dumb question).
2. If I had conducted a MANOVA, I would have had to conduct follow-up univariate ANOVAs afterwards to determine which domains were driving the significant differences, right?How is this different from simply starting with the univariate ANOVAs, finding that there are significant differences, and then adjusting the threshold for multiple comparisons in the post-hoc planned comparisons? 
3. As well... is it even possible to run a MANOVA on GraphPad PRISM? I can't find the option in the Analysis drop-down, and nothing online is giving me a clear answer.

Thank you again!",136vt9e,worstgurl,1683139547.0,2,1.0,"['>If I had conducted a MANOVA, I would have had to conduct follow-up univariate ANOVAs afterwards to determine which domains were driving the significant differences, right? How is this different from simply starting with the univariate ANOVAs, finding that there are significant differences, and then adjusting the threshold for multiple comparisons in the post-hoc planned comparisons?\n\n[Here\'s](https://stats.stackexchange.com/questions/133297/what-is-the-null-hypothesis-of-a-manova) a relevant nugget:\n\n>The answer is that running all univariate ANOVAs, even though would test the same null hypothesis, will have less power. See my answer here for an illustration: How can MANOVA report a significant difference when none of the univariate ANOVAs reaches significance? Naive method of ""combining"" (reject the global null if at least one ANOVA rejects the null) would also lead to a huge inflation of type I error rate; but even if one chooses some smart way of ""combining"" to maintain the correct error rate, one would lose in power.\n\nAnyways in the scenario where you start with a MANOVA, follow-up tests would be contingent on rejecting the null. You\'d be using it as an omnibus test to (weakly) control the type I error rate, and under certain circumstances pick up on differences that you otherwise wouldn\'t have with individual ANOVAs.', 'Sometimes there are interesting multivariate interpretations of MANOVA. See [this article.](https://psychology.okstate.edu/faculty/jgrice/personalitylab/Grice_Iwasaki_AMR.pdf)', 'Thank you for sharing this!', ""Thank you for sharing this article - I'm reading it right now!"", 'No problem!\n\nI\'d suggest digging deeper on the topic because there are [additional considerations to be made](https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=2185&context=jmasm) in controlling error rates. The omnibus test may only have an X% change of being a false positive in the null hypothesis is true, but if you end up with a false positive you\'re exposing yourself to the risk of additional false positives when following up with univariate tests. Conversely, a significant MANOVA might only reflect a ""true"" difference in one of the follow-up tests, but won\'t protect against the rest being false positives.']"
[Q] kruskal test is significant but Wilcoxon rank sum test shows none are,"Hello everyone,

I was wondering if someone could please help me in that. I am trying to see whether habitats are microbes found in controls or influences the number of genes in a specific group (e.g. number of transporters or CADzymes or COGs).

More specifically is to compare whether different habitats have different number of genes. I was told to first do a kruskal test to see if there is significance difference between groups, followed by a Wilcoxon rank sum test to see which groups are different. 

Therefore the kruskal test has found significance (p-value = 0.0006427)  difference between habits and number of genes. However when I do Wilcoxon rank sum test all groups are highly insignificant (p > 0.25).

As a result could someone please help me in why this might be so or why this is occurring?",136veo5,MountainNegotiation,1683138638.0,9,1.0,"[""1. Don't use plain rank sum tests as post hocs for kruskal wallis. There's a number of closer-matching alternatives.\n\n   e.g. see the answer [here](https://stats.stackexchange.com/a/515697/805)\n\n2. Whatever post hoc you use, this sort of thing will still happen. It happens with anova and t tests for example\n\n  The joint acceptance region of pairwise tests is a rectangular (hyper-)prism. The joint acceptance region of a single omnibus test is not a prism. E.g. for anova its an ellipsoid. Even for the best possible choice  of pairwise post-hocs, parts of the ellipsoid stick out, outside the prism and vice versa. See this diagram https://i.stack.imgur.com/4woXw.png (from here: https://www.reddit.com/r/AskStatistics/comments/wygvj8/insignificant_ftest_but_significant_coefficient/ilxovzi/)\n\n   Kruskal Wallis has [a more complex boundary](https://stats.stackexchange.com/a/76080/805) but the story is quite similar in that parts of it can stick out of the pair-wise test's acceptance-region (still a rectangular prism).\n\n3. There's an additional issue with pairwise rank sum tests vs a Kruskal-Wallis test beyond what happens with t-tests and ANOVA (though it goes in the opposite direction); the ordering relation that the rank sum test looks at is *non-transitive* (A>B and B>C does not imply A>C). You can get a cycle of pairwise differences (A>B, B>C and C>A) that don't show up on the overall test. This is not what's going on here though, the problem is the other way around."", 'Did you do the wilcoxon test pairwise for each group? You should have different p values for each comparison.\n\nAlso, consider the Dunn test as well for your point estimates.', ""It doesn't seem right that the KW test result would be < 0.001 and all the pairwise rank sum tests would be > 0.25.  It sounds like there's an error there.\n\nBut don't use pairwise WMW rank sum tests as a post-hoc.  Use Dunn's (1964) or another appropriate post-hoc for KW."", 'what a great feedback you gave there,those resources are pretty interesting. Thanks for sharing', ""I did do a wilcoxon test pairwise for each group yes and in my output I get p-values for each combination of groups. However, when I look at the output all combination's p-values are over a p-value of 0.05.\n\nHowever a colleague recommended the Dunn's test of  the Kruskal test and it showed the combinations that were significant!"", ""Yes it is super strange and odd and tomorrow I am definitely going to dig into this more.\n\nBut thank you for telling me not to use WMW and instead use Dunn's which I have and I have been able to show which groups are different.\n\nThank you for the help and telling me this!"", ""I omitted a reference for that last point, but since it's not the OP's current issue, I figured they probably don't need it.""]"
[Q] Why is the t-distribution said to be leptokurtic rather than platykurtic?,"Mesokurtic distributions are similar to the normal (Z) distribution, leptokurtic distributions are more narrow and high (less variability), and platykurtic distributions are more flat and wide (more variability).

So if the t-distribution (especially with n < 30) is flatter and the tails wider, then why isn't it regarded as platykurtic? Several sources say that it's leptokurtic (see link below to Rice University statistician):

*The  distribution is very similar to the normal distribution when the estimate of variance is based on many degrees of freedom, but has relatively more scores in its tails when there are fewer degrees of freedom. Figure 10.8.1 shows  distributions with 2 , 4 , and 10 degrees of freedom and the standard normal distribution.* ***Notice that the normal distribution has relatively more scores in the center of the distribution and the  distribution has relatively more in the tails. The  distribution is therefore leptokurtic.*** *The  distribution approaches the normal distribution as the degrees of freedom increase.*

It would seem to me that fewer scores in the center and more in the tails is the definition of platykurtic.

[https://stats.libretexts.org/Bookshelves/Introductory\_Statistics/Book%3A\_Introductory\_Statistics\_(Lane)/10%3A\_Estimation/10.08%3A\_t\_Distribution#:\~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Lane)/10%3A_Estimation/10.08%3A_t_Distribution#:~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8).",136r19x,Papancasudani,1683128789.0,9,0.85,"['It looks like you may have gotten yourself confused here by mixing up kurtosis and variability. You are probably mentally comparing distributions where the variance is changing at the same time as kurtosis is changing. \n\nKurtosis is about heaviness of tails primarily. If you are comparing kurtosis to the normal distribution you ideally should be considering distributions that vary in kurtosis while variance is held constant. \n\nIf you compare a t-distribution to a normal distribution with the same variance, you will find the t-distribution has a narrow point with high density, but decays rapidly from that point. The probability of being in a neighborhood around the high density point is less than the normal distribution. This is leptokurtic because there is reduced total probability in the central region with increased probability out in the tails.\n\nFurther, consider two normal distributions. One with high variance and one with low variance. The one with low variance is very pointy compared to the flatter high variance distribution. However they both have exactly zero excess kurtosis as they are both normal. The pointiness here is a measure of variability, not kurtosis. Kurtosis is only related to the pointiness after you have already fixed the variance.', 'David Lane (u/dmlane) sometimes posts here.  Maybe he can offer a clarification.', ""1. Your link is not working in old.reddit; hopefully this would work for people who use old:\n\n   https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Lane)/10%3A_Estimation/10.08%3A_t_Distribution#:~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8\n\n2. What's wrong here is there doesn't seem to be correct accounting for the variance of the t-distribution. You have to make the t have unit variance to compare tails and heights.\n\n2. Beware, however, that sometimes the tails and the height in the center can give contradictory messages; *usually* (but not always) it's the tail that has the most impact, and it's normally the thing you'd focus on. \n\n   It will be fine in this case (after making the variance constant, at least as long as the variance is finite), but as a general rule if you want to compare kurtosis the best way to do it is to actually compute the standardized fourth moment either algebraically or via numerical methods."", '> It would seem to me that fewer scores in the center and more in the tails is the definition of platykurtic.\n\nIt\'s not just a comparison between center and tails. It\'s a tradeoff between many points about 1 standard deviation from the main, giving a broad stumpy central plateau (platykurtic), and between many points very close to the mean and some points far beyond 1 standard deviation, giving a narrow but tall central spike, and long tails (leptokurtic).\n\nLowest possible kurtosis is ""all mass 1 standard deviation from the mean,"" as in a fair coin flip. \n\nPart of the confusion is that if you just remember platy- and lepto- mean ""thick"" and ""thin"", it\'s easy to get it backward by thinking that means the tails. it doesn\'t. ""Platykurtic"" is ""thick-arched"", thin tails, while leptokurtic is \'slender-arched\', compensated by longer or thicker tails.', 'Ahhhh that makes sense. Thanks very much.', 'The t distribution is leptokurtic because it has relatively more scores in the tails. Compare critical values of the z and t distributions to see you have to go out farther on the t distribution to include 95% of the scores. [This page](https://www.scribbr.com/statistics/kurtosis/) has a nice graph. Note that the platykurtic distribution has very few scores in the tails.', ""The linked discussion doesn't seem to standaridize the t's, confounding higher variance with higher kurtosis.""]"
Difference between shrinkage estimator and shrinkage priors?[Q],"Hello, Ill be working as a RA with a professor related to developing a modified shrinkage estimator for high dimensional omics data. More specifically, we are interested in recovering moderately strong signals, that would be zeroed by lasso. I was reading about horseshoe priors and found it interesting how you could essentially control the shrinkage by reformulating the feature selection problem in a Bayesian way.  I wanted to know however, what the difference is between a shrinkage estimator and these shrinkage priors. Are shrinkage priors a form of the estimator? For example there horseshoe prior is a scale mixture of gaussians, where the scale parameters are modeled by half cauchy priors. Is the specification of this shrinkage prior distribution , equivalent to calling it a shrinkage estimator?

Also, I was wondering what other books or literature may have more information on this area of statistics. Basically Im interested in signal recovery in high dimensional data with shrinkage estimators. I was going to checkout tibshiranis statistical learning with sparsity book. But I was wondering what else there may be.",136prxv,Direct-Touch469,1683126665.0,19,1.0,"['A shrinkage estimator is any estimator that applies shrinkage in any way. In contrast, a shrinkage prior has to be explicitly a prior, used in a Bayesian context, that results in shrinkage. The use of a shrinkage prior usually comes with a more general Bayesian setup where you end up with a full posterior distribution. However we can use summaries of the posterior, like the mode or mean of the posterior, as estimators. These specific estimators generated via a shrinkage prior will then also be shrinkage estimators.\n\n\nShrinkage also goes by the name regularization and this keyword will also be helpful for looking for more information. One other book I would recommend is Simon Woods book Generalized Additive Models. Generalized additive models are usually based on regularization that penalizes curvature (or other aggregate characteristics) of functions in a dense basis rather than sparsity of the basis. \n\nRegularization as whole is a very active area of work, but Im not aware of any singular treatments. You would be best off looking into journal articles after familiarizing yourself with some basics.', 'Following!', 'I see. So one thing Ive been trying to wrap my head around is, the posterior distributions here are the posteriors of the coefficient estimates of parameters right? I would imagine some of these are centered around zero, and some centered non-zero (these are the strong signals). Posterior of the betas in regression that is. Is this correct?', 'You get posterior distributions for all model parameters you infer, which includes the coefficients, and potentially other parameters and hyperparameters as well. \n\nIn my experience using shrinkage priors, you get basically three classes of posteriors on parameters with shrinkage priors:\n\n* Rather unambiguously shrunk. Strong/sharp peaks at 0, often symmetric but sometimes youll get larger tails pulled one way or the other. These look a lot like the prior. \n\n* Rather unambiguously in the model. Non-zero modes, generally less spiky distributions. These dont look like the prior.\n\n* Ambiguous support. Much like with spike and slab priors (which are in a sense discrete shrinkage) you can get a mixture distribution for a posterior. In this case, a mixture of the above two kinds of distributions. A multimodal distribution with a spike at 0 and another at a non-zero value.\n\nSomewhat related to all this is what counts as a shrinkage prior. Youll see people calling g Laplace priors Bayesian LASSOs but the Laplace is only exerting a pull on the mode. To get the means, medians, and overall distributions shrunk, you need something more powerful. Hence things like the Horseshoe.', 'I see gotcha. It feels like people who try and come up with new methodological advancements in this area try to choose the priors on the scale so that the tails are fat and sharp peaks (ie. Cauchy). Is new methodological advancements really as simple as trying to find a new mode combination such that the distribution to scale prior/likelihood attains certain theoretical conditions? \n\nIve been trying to find a place to start doing a literature review in this stuff and been trying to find patterns. This is something I observed when I read the horseshoe paper. Different distributions for priors and likelihood combinations.', 'Yes and no. \n\nMost shrinkage priors are scale mixtures of Normals, and anyone can slap a distribution on the scale and see what happens. Actually saying something about what you get out of that, like proving that it has useful properties, can be a bit harder. (Plenty of interesting papers never leave the realm of models that have simple expressions for the maximum likelihood estimate so that they can either inform the prior or make statements about shrinkage.) There certainly is a cottage industry of people testing out new combinations, others working to prove properties of existing approaches, and yet others pushing for various refinements of existing distributions. Though those refinements can go many different directions. Ive seen people argue that we need heavier tails and others argue that we need to calm the tails down a bit. Ive seen reasons to agree with both sides of this.\n\nBut theres also computational tractability to consider. I love me a Horseshoe, but the global scale doesnt mix very well and thats kind of a pain. Other distributions like the Bayesian Bridge (sometimes called the generalized normal) have global scales that mix better, in the case of the Bayesian bridge because theres a closed-form density for the distribution conditioned on the global (but marginalized over the local). The trade off Ive found are that this convenience requires a prior on a transformation of the global scale which is less intuitive to set and harder to think about. Then there are some modifications to the shrinkage priors (like tail-regularized versions) which can make sampling difficult (and the prior even harder to conceptualize and understand). I havent seen a ton of papers on all this, but [this one](https://projecteuclid.org/journals/bayesian-analysis/advance-publication/Shrinkage-with-Shrunken-Shoulders--Gibbs-Sampling-Shrinkage-Model-Posteriors/10.1214/22-BA1308.full) covers a decent bit of ground and has some good references. Its a bit of a dense read.', 'I see, that was a good breakdown. The computational side of it is something thats worth considering for sure. Ill checkout the reference. One question I have is, how often do people really use a different type of specification of the mixture or something very specific vs just a simple lasso? Of course lasso is one of these, but its the most commonly used. Would one ever need to abandon the lasso for their own needs? And make a special custom mixture for the specific problem?', 'Few people running statistical analyses are going to go out and create a new shrinkage distribution to tackle a problem. Most of them don\'t have to, and most wouldn\'t know how anyways. This is why things like the LASSO and Horseshoe models come up again and again: folks like having standard approaches to problems.\n\nIf you want to start comparing across inference paradigms, it depends a bit on what the person doing the analysis wants out of it, their goal, and why shrinkage has entered the picture. \n\nIf we were talking to a scientist, say a genomicist, facing an n << p problem, and they just want _some_ answer (that is, they\'re cool with a point estimate and don\'t give a shit about ""oracle"" properties), we\'d probably advise them to turn to something relatively standard, widely implemented, and which seems easy enough to understand. So, a LASSO. We could recommend an equivalent model in, say, `brms`, with a Horseshoe, but given that Bayesian inference seems to be a tough sell in some disciplines the LASSO is the safe answer. \n\nWhat if you\'re talking to someone who cares about uncertainty quantification, though? This is [trickier than you may think for the LASSO!](https://stats.stackexchange.com/questions/402267/how-to-obtain-confidence-intervals-for-a-lasso-regression) In this case, I\'d probably suggest something Bayesian. There\'s no difficulty at all using some sparse Horseshoe regression to get credible intervals for parameters.\n\nWhat if someone wanted to quantify the strength of evidence for a coefficient being shrunk out of the model? Again, I\'d suggest the Bayesian approach because while this isn\'t an _easy_ question to deal with in a Bayesian framework, I have no idea how the hell you\'d approach it in a LASSO sort of context, or if it would even be a sensible question there.\n\nDo most people fit into that first category? Probably. Is it important to have shrinkage estimation techniques in multiple inference paradigms anyways? Definitely.', 'I see, gotcha.  That makes sense that theres no needed to reinvent the wheel when things like horseshoe and lasso exist. Funny enough, my RA is with a biostatistician who works on small n and big\nP problems. Hes interested in weak signal recovery, essentially post estimation of the lasso, we have strong signals (nonzero) and sparse signals (weak, zeroes out coefs), but we think some of these are moderate signals (ie. Can we recover signals that could be moderately strong, and not entirely zero), so we want to, post lasso, try and recover weak signals that could be predictive. Of course Im gonna talk to my PI more, but Ive been looking at trying to apply a classic shrinkage prior (horseshoe, different scale mixtures in the literature etc), and see how it does. However, as a (incoming MS) student, it feels like I am just using whats existing than creating something new, which I feel like a PhD is, and what Id hope to do. So maybe Im biting off more than I can shew by trying to reinvent the wheel, but is using off the shelf or methods in the literature to apply to a problem not really novel? I have been spending time on how to create my own shrinkage estimator vs using existing methods, which I think may be a tall order for me (I have just completed a BS in stats and havent taken any graduate stats yet).']"
[E]Does anyone have acess to statista? I need for my final project in collage,"Hi everyone I Found a document i've been looking for to use it on my thesis.however, it costs a huge amount of mony and i'm just a 3 rd world country student, can anyone provide it for me. Link:https://www.statista.com/statistics/1186820/co2-emissions-commercial-aviation-worldwide/ Email:segniziad@gmail.com",136kfw3,raouf__,1683120200.0,0,0.22,['Talk with your university IT people:\n\nIs there student licensing?\n\nis it on computer lab machines?']
Which type of omega for scale reliability? [Question],Hi! I have a question about which type of omega I should be using to assess the reliability of a scale and its subscales. I have a scale which has two separate subscales derived from confirmatory factor analysis. Should I be using omega total or hierarchical for reliability? Would it be one for both the overall scale and its individual subscales or should it be omega hierarchical for the overall scale and omega total for the subscales (or vice versa)? Thanks!,136jl9v,ParzivalHalliday,1683118087.0,0,0.5,[]
[Q] Ranked test result necessary fof testing significant difference,I have ranked test results from rank 1 (highest) and rank 6 (lowest). Is it necessary to use One-way MANOVA to determine significant difference to ranled results? The reason MANOVA was used it because there are two dependent variables but that's not the point. Just want to know that ranked data needed to test its significance.,136hmxx,drwmda,1683112841.0,6,1.0,"[""if you're wondering whether you need to use One-way MANOVA to test the significance of ranked data, I would say it's not always necessary. You could use other tests like the Kruskal-Wallis H test or the Friedman test, which are more appropriate for ranked data. These tests are nonparametric and are used to determine if there are significant differences between groups in your data.\n\nSo, if you're only concerned about the significance of ranked data, I'd suggest looking into these other tests instead of MANOVA. However, if you want to consider the two dependent variables, then using MANOVA might still be a good choice."", 'Sorry for the late reply. Thanks for the answer, I learned a lot!']"
[Q] Can I compare coefficients in the estimated multiple regression equation?,"Hi all,

If coefficient b1 is higher than b2, can I say the independent variable x1 has more influence on dependent y than independent variable x2?

For example:

y\_hat = b0 + 3\*x1 + 2\*x2

Is it safe to conclude that x1 has more influence on y\_hat than x2?

Assume the overall regression relationship and each independent variables are significant.

Thank you in advance!",1367zj2,Complex-End402,1683081157.0,8,0.91,"[""Not if your b's aren't standardized coefficients.  Imagine x1 was measured in cm.  Now change it to being measured in m or mm."", 'Adding to salvarore\'s excellent answer. Be really careful about the following (ordered by importance):\n1) p values: if the p value of beta2>beta1, and the second is not statistically significant, you cannot reliably state the ordering you are looking for\n2) how you state your results. Just a reminder that your regression model, unless otherwise specified, is not a casual one. Interpreting coefficients as ""influence"" sounds a pretty demanding task to put on a linear regression model\n3) linearities: the linear regression model assumes a certain shape of the coefficients (linear). If, for whatever reason, this assumption is violated (for instance if what links an input-outlut model is a sum of a sin+cosin), there may exist a transformation of your variables that has different coefficients. In this case your statement will still be true -> the linear coefficients are ordered, but saying that all the possible coefficients that can be used to express the relationship between dependent and independent variable in have an ordering may be incorrect.', 'Assuming that your basis functions for each coefficient are fixed, then yes. But careful. That will still be a model assumption, not a data assumption. The way you are framing the question, sounds like you are using your model as a replacement for a test, and most models are bullshit.', 'If you standardize your coefficients, then they are comparable because they are on the same metric. There is also something that I read about recently called dominance analysis that allows you to compare the relative importance of predictors', 'No, because X1 could be anything', 'Thank you u/SalvatoreEggplant, I divided an independent variable by 1000 and its coef increased. As my understanding, unless all my independent variables have the same scale, comparing coef is wrong.', 'Even if beta2> beta1 and they are both significant even then you cant say the difference is significant!!', ""Thank you u/scraper01, I didn't think about fixing basis functions."", ""Thank you u/Sk8FastEatAss, it's first time I heard about dominance analysis. Very useful!"", 'You can say that one estimate is larger than the other estimate if standardized but you cant say the difference is significant just by looking at them.', 'Look up ""standardized coefficient"" ""beta"".', 'Also look at relative importance (or weights) analysis', 'Youre right, good point. I had not specified that.']"
[Q] Is this left-censored data or something else?,"I have some longitudinal event data where a subject (potentially) repeatedly takes an action over the course of some observation interval, and basically want to figure out how the rate of this event occurring depends on the time that has elapsed since the previous event occurred (I suspect the rate is much lower immediately after an event occurs, then increases over time). Normally when I want to determine event rates I reach for survival analysis since it is able to handle censored observations in a robust manner. I am mostly familiar with how to handle right-censored data where the observation interval ends and all we know about the next event is the time that has elapsed since the previous one.

But I am a little perplexed about how to handle censoring where the time the previous event occurred is unknown, but the current event has been observed. As an example, say I observe a subject for one week, and they take the action once during that time. I know for a fact they also took the action at some previous time before the observation interval started, but don't know how much before. I initially thought this might be considered left-censored data since it is due to the effect of the beginning of the observation interval, but upon closer inspection I'm not so sure anymore because I do observe the time the event occurs.

So to summarize my questions:
1. What is this kind of censoring called?
2. How do I handle it appropriately? Specifically, if I want to write down a likelihood function, how do I do so taking both this type of censoring, and normal right-censoring into account?",1366til,physicswizard,1683077868.0,4,0.83,"['In survival analysis, the failure event is assumed to only happen once.  After you die, you stay dead forever. No resurrection is permitted, so no second death is possible.\n\nIn your situation, the fact that the event of interest is repeated means survival models are not the appropriate methodology. \n\nSince you are interested in the counts of these events over intervals of time, you might think about point process models, indexed by time. They will allow you to incorporate the dependence coming from the fact that the rate at which the event occurs depends on the time elapsed since the previous event.', 'This problem reminds me of Poisson distribution (e.g., number of customers during next time interval) and the related exponential distribution (e.g., time between customers).', ""Sounds like interval censor. Say you make an observation at `t1` and `t2` and the actual time of event is unknown. \n\nIt's interval censor if **the only thing known is that it occurred somewhere ***between***** `t1` **and** `t2`.\n\nIt's left censor if **the only thing known is that the event occurred ***before***** `t1`."", ""I think I might have figured it out. In normal right-censored data the contribution to the likelihood of an unobserved event would be `\\int_T^\\infty dt\\, f(t-s) = 1 - F(T-s)` where `s` is the beginning of the interval (for the event), `T` is the end of the observation interval (the censoring time), and `f` is the event density (i.e. `f(t) = F'(t) = -S'(t)`). The interpretation of this is the total probability for the event to occur in the censored region of time.\n\nFor this start-time censored data (which I think is still technically considered right-censored because all we have is a lower bound on the time to event), the contribution to the likelihood is of a similar form `\\int_{-\\infty}^0 ds\\, f(t-s)`, which through suitable u-substitutions can be shown to be equivalent to `1 - F(t)` where `t` is the time elapsed from the beginning of the observation period to the occurrence of the first event.\n\nSo in summary, this is also right-censored, just in an unusual form."", 'I think you\'re fixating a bit too much on the ""survival"" moniker. Perhaps a more neutral name would be ""time to event model"". There\'s no reason you can\'t use SA techniques for recurring events; in fact [recurrent event analysis](https://en.m.wikipedia.org/wiki/Recurrent_event_analysis) is built on top of it.\n\nI will look into your suggestion of point process models though, that sounds like it could be relevant.', 'recurrent events are definitely a thing in ""time-to-event"" analysis. People often use the term ""survival analysis"" even when you do have recurrent events.']"
[Q] Can I create a PCA time series by using the eigenvectors for each date?,"Long story short, I have a set of data over each summer for four years. I'd like to show the PCA across each season, so see if there is a pattern/how the pattern changes (these are environmental variables).

I have done some reading and I'm wondering if I can take the eigenvector for each date/set of variables and plot it across the season. I've already completed the PCA in python, but it is not great for looking at changes across seasons because it produces ~56 plots per study area.",1365au0,hatcatcha,1683073761.0,1,0.6,"['> I have a set of data over each summer for four years\n\nWhat does the data represent? How many variables are you working with?\n\n> I\'d like to show the PCA across each season\n\n""Show the PCA"" doesn\'t really make sense. PCA is a technique, not an object or a visual. Do you want to project the data on the top 2 PCA components and plot it?\n\n> take the eigenvector for each date/set of variables\n\nSimilarly to above, ""take the eigenvector & plot it"" doesn\'t make any sense. The top eigenvector of the correlation matrix gives the first PCA component, which you could use to project the original variables. \n\nIf each row of the data represents a distinct time, then you could project this data into the PCA space and view its pattern over time. It\'s not clear exactly what this plot represents though, nor what question you\'re trying to answer with it. What do you want this plot to accomplish?', 'Thanks for the reply. As you can tell, this isnt my area of expertise but I was asked by my advisor to do a PCA on my data (grad student here). \n\nThe data contains five variables (these are topographic variables derived from remote sensing data). For each date, I have a spreadsheet that contains about 400,000 points that represent a pixel in the remote sensing data. Each pixel contains a value for each of the five variables. There are five columns for each variable, and each row (1-400,000) is just a different pixel within a specific geographic area. I hope this makes sense \n\nSo, the rows of the data do not represent time, but represent a different pixel on the same date. So, I have many spreadsheets (for separate dates) that have a ton of data on them. \n\nThe goal would be to retain a value from each spreadsheet to plot on a time series. So far I havent figured out if there is a way to do this using PCA. The goal is to show, using PCA, how relationships between these variables change across season. I work in the Arctic where vegetation changes very rapidly, and is possibly affected by topographic variables like slope, elevation, solar insolation, and topographic wetness, but these relationships change depending on the time of the season. We would like to know how these relationships change across the season and will be also sampling stream chemistry for the corresponding dates. \n\nThanks again for your reply!', ""Based on what you describe, I'm questioning whether it make sense to lump all pixels together into a single PCA. Is it possible that pixels in one area may not have much in common with pixels in another area, or perhaps the patterns could be different? Specifically, could the correlation structure of pixels in one area be entirely different from pixels in another area? Where correlation structure essentially denotes the relationships between your 5 variables.\n\nIn general, I'd be weary of the sparse nature of information across 400,000 pixels. If there is a way to distill the pixels into something more meaningful, that'd probably be a useful step.\n\n> The goal would be to retain a value from each spreadsheet to plot on a time series\n\nSo you want to summarize each date's spreadsheet into a single value or small set of values? \n\nDepending on the nature of the dataset (e.g. discussed above, how much diversity we expect across pixels) this may be straightforward or it may be difficult.\n\nLet me phrase the question this way. Which difference do you expect to be more substantial: the variety of pixels within a single date's spreadsheet, or the variety in behaviors across dates for a single pixel?""]"
[Q] Any learning tools that incorporate sports stats?,"Im a prospective data/business analyst and its my understanding I should develop a strong background in statistics and excel to get started. Ive already taken some of Eddie Davilas stats courses on LinkedIn so Ive learned some of the fundamentals of stats, but I want to take it to the next level and begin applying my learning, specifically working more in excel. 

1. Are there any resources you suggest I look next?

2. Are there any resources that use sports statistics to teach these concepts? 

Thanks!",136412s,jarjarp,1683070377.0,1,1.0,"[""[David Robinson](http://varianceexplained.org/r/empirical_bayes_baseball/) has a nice blog, and a nice pay-what-you-want ebook, about using baseball statistics, specifically for a technique called Empirical Bayes Estimation (in effect, using the distribution of the events you've already observed as a prior for what you might observe in the future.) It is just one tool in your toolbox, not a whole way of doing statistics, but you may enjoy looking at it.""]"
[Q] How to perform ordinary least squares with possibly correlated data points?,"Hello!

I'd like to present an example to illustrate my question.

I have a number of athletes and would like to test whether there is a relationship between their running time (measured on the first of Jan) and how much they trained in the previous year. I have multiple data points for each athlete, one for each year.

I can perform OLS and get back some statistics metrics such as p-value etc., but I'm wondering whether these are really accurate since the data points from a single athlete will each be correlated to each other? But at the same time, if the same athlete trained less and therefore got a faster time, is that really correlated to the previous year?

I was thinking I could perform PanelOLS, but just wanted to check here whether anyone has any other ideas.

Thanks!",1361jmf,Coxian42069,1683064367.0,3,1.0,"['By you *specifying* to use the specific estimator ""ordinary least squares"", the correlation among the data points is simply irrelevant and would be ignored.\n\nSo ""how to perform ordinary least squares"" is simply to do it. The question would instead be whether such an algorithm makes sense.\n\nThe problem is that by talking about the algorithm, you\'re placing the cart before the horse.\n\nSo the better question would focus on a suitable *model*, not the estimation algorithm.  Then with a model in place, you might consider how best to estimate that model, and hence how to perform any additional statistical analysis (like say, tests, confidence intervals, prediction intervals or whatever else).', 'Sure seems like panel data', 'Youll want to use a panel regression that estimates the variance-covariance matrix using the group structure of the data; the assumptions of the OLS estimator for the variance and derived statistics (i.e., standard errors, t-values, confidence intervals) will be incorrect in your setting.', 'Great, thanks, I did think so. Are there any out-of-the-box tools you can suggest I look into?']"
[Q] what is the name for this quantity in the covariance matrix?,"I am trying to find out if there is a consistent name in the literature for a term in the calculation of the covariance matrix.

If you have N random variables in a N dimensional vector x, the covariance of x is given by:

Cx = E[ (x - E[x]) (x - E[x])^T ] = E[ x x^T ] - E[x] E[x]^T

where E[x] is the N-dimensional mean vector of x.

I am interested in this first term, **E[ x x^T ]**, and want to know if it has a name. It isn't called the [autocorrelation matrix](https://en.wikipedia.org/wiki/Autocorrelation#Matrix), since I am thinking of a general form where variables in x can be anything, not necessarily representing a signal and delayed copies of itself.

does anyone know if this particular quantity has a name? I looked up ""expected sample outer product"", but couldn't come up with anything meaningful.",13614dx,Sasqwan,1683063450.0,2,1.0,"['Maybe the second moment of multivariate gaussian sample?', 'the signal processing folks call that the autocorrelation, since signals generally have mean zero', 'thanks alot! I looked into what you said, and it seems like it could be called the second noncentral moment, or the second raw moment, based on this page (https://en.wikipedia.org/wiki/Moment_(mathematics)). regardless, I was hoping it had a simpler name :) guess not.', ""and I don't mean to be mean, here"", ""Your own name was sufficiently descriptive that people would figure out what was meant. \n\nI can think of other potential terms but none are simpler than your suggestion (nor the one by /u/vermillion50), so I don't see much point there.""]"
[Q] Is this normal? Grade distrubtion in master level statistical inference?,"Hi,

I am currently a graduate student in biostatistics, who just finished my final for statistical inference (casella and berger chapter 6 to 10).

I got my ass whupped by the final (66), which left me with course average of 82. But I got the final grade of A.

Which got me thinking.. Is it normal for theory heavy classes to kick a lot of people's butt especially during finals? The last midterm average was somewhere around 70's to low 80's.

Did you class usually ace the finals for statistical inference?

I notice that this year's final is much more harder than the previous years?

I feel little dumb after taking that final.",13601yv,edsmart123,1683061081.0,12,0.83,"[""Numeric grades in graduate school don't mean that much. Pay much more attention to letter grades. \n\nA= you are doing fine\n\nB= we will fail your ass on quals if you don't improve between now and then \n\nC= how the fuck did you get in here to start with?"", 'Might just be a graduate course thing. At my institution, generally the only grades given in graduate courses were A or B. The latter meant you were probably not ready to pass the quals.', 'I\'ve seen plenty of stats subjects (not just at masters level) that pitch the subject fairly hard (especially exams) but then have to push the grades up (so a seemingly very ordinary mark on the exam results in a high grade). It\'s not that unusual.\n\nI\'ve also seen plenty that don\'t do that.\n\nDon\'t worry overly much. While it\'s important to realize that there\'s a lot there that you haven\'t understood well enough to just do it in exam conditions (so you may find yourself relearning parts of it, if you ever need them),  you\'re in a relatively good position to be able to do that. You may not have all the knowledge firmly in hand but it sounds like you have the meta-knowledge (you have a decent coverage of what things you ""should"" know about, and the knowledge needed to mostly get the deeper knowledge when you need it). \n\nI see no reason to feel bad about it not all being at your fingertips. It sure as hell isn\'t all at mine. If it comes up that you need some of that stuff again, you\'ll need to do some work is all.', 'I got 28% in statistical inference first time round... I passed the resit thankfully. \n\n40% of the class failed statistical inference the first time round. \n\nIt was known as the killer on our MSc Stats course.', ""I've taught a similar course for years. The distribution of grades is hard to control because\n\n- small sample size (compared to undergrad)\n\n- students vary widely in the prep they've had \n\nThat said, the most common rules of thumb we tend to follow are\n\n1) make the midterm harder than the final, so students will work harder\n\n2) worry about the distribution of letter grades, but not the numerical scores.\n\nGuaranteed you're not the only one that feels dumb after taking that exam.... introductory stats courses often have that effect."", 'Statistical inference and probability theory were two my of my most difficult courses during masters. Grade inflation is common given the difficulty', ""Its not unheard of. Just like in undergrad, some profs curve, other don't. Its a lot better to care about the final grade rather than the raw %. \n\nI took statistical inference using casella and berger as well (Chapters 1-10 in 1 semester). Nobody in my course got above an 80% on the exams. 55% was a C, which was passing in my program (requires A's in everything else to meet 3.0 req tho). Needless to say, grade distributions and academic rigor vary widely. Some programs fail students out for getting a C, others dont."", 'Youre fine I also got whooped by my grad statistical inference class this semester, but unlike you did a lot worse and will have to retake the class. Its just tough material and its definitely the hardest class Ive yet to take in my masters program.', 'Depends a lot on the program, but usually somewhere in the handbook theyll spell it out for you. And if not there, the departments Ive been in cover this pretty thoroughly during orientation. The last place I worked, the youre going to need to improve for quals grade was a C. When I was in grad school, we got simple pass/fail and a yearly written evaluation from our advisor (1st year) or thesis committee.', 'Um I guess that\'s true some places. I\'d be inclined to say that if you aren\'t getting kicked out your grades don\'t matter at all in grad school.\n\nFor my PhD Bs were the norm...like 80% of the class was given Bs. A was ""we need a way to highlight this person for internal grants"" and C was ""you\'re going to fail"" and this is at a high ranked university (which almost always have rampant grade inflation for undergrads, including us).', ""You are correct in the sense that qualifying exams and funding are really the only things that matter (other than your dissertation). So yes, you'd rather be a 3.2 and pass quals than a 3.9 that fails quals 10/10 times. \n\nBut it's extremely unusual for someone that crushes the core classwork to fail quals overall."", ""Well yeah the last bit is true, but also there's no difference here really between a B and an A was my main point. Like literally no difference really.""]"
"[Q] When it comes to evaluating the Time Series model, do we choose the model with the lowest Akaike Information Criterion?","If yes, can we then choose the model that has negative AIC? For example, model 1 with AIC of -100 vs model 2 with AIC of 0 vs model 3 with AIC of 10. In this case, would I choose model 1?",135yjp3,PythonEntusiast,1683057678.0,1,0.67,"['Yes, it can be negative. Just pick the lowest one, although BIC tends to be somewhat more popular in time series because it favours parsimonious models.', "">  do we choose\n\nThere's not enough context to be so normative; you *could* do all manner of things. *If* you decide to use AIC as the basis for choosing between models, then choosing the lowest AIC is generally what you'd want to do. just as you'd maximize likelihood for situations where that would make sense. Indeed that sense is quite directly related; in effect the AIC adjusts -2log L for the expected effect (asymptotically, at least) of maximizing L over some parameters.\n\n> If yes, can we then choose the model that has negative AIC?\n\nyes, -100 is smaller than 0 which is in turn smaller than 10""]"
[Q] Quality Control sample calculation,"I believe this is probably very simple, but its a been a long time since Ive done any stats work and Ive tried googling and havent found exactly what I need. So sorry if its kind of a dumb question. But heres what Im looking for. 

I have a product that should have unique serial numbers between 1 and N (probably greater than 100,000 for reference). I need to do QC on my orders to get some level of confidence that the numbers are within that range and are unique (say 95%). What is the equation for figuring out how many samples I need to take in order to get to that confidence level?",135vjce,yogurtman,1683050756.0,2,1.0,"['This seems a very odd problem to solve with statistics. Your serial numbers should be assigned deterministically. You should have 100% confidence in their uniqueness, they should not be randomly assigned; and it should not be possible, in theory, to have unexpected duplicates. If duplicates are suspected, an exhaustive search to identify to problem is probably going to be a better approach.', 'What software are you using for this calculation? For most, you could easily import all \\~100,000 serial numbers.', ""You're looking for the confidence interval on a binomial distribution.  Basically, 95% confidence that the failure rate is below X% requires a sample size of N with 0 observed failures.\n\nhttps://www.weibull.com/hotwire/issue118/relbasics118.htm"", ""So, you're looking to check for the absence of an event, rather than the occurrence of it? Or do you want to measure the proportion of dodgy numbers given that you know some will be dodgy?\n\nIf you're looking to check for the absence of an event, the simplest way is to use the rule of three, which says that if you observe zero events in N tries then an approximate 95% CI for the risk of the event is (0, 3/N), as long as your sample is at least 20-30 (because the maths relies on the Central Limit Theorem).\n\nSo you just need to decide how much of a real underlying prevalence of dodgy numbers is too small to care about. If it's 1 in 100 then a sample of 300 with no events observed would be required to be reasonably confident that the prevalance is no more than 1%.\n\nIf you expect to observe one or more dodgy numbers then you may prefer to do the sample size calculation on that basis. [Here's a sample size calculator](https://select-statistics.co.uk/calculators/sample-size-calculator-population-proportion/)."", 'Overall I agree. Its actually RFID numbers and Im using a new supplier. The numbers should be in a range that we determine, but I am trying to figure out a way to sample test them to make sure I have a reasonable expectation that they are correct.', 'I expect to have 0 incorrect serial numbers, but I want to at least do some sample checking to see if over time we get any serial numbers that are outside the expected numbers. If we do receive any serial numbers that are incorrect then we will do a much larger sample to see if we can figure out how many are incorrect and then have our supplier give us new ones.', 'Then the rule of three is the simplest way.', 'Ok that works. Thanks']"
[R] [E] [Q] Violating Assumption of Logistic Regression,"Hello, I built a dataset that includes repeated observations of student retention through each term of a student's enrollment.  One observation for each student in each term of enrollment.

My question is: one of the assumptions of logistic regression is that observations need to be independent of one another: ""Logistic regression assumes that the observations in the dataset are independent of each other. That is, the observations should not come from repeated measurements of the same individual or be related to each other in any way.""

I think I'm violating this assumption, but when I run the same dataset through both regression based models and tree-based models, I get very similar results. I would expect that the LR model would preform worse?",135swwd,masterofzion1,1683045066.0,0,0.33,"['If you violate them, I think youre mainly inflating variances and covariances inside the model. Therefore your inference on the inputs might draw you to incorrect conclusions. Would you mind sharing why you think youre violating this assumption?', ""Those repeated observations may not affect the quality of fit or the predictions, but they will make the coefficient standard errors and p-values invalid.  It's like artificially increasing the size of the data set, which has the effect of decreasing standard errors."", ""I haven't checked the residuals yet, which I know is how to check this assumption.  I'll do that this afternoon If I have time.  I was assuming that I would violate it based on the reading of the assumption, which I know isn't definitive.  Does that answer your question?"", 'Not yet. Was there something on the sample selection or is there a particularity in the data that says observations are not independent? Like time dependence or something? Were individuals measured twice?', 'Many students were measured twice at different times.  \n\nFor example, I started the dataset with the 2019 Fall file, and then added the 2020 Spring file (the next term).  So, students who were in the 2019 file would be duplicated (with different values for some of the variables) if they were also present in the 2020 Spring.', 'So yeah, indeed, you have a violated assumption. May I suggest a mixed model?', ""Thanks.  I'll do further research and either change my dataset so that I have one observation with features that represent time) or look at a mixed model.  \n\nAppreciate it!""]"
[Q] How to interpret non-significant results?," Hello,

Im working on my diploma thesis whether airplane crashes have an impact on the stock price of the airplane manufacturers. The thing is, my results are not statistically significant. How do I interpret these results.

Im sure I can say: ""I didnt prove that airplane crashes have statistically significant effect,..."", but that is lame, it sounds like I didnt find out anything. So can I also say that: ""Airplane crashes dont have statistically significant effect,...""?

Because I dont think this is true, that no significance means automatically no effect, but my teacher who helps me with my thesis told me this.

Thank you",135oyfa,ericson1998,1683040477.0,1,0.57,"['Dont say prove. Say something like, according to the model the estimated effect is this, and is not significant at this alpha level. Thats all you can say.', 'Your intuition is broadly correct. The oversimplified way of explaining this, is that your effect estimate still represents the ""best guess"" for the ""true"" effect (if you study is otherwise valid), just with higher than usual uncertainty. For the result we would report the estimate with the confidence interval/SE and discuss the limits to the study. \n\nHowever, I would also caution against going into this discussion if (1) your teacher specifically told you otherwise and (2) if you need to be asking this question. P-values are the first thing we typically teach because it\'s simple and easy(er) to understand. Moving beyond them before you are ready is mostly just risking making a lot of mistakes.', 'Instead of I didnt prove say I didnt find conclusive evidence that  You may have hint which should not be ignored. Clearly, lack of significance does not mean lack of effect.', ""What is your p-value?\n\nA p-value that's smallish, but still greater than your alpha, suggest further research is warranted, but the current sample and related analysis does offers insufficient justification for rejecting the null hypothesis.\n\nLarger p-values aren't as suggestive that a larger sample might improve things.\n\nResults that are not statistically significant do not indicate *no effect*. The indicate that the effect cannot be confirmed at the desired level of confidence. The null hypothesis is never confirmed, only rejected or not."", 'your teacher is wrong. strictly, either of the statements you think you can say, you can say, but not much more. this is where you think about what might have been missing from your analysis and discuss  that', 'Be sure to present some kind of observed effect size.  I\'m not sure what your model is, but something like an r-squared value, or the difference in mean change of stock prices for those with crashes vs. those without crashes.\n\n(These are two kinds of effect size statistics:  standardized and not standardized.  Presenting both is helpful.)\n\nIf you can present the effect size with a confidence interval, this gives a lot of information.\n\nIf there is a relatively large or suggestive effect size, without a significant hypothesis test, you might say the  results are ""suggestive"", implying future research is warranted.\n\nIf the effect size is small, well it\'s small (even if the hypothesis test were significant).  It may be that that the results don\'t suggest any *practical importance*.', 'I wouldn\'t say, ""There is no effect.""  I would say this study failed to show a statistically significant effect.  As per my previous comment, you can estimate the size of the effect.  It\'s just that you haven\'t shown that it is e.g. statistically different from zero (no effect).', 'The sampling plan did not provide sufficient evidence to reject the null hypothesis (there is no statistical difference at designated confidence interval).\n\nIt is correct that, the null hypothesis is never fully accepted as additional sampling may prove otherwise (because technically its a confidence interval, stats are never 100 percent certain but you can have diff levels of confidence.)\n\n\nYes this does mean that new tests and further analysis is open to rejecting the null hypothesis. Just not this particular test.', 'An appropriate hypothesis test should be interesting regardless of the outcome. So what hypothesis are you testing?', 'You dont prove, you support.\n\nAlso, null results are results. Science needs to get this! If you were disappointed in the lack of support for the alternative hypotheses, this means you were expecting to reject the null which means you would not have to study it but you did because, apriori, you cannot know the results.', 'Okay here is more context. Interpreting the magnitude of p values was first done by Fisher. Neyman-Pearson proposed the whole hypothesis testing thing (with all the Type-X error and all) as an alternative to that. Doing hypothesis testing *and* interpreting the magnitude of p-values, although not rare, is a malpractice. You can be Fisherian or not. But mixing the two was never the intention of the people who created these methods.', 'You cant interpret a p value in a way that adjudicates between whether more or less research is warranted.', 'Exactly, the latter is the most important part of it all really', 'This guy/gal stats.', "">Doing hypothesis testing and interpreting the magnitude of p-values, although not rare, is a malpractice.\n\nAs part of the present study, yes. *No one is suggesting otherwise.*\n\nWhat has been (correctly) stated is that we can fail to reject the null hyothesis, and still learn something in our research. P-value less than alpha doesn't mean there's nothing to learn.\n\nAs long as we start any new hypothesis test with a fresh sample, we can use our old sample to *raise* whatever questions we like. We just can't use our old data to *answer* any of those questions. If our calculated p-value is only slightly greater than our arbitrarily selected alpha, then we absolutely can leverage that information moving forward. Not rejecting a null hypothesis is not equivalent to concluding there is no effect.\n\nNow, if we increased our alpha value or modified our hypothesis *without* subsequently obtaining a brand new independent sample to analyze, *that* would be statistical malpractice. No one is suggesting we do anything like that, though.\n\nAs long as we make all of our changes *before we collect the new sample we're going to use for the analysis*, we can make whatever changes we like. The path to our design doesn't affect the study's rigor."", ""Yes, actually you can. Whether or not more research is warranted is entirely subjective. A small p-value greater than alpha is pretty much the best indicator one has that further research is justified. Are you seriously suggesting that a p-value of 0.02 with an alpha of 0.01 is sufficient to close the door on the alternative hypothesis for good?\n\nRecognizing the further research might be prudent doesn't affect the conclusions of the *current* study at all. The null is still not rejected."", 'If our calculated p-value is only slightly greater than our arbitrarily selected alpha, then we absolutely can leverage that information moving forward.\n\nWhoever told you this owes you money. For the OP: this is 100% what you should not write.', 'No Im saying that the value p value of a hypothesis test should not be used to inform whether more or less research is justified.', ""Go back under your bridge and leave stats to those of us who know what we're talking about."", ""It absolutely is one of the things that should inform that decision. I'm not sure why you would think otherwise.\n\nEdit: Whether or not to reject the null based on *this* sample is an objective decision determined by the p-value's size relative to the preselected alpha value. Determining whether or not the alternative hypothesis is worthy of further consideration in the future is not nearly so cut and dry. Not rejecting the null hypothesis is not the same as accepting it as true or determining the alternative hypothesis is false. It is simply a statement that there is not sufficient evident in the present sample to confidently reject the null.\n\nIf I toss a die four times and it comes up 6 three of those times, that's not sufficient to assume the die is not fair, but it *is* enough to make us wonder. Further research is warranted."", 'Lol. Cute. I have PhD and taught stats. ;)', 'Youre speaking about p values independent of the power of a test. A p value cannot be used to assess the power of a test, and only if a test is powerful or not is useful in determining if more research (more powerful study) is worthwhile.', ""So you're saying it's your reading comprehension, and not your stats knowledge, that is severely lacking? Or are you just trying to say your PhD program sucked without saying your PhD program sucked? Did you get the degree from Cracker Jack U?\n\nYou're objectively quite wrong on this topic. It's not just that you don't understand what *you're* talking about. You also don't seem to understand what *we're* talking about. Downvote all you want; you'll still be ignorant about the topic at hand."", 'The power of the hypothesis test is irrelevant. We aren\'t talking about whether or not to reject the null hypothesis.\n\nEvidence that is insufficient to justify rejecting the null hypothesis can be plenty sufficient to indicate that the reason we can\'t presently reject the null *might* only be that our sample size is too small. In such a case, we might want to proceed planning another analysis, using a new, larger sample (independent of the current sample).\n\nEdit: Rejecting the null has a standard of evidence similar to criminal court, *beyond a reasonable doubt*, where we quantify ""reasonable doubt"" with alpha. Determining whether or not further research is necessary is more like the standard in civil court, a *preponderance of evidence*.', 'Im sorry but you absolutely cannot use the result of a study to judge whether the study was sufficiently powerful or not.', ""That isn't even the topic of discussion.\n\nHave you been reading along before replying or are you simply arguing for the sake of arguing?"", 'You can only know whether your sample size is too small before you do the study!!! The p value cant tell you that!!']"
[Q] How to combine four highly correlated variables into one?,"Hi everyone, I'm fairly new to econometrics and have to build a model that predicts the price of maize, rice, soybeans and wheat.

The thing is that their prices are extremely correlated with each other. The coefficient matrix shows values higher than 90% for every combination.

How can I transform the four commodities into one, in order to have their price as one Y variable to be predicted? My main concern with aggregating them would be unnacounting for substitution effects between some of them, given that, for example, both maize and wheat and probably soybeans are fed to livestock.

Is there a way to combine them and ignore possible substitution effects? I have a paper that transformed them into caloric equivalents and aggregated them, but I don't know the proportion of each one.

What other techniques can I use to combine them? Is it even logical to do that? Can't I just predict the value of just one of the four?

Thank you in advance.",135o7ml,Michigan999,1683039793.0,0,0.33,"['Principal Component Analysis or Factor Analysis might be exactly what you are looking for. :)', 'I agree, PCA is the way to go', 'What variables are you using to predict the prices? Are the prices your independent variables or your depend variables? Could you describe the problem in a little more detail?', 'Throwing my hat in with the first component from PCA responses.', ""The first principal component would be one typical way to combine such variables.\n\nIf they have similar variances, you typically end up with something very close to simply adding (/averaging) the components (up to a linear rescaling of the result).\n\n> I don't know the proportion of each one.\n\nThat's unfortunate; if they're being fed to livestock, presumably the relative totals are available."", 'Thank you. Sorry for the late reply!', 'Thanks! I just learned what it is', 'Sorry, I missed that. The prices are the dependent variables. I am still building the model, but the independent variables are weather related (shocks from the previous period, or simply temperature, for example.) And I think consumption would be another one.', 'Thank you!', ""They don't have similar variances. Maize has a variance of 21, rice 219, soybeans 100, and wheat 38. All of them are in usd/bushel and rice is usd/100 pounds.\n\nI calculated consumption for each of them and assigned them weights based on total consumption of the four. Then I calculated a weighted average of the priced and the variance is 68. Would this work?""]"
[Q] How to calculate r^2 with missing data,"Hey guys.

I have been trying to solve this problem for more than 2 weeks now and I just can't understand it properly. Can you guys give me a hand with it? 

I need to solve the r\^2 and the f-test of an estimated simple linear regression model using OLS. But the only info that I have is the estimated model itself, with the standar errors of every coefficient. (b0 and b1) since it has a single independent variable. And as an ""additional"" information, I know the n (total observations) and the sum of squares of the observations minus their mean. That's all I have, I don't have access to the observations, anything else.

What do you think i could do to solve it?

thanks in advance",135o0qu,AirXval,1683039628.0,1,0.67,"['Write down the formulas for each of the pieces of information you have, along with the formulas for R^2 and the F statistic. See if they have anything in common.', 'whats your point? I already know that relationship between the formulas, the problems is to get the SSE or SST without the additional info... btw since ur username checks out lmao. \n\nis it true that you can use the standard error of the regression to estimate SSE as:  \nSSE  s\\^2 \\* (n-2) ? or that you can use the sum of squares of X as a proxy for SST, since they are equal in a simple linear regression model. That is: SST = (Xi - Xmean)\\^2 ??', "">is it true that you can use the standard error of the regression to estimate SSE as:\nSSE  s^2 * (n-2) ?\n\nThat's exactly the type of questions you should be able to answer by following my advice above."", 'bruh you left me in the same doubt hahaha I thought u meant the normal thing about R\\^2 = ssr/sst and f-test = r\\^2/k/1-r\\^2/n-k-1 lol.\n\nAny idea where I can find sources to understand better this? im struggling  no joke I couldnt find anything about what I mentioned in my previous comment. Every video or website tells the same thing but none break down the formulas', 'You need to go deeper - what are the formulas for SSR and SST? You should be able to find everything you need in your textbook - if not, Wikipedia has detailed formulas for all of these things.']"
[Q] Are there any situations in which a smaller sample size is a good thing?,"Hi all,

Im a Data Analyst with a Physics background, and just wanted to get thoughts on my question above.

Typically, I would think that a larger sample size means youve got more statistical power, a smaller confidence interval, and ultimately more evidence to back up any conclusions that can be drawn from the data.

Is there literally any situation where its better to have a smaller sample size, as opposed to a larger one? (Other than less data perhaps being easier to work with)",135jx2u,JLane1996,1683030873.0,48,0.91,"['Easier to work with, and also cheaper to collect.\n\nOtherwise, there are edge cases like when it is still a sample but tending towards the population, then it is likely that your sample is not randomly selected but introduces specific biases. For example you have data from almost all students in a district because they all received the questionnaire in class, except the ones being absent from school. Well, the absent ones probably have some characteristics in common.', 'Situations where the treatment you want to test has a direct cost associated with it or its ethically questionable\n\nFor example, if you want to test whether giving coupons is beneficial to your business, youll want the lowest number of individuals receiving coupons that are necessary to validate the effect. This is because giving cupons has a cost associate with it, and giving coupons to everyone (despite giving a better estimate of the effect) would hurt revenues a lot\n\nAnother example would be evaluating the impact of higher body fat on heart rate. You wouldnt want thousands of individuals having an unhealthy lifestyle just to get a higher precision that doesnt really affect your overall conclusion', 'Yes! **More data does not offset the problems caused by non-random data collection** -- in fact, more data will exacerbate the problem by making you not only wrong but **confidently wrong**.\n\nCheck out the (incredible) paper ""Statistical paradises and paradoxes in big data"" by Xiao-Li Meng. He shows that the bias of a sample average can be decomposed into a product of three terms:\n\n1. A ""data quantity"" measure, sqrt((1-f)/f) where f is the fraction of the population covered by your sample\n\n2. A ""problem hardness"" measure, simply given by the population-level standard deviation of the quantity being measured\n\n3. A ""data quality"" term, which measures the correlation between your sampling procedure and the quantity being measured. If your experimental design is not *perfect* or your data is observational, this term will almost certainly be nonzero.\n\nWhen your sampling procedure is systematically related to the quantity of interest, the data quality term contributes to a bias in the direction of this systematic relationship. While a large quantity of data will drive the overall bias closer to zero, it will also rapidly decrease the variance of your sample mean, causing any confidence intervals you construct to typically exclude the true population mean.\n\nOf course, you can correct for this bias if you somehow know your data quality term a-priori, but this is not a very reasonable thing to know in practice. \n\nI hope this is helpful!', 'Not so much small as big enough. Clinical trials may be [stopped for futility](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7954786/) (and other reasons).\n\nIn the case of oncology and rare diseases in single-arm trials, there is an opportunity cost in studying each additional participant. Studies should be as large as necessary but rarely bigger. This veers into ethical and statistical considerations.', 'As others have mentioned, cost and ethics of collecting extra cases. Id also like to add a new scenario: even though adding extra cases will generally improve an asymptotic estimator, the amount by which it does so tends to fall off as n increases. If your estimation procedure is computationally expensive and scales with n, there is almost certainly a point where the additional computational resources arent worth the additional computational resources.', ""I would say this is less of a statistical and more of a methodological argument: a smaller sample size can be better if it means the data are higher quality (ie less biased.) \n\nLet's say you want to collect some data on cognitive functioning. Sure, you could code some simple measures and put them on the internet and pay anyone a dollar to finish them, or you could do an in-person measure in a controlled environment with a trained person administering the measure on people whom you can verify meet selection criteria. The first one will get you many more data than the second, BUT those data might not represent the construct you're interested in."", ""If *more data* = *more data collected in the same way with the same characteristics*, then more data gives more power. But, more generally, you shouldn't assume that larger sample gives more power. \n\nPower is the relationship between signal (effect) and noice. Larger sample is a way to reduce noice, so it increases power, unless the larger sample dilutes the signal or adds to the noice in other ways.\n\nA common mistake is to dilute the signal. For example, if you want to know whether masks prevents the spread of covid, then including people were never exposed to covid at all is going to reduce your power. A more subtle example: in a CVR analysis, you can get more power by excluding power users.\n\nAdding to the noice was already mentioned in other replies. Basically, if you have sub-populations defined by some confounding variable, then you can have more power on the sub-populations (despite the smaller sample sizes), than on the full population. Even if effects are equal across all sub-populations."", ""Yes. \n\nBack in the 1990s there was an equivalence trial of cisplatin vs carboplatin in testicular cancer. Cisplatin was known to be extremely effective, it was hoped that its analogue, carboplatin, would be equally effective with fewer side effects. The trial recruited very fast relative to the follow-up required and so it was decided to recruit a larger sample while data were still accumulating. In the event, carboplatin turned out to be less effective and recruiting a larger sample size did harm. Devastating, in hindsight, for all concerned.\n\nThat sort of ethics doesn't often come into physics experiments but resource use is always an ethical question. If you use more resource than necessary on one question, there will be less resource available for other questions. In the example above, it was an equivalence trial and so it was felt that the opportunity to collect more information to demonstrate (the hoped for) equivalence should not be missed. For other questions, we're often looking for evidence that a difference is big enough to make changing practice worthwhile. Setting out to collect more data than necessary for a decision to be made is (usually) a waste of resource."", 'Well, with Huge sample sizes and frequentist stats p is almost guaranteed to be very small. So things get complicated. Small is relative I guess?\n\nEffect size measures become very important', 'If the larger sample doesn\'t introduce new biases (selection bias etc.) then from a statistical standpoint I don\'t see a downside.\n\nThat said, if you\'re planning to do a hypothesis test, it\'s important to remember that it will _not_ lower the Type I (false positive) error rate, unless you decide to do that yourself with a lower alpha. \n\nIt\'s also important to realize you will increase the power to detect small (but likely \'real\' associations). I work in medicine, and that\'s why we (hopefully) worry about _clinical_ significance of results just as much as statistical significance. We often define a priori what a ""clinically important difference"" would be. \n\nA hypothetical example one of my profs used to use is, with a really large sample size you might be able to find evidence that eating some specific food is associated with higher mortality (with a tiny p-value), _but_ if the difference in life expectancy between someone eating the food and someone not eating the food is ~1 day over their whole lifetime, it\'s unlikely anyone is going to avoid that food based on those results.\n\nAnother practical downside is the cost of collecting more data.', ""when u're analyzing the effects of nuclear meltdowns lol"", '1. It\'s a bit weird but ... arguably (... sort of?), because the sources of bias are manifold.\n\n  As sample sizes become large, systematic effects/bias will tend to dominate sampling variation. \n\n   When sampling variation is larger, the ""correct"" value would not be ruled out even though there\'s a little bias in there. As you get an estimate with less and less sampling variation in it, the thing you\'re trying to estimate no longer becomes ""plausible"". \n\n   Consequently as sample sizes increase, it becomes more and more important to eliminate even the tiniest effects that can lead to bias, and some may be extremely subtle - almost impossible to identify, let alone do something about.\n\n2. Any time sampling is expensive (which in some sense is almost always a factor), or destructive of a limited resource.', ""Stochastic gradient descent employs the idea that approximations of the gradient using a subset (or even single data point) to approximate the gradient of a loss function for a step direction is much faster (computatonally)  than doing standard gradient descent and finding the true loss function gradient at each step. At least I think that's what's up. If anyone wants to correct me feel free!"", 'Generally speaking;  when there are potential ethical concerns; more samples are bad lol\n\nFor instance;  if I want to test the efficacy of a drug to reduce smoking addiction-I probably dont want to create a random complete blocking design where we select 1000 people out of the population and get a bunch of them to start smoking lol. Thatd be sucky for a lot of individuals (GET IT)\n\nMany posters here are calling out cost: and I just wanted to clarify that from a statistical perspective estimators will always perform better assuming that your sampling method isnt compromised (so for any size samples you dont want to use self reported data, or to a far less dangerous degree: stratified sampling ; which is totally legit but designed for smaller datasets so you might lose some performance). Large samples that use self reported data will give you high confidence in poor data.   But cost in operational overhead (be ir dollars or hours or whatever) is something to keep in mind;  because it can prohibit you from doing other experiments. \n\nThe question is always going to be; at what point does a large sample stop giving me practical gains in the quality of my estimator as a function of n?  And this is a tough question. Because depending on the distribution of your data the asymptotic  e behavior of the estimator will vary.', '[deleted]', ""When you want to shape inference for some different reasons like marketing for example. 9 out of 10 dentists recommend Colgate. If you look at the original study, they ask dentists to recommend 3 or 5 toothpastes. And there were 300 dentists i think. Obviously most do mention Colgate as it's one of the largest players but they also mentioned most other big brands. Ergo, you have the famous tagline, all backed up by a small flimsy study."", 'Generally, higher quality (more representative) data is better than large amounts of biased or flawed data (even if you capture to close to 80% of the population). A moderate amount of self-selection bias in the data can result in a severe reduction in your effective sample size.\n\nTrue random samples are king.', 'When there are industry standards for testing that call for specific sample sizes or test procedures, more testing is not only an added cost but may produce unexpected accuracy in summary statistics that can degrade decision-making. Remember that the purpose of collecting data is always, as Deming wrote, to provide a basis for taking action or a recommendation for action.\n\nFor example, if I collect data and report the standard deviation of the mean, collection four times as much data as usual will reduce the standard deviation to half its normal value. If this result is not understood by decision makers as simply an artifact of N, incorrect actions may be taken.', ""Having really large effect size that's easy to detect doesn't need big sample size and it is better not to collect big sample size in this situation"", ""When you're trying to win an argument and rely on the law of small numbers /s"", ""This depends on the question you're asking and the feasibility of collecting more data. \n\nYou don't need large N's to know that volcanic eruptions, nuclear plant meltdowns, and assorted other processes can be disasters.  What's more, they can't be easily replicated, so studying the mechanisms of their disasters will need to be limited to a relatively small n.\n\nPoliticians like to hide behind small sample sizes to avoid making important decisions."", 'There is such a thing as too much data. I\'ve worked on a couple of projects where we thought we were being smart by requesting the entire eligible populace with statements such as ""most precise estimates to date"" in the power/sample size section of the data application protocols. We later realised that we (the University) didn\'t have the computing power to analyse that much data via the models we proposed. Ended up taking a random sample and analysing that. Made the whole thing seem rather pointless.', 'cheaper to collect, easier for the explorative analysis, sometime smaller size but a good quality on the data is better than large size but poor quality. But overall when working with small sample size you need to somewhat make sure that its represent the population.', 'Well but of course. For OLS you generally have to invert a matrix to solve the residual equation and this is an outrageously costly computation to do with any ""real"" amount of data. I look at some of the tables at my job where we count with Ms and Bs and think to myself that no OLS regression here is worth the compute it would cost to run.', 'Depends upon the coupon though doesnt it? Sometimes the impact does come from as many as possible simply using it in a loss leader type of sale incentive. I suppose theres always a tipping point where your coupons can become plentifully in the hands of those who would have been there and paid full price though.', 'paper link:\n\nhttps://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf', ""> in fact, more data will exacerbate the problem by making you not only wrong but confidently wrong.\n\nOnly if you are doing it wrong. A proper data analysis will include systematic uncertainties that do not decrease with the sample size to cover issues with the data collection. If you cannot quantify that, don't start the study because it will be useless anyway."", ""As I recall, this is one of the reasons some major polls got the 2016 Trump election prediction wrong: their samples were nonrandom (possibly in ways they didn't understand), but the huge amount of data convinced some people any sampling problems could be ignored.\n\n*Edit*: Because I was very impulsive, I did not notice that the article linked deals with that issue explicitly."", 'Right, with a large sample size you will likely know the direction of the effect and if the effect is small you will almost certainly know it. The problem with small sample sizes is you might mistake a small effect for a large one.', 'More data using a poor sampling design can degrade the performance of an estimator \n\nClassic case:  large samples of self reported data.', 'Thank you!', 'I agree with you in the context of a study where you have good control over experimental design. But this is a major, major problem in the more observational context where ""big data"" is relevant or anywhere you have unquantifiable selection bias, e.g. survey responses or election polls.', 'I really recommend reading even just the abstract of the paper (linked by another user above). They address exactly this point!', '[deleted]', ""If you have no way to estimate your selection bias, don't start the study. Its result will be worthless.\n\nOf course, in practice you will always have some idea about the selection bias - from similar studies in the past, from other studies about the population you study, and so on.\n\nHoping that the statistical uncertainty will cover your selection bias is not a scientific approach."", 'I can tell you didnt get past easy stats theory.\n\nIf you did, youd listen to those of us with postgraduate degrees correct you on literally the basic shit.', '[deleted]', 'Every asymptomatic estimator assumes there is no inherent sampling bias.  \n\nIf one were to perform an analysis using larger datasets of non random, I.e. self reported data it vs then redoing the analysis with a random sample; the latter will always outperformed asymptomaticly.  You have violated the basic hypothesis \n\nIt will most likely be outperformed for small to moderate samples. This is a well documented phenomena. \n\nTry again.  I suggest also passing a basic stats course.', '[deleted]', 'Learn something from the other posters here\n\n Because right now there is no doubt that what we youre teaching is probably low quality, and probably not advanced level lol.', '[deleted]', 'Keep doing things wrong and teaching others to do things wrong. \n\nIf you cant see how non random data biases an estimator, and more of that data increases that bias, then we cant help you because you literally dont know the basics.\n\nStop hiding behind ignorance and just admit youre wrong. Its okay to be wrong. But youre never gonna grow if you cant you do that. And I can tell you that theres undergrads that can pass this part of a job or research position interview. You cant.\n\nAlso my post degree wasnt necessary to know this. My basic degree was because we covered it in an intro to stats class. Which you clearly have not. Stop being a proud idiot.\n\nTLDR:\nLess data that follows random sapling is always better than large data sets that are not randomly sampled. End of conversation. See stats 101, get humbled.']"
[S] I made an app to brute force demonstrate the answer to my favorite stats puzzle questions,"I thought it would be interesting to let people see the answer resolve for each of these 2 questions, as both answers are counterintuitive to most. The code is also included, so doubters can actually verify a fair simulation is being performed.  Very simple app, but maybe some here will enjoy!

https://codesandbox.io/s/echarts-playground-forked-1qzwkz?file=/src/App.js",135bpcb,rollie82,1683003530.0,9,0.74,"[""I'm not clicking a random link if you're not explaining what's going on."", 'If solved analytically, the answer to both questions is 33.33%, right?', 'codesandbox is a service to have a web-based IDE for snippets of code or small projects without needing to host/deploy anything. It\'s just running javascript, so under normal circumstances it can\'t hurt you.  I could provide a screenshot of the IDE and resulting output, but that doesn\'t prove anything either.\n\nIf you just want a description - it\'s a React app with Apache E-Charts to create two donut charts. It randomly generates between 1-~100 samples every 250ms of two children (with sex + day of the week they were born), and graphs the number of times one of two situations occurs, ""A family has 2 children; they have a son. what is the probability they have a daughter?"" and ""A family has 2 children; they have a son who was born on Tuesday. what is the probability they have a daughter?"".  Generated samples are discarded if they match neither of the two conditions; if they do match, the sex of the \'other\' is recorded and graphed, which updates in real time as samples are generated.', ""No, the answer is not the same for both. \n\nYou can obviously Google to get the answer, but it's worth trying to figure out the answer using pen and paper too."", 'Nope! The first is 2/3, while the 2nd is 14/27, which matches the results of the demonstration', 'No. The first one is 1/2.', 'Yeah i found out. You have 7*7*2*2 possibilities in the second question. You count how many of these contain at one boy born on tuesday and one girl born on any day, lets say that is n, and then the answear is 1/n\n\nSomething like this. I could say more concrete answer with pen and paper but its not that interesting. But i think i understand the general idea']"
[Question] Help to interpret Two-way ANOVA results.,"Dear all,

I am a PhD student working in the field of wildlife management. I am new to ANOVA; I did my analysis on SPSS. I wanted to know if any relationship exists between communities (indigenous and non-indigenous) across different administrative divisions (sectors).

Variables: 1) Ratings on strategies (dependent variable), 2) communities (indigenous and non-indigenous), 3) administrative divisions (sectors).

Tests of Between-Subjects Effects:

1. Shows that the community (F(1,498)=5.123,p=0.024).
2. Shows the sector (F(3,498)=21.051,p=0.001).
3. Interaction (Sector \* Community): (F(3,498)=5.020,p=0.002).

Link: [https://ibb.co/cxMZfKd](https://ibb.co/cxMZfKd)

Graph:

Link: [https://ibb.co/2kZb6Sd](https://ibb.co/2kZb6Sd)

Post hoc results:

Link: [https://ibb.co/b5sZn24](https://ibb.co/b5sZn24)

Which results are essential? I am confused with the Interaction term. What does the meaning of significance for all the results under 'Tests of Between-Subjects Effects'? Could someone help me explain my results? Thanks in advance.",1356k6q,n1ght_w1ng08,1682988269.0,1,1.0,"[""The ANOVA shows a sig difference, but you need to run post hoc tests to get the more in-depth info. The ANOVA just says there's a difference, but not what they are, thus the need for post hoc tests. There are guides on how to perform them on SPSS. They're relatively easy to do, especially if you know how to do ANOVAs. Those results will give you the info you want"", 'I dont think it is that helpful to do all pairwise comparisons. It would make sense to follow-up effects with more than 1 df such as Sector with the Tukey hsd comparing  the three sectors. For the interaction, I would start by with a graph of means that has sector on the X-axis and separate line for each community with the  DV on the Y axis. Then describe the interaction in a way such as: the effect of community was larger for Sectors 1 and 2 than it was for Sector 3. You might want to test simple effects such as testing the effect of sector separately for each community.', ""Hi, thanks for your suggestion. Here are the post hoc tests: [https://ibb.co/b5sZn24](https://ibb.co/b5sZn24)\n\nI am not sure how to read it, well it's because I'm the first one at my lab to run ANOVA and my Prof is reluctant to guide me as he doesn't know how to run it."", 'Hi, can I DM you? Thanks', 'Sure. Read [this article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0271668) first and then ask me questions about it if you have any.']"
[Q] Hypothesis testing and statistical testing. Is this procedure correct?,"Lets say I am given a dataset and told to figure out why customers are leaving our company. I take a quick look at the data and see that each row is a customer and each column is a feature. Within the features, among other things, I have a column named ""churn"" which indicates whether or not a customer has left the company.

From here I take a look at other features and notice a ""monthly charges"" feature. I think to myself, ""Maybe certain customers are leaving because they are being charged more than the customers who are staying"". I want to form a hypothesis test off of this statement but before I begin have I already committed an error? The first question I have is should I form my hypotheses before I have even looked at my data?

Lets say I didn't commit an error and move on to form a null and alternate hypothesis. My null hypothesis would be:

H\_o: Monthly charges of customers who have churned <= Monthly charges of customers who have not churned.

H\_a: Monthly charges of customers who have churned > Monthly charges of customers who have not churned.

I set my significance level of 0.05

From here I would like to visually confirm my test by checking the monthly charges distribution of each group and comparing them to each other. In this example the distributions don't overlap much at all and would leave me to believe that I can reject my null hypothesis. My next question is, should I stop here? I would like to follow this up with a T-test to confirm my findings with a significant p-value. Is this appropriate?

Let's say it is appropriate and I conduct a T-test and find that my p-value is less than my alpha. I then reject my null hypothesis and say ""Monthly charges of customers who have churned > Monthly charges of customers who have not churned.""

Does all this seem like the correct way to conduct a hypothesis test, visualization of the data, and a statistical test?

Also, Let's say I am later tasked with creating a logistic regression model to predict whether or not a customer is likely to leave based on their current features. I have already done the analysis and found there is a relationship between customer churn rate and monthly charges. Would my assumptions that this would make a good variable to include inside my model be a bad one to make because I don't know how other variables would effect it(confounding variables)? What about variables that I test and find no significance. Should I leave those out of my model? Or should I just include everything and see how they interact and do some PCA/Regularization to slim down my feature space?",1352t8v,AKing2713,1682978761.0,2,1.0,"[""> The first question I have is should I form my hypotheses before I have even looked at my data?\n\nYou should not be using the same data to formulate a hypotheses and to test them. If you really have no sense of what you want to find out/what hypotheses to test you could split your data up (at random, though not necessarily in equal proportion) so that they're not done on the same values. That will at least help you not to screw up your type I error rate with cherry-picking.\n\nhttps://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data\n\nWith some looking around, however, you might be able to identify research into causes of customer churn more broadly and consider investigating those."", 'Here a a few questions to think about:\n\n1. Do you have the entire population of your customer data, or just a sample? If you have the whole population you would not test for statistical significance - if the two means differ then they differ. At that point, you need to decide whether the difference has practical significance in that even if the numbers are different are they meaningfully different in a way that helps you. This will have to be determined through business knowledge. \n2. If you have a sample, doing a t-test would be a way to statistically test for that particular relationship. Of course, even if there is a statistically significant difference it does not immediately imply causation, particularly with other features available. This is a good exploratory step\n3. When fitting a logistic regression, you may find variables that have statistically significant differences between levels of your binary variable are not useful in the model when accounting for other predictors. \n4. Discriminant analysis is another avenue for you to explore as well.', 'Is this panel data?', 'Im sorry if I did not explicitly state that I would form my hypothesis and then later split my data into three sets, a train, validate, and test set. I would only be doing the statistical testing on my train set.', 'Thank you for the reply. Its funny, a few weeks ago I asked a question to response number 1s line of thought when it comes to sample vs population when we have what I would think to be a population. https://www.reddit.com/r/statistics/comments/12cr9z6/q_sample_vs_population_do_i_have_this_right/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1 . More or less the counter argument to that was I would still want to treat my company data as a sample even if I have all the data of every customer. This is due being able to make inferences on data that would be collected in the future and or policies that would effect my company through time. Still not sure how I feel about that response though.', ""This was more of a hypothetical situation to check if my logic was correct. In this instance let's say no, its current customers as is in the company at present time and the data is updated whenever there is a change in a customer's information."", ""Okay. A large number of people don't know to do this, so if it's not mentioned, it's important to bring it up."", 'Understood. Ya my fault for not explicitly stating it.']"
[Q] Figure ideas to compare genes present/absent for 57 archaea species,"Hello everyone!

I am comparing 57 archaea species (which can be divided into 4 orders/groups) in terms of their potential metabolisms based on their genes and pathways present. I have annotated my species all with a RAST + DRAM combination on Kbase.

I have collected quite a bit of data using combinations of eggnog-mapper, KAAS, and interproscan.

With this data in hand I want to start making figures to show my data. Therefore, I have decided on showing my data via heat-maps, venn diagrams, bar graphs, and PCA plots. Moreover, as my data is not normally distributed I am using Kruskal Wallis for my statistical tests.

However, does anyone else have ideas for graphs or figures to show my data, in particular figures showing the difference between species and groups in terms of having genes/pathways present or absent?

If so, I would be very much appreciated of the help.",1352h2r,MountainNegotiation,1682977951.0,2,1.0,[]
[Q] How can you determine if two survey questions are dependent?,"I want to test the dependence between 2 likert scale questions. Both are answered by the same population. The first question asks how often the respondent watches YouTube related to a certain field and the second asks if their career choice is affected was a result of watching YouTube videos. The options for both are:  


1. Strongly Disagree
2. Disagree
3. Agree
4. Strongly agree

I was initially going to use chi square test, but from what I've found it is usually used when there are different sample groups. Thanks in advance!",1350m98,robot-potato,1682973615.0,5,0.86,"['Many studies compare likert scales as continuous variables. But if you want to treat these like rank ordered variables, then you could use the Spearman rank correlation test.', ""You have the results from two Likert-type items.  You probably want to treat them as ordinal variables, and you are looking to find an association, or correlation, between the two.  Kendall *tau*\\-c is an appropriate measure of correlation. Because there are equal number of levels between the two, Kendall *tau*\\-b is also appropriate. Also, Spearman correlation will also work fine (and may be more available in some software packages).\n\nIt's possible to treat the data as nominal.  In this case you could use a chi-square test of association, with the corresponding measure of association Cramer's *V*."", 'It makes no sense at all to respond to a question of the form ""***How often*** do you..."" with answers like ""Strongly Disagree"" or ""Agree"".', ""Look into a chi square test of independence. It's used for just this. If your data is coming from a random sample with a complex survey design, there are packages in all statistical software to account for the weights and sampling design"", ""Unrelated, but I believe you always need an odd number of options for these types of questions, so you would need a neutral answer among those, otherwise you may be forcing a false answer in case the person doesn't know the answer to the question or doesn't have an opinion about something"", ""I'll look into it. Thank you so much!"", 'Its probably ""im watching youtube vides often"".  OP wasnt literally saying the question, s/he just said what the question is about', '~~Of course it does. Theres nothing inherently wrong with looking at this relationship.~~\n\nEdit: How often do you say retarded things?\n\n*Strongly agree*', ""That's... not the point /u/efrique is making? It's not whether looking at the relationship between responses to the two questions makes sense, it's whether one question and *its* response options make sense *by themselves*."", ""As /u/Binary101010 says, I'm not talking about the relationship, I'm talking about a single variable.\n\nHowever, it has implications for trying to investigate the relationship, since if the variable is nonsense on its own, the relationship would be nonsense as well. Your variables have to make sense *first*."", 'My bad yer totally right . I fixed my original comment.']"
[C] Stochastic calculus or AI,"Edit: This should also be flaired as education \[E\] as well.

I am a PhD student in my department in the stage of identifying my advisors. There are two areas in my department that I am particularly interested in, (1) Stochastic calculus and (2) AI specifically, RL type problems.

I know there are ways to bridge the two, but due to departmental politics, it is unlikely I can even get faculty from (1) and (2) to chair my dissertation. RL is too applied for the interests of faculty in (1) and stochastic calculus is too theoretical compared to the interests of the faculty in (2).

At this point, it seems that I should pick one or the other, and I want to pick a topic that will prime me for a career in some sort of quantitative research in tech or finance industries. My current training is more theoretical, and thus I have an easier time reading books on stochastic calculus, compared to reading say barto and sutton's RL book. Actually RL is still one huge black box to me, but it seems more and more job postings in quant research seek general ML skills. That said, I am willing to learn it, as their research questions they are asking these days is quite interesting to me.

TLDR: ""which technical skill is more useful for a career in quantitative research in industry, stochastic calculus or RL?""",134zcot,alberto-matamoro,1682970686.0,35,1.0,"['I started out my PhD working SPDE theory, specifically large deviations, and I can tell you that I **never** use any of that knowledge in my career.  Might be useful in finance, but personally I think RL has much better promise right now.', ""I'm sorry you're getting hit by academic politics so early into your career. \n\n*Everyone's* doing ML right now. It's probably way less competitive to be the top of your research domain if you do the more theoretical route. Also, the more deeply theoretical stuff is harder to self teach. Once you get the main ideas and understand the notation, you'll be able to keep up with the ML/RL/AI research on your own. I bet there's a larger fraction of stochastics people who are comfortable reading RL research than RL people who are comfortable readings stochastics research.\n\nIn any event: focus on whichever topic interests you the most. This is *your* PhD, and the goal is for you to exit the program as the world expert in <incredibly focused niche>. Pick a topic that feels like you can commit your life to it. That you would look forward to spending all of your time thinking about and trying to push the envelope. Forget about the industry, you'll be fine."", 'Job wise, you can do better in AI.', 'If you have an easier time with stochastic calc books, do the RL topic and keep on top of stochastic in your personal time.', 'You should definitely ask r/quant but my best guess would be to do RL because it seems like you could self study stochastic calculus with greater ease.', 'As far as Im aware, stoch calc in finance is primarily used in derivatives pricing type roles (Risk quant, Model validation etc), unless you find yourself in one of the few shops where they may be doing some research on impact models and market microstructure for HFT.\n\nIMO ML is far more applicable in QR, if thats what youre aiming for. \n\nUltimately however, do not think that this will tangibly impact your career options (for finance at least, no clue about tech), so you may want to pick the topic you enjoy the most.', ""To clarify: stochastic calculus is increasingly used in AI to model the dynamics of neural networks and other deep learning algorithms. \n\nAnd as for reinforcement learning being separate from stochastic calculus, Bellman (of the Bellman equation) would disagree with your faculty. The theory of Markov decision processes was developed in the '50s and '60s for RL and related problems, under the rubric of dynamic programming and optimal control. These days what is trumpeted as the bleeding edge of ML often turns out to be a special case of something that was well-known to that generation of researchers, but applied to a large dataset and implemented in high-performance code.\n\nThere are a lot of people in ML who have weaker theoretical backgrounds but who code fast and are really good at putting together pieces of techniques they find into a paper. The limitations of this approach are becoming more apparent. Theoretical understanding does matter. As the need for transparent, secure, interoperable, performant, scalable, robust ML grows, so too does the need for a theoretical understanding of the field.\n\nI would advise you to go with whichever program you feel is most interesting. You may not get another opportunity to seriously study a subject you're interested in, let alone make an original contribution to.\n\nPhDs in math, physics, stats, CS, etc. are often not hired for their specific expertise in a particular area. They are hired for their analytical thinking skills, their ability to work through a challenge in depth and develop original approaches to a complex problem where there is no cookbook solution. \n\nThis means it matters somewhat less whether you pick a trendy field."", 'Rl', ""Combining stochastic calculus with AI doesn't strike me as a very promising research direction. And being an expert in AI / reinforcement learning is way more marketable than being an expert in stochastic calculus.\n\nIn the old days finance firms were solving the Black-Scholes equation or something and stochastic calculus was important for that. But these days aren't finance firms mainly doing machine learning and convex optimization? Stochastic calculus seems less important in finance than it used to be."", 'thanks for sharing this experience', 'SPDEs are too remote from serious application in finance. The good thing in the field is that, whatever mathematical object you have there will be some obscure situation in finance which you could formulate with it but the chance is low that this becomes actual standard. There is some application to limit order books but Im unsure how deep the application really goes.', 'Awesome, thanks for the perspective', ""thats what I'm seeing too. AI seems to have immediate use cases for business stakeholders whereas stochastic calculus seems to be a high brow topic to most stakeholders."", 'thanks, the motive here would be to focus on a challenge that I am less familiar with where I can have some expert guidance', ""would you mind if I dm'd you about the career in finance? seems like you know something about it."", 'SPDE and limit order book ?', 'Sure absolutely', 'Yes, google that and youll find some material']"
[Q] Sudden significance in OLS without mediation or correlation?,"Hi all,

I am conducting some data analysis and am having trouble interpreting significance in my multiple linear regression results. Specifically, when I go from \[Regression 1: DV \~ Treatment 1 + Treatment 2\] to \[Regression 2: DV \~ Treatment 1 + Treatment 2 + Covariate\], I am finding that\*,\* in R1, Treatment 2 is not a significant predictor, but that this is the case in Regression 2 *despite* Treatment 2 and the Covariate seemingly neither being a) correlated (r= 0.008) nor b) in a mediating relationship.

Why might Treatment 2 suddenly be significant without mediation or correlation? For reference, the Treatment variables are both dummies, and the Covariate a variable that takes on integers 1 through 7 (Likert scale).",134qalk,GAKM,1682956476.0,1,0.67,"[""Here's a screenshot of my Stargazer output, for reference: [https://imgur.com/a/JKUiQzs](https://imgur.com/a/JKUiQzs)\n\nI am referring to regressions (1) and (2)"", 'Statistical significance is a generally meaningless term. Adding variables decrease power. Think about a Bayesian model: I added a variable and now the posterior distribution for one of my coefficients crosses zero a little bit more than it used to. It makes no sense!  We cant make any inferences from this.', 'Moreover: adding for covariate could explain more of the variation in your DV and change the which observations contribute to the parameter estimates.']"
[Q] Power analysis post hoc?,"Hi all,

I just received a manuscript back from a journal, and I am trying to wrap my head around one of the reviewer's comments. Some background: I am not very knowledgable in stats.

Basically, we concluded that a difference we found was likely based on a large difference in sample size (one group had a n of 50, the other an n of 11). The differences were no longer significant at follow-up.

The reviewer mentioned that to justify this conclusion, we need to conduct a power analysis. I've watched a couple youtube videos on this, but I'm still confused on the basics of what it is/how to do it.

Any help/guidance is appreciated!",134pb7o,TrafalgarLawMD,1682954240.0,17,0.92,"[""Post-hoc analyses are of no use and a reviewer should know better (unfortunately many don't). Here is a [good practical demonstration](https://data.library.virginia.edu/post-hoc-power-calculations-are-not-useful/) of why they're useless, with references to some of the primary literature. [Here](https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/) is a more acerbic account of why they're rubbish."", ""Power analysis is important, conducted before the study. Power analysis based on the sample you collected (*post hoc power*) is not helpful or informative; it's (almost purely) a monotonic transformation of the p-value.\n\n/u/dmlane gave a good list of references here:\n\nhttps://www.reddit.com/r/statistics/comments/12ucxre/q_help_replicating_power_analysis_from_a/jh6keif/"", ""> we concluded that a difference we found was likely based on a large difference in sample size [...] The reviewer mentioned that to justify this conclusion, we need to conduct a power analysis. \n\nthe sample sizes you observed were sufficient to detect effect sizes above a certain threshold. Your assertion is that this threshold is actually higher than the lowest effect size you actually care about. \n\nThey're asking you to clarify concretely:\n\n* what is the lowest effect size the existing experiment design is sensitive to\n* what the threshold effect size you are interested in is\n* how this experiment design would need to be changed to be sensitive to this lower threshold"", 'GPower is a good option, as it allows you to put in your values and spits out your power (or sample size).\n\nI recently had to do this for a manuscript and it annoyed the hell out of me given that posthoc power calculations are inherently problematic (see Heinrich et al, 2010 if you want an explanation).', ""Thanks for your input! I'll take a look, and show it to my PI. Maybe we will disregard this part of the reviewers advice"", 'I dont think it is justified to assume you have an effect and explain away the lack of significance based on a power analysis. Better would be to state upfront that you dont have a conclusive outcome and give a confidence interval. The confidence interval would likely reveal that your data are consistent with large effect sizes but are also consistent with no effect. Better to be explicit about the uncertainty than to blame your experimental design. You could comment that a second study with a larger sample size would likely be informative.', ""thank you! I'll look into it!"", ""Hey, quick follow up!\n\nGot the program and tried watching a couple videos on it. I assume that I'll be looking under the settings: Post hoc: Compute power - given a, and noncentrality parameter (lambda)\n\nDo you know where I can find what the noncentrality parameter (lambda) and Df value that I need to input would be? \n\n[https://imgur.com/a/JZ57Bea](https://imgur.com/a/JZ57Bea)\n\n\\^\\^here is what I am referring to for clarification"", 'Post hoc power analyses merely restate the p value in different mathematical form. If you search, there are many articles out there showing they should not be run and how to push back against reviewers who ask for them.', 'Just because its rubbish doesnt mean you dont need to appease the reviewer in some way. Perhaps you can try it and if the results come out favorably just include it and if they dont include a note about criticisms of these tests and why you still think the results are important and valid', ""> I dont think it is justified to assume you have an effect and explain away the lack of significance based on a power analysis\n\nWith pure equality nulls, that's nearly always what it's going to be, though. \n\nIndeed, that's why very, very large studies almost always reject -- they pick up all those extremely small effects. *Exact* equality is almost never true.\n\nThat said, I mostly agree with your points there."", 'Thanks for the comment. I agree that exact equality is almost never true. However, when a difference is significant you can reach a confident conclusion about the direction of the difference. Otherwise you cannot.']"
Help with a probability problem [Q],"My friend and his partner are considering joining the police force. Their apprenticeship rota consists of 6 consecutive days on followed by 4 consecutive days off and we're assuming the choice of rota is random and so all are of equal probability.

Can someone please work out the probability for each number of 'off' days that line up between them? The maximum being 4 and the minimum being 0 of course.

I could brute force all permutations but I'm convinced there's a statistical approach that's way easier and I'm just not clever enough to work it out!",134oqoy,goneChopin-Bachsoon,1682952977.0,1,1.0,"['I\'m assuming a rota can start on any day. \n\nCall the first day on for person 1 ""day 1"" and look at the possibilities for Person 2 relative \nto Person 1:\n\n       Person 1:\n         + + + + + + - - - -  \n       Person 2:            \'off\' overlap\n         + + + + + + - - - -     4\n         - + + + + + + - - -     3\n         - - + + + + + + - -     2 \n         - - - + + + + + + -     1\n         - - - - + + + + + +     0  \n         + - - - - + + + + +     0 \n         + + - - - - + + + +     0 \n         + + + - - - - + + +     1  \n         + + + + - - - - + +     2 \n         + + + + + - - - - +     3 \n\nThe average overlap across all person 2\'s possible start days  is 16/10 = 1.6 days', 'This is exactly what I needed thank you dude!']"
How to find the relative dispersion of a list of numbers over a number in it? [Q],"In the game of chess a white advantage is defined as a positive number, a black advantage is a negative number, a draw is 0. 
Let me give an example of what I'm trying to calculate; let's say an engine from a certain position on the board calculates 3 best variations (from white perspective): 

| Variation | Evaluation |
| -------- | -------------- |
| Best One    | 0            |
| Second Best   | -50            |
| Third Best   | -120            |

How would I be able to find the relative dispersion of the various variations over the best one? 

I'm leaning towards using a coefficient of variation; my pain points are:

 - would zero here be considered ""meaningful""? From Wikipedia,

> The coefficient of variation should be computed only for data measured on scales that have a meaningful zero (ratio scale) and hence allow relative comparison of two measurements (i.e., division of one measurement by the other).

 - the dispersion would be more significant as much as values gets closer to 0 (e.g. a list like [0,1,2] would be much more disperded than a list [998,999,1000]). How would I be able to account for it?

Sorry if these questions sound naive but I'm a noobie in the field, and thanks for your time.",134m6rb,aerdna69,1682946848.0,1,0.57,"[""I don't have a perfect answer for you.. CoV will account for the second point as the mean will be higher for the 999 case vs for the 1 case. You could consider these on interval scale for sure. There are third or more degree moments if you want to look beyond standard deviations.""]"
"[Q] How to calculate Var(X+Y) given Var(X), Var(Y) and correlation coefficient between the 2 variables?"," 

I am adding the entire question below for context. **This isn't homework** and I already have the solution. I don't understand it though, so I would be grateful if anyone could explain.

While computing ""Value at Risk"" in finance, we are first required to calculate the standard deviation for the entire amount invested. In the given problem, our investment consists of 2 smaller investments, but the catch is that they are correlated. I'll add the question below:

This was the question given in 1 of my textbooks (Financial Management):

>Consider a portfolio consisting of a 20,000,000 investment in share XYZ and a 20,000,000 investment in share ABC. The daily standard deviation of both shares is 1% and that the coefficient of correlation between them is 0.3. You are required to determine the 10-day 99% value at risk for the portfolio?

The first step in the problem in the solution is to calculate the standard deviation of the entire portfolio of 40,000,000. If the 2 investments, which I take to be random variables here were independent, we would get Var(X+Y) = Var(X) + Var(Y). But here it's given that they're correlated. Plus the variance given in the solution is 0.65 (i.e. a standard deviation of 0.85%). I don't understand how this has been calculated.

Any help would be much appreciated. Thanks!",134lag2,Minato_the_legend,1682944522.0,1,0.54,"['var(x+y) = var(x) + var(y) + 2cov(x,y)\nyou can use the correlation coefficient to solve for the covariance', 'The explanation for this problem is written out here:\nhttps://www.ssei.co.in/wp-content/uploads/2019/05/SFM-DAWN-PAGE-219-TO-231.pdf\n\n1 lakh is an Indian term for 100,000\n\nIf you want to know where 0.65 came from, you may need to include a little more context surrounding that number.', 'What you are finding here is actually VAR(0.5X, 0.5Y), 20M in each investment so 50% weights in both assets. \n\nAlso note you are not given the covariance but the correlation, so to get the covariance you should use, corr(X,Y)\\*SD\\_x\\*SD\\_y\n\nThe provided solution of 0.65 is correct.\n\n(0.5\\^2 \\* 1\\^2) + (0.5\\^2 + 1\\^2) + 2\\*0.5\\*0.5\\*0.3\\*1\\*1 = 0.65', 'I did try that but then I get Var(x+y) = 1+1+2(0.3x1x1) which is 2.6\nThe variance given as the solution is 0.65', ""[https://imgur.com/a/6FSyD3p](https://imgur.com/a/6FSyD3p)  \nI could not directly add the image in the post, so I posted it on Imgur and here's the link.   \nAs you can see, they calculate the standard deviation for 1 day first and the subsequent calculations come later.   \nOnce the standard deviation for 1 day is calculated, I know how to do the rest of the problem, it's just this part that I don't understand.  \nAgain, thanks for your help"", 'Thank you  for  the  explanation, I get  it now.  But one question. The  forumla  is  V(X+Y)  =  V(X)  +  V(Y)  +  2cov(X,Y)  right?  So  when  we  take  X+Y  and  X  and  Y  are  identical,  doesnt  that  automatically  make  the  standard  deviation  of  X  and  Y ,  expressed  as  a  percentage  of  X+Y  0.5x  of  whatever  it  was  as   when  expressed  as  a  percentage  of  X  and  Y  individually?  As  in  does  the  formula  not  already  account  for  that?  Or  is  it  used  in  a  specific context With some implicit assumptions?', 'I think this more nuanced than the Var(X+Y) formula. You were given daily standard deviations and are being asked the 10 day value at risk. Im not sure about the specifics of the math but Im assuming that you have to factor in the daily variations in this problem. Applying the formula strictly gives .0001 + .0001 + .6 = .6002. Summing the daily changes in some way should probably account for the missing portions', 'Can you show the whole problem?', 'You correctly found that Var(X+Y) was 2.6 *in your original set of units*, fractions of 20,000,000, but if you are expressing your SD as a percentage of the amount at risk, rather than an absolute number, you have to remember that the combined portfolio is twice as big as the original was.\n\nNotice that 0.65 = 2.6 / 2^(2), or 0.81 = sqrt(2.6) / 2. \n\nIf you started with two $20,000,000$200,000 investments, you combined portfolio has value $40,000,000$322,500.', 'Sure.  Var(aX) = a^2 * Var(X).\n\nSo with the line starting ""Alternatively..."" they are calculating the variance in terms of percent.  The tricky part, though, is that 1% of the whole portfolio is composed of 0.5% of the first asset, and 0.5% of the second asset.  So, the equation is more like:\n\nVar(0.5X + 0.5Y) = Var(0.5X) + Var(0.5Y) + Cov(0.5X, 0.5Y)\n\nallowing the numbers inside the Var and Cov to be expressed as percentages.\n\nThus:\n\n= (1)^2 (0.5)^2 + (1)^2 (0.5)^2 + 2(1)(1)(0.3)(0.5)(0.5)', 'no, the actual/more general formula for the variance of the sum of 2 random variables is\n\nVar(aX + bY) = a\\^2 \\* VAR(X) + b\\^2 \\* VAR(Y) + 2(a)(b)COV(X,Y).\n\nby using V(X+Y) you are assuming that your portfolio is has a weight of 1 on X and 1 on Y, which is impossible (in terms of portfolio construction), since the weights of assets in portfolio must always sum to 1.', ""https://imgur.com/a/6FSyD3p\n\nI could not directly add the image in the post, so I posted it on Imgur and here's the link. \n\nAs you can see, they calculate the standard deviation for 1 day first and the subsequent calculations come later. \nOnce the standard deviation for 1 day is calculated, I know how to do the rest of the problem, it's just this part that I don't understand.\nAgain, thanks for your help"", '[https://imgur.com/a/6FSyD3p](https://imgur.com/a/6FSyD3p)  \ncould not post the pic in the post itself so Ive added a link here. Thanks for looking into it', 'Thanks a lot, yeah that makes sense now. But just 1 question. the Formula says V(X+Y) = V(X) + V(Y) + 2cov(X,Y) right? So here X+Y already means that SDx or SDy when expressed as a % of X+Y should be 0.5x what it would be as a % of X or Y individually. My question is, does the formula not already account for the weights? Because it says V(X+Y). So V(40 million investment) = V(20 mn inv) + V(20 mn inv) + 2cov(X,Y). Why do we need to specifically include the weights in the formula? Or are there some implicit assumptions in the formula that Im missing?\n\nsorry if Im being stupid, but I feel like Ive semi understood it but not yet 100%', 'Thanks a lot, that makes sense. But then shouldnt the Var(X+Y) formula be adjusted for weights? That is:\n\nVar(X+Y) = (X/X+Y)\\^2 \\* Var(X) + (Y/X+Y)\\^2 \\* Var(Y) + 2\\*Correlation\\*(X/X+Y)\\*SDx\\*(Y/X+Y)\\*SDy\n\nBecause X+Y here is already the sum of the 2 random variables and of X and Y are identical, then SDx and SDy as a percentage of X+Y is obviously going to be half of the percentage it was for X and Y individually. So shouldnt the formula Var(X+Y) = Var(X) + Var(Y) + 2cov(X,Y) be modified to adjust for weights? Or is the formula given in a different context?', 'Oooh okay okay got it finally. Thanks a lot! Best answer ', ""Variances and standard deviations are, strictly speaking, *never* expressed as percentages; X and Y each have mean 20,000,000, standard deviation 200,000, and variance 40,000,000,000. The basic V(X+Y) formula is intended to apply to those numbers, and when applied to them, will give you V=104,000,000,000 or SD~322,500. You have presumably been given a shortcut to save you from working with such large numbers (probably a poorly-explained one, and one that will only work under certain circumstances.)\n\nYou will occasionally (in pure-math statistics) see the 'coefficient of variation', a dimensionless quantity defined as standard deviation divided by the mean (here .01 for the original investments and .0081 for the combination.)"", 'Ooh okay yes yes correct, now that you mention it Ive never seen stddev expressed as a % before. And yeah the book does not at all explain how to calculate any of these terms, its a finance book and it simply gave a formula. So yes, your explanation clarified it for me and I have understood it now, thanks a bunch!']"
How should I set myself up for a good masters program? [E],"Its basically exactly what the the title says. Currently Im at the end of my freshman year, and Im a finance and statistics major. Ill probably end up dropping the finance and adding on a math major this summer, hopefully graduating in 3 years from my undergrad institution. 

Profile:
School- Indiana University Bloomington

Demographic- Latino, Domestic 

GPA- 3.46

Finished Calc 1 and 2, probably taking Calc 3, 4, Linear Algebra next year and will be on course to take 2 Real Analysis courses as well as Diff. Eq. Took 2 applied stat courses my freshman year, and some Python-based coding 

Is there anything else I should ideally aim for? Really just want to know what a good graduate program expects from their candidates.",134ezhu,Snoo_85604,1682923741.0,1,0.54,"[""Good math skills.  You should take Real Analysis, and the 400-level Linear Algebra class.\n\nIf you could take the core master's prob and stat sequence while in undergrad, you would be a shoe-in."", 'Agreed. Also, make sure you get to know some of your math/stats professors well. This will give you solid letters of recommendation for grad school and the potential for doing research during undergrad, both of which will be very helpful.']"
[Q] how to account for covariate in point biserial correlation in SAS?,"So I got a lot of good advice in my last post which I really appreciate; I did an ANCOVA and found that age is indeed a covariate. I wanted to do a t-test originally, but I don't know how to account for a covariate. I also wanted to try out a point biserial correlation but I also don't know how to account for a covariate in SAS. Ive spent a good hour searching the Internet for an answer and I haven't found anything. I'd appreciate the help!",134daed,flickerflame13,1682917816.0,0,0.5,[]
[R] Two-way non-parametric ANOVA tests?,"Hey there, I'm thinking that with my data set I need to find a solution for a two-way non-parametric ANOVA but wanted to confirm that and see what my best options are.

I have a study that measures the level of 48 different analytes in samples. The samples being tested received 2 different initial treatments (A or B), and then a later treatment (X or Y) before analysis. So with the testing I have groups of:

* A + X
* A + Y
* B + X
* B + Y

With the data I have I tried running a normality test in prism on the values for the analytes and only 5-10 came back with normal distributions. I'm under the assumption I need to run a 2-way non-parametric ANOVA but wanted to (1) make sure that is right, and (2) if so what test can I run.

Thanks!",1347xil,turtle_flu,1682901510.0,1,0.67,"[""2-way anova doesn't have an assumption about the distribution of values in each group, or the distribution of the dependent variable.  (I can't quite tell what you're looking at.)  \n\nYou can look at the residuals from the model.\n\nAlso, normality testing in this context is not useful at all.  It doesn't tell you what you want to know.  It doesn't give you any indication if it's reasonable to use the model.  Perhaps see, [https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90](https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90) .""]"
[Q] Parameter vs Statistic,"  

Assuming I have a data sample of sugar concentration found in 25 regular coke and 25 coke zero where coke = 10.9 g, s = 0.5 and zero = 2.5 g with s = 1. Since these are the population data, they are considered parameters. 

However, if I am only interested in regular Coke and hypothesize that 25 regular Coke drinks have a mean sugar concentration lower than 11 g, then my statistical hypotheses are H0:  = 11 ; H1:  < 11. 

Since this is a sample taken from a population of both regular Coke and Coke Zero drinks, the data for regular Coke drinks are considered statistics (x-bar =11) as they refer to the characteristics of the sample of 25 regular Coke drinks that I have collected.

Am I right on this?",1346oxu,limkel,1682898177.0,0,0.5,"[""I am confused with your setting. If you have population parameters, there's no sampling/estimation needed. No need to make hypothesis because you know the distributions already.\n\nIf you do not have the population parameters and instead you have a a sample and want to do hypothesis tests about the mean, yes, you could use the sample mean to create estimates, try to test stuff and determine confidence intervals.""]"
"[Q] Should I use OLS robust standard errors, FGLS or PSCE for regression?","I am at the end of the road as I have posted this almost everywhere and I got no responses. 

My sample size is 8 banks over 10 years. There is heteroskedasticity but no serial autocorrelation. I use stata 13.

Correction: PCSE*",1340t1a,EbiraJazz,1682883223.0,0,0.5,"['What do you think the pros and cons are of those methods, given your panel data?\n\n(Instead of just posting the same thing for the 8th time in two weeks. Maybe try a different approach?)']"
"[Q] How do I calculate ""effect size f^2"" for post hoc power analysis in G*power for a hierarchical regression model?","While I understand that doing a post hoc power analysis is a bit rubbish thanks to the wonderful response I received on one of my previous posts here, I still have to provide the results of one to a reviewer anyways (with some added explanation as to why it is not good practice). With that said, I am doing a post hoc power analysis in G\*power using the F tests test family with the Linear Multiple Regression: Fixed Model, R\^2 increase option. To do so I need to input a value for ""Effect size f\^2"". Does anyone know how to calculate this? I tried reading the manual, but I'm totally out of my element. 

Thanks so much for any help on this!",1340e40,TheEudaimonicSelf,1682882127.0,2,1.0,"[""I don't humor dumb requests from reviewers and neither should you"", 'Sorry the reviewer is giving you such a tough time. They probably learned in grad school you should do post-hoc power following a non-significant effect. You absolutely dont have to give in to the reviewer. I dont know but I have been around a long time (retired two years ago) and have never found an editor who would require a meaningless analysis if a good explanation and/or proper references are given. You have to please the editor and not the reviewer. More important, please yourself. Its your article and it reflects very badly on you to report statistics that never should be done. You could say that to the editor. The critique of post-hoc power should be in your letter to the editor and not the article itself.', 'Calculate your observed f^2. The reviewer asked a stupid question and should get a stupid answer.', ""It's Cohen's f-squared.  You can find this online or in Jacob Cohen's book, c. 1988."", 'Okay, thank you. That makes sense. Most of my analyses were significant though, and I did not run a priori power analyses. Does that matter?']"
[Q] Course about prescriptive statistics,"I recently started working as a data scientist in one company and they require me to do prescriptive statistics there. What they want me to do is find/prescribe rational values for some products that will reduce the cost of an action X. I use Python and I've never worked as a prescriptive engineer before so I need to find a course that will make me learn the things. I know some algorithms used for optimization, mostly for constraint based optimizations, like simplex method, simulated annealing, particle swarm optimization, karmarkar's algortihm etc. but I'd like to learn more. 

&#x200B;

If anyone knows a good course or a tutorial about prescriptive statistics with good examples, please do let me know.",133ylie,AnthinoRusso,1682877703.0,3,0.8,[]
What do top 15 departments have that “lower ranked” departments don’t? [Q],"I noticed when looking at phd placements at some top stats departments (by top I mean anything top 20-15) they place into really prestigious companies (research labs at FAANG, Quant Researchers at premier hedge funds). I compared this to  some other Lower ranking  departments (anything 30th or lower), and noticed that the placements are still solid, but there are not a high amount of placements into the same type of companies people from the top departments go to. 

My question is, with regards to industry prospects, what do the top departments have that can place students into these prestigious roles in industry, that a lower ranking department cant? I know people here have said phd stats dept rankings dont matter for industry, however, when looking at the difference in placements between these two baskets of schools schools, it seems that the top 20 programs place students *actively* in such companies, whereas in lower ranking departments its quite rare. 

Can anyone explain why? My goal if I was to pursue a PhD in stats is to get into such roles in the industry, (which I need to reevaluate if this is even a good motivation for doing a phd anyway), so I was thinking about reapplying to these programs after my MS in stats. Of course, they are quite competitive, but I feel as though Im missing out on something that these top 20 departments have if I dont go to one for my PhD.",133vdib,AdFew4357,1682869750.0,35,0.87,"[""Reputation often breeds reputation. It also breeds increased access to world-class faculty, funding, and students (and, consequently, training). It's a cycle."", 'They have more famous professors. Also they can be more selective of their students.', 'Look if you can get into an excellent department and graduate, then by all means do it. \n\nTalented undergraduates have a tendency to way underestimate: (i) the attrition in PhD programs and (ii) the use of a PhD in a lot of careers that are not necessarily academic posts or the ""most"" prestigious industry jobs. They also have a tendency to overestimate their own talents and the likelihood that they are going to be a top student in a top PhD program.', 'A lot of it is self-selection. Folks gunning for those ""prestigious"" jobs preferentially sort into the big-name schools. There\'s also some peer pressure in play at the top schools: if you see your peers are going into these jobs, you feel like you should too.', 'Companies hire regionally. Most of the top programs are regionally located near tech and finance centers.\n\nNext you might ask, why is this? The answer---heavy investment from the US federal government, largely since WW2, especially Harvard, Stanford, MIT.', 'Selection bias. Top schools and top companies have the same type of barriers to entry. People who are well connected can get into both.\n\nStatistics is the same regardless where you learn it.', 'You do understand that these rankings are 100% bullshit done by no reliable methodology and lacking any statistical measures of reliability or validity?', 'Reputation. Quality of students. Training.', 'Id say quality of students is just better at top programs. The education is more rigorous, the environment is much more competitive, professors are generally better, etc. you will still get interviews from lower tier program Im guessing but you probably wont get far', '1. School name recognition and reputation\n2. Signaling that you earned admission to an elite university by passing through a rigorous gauntlet of GPA and standardized testing\n3. Higher profile PhD research therefore higher profile R&D experience\n4. (The classes may be slightly more rigorous and better taught at an elite program, but I think this is less significant than 3.)\n5. Peer effects - some learning will be from your fellow students, and your fellow students will be higher-performing at an elite university', 'Networking, which means opportunities. Its as simple as that.', ""I think it's a mix of everything people are saying. All else being equal, companies prefer to hire from higher-ranked schools because the graduates come with less risk (perceived or real), and because better pedigree of employees at a company/unit improves the reputation of that company/unit. Taking program reputation out of the equation, higher-ranked programs tend to have (on average) better PhD students in the first place, in addition to better connections and training due to attracting better faculty, funding, etc."", 'Resources (send student to conferences, have symposiums, have weekly activities for education and also fun, job fairs)', 'Top-ranked schools receive more non-regional corporate recruiters, particularly for internships. Corporations tend to prefer former interns to other applicants for permanent roles, all else equal.', 'Do you think its fair of me to only want to do a phd in stats if its at one of those departments? Frankly I cant see myself being motivated to do it unless I was at one of those institutions.', 'I agree. Of course I dont think Im even remotely qualified as an undergrad right now for these places. Frankly I wasnt when I applied for PhD programs this cycle. But I feel like this cycle I dont think I got a fair evaluation of my abilities as a student. The first two years of college my grades suffered due to not knowing what I wanted to do, and taking classes which werent even relevant to stats and doing poorly in them. I completely turned my gpa around in my third year and then applied this year in my fourth year, but I simply do not believe the first two years of my college career are representative of me as a student in a PhD program. I learned how to study differently, learned the relevant material and developed interest in stats, and found my strengths. I just feel like if I have a shot at those programs again through doing research in my MS and other things, its worth reapplying. I think I wouldnt care if I didnt get in at that point cause I know they have a fair evaluation of me as an applicant (an actual MS student who is performing at a high level in their MS), vs me this year (a confused undergrad who did kinda okay).', 'Its not 100% BS. They poll academics to get these rankings. Going to a school that your field thinks is very good is often a good idea if you can.', 'I do get that yeah. But like how can you write off a Washington, CMU, Columbia, Duke, or Harvard? These names carry weight when you have a PhD from it and want to go into FAANG or Finance. This is just what my hunch is.', 'Thanks. This confirms that my decision to reject a PhD offer from a lower ranked department for a funded MS elsewhere was a good idea. Hopefully in two years my funded MS will get me offers at top departments. If they dont, then industry is always there and Im okay with that. I just dont think I can justify spinning my gears of 5 years if Im not reaping the benefits of 1-5.', 'Number 4 is definitely not the case. Content expertise does not necessitate pedagogical expertise.\n\nThat said, I think you are right on the rest in the context of going to industry.  For going into academia, the reputation of the concentration and your advisor are more important.', 'Who cares if it\'s fair? If it\'s what you want, it\'s what you want. \n\nYou may want to consider what\'s good for your future and compare what options you have with no Ph.D. at all to what options you have with a Ph.D. from a ""lower ranked"" school, but you don\'t need a Ph.D. to have a happy life, so don\'t get the degree just to have the degree (unless that\'s something that is worth 5 years of your life to you).', 'Its only fair for you if you even get a place in these thats not too easy.', 'They are not doing scientific polling by a reputable polling organization.  In fact, they are also factoring in a lot of data provided by universities.', 'Agreed. 90% bullshit... 95% tops.', 'I did not say that every highly ""ranked"" department was terrible, although some of those you name would not be in my top 15 (and I am not going to bother to argue which).  I only said the rankings were worthless.  Your idea is just as good as US Snooze.', 'Yeah. I think Ill have to see what sticks in two years. Who knows if after my MS I even want a PhD. I always tell myself that if I got the jobs and career I wanted to be in post MS without a PhD, I wouldnt even think twice about the PhD. To me the phd is the means to an end (how can I get the degree to get me in X career), the way I have been navigating this whole PhD or no PhD thing is saying yeah a PhD in stats from Harvard will get me to xyz, therefore maybe I should consider it, thats the only reason why. Maybe thats not a strong enough reason to do a phd either. I just feel like if I got my bachelors in stats from a top 25 department I should be going to a PhD from a higher institution. This year it was a complete wash and I got into 1 phd program which was ranked much lower; not placing students where I wanted to be; and frankly wasnt my top choice. I got into a funded MS with no PhD program (the MS is a terminal degree, its at a R2 institution), and Im just thinking about trying to do things to beef my resume for phd apps. And just seeing if I get funding anywhere in the top 20 and revaluate from there.', 'Theres not a lot of transparency about how they do the polling exactly as far as I know. But the results are fairly meaningful, it is good advice to tell people to go to the higher ranked school, all else being equal.', 'I agree that the rankings are a wash, but I guess what im saying is Id rather be in programs which place people in the careers I want to be in than go to departments that dont. And it just seems like some departments do this better than others and those departments are ranked higher than what I got this cycle.', 'The advice I\'ve always heard and given is to only do a PhD if you\'re truly, deeply interested in research. Of course many people who get PhDs go into industry, but I think it only makes sense to go for a PhD if you really care about research. Otherwise economically (and probably for mental health) it just makes more sense to get an MS and go straight to industry. But I don\'t think I\'ve heard of the goal of getting a PhD in order to land a job at a ""top company"" before. I don\'t know enough about the tech/finance landscape to know a) how much better working at a top company is than working at the average company, and b) how difficult it is to get a job at a top company with a PhD from a lower-ranked department. My instinct about b) is that since industry is more meritocratic than academia and that the demand for stats is pretty high, it would be marginally harder but by no means impossible or even that difficult to get a job at a top company with a degree from a lower-ranked department, especially after building experience, connections, and a resume while working at non-top companies. But maybe I\'m wrong.', 'So you believe.  Lots of people believe lots of ridiculous things with no evidence.  So what?', ""Whatever.  A lot of people believe in this stuff, so you won't be wrong to rely on those fools being foolish."", 'Yeah. Im not aware either. I guess my own internal belief is I just want to be at a top department. Im gonna apply and see what i get. Of course theres the chance I dont get in but at least I know I gave a real effort in my MS to be a good enough candidate. Im gonna plan on having a job offer in hand by the time I graduate so I can have something if I decide against a phd or if I get dont get into any programs I want']"
[M] What is the etiquette on this subreddit for asking for help without a statistics background?,"## Question to Answer

Is there a general etiquette, required set of information, and required background in statistics for the community to see a post as value-added?  I'm posting this not just for my own information; if I'm having these same questions, maybe others are as well?  And the subreddit description doesn't detail much regarding information required in a [Q] post.  

## Background
I'm new to statistics work (started learning in November) - engineer by trade.  And I believe many others in this community are in a similar boat.

I  was hesitant to post this since my previous post requesting help got downvoted immediately.  See link here: [Reddit Post Link](https://www.reddit.com/r/statistics/comments/1335jqh/gage_rr_help_next_steps_question/).  For my first post in this community, the response was discouraging.  And I'm 100% open to criticism, constructive or otherwise.

I see post after post of OPs asking for advice on which statistical test is best suited for a specific dataset, hypothesis, and questions to answer.  And it seems the majority of posts get supportive responses from the community.  I've very much enjoyed reading many of these posts and have been sent down many a rabbit hole trying to learn more.

However, my post was immediately downvoted.  Maybe I didn't use the right terminology or didn't provide all of the appropriate information, or maybe I provided too much information?  A reply in the post indicated that I should be paying for the help.  If so, no problem; however, the problem I'm trying to solve is more for my own learning.  In my uneducated opinion, it was a pretty simple request: what statistical test would be best suited for the problem statement, dataset, and hypotheses? 

Thanks for your input and feedback!",133tqbc,STFUandLOVE,1682865787.0,11,0.76,"[""If you can't summarize your request concisely in 60 seconds you're unlikely to get good replies. For your previous question in particular, it's not immediately obvious what's going on (maybe it is if you work in this specific field, but then you've already narrowed down the pool of people that are willing to help you). So people have to spend a few minutes looking at your graphs and trying to decipher what it all means. It may be obvious to you but not to a random person looking in from the outside. So if you can summarize that all better you're likely to get better responses.\n\nBasically, rule of thumb for asking for help (at least in my experience) is make it as easy as possible for the other side to help you."", 'People are grumpy in every sun. I wouldnt read into downvotes too much. It do agree with your assessment that this isnt a super beginner friendly sub.', ""That's a bummer that you got discouraged. IMO, giving people access to expertise for basic questions is one of the best things about reddit."", ""For basic questions you may get fewer downvotes in /r/AskStatistics than here. But it's mostly the same folks answering in both forums, so as long as you don't appear to be posting homework questions you'll probably get answers."", ""1. Newb questions are definitely okay; I would say that a basic level of search/research is expected, and you should make sure your question is as clear and concise (but complete) as you can make it. I don't think any of those were issues with your post, not that I read it in detail -- nor would I call it a newb question.\n\n2. On downvotes, you can't normally infer what might have led to a downvote\\*. A few downvotes might each have happened for different reasons, so if they're not explained, and there's nothing obvious to improve, *just ignore them as noise*. This isn't stackexchange, but even there downvotes are not necessarily a very clear signal of any issue in particular. On reddit even less so.\n\n  If you get a dozen downvotes, you probably did something wrong but if you get that many, someone will probably identify what they see as the problem.\n\n\n\n---\n\n\\* well sometimes you can, like if you were trolling or obviously breaking some major rule (like posting homework), but outside that sort of situation, you really don't know why someone downvoted if they don't say."", ""R/statistics doesn't actually get a lot of traffic from knowledgeable people who could answer questions like the one you posted -- my advice is take it up on stats.stackexchange.com instead. \n\nStackexchange has the same potential for echo-chambery in-group reinforcement as Reddit, but there are just a lot more people reading it. While it's still a roll of the dice, your chances of getting a constructive response are greater.\n\nThere may be still other suitable forums. Cast the net wide."", 'I agree with this. Many of the beginner questions posted are basically asking ""How do I analyze this?"". For someone to answer the question, they have to explain 2+ stats classes. That\'s assuming the poster had provided enough information about the data and problem... Which they usually haven\'t.\n\nOn the flip side, if you do post enough info, nobody is going to read it all.\n\nFor something like this, you just need to hire a consultant or spend the time to learn yourself.']"
[D] What is the chance of two people with the same phone number but with different area codes meeting?,,133krrx,Lopsided_Internet_56,1682847408.0,6,0.67,"[""This, and questions like this, is unanswerable without a model. Once you have a good model for how people with specific phone numbers and area codes interact, *then* you can start asking questions like the one you're asking."", 'The probability that both have identical numbers not counting the area code minus the probability that both have identical numbers, including the area code. Assuming persons and digits within number are independent draws, each of these two terms is a product of two identical factors, which is then summed over all possible assignments in each case. Since the summands are constant, the sum results in a third factor, the number of possible assignments in each case,  which cancels one of the other two factors in each case. That would be 1/\\(10^7 \\)  - 1/\\(10^10 \\), but we assume that the first digit of the area code and of the exchange must be nonzero. Hence\n\n     1/(9*10^6) - 1/(81*10^8)', 'I am a high school student from Asia. I find statistics and probability really interesting and want to solve questions like these. Using the basics, how to approach this question?', 'First of all you need to turn the question into something that has a well defined answer. For example, compare the following:\n\nWhats the chance someone gets struck by lightening?\n\nWhats the chance that I get struck by lightening if I stand outside my house for a year?\n\nObviously the place and time period are necessary to give an answer. But the subject of the question and how its framed is just as significant. Its almost always easier to consider the probability of an event happening to a specific individual as opposed to the probability of an event happening in general within a population. You can also use the individual probability to estimate the general one.\n\nNow looking at the original question, we can rephrase it as:\n\nWhat is the chance I meet someone with the same phone number as me in my lifetime?\n\nNow in theory, this probability should exist. But it certainly isnt one that is possible to calculate precisely especially using only basic theory. We have to make simplifying assumptions that turn the question into one that we can answer. Rather than make those assumptions right away, well instead break the problem down first and make any assumptions we need as we go.\n\nOne way is to initially consider the probability that a new person I meet has the same phone number as me, then by estimating the total number of new people Ill meet in my lifetime we can answer the problem with the binomial distribution (if youre unfamiliar look this up because this is the core of many problems like this). \n\nSo to find the probability of the next person I meet having the same phone number as me, Im going to assume that they have the same format as number and that they definitely have a different area code. I just need to calculate the probability that their 7 random digits are the same as mine. This is easy since theres no constraints on the numbers (Im not familiar with US numbering systems so Im going with that) and its simply 1/10,000,000.\n\nFor the total number of new people, Ill make the baseless assumption that its 50 a year for the next 60 years which totals to 3000. \n\nNow to finish the question of and get a solution, we need to calculate the probability that an event with p=1/10,000,000 occurs in n=3000 trials. Using any method you wish, you get an answer of ~0.03%.', 'Check my reply in the original thread :)', 'Thank you Sir, that does make sense to me. My gratitude for explaining me in detail.']"
[E] I tried to teach some basic statistics. Would love to get some feedback!,Here's the Link : [https://youtu.be/4AVBhBM8BgA](https://youtu.be/4AVBhBM8BgA),133islu,heydonlyone7,1682840101.0,9,0.84,['Need to fix the audio. Lots of background noise. Use Audacity. Its free and there are lots of YT tutorials on how to use it']
[Question] Calculating confidence intervals of AUC?,Theres a study that reports AUC in addition to Intercept (95% CI) and Slope (95% CI). I am trying to pool this data with some other studies that all report AUC (95% CI). So Im trying to figure out how I can figure out the overall CI for the AUC. I dont completely understand what the intercept and slope CIs indicate.,133evrt,Hoodly3,1682825968.0,3,1.0,"['Maybe using some sort of bootstrapping on the validation data?', ""I was thinking along the same lines... As far as I know there's no rigorous way to connect CIs on the underlying model to the AUC itself - so a hacky approach is to just simulate many ROC curves based on the variance implied by the underlying CIs. It may take a while to converge though, and if anything the resulting CI is just a heuristic."", '[deleted]', 'Sorry, not able to help with that one.']"
[Research] Need help choosing my statistical test.,"
Its been a long while since stats class, and Ive decided to drive myself crazy and write a paper for work. Any help is appreciated. 

I am doing a chart data review of transgender patients with intentional ingestions. Factors I will be looking at will be age, location, gender identity, medications ingested, treatments needed, and medical outcome. 

Am I correct that a MANOVA is the correct test for this?",133cimd,Euthanaught,1682818424.0,11,0.93,"[""You need to clearly identify your DV or DVs, how they're measured, and what research question *exactly* that you're trying to resolve (what's the research hypothesis?)."", 'MANOVA is a test with multiple RESPONSE variables. For example, if I wanted to test if average height and average weight and average age were equal between United States and China, MANOVA would be an appropriate test. \n\nIf, however, I wanted to know if the average age differed by a set of variables, you would not use MANOVA since you are trying to test a single response. \n\nThink about what question you are trying to answer with your data - what is the variable of interest and what are your factors. Then use that to find the appropriate hypothesis and test it.', 'Please see my other comment. I am just doing a chart review, and not actually doing an experiment.', 'Its difficult because Im simply doing a chart review- I have a pile of data and just want to review potential trends, things like x cases in y location, x cases with y Med category, number of cases in each gender category, etc. Im not actually manipulating any data. Apologies if Im just thinking too hard about it.', 'That doesnt negate the importance of the questions in the comment you replied to. Whether youre the one collecting the data or not, you have to be able to clearly articulate what kind of data it is and what you want to find out from it.', 'Apologies, my experience up til this point has been having someone else tell me which test Ill do, so I am floundering a bit on my methods. Between Reddit and YouTube, its been tolerable. \n\nMy data (I believe) will all be qualitative, and will include things like cities/states where cases occurred, ages of patients in the cases, genders of patients, medications taken, symptoms, etm. \n\nWhat Id like to do with the data is compare such things as number of mtf pts vs number of mtf pts vs number of NB patients; how many anti depressants vs how many pain killers vs how many antihistamines; this state vs that state, and what % of the total cases of that state over that time they are; number of cases with severe vs moderate vs minimal symptoms. *Maybe* a little bit of number of mtf pts who took antidepressants, etc,  etc. \n\nI am just a bit at a loss as to how best to go about that, but I am about to go do some digging now.\n\nEdit:\n\nAm I thinking about this way too hard and I dont need a test at all because it is just a straight reporting of data?']"
[Research] Multiplying odds ratios together in moderation analysis?,"I am a public health student and I ran a moderation analysis in STATA. I am looking at age of first marriage and the outcome of intimate partner violence in Uganda. I ran a moderation analysis, controlling for husbands alcohol use, and the interaction between age at first marriage and husbands alcohol use. I ended up with three significant odds ratios for age at first marriage (age 15-17; reference group 18+), husbands alcohol use (binary variable yes/no), and the interaction between age at first marriage and husbands alcohol use.
Can I simply multiply these odds ratios together to get the odds of intimate partner violence compared to my base case (being married at age 18+ and husband doesnt drink)? Thanks in advance!",133byjm,peachinthemango,1682816729.0,3,1.0,"[""I'm assuming you're talking about doing a logistic regression. If that's the case then I believe the answer is yes, but depending on what all your base case factors are that could make sense (from an interpretation point of view) or it could not. It would be most powerful to use it this way when you're careful about picking which values represent the base case and encode them respectively"", 'Yes you can multiply to get OR. Be careful this does not apply to confidence interval though.', 'Yes, logistic regression. Ok great! Yes a base case would be a woman who was married as an adult (18+) whose partner does not drink, as those are considered low risk categories for intimate partner violence.', ""It that sounds great. I'm pretty sure that would work fine. I'm not an academic, but it sounds like being clear that the base case does not represent the average woman is important""]"
"[D] I'm doing dual credit this summer online, and am taking Elementary Statistics and College Algebra. I have a few questions, any advice would be greatly appreciated.","So I'm a junior in high school doing dual credit and have chosen to do Elementary Statistics (MATH 1342) and College Algebra (MATH) for the first 5 weeks during summer. I am also doing 2 courses, Government (GOVT 2306) and Speech (SPCH 1315), for the second 5 weeks. I am trying to shave off 1 year of community college and am pursuing a career in Quantitative Finance. I plan on getting a Mathematics degree with a minor in statistics and spending my personal time learning programming.

I was wondering if I could get some advice on this since I feel like I might have signed up for too much for the summer. I do like math and enjoy helping others with their questions if needed in class. I'm doing pretty well in Algebra 2 right now, but I feel like I haven't exactly retained a lot of what I've been learning. I also have never taken any stats course and I'm afraid that taking an intro class to statistics could be a big rush and I might not retain most of it later on. Couple that with College Algebra and I'm going to have to deal with a lot.

I have no idea if I'm actually ready. I think I could do it since I actually enjoy math and I have a lot of free time that could be spent studying, but I need some advice. What are good studying habits that can help? And how can I remember all that I'm going to be learning? Any apps or school life hacks from anyone studying mathematics/statistics? What works and doesn't work? I'm doing all this online so maybe it won't be as bad as in person.

I would really appreciate all the advice given to me. Thank you for reading my question. Have a wonderful day!",1338x2v,Raskrj3773,1682808091.0,2,1.0,"[""I do not any apps, but I love mathematics too.\nThough I am in Accounting now, but whenever I face mathematics I instantly remember what I have learnt.\n\nSo, I would suggest do not worry if you don't remember anything. If you love mathematics you will be doing great. When needed you will retain everything.\n\nGo ahead don't worry about retaining."", ""I wouldn't recommend doing two 5 week math courses at the same time, unless you're _really_ good  at all of: time management, general study skills, and learning math in particular.  Keep in mind that most college math courses are 14-15 weeks in a normal semester, so condensing that down to 5 weeks means you have to learn about three times as much material each week as you would if you were taking it on the regular timeframe. \n\nDoing two 5 week courses at once is probably roughly equivalent to doing six 15 week courses at once in regards to how much material you'll need to learn each week.  Can you handle that kind of workload without getting overly stressed, especially if it's all math? If you really think you can handle it after really thinking it through and considering your previous experience with similar workloads, OK great.  But if not, saving a bit of time in getting your degree isn't worth making yourself miserable for weeks at a time due to the stress of the workload.  And another consideration is whether you'll retain the information as well if it's all condensed into a few weeks.\n\nTLDR: If you're not reasonably certain you can handle a very intense workload, don't take two 5 week math classes at the same time.  Honestly, even doing _one_ 5 week math course will likely be a real challenge due to how fast things will go.  If only condensed courses are offered over the summer and you just want to get some credits over that time, you might be better off getting some introductory humanities, social science, or other gen ed courses out of the way over the summer (since, even if you plan to major in math, must colleges in the U.S. will require about half your credits be gen eds, though the particular requirements will of course depend on what college you plan to go to) and wait until the regular semester starts to take the math courses as regular 14 or 15 week courses."", ""I can't change it anymore, sadly. I honestly just wanted to get the 1 year shaven off, as that's what a lot of people aren't retelling me to do. Some people at even getting 2 years of college shaven off doing all the ap and dual credit stuff, and skip CC. It honestly just makes me feel horrible. \n\nAbout the difficulty, idk. I like mathematics, and I like to figure out all its patterns and little nuances that it has in it, but yes, I really don't know if I can handle 2 math classes. I did wish I had an additional math class my junior year, not just 1. But that spans around the whole year. \n\nI plan on doing the prereqs my 1 year of CC, and doing calc 2 1st semester, and 3 and diff eqs second. \n\nDo you have any advice for handling the workload, or any tips or tricks that can work?"", "">Do you have any advice for handling the workload, or any tips or tricks that can work?\n\nDoes the CC or your high school have a tutoring center where you can get help as needed?  If so, I recommend making regular use of it, even if you wouldn't normally need a tutor for math, since you'll need to learn more material more quickly.  Also, see if any of your classmates in either class are interested in forming a study group.  \n\nAlso, maybe try to get a head start on the material now, before the classes start.  If you already have the textbooks, start reading the first chapters and trying to work through the problems at the end of the chapters.  You can also use Khan academy -- they have loads of algebra content and statistics content too.  The algebra course actually probably won't be too bad if you're doing algebra 2 right now, since, at least the community college I tutor at, the college algebra course was mostly a condensed version of algebra 1 and 2.  If you're reasonably solid on most of the material covered in algebra 1 and 2, it will likely be mostly review for you, though of course I can't say for sure without seeing the course syllabus.\n\nAnd if you want additional supplemental resources:\nMathsisfun.com\nBetterexplained.com \nProfessor Dave Explains YouTube channel \nPurplemath.com \n\nAs for study tips:\nYou might be doing this already, but make sure to organize your work clearly as you work through problems. Use boxes or the like to separate subproblems from the main problem and arrows to indicate the order of your steps, etc.\n\nAnd write out, _in words,_ the reasoning for the steps you're taking to solve a problem, either in your notes with example problems from the lecture or textbook, or as you're working through problems yourself.  If it takes you too long to handwrite a lot of sentences, you can use an equation editor to write out all your work and also type sentences, such as [mathcha](http://mathcha.io).  To be clear, I don't mean you should use words _instead of_ algebraic notation, as that would be silly.  Rather, I mean, in addition to showing your work of, e.g. solving an equation, write down what the reasoning for taking each step was, if you haven't fully internalized the reasoning already, because the act of writing it out will help you to remember it better and it will give you better study material to use when reviewing before an exam or quiz."", 'They have tutoring but its all online, all my course work is too. I have to make an appointment for it.\n\nAs for books, id have to ask them what kind they would recommended me for stats and college alg. Gonna ask my high school counselor on info for that.\n\nI generally have some messy notes, and sometimes when i look back at them, theyre super disorganized. I really should improve on that. \n\nNever tried the equation editor, or mathcha, seems interesting.', ""Maybe make an appointment for tutoring each week, and when you study, write down any questions you have or concepts you're struggling with and bring them to tutoring.  \n\nDefinitely work on organizing your work and notes better.  The better you organize your work as you're working, the more it will force you to organize your thought process and better internalize the material. For me personally, I find it works well to use the aforementioned equation editor and just type out the reasoning for steps where it's not obvious.  I find it MUCH easier to organize my work with an equation editor because that way I can't start writing in random places on the page, all the text is the same size, and it's much faster to intersperse text with mathematical notation than handwriting on paper.  Plus, I can copy/paste expressions so if I have several similar mathematical expressions, I can just type the first one, copy/paste it and then modify it rather than typing the whole thing again.  It's much less tempting to skip steps in showing my work when I don't have to rewrite the whole expression several times to show everything, but can just copy/paste instead.  I'm an engineering student nearing the end of my degree and so my HW often involves much more complicated problems than will be encountered introductory statistics and college algebra, so you probably won't have as much of a need for an equation editor right now as I do, so if you prefer to keep working with pencil and paper it should be fine, so long as you find a way to organize your work well enough that you can look back in what you did a few weeks ago and understand it.  \n\nAs for books, do the math courses not have specific textbooks you're required to get?"", ""Alright. I think I'll do it with paper/pencil as a starting point, and maybe use the equation editor later on if needed. There must be tutorials for it online, right?\n\nAlso, I don't get to see a lot of info on the classes online. just their class number, credit hours, prereq, vague description on course teaching, ect. Not really anything about textbooks needed or specific course material. I'm going to have to either find the emails of the teachers of the math classes classes, or ask my hs counselor on info for that."", '>maybe use the equation editor later on if needed. There must be tutorials for it online, right?\n\nThe first time you use the website it takes you through a quick tutorial, but it\'s overall really easy to use and doesn\'t require following a detailed tutorial.  Basically, it\'s in text mode by default, and if you want to type mathematical notation, you press the backslash key and a menu opens.  Select ""inline math"" if you want to have some mathematical expressions in the middle of a sentence or ""math container"" if you want the equation/expression on its own line.  Then, once you\'re in either math mode, press the backslash key again to access the menu of different mathematical symbols and just start typing the name of the symbol and it will quickly move that symbol to the top of the list and you can just press ""enter"" to insert it.  e.g. If you want to type a fraction with a horizontal bar, type ""f"" and ""fraction"" should be the first thing that goes to the top of the list.  \n\nThere\'s also a feature where you can draw the symbol and it will suggest several things it thinks you might mean, but that\'s only necessary if you\'re trying to use some very specific notation that you don\'t know the name of and I don\'t expect you\'ll need it.  There are some other nice features, like making folders to store different files in, which I definitely need since I\'ve used it for several different classes and it would be way too hard to find my past work if I didn\'t have it organized by class.  And there\'s an option to create custom shortcuts; for example, in one of my classes I  had to use the expression 4/ a lot, so I added a shortcut where I could just type ""K"" into the search bar of the math mode, press ""enter"" and it inserted that whole expression.  But that\'s all extra stuff that I eventually noticed just from using it normally.  Overall, it\'s the single most user friendly equation editor I\'ve found and one of the most user friendly products I\'ve found in general and the online version (which is what I use) is completely free.  As long as you\'re reasonably component at using a computer for normal stuff, I don\'t imagine you\'ll have any serious difficulty figuring out how to use it.  If you decide to give it a try and do have any questions about how do something with it though, feel free to DM me -- it is possible I\'m overestimating how easy it is for the average person to use it, since I probably have a lot more experience with teaching myself to use software tools than the average person (though certainly nowhere near the experience of a professional software engineer or the like).  \n\n>Also, I don\'t get to see a lot of info on the classes online. just their class number, credit hours, prereq, vague description on course teaching, ect. Not really anything about textbooks needed or specific course material.\n\nYou might try Googling ""syllabus for [course name and number] at [school name]"" and see if the textbook is listed on the syllabus.  Otherwise yeah, probably a good idea to find the professors\' email addresses and ask what textbooks to get.  Even if you don\'t have time to get a head start on studying, best to order the books early enough that you don\'t have to worry about paying for expedited shipping to make sure they arrive on time (at least, assuming you order physical copies instead of ebooks).', 'OK, so I found the syllabuses for the classes, and the stats class needed a book called ""Introductory Statistics"" by openstatx. The college algebra course needs something called ""Lumen Learning"". I have no idea what that is. But i talked to my counselor today and asked if i needed any textbooks or anything, ad she said she doesn\'t think I do. So I don\'t think I need anything actually? I\'m doing all of my course work on Canvas I\'m pretty sure.\n\nShould I still read the book on stats just to get a small leg up tho? It is completely free on amazon, and I have it on kindle. It is 900 pages long tho.\n\nOh, and i think I have used something similar in the past to he equation editor you mentioned. I have used an equation creator type thing that has symbols and lets  me write equations in google docs, but I don\'t think it actually solves anything, just sets them up. I\'ve used it to copy some equations listed on a  paper that I had to copy onto my computer, and used the equation thing on docs to make it exactly like the paper shows. Is Mathcha the same way? Or does it solve equations?', ""No matcha doesn't solve the problems, just makes it easy to type everything out.  Desmos.com works as an equation editor for basic notation and will do some calculations for you, so you could try that if you want.  Honestly though, the most important thing is just make sure you organize and explain your work really well, or otherwise take really good notes while watching the lecture videos/doing the reading, because you'll want to make sure you have good study material to prep for exams.  If that's easier to do on paper, do it on paper.  If it's easier with an equation editor, use an equation editor.  \n\nPersonally, I rarely have the patience to sit through a lecture or read a whole chapter and take good notes in one sitting, so I alternate between doing the reading and working through problems, and I basically take notes as I work through the problems by explaining the reasoning for my steps in words.  You can do it that way, or you can do the reading first and take notes on the reading and reference those notes as needed when you work through the problems later on.  Either way, make sure you organize your work/notes well enough and provide enough detail that your future self won't be wondering what the heck you meant.  And especially with an accelerated timeline, you'll need mental reinforcement of the material you'll get from writing everything down and organizing it.  Just the act of doing that tells your brain the information is important and so it should hold on to it.\n\nAs for the book, I certainly wouldn't try to read the whole thing ahead of time, but it can't hurt to get a head start just reading the first chapter and trying t9  answer the questions/problems.  Alternatively, you could email the professor and let them know you're trying to get a head start on learning the material and ask what they recommend."", ""Oh, OK. thank you SO much. It means a lot.\n\nSo you're studying engineering, right? I wanna know how you study it all. Like, how do you remember it all? Spaced repetition? Encoding? I've heard all kinds of techniques, ad some say those don't work, but other do, or some other combination of niche technique. What has worked for you to study engineering?""]"
[Q] Does there is a certificate(s) or exam(s) or designation(s) I may set for to prove my knowledge of Statistics,"There is CPA, and CMA ...etc for accounting as an example.

There is CFA for finance as an example.

So, does there is something like that for Statistics?

\-Later Edit-

I am from 3rd world developed country in Africa.

I was thinking about post-graduate or a master's in Statistics, but after searching in local universities I've found it very time-consuming and may extend to 3 or 4 years plus the curriculum is not updated.",13380l9,mgbsher,1682805681.0,1,0.55,"['The ASA may have something', 'The closest that comes to mind is the Exam-P test for actuaries.\n\nEDIT: I understand this is not a pure statistics exam, but it is difficult for many and could showcase knowledge of probability concepts.', 'A degree from a good institution is hard to beat.\n\nWhy and to what level do you need to prove your knowledge of statistics?', ""To my knowledge, there is not a certificate in the field of statistics.  There are (or used to be) statistical software packages that offered certification for their specific software.  SAS used to do this, but I don't know if they still do."", 'Depends on your location. Canada and the US have the PStat accreditation through the ASA and the SSC. The US also has the GStat and Canada also has the AStat. The UK has GradStat and CStat through the RSS.', '>ASA\n\nWhat is it? as google brings many abbreviations not related to statistics.\n\n\\-later edit -\n\nDo you mean [American Statistical Association](https://www.amstat.org/), and I sent them an email.', '>Why you need to prove your knowledge of statistics?\n\n2 reasons, get promoted at my job and find an online chance to gain extra money.\n\n>what level ?\n\nI do not know the right levels, but I read and learn a lot on my own, and I studied many textbooks on my own.', ""I've added the location to my 1t post above.\n\nI am afraid that I am not in US or UK, and sorry for not adding this info earlier."", 'What do you need to get PStat and AStat accreditation for Canada?', 'I did  apologies, shouldve clarified haha', 'I see. Degree seems like the only realistic option.', 'Oh, thats okay. I just posted the ones I am familiar with. I think Ive heard some African countries have statistics organizations. Its possible they have some affiliation with the International Statistical Institute (ISI) or something similar. If so, maybe there are international accreditations available through something like that, though I really have no idea. Hope youre able to find something!', 'Some information is here: \nhttps://ssc.ca/en/accreditation\n\nTheres an application, and from memory for one of them you need to check certain boxes in terms of which courses youve taken.']"
[D] Documentaries or movies that use statistics to solve practical problems?,,1337xfc,rankdoby,1682805468.0,28,0.92,"['Moneyball and The Big Short are the ones that come to my mind now.\n\nPS - I guess many works about gambling like movies about poker and blackjack too', 'I really enjoyed the BBC documentary [the Joy of Data](https://www.youtube.com/watch?v=l6oKriR-RjM).', 'Moneyball?', 'The imitation game', 'I teach international relations to high school kids focusing on tech/science/math problems and how they are discussed politically. I hope more people comment and make this thread a great resource!', 'The early seasons of Numbers is decent, but I havent watched an episode probably 12+ years so I may be misremembering.', '21 or the documentary Breaking Vegas of the real life MIT Blackjack team. There is also an episode on a similar series on history channel called Professor Blackjack.', 'Cant wait for them to do a documentary/movie on 3 point shooting becoming the meta in the NBA', 'Turns out that 3 pointers are worth more points than 2 pointers']"
"[Q] What statistical techniques are used for ""item-level analyses""?","I have run an experiment where we have participants see a sentence, with or without a photo, and then decide whether the sentence is true or false. I have been tasked with doing an ""item-level analysis"" on the sentences to see whether some sentences were more likely to be seen as true or false. 

I am somewhat knowledgeable in social science statistics, but I have never done an item-level analysis. I am wondering:

1. What techniques are typically used? I have read that a multilevel model is often used for this, but I can't seem to wrap my head around how.  
2. Are there any resources that you know of that walk through this stuff from beginning to end?

Thank you!",1337x3i,bennettsaucyman,1682805444.0,3,1.0,"['You can probably get pretty far by adapting the latent variable setup in [item response theory](https://en.wikipedia.org/wiki/Item_response_theory) to this problem.\n\nIn IRT, each _question_ has a latent difficult parameter and each _individual_ has a latent strength parameter. You have lots of modeling options here, for example you probably don\'t actually want a strength parameter for each of the _individuals_, so maybe just use a single ""strength"" (truth-answering) intercept parameter for everybody and let only the sentences have their own truthiness *b_i* parameters.\n\nPseudocode (following the [STAN example](https://mc-stan.org/docs/2_20/stan-users-guide/item-response-models-section.html)) might be something like:\n\n    model {\n      ... // priors\n      for (n in 1:N)\n        answered_true[n] ~ bernoulli_logit(\n           true_intercept\n           + picture_was_shown[n]\n           + b[sentence_idx[n]]\n        )\n    }\n\nEdit: just to clarify, it\'s believable that individuals vary in their truth-leaning (credulousness? suggestibility?) so you definitely could add individual-specific truth-leaning parameters -- in fact, you\'d probably want to do this for a better model fit if you had plenty of data. The reason I assumed not above is just that (1) it didn\'t sound like you would have tons of data and (2) didn\'t sound like a quantity of interest for the research, but obviously it\'s easy to add to the linear predictor if desired.\n\nAlso, to respond to this part:\n\n> Are there any resources that you know of that walk through this stuff from beginning to end?\n\nThere are tons of good articles and libraries for IRT, just depends what environment you\'re using (e.g. R, Python) and modeling approach (e.g. max likelihood or probabilistic/MCMC with Stan/PyMC/etc).', 'Another, simpler way to approach it is to compute mean response times for each item in each condition for correct responses. Then do the anova. See Clark (1973).']"
[Q] What conditions are necessary to use the Partial F-test?,"I know that for the Full F-test we have to satisfy the regression conditions:

1. Linearity
2. Homoskedacity of residuals
3. Normality of residuals

However in my statistics class they never access the conditions before using the Partial F-test. **Is this because we are using the partial F test to compare to the partial linear regression model to the full regression model?** Meaning that we are going to fit the regression line/curve no matter whether or not the conditions are satisfied.",1336u2q,Sun_of_son,1682802661.0,8,1.0,[]
[Question] Trying to pick a stats analysis programming language to learn as somebody without CS background. Any recommendations?,"After years of procrastinating on actually learning how to stats program to speed up data analysis / graph generation, I think it's time I actually learned a script-based stats language rather than just relying on pre-made stats packages.

In the past, I've just been relying on stats software packages that don't require any CS/programming skills, such as Minitab, SigmaPlot, GraphPad Prism, etc. However, I've been increasingly finding that I'm having to use more complex statistical models that those programs simply do not currently support, which is ofc delaying some manuscript/abstract/publication submissions, as well as ofc my thesis.

Currently, I'm contemplating between learning one of the following stats languages that are based on or compatible with custom scripts written with computing languages such as Java or Python:

1. IBM SPSS
2. SAS
3. MATLAB
4. R

Some statistical models that my current stats packages can't really support (and I kinda need for deeper layers of analysis) including PROC MIXED and Within-Subject adjusted Repeated Measures Two-way ANOVAs with Covariates, as well as some more specific Post-hoc tests. The ability to further customize graphs using the same software as for stats analysis would also be a plus.

Admittedly, my background in CS/biostats is kinda limited, so relative ease of learning is also a factor I'm trying to consider atm.

Thanks in advance for any input!",133654z,asparagustasty,1682800889.0,22,0.84,"['R is free, which is a big deal if you dont have an employer footing the bill. SAS has some biostat advantages, I suppose, depending on your goals. (And frankly, that may be dated info anyway.)', ""In order, for stats:  \n1. R.  \n10. SAS  \n18.  Matlab  \n1000. SPSS \n  \nJust go with R and don't look back. Matlab is an engineering language with some other functionality hacked on. SAS is a walled garden ecosystem. SPSS is a pile of shit."", ""Hey, before I comment: I only have knowledge on R,Matlab and Pyhton. \n\nFirst of all, don't bother with matlab. I think the most relevant choices are either R lr Python. I use R in my master and used it mainly for academia stuff, and then I use python for my company. \n\nPersonally, I prefer R for stats/data handling, as I really like some niche features and handling data with tidyverse package feels more natural to me. On the other hand, Python I seems as a more flexible language that allows me to go into more complex programs (genetic algorithms or  more object oriented programming for example come to mind). However, I think a lot of stuff can be done in either R ir Pyhton, specially basic statistical tests/simulations and data analysis.\n\nIn conclusion, I recommend either R or Python, as both are great. If you will mainly use your coding in industry then probably Python will be better as it is more widely used."", 'As a die hard Python evangelist I suggest R.', 'Another vote for R. \n\nGiven any problem, X, in statistics, just Google ""R package X"" and find that two or three folk have written all the code you need.', 'I taught a course on R for students with zero CS background a few years ago. The lectures are [here](https://youtube.com/playlist?list=PLu77iLvsj_GNmWqDdX-26kZ56dCZqkzBO)', 'I found myself in a similar boat as you a few years ago. From my experience Id encourage you to try learning R. The tidyverse ecosystem and ggplot2 package are great for folks without a CS background. Learning is definitely a journey regardless of what you choose, but if stats and viz are your main goal, I vote R over Python.', 'Of those, the answer is very easy: R. \n\nSPSS is trash. SAS is pretty specialized. There are some orgs that use it because of its capabilities with large datasets, but its not a common language and learning it is unlikely to give you a huge leg up when youre searching for a job. MATLAB is somewhat similar. \n\nR, hands down, is the best choice among these four.', 'R', 'You should probably learn R because thats what the cool stats kids use. \n\nHowever, you should also consider Python. It also has a large number of statistical tools, and would give you a bit more versatility in future work opportunities outside of stats (especially if deep learning has any potential in your future).', ""I've used all four of those in the past - though some not for many years. \n\nI'd put R at the top of that list. It does take some work to learn but you can pick it up as a language while you use it as a stats environment, it's not hard to get the basic recipes for reading in data, using basic plots, regression, common tests and so on and then moving on to learning what R is good at."", 'R. As others have said, the FOSS aspect adds another dimension to things. Chances are that whatever you need will be an internet search away. You may find more difficulty with the others. SAS is only worth considering if you want to go into industry and do clinical trials.', 'If you want to learn Python, you are the target audience for my book, Elements of Data Science: https://allendowney.github.io/ElementsOfDataScience/README.html', 'I suggest instead of thinking about what fits your situation right now, think about what will benefit you the most in the long run. In my opinion, R and python are the most promising for your future job prospects, and python more than R. In most industries python is the norm because you can have one language for the entire org, and internal teams can build tooling to assist you.  However, R had a lot of statistical tools that arent available in python. If youre looking at a career in straight biostats, eg clinical trial design and analysis, SAS is probably good. Matlab is for electrical engineering who refuse to learn another language. Finally, I dont know anyone who uses SPSS. Maybe banking?', ""R, definitely. It's the biggest stats analysis programm. SAS, SPSS and MATLAB are very industry-nice."", 'Why not Python?', ""I say none of the above -- go with Python.  It's the most popular language for statistical programming and data analysis and for good reason.  It has a huge online support community and hence there are _way_ more free online tutorials and examples of how to do x in Python than in R or MatLab.  \n\nI do actually think that the _design_ of R _is_ better suited to statical coding than Python is, probably because R was designed specifically _for_ statistical coding whereas as Python was designed as a general purpose language and the statical coding aspects rely heavily on libraries like NumPy, SciPy, and Pandas.  But, IME, that better design doesn't make up for it being much harder to learn how to use it properly -- in addition to there being fewer resources to help you learn R, a lot of the documentation for R is just _horrible_ -- it leaves out WAY too much information and the information it does give often isn't presented clearly.  On the other hand, Python documentation, both for the base language and for the popular libraries, is some of the best written technical writing I've seen -- it's clearer and easier to understand than any other documentation I've seen.  \n\nOne tip though -- if you start working with the Pandas library a lot, make use of the dfply library too.  Pandas syntax for dataframe manipulations is often needlessly ugly, verbose, and counterintuitive.  It's not that big of a deal really, but the dfply library lets you work with the same Pandas dataframes using much nicer syntax that's both easier to read and easier to remember.  And the readme file on the GitHub page explains very clearly and concisely how to use it.  The learning curve is very short."", 'I agree with the ordering provided by another commenter as well the one that said SAS some biostats advantages. One thing I will add is that since R is free, it changes VERY often. There is no guarantee that code you right today will work tomorrow. As a big R enthusiast, that is one major thing SAS has going for it. Consistency.', ""Go with R, there's many good libraries, the syntax is easy to understand and the visualizations you can make with ggplot2 can look really good. There's probably also more resources out there for it, but I don't know enough about the others to say this with certainty.\n\n I wouldn't go with Matlab for sure, it's not really made for statisticians and it's also not open source."", 'R is my recommendation. SPSS, SAS, and Matlab are all worthless nowadays IMO. If you already know Python, learning its stats and plotting packages will be easy for you, but I find that they are less well-designed than Rs equivalents.', 'R', 'R is your first choice but as some have said, its documentation can be very poorly written for some libraries or functions. \n\nPython is very good for general data science, not just limited to stats analysis, does everything R does but more. You can easily pipeline an entire data analysis system that automates data fetching, analysis, visualization in one go all within Python.\n\nSAS is a lot more accurate than R, but also much more expensive. Depending on your use case, if accuracy and precision matter a lot, then SAS is preferred.\n\n\nR is quite convenient to use though. Setting up Python can be a hassle to some people.', 'R', ""Like most everyone else in this thread, I recommend R. It may not be the easiest to learn at first, but the payoff is enormous. It was designed from the ground up for analyzing data. Once you learn a few basics and get some practice, data exploration is almost effortless. I work at a university doing stats consulting and I often help students and faculty using SAS or SPSS. It's maddening how much work it takes to explore data and generate basic summaries or plots in those programs. In R it's usually a simple function."", 'Starting from scratch..Id probably go for R. \nI used to use MATLAB (read that as still use it, but only when required).\nIm mostly in python now, but that is because my industry has shifted towards python after two decades in MATLAB.\nI would consider Julia as an option too', 'Thats true, and Ill definitely factor that in now.\n\nMy current lab does include licenses for the others besides MATLAB plus in-house guides for how to quickly learn SPSS, SAS, and R.  Ill lean away from MATLAB then.\n\nSAS does offer a couple stats models, including PROC MIXED, that the papers Im basing my current study on have been using, which would simplify the discussion section a bit.\n\nIm not sure if they would be programmable on SPSS or R.  My PI also has no background in stats programming so hes not sure either.  I guess Ill have to look into it.', ""Python is also free and has a much larger online support community.  Also, R's documentation is pretty hit or miss -- some of it is decent, but a lot of it is pretty terrible, which can make it unreasonably difficult to figure out how to properly use particular functions.  OTH, documentation both for base Python and for the various Python libraries commonly used for data science and statistics is some of the best written and easiest to read technical writing I've come across.  I'd say, as a language, R _is_ better for statistical coding in particular, but the poor documentation makes it needlessly hard to _learn_, so Python is probably a better choice as a first language. The only downside is the syntax for the Pandas library is rather ugly and verbose compared to the syntax for R's built-in dataframe operators, but that's not too big of deal because there are several helper libraries that let you use R-like syntax with Pandas dataframes."", 'R is also top of class for visualisation (as someone that switched to python after a couple of years in R).', '>SAS is a walled garden ecosystem.\n\nOwned by pricks. That needs to be emphasized.', 'Well said', 'Thats what the P and S stand for!', 'Strongly agree', 'Btw, just remembered what I used to start on R in my bachelor, it is a free to use website so might as well leave it here in case it is usefull :)\n\nhttps://r4ds.had.co.nz/index.html', 'Ah yes, my go to solution for any applied course in my stats degree.', 'Thanks for the breakdown!\n\nCurrently, my career is in the direction of academic university teaching in biology/neuroscience/behavioral psychology upon finishing my grad degree, but a lot of tenured positions in my area will also require concurrent active research/PI-ing within the fields Im teaching, plus also job advancement/job security/ seniority is at least partially based upon publications.\n\nWith that in mind, biostats on experimental trials with rodent subjects will definitely make up a part of the overall work week.\n\nBased on your summary tho, definitely it seems that MATLAB and SPSS are out of the window as certainly engineering and banking arent in the picture.  R and SAS are looking like the hotter options.  Ill have to look deeper into whether others at my institution recommend in terms of Java or Python.  It seems that R is compatible with both?', "">On the other hand, Python documentation, both for the base language and for the popular libraries, is some of the best written technical writing I've seen\n\nReally? This is an embarrassing mess that sucks:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"", '> that is one major thing SAS has going for it. Consistency.\n\nhowever, they have been consistent for 50 or 60 years or so. Any other language would have created a version which fixed inconsistencies and design flaws. Not so SAS, its all still there.', 'R can do everything SAS can do and everything it cant.', ""R has excellent tools for mixed effects models, plus there's an easy interface to Stan if that's your thing.\n\nI can't emphasize this enough: _fuck SAS_. The SAS Institute is a bunch of litigious assholes with a crappy, dying product. Learning SAS is career poison."", 'If your Prof applies your entire class can get free acces to datacamp', '> R is better for statistical coding in particular\n\nR is ideal when you need to manually construct some non-standard test statistic using design matrices', 'All good points. The OP didnt list Python in his options, so I went straight past it. \n\nFWIW, I love working in R. But as you say, it may depend on how comfortable OP is with varying quality in documentation.', 'To use R like libraries like siuba then you end up needing to learn some R tidyverse anyways and tidyverse is easier than pandas', 'I honestly havent encountered documentation issues with R at all. Where are you seeing that? Or is is a talking point making the rounds?', 'Why did you switch to Python? If someone was choosing between R and Python today, what would you recommend they go with?', 'Wait somebody recommended Java? Hard no there.', ""That page in particular definitely isn't the best -- just the visual organization of the page makes it hard to figure out what's going on.  I've seem some similar stuff from sklearn before and not been very happy with it. But just not *all* of the documentation is great doesn't mean that, *overall* Python documentation isn't some of the easiest to understand.  And even that sklearn page is still way better than than some of the R documentation I've seen, because at least it actually has lots of relevant information, even if it's not organized as well as it could be.  My biggest complaint about R honestly is how **sparse** so much of the documentation is -- it's not necesarrily that what's there is poorly written or whatever, but that it's just missing SO MUCH important information.  That's not true of *all* of it of course, but it's an experience I've had many times."", ""I suppose it somewhat depends on how complicated the data cleaning/manipulation you need to do is.  I typically just use the dfply library, which makes the syntax for basic dataframe manipulations SO MUCH cleaner than the default Pandas syntax and it's incredibly easy to use; it takes about 5 minutes of skimming the readme doc on the github page to see how to use the main functions.  There's no need for any familiarity with tidyverse.  I haven't had a need to use any of the other dplyr-like Python libraries, although I do agree that dplyr and similar tidy-verse libraries are definitely one of R's biggest strengths.  But even if OP does need to do more complicated dataframe operations that would be better facilitated by a larger library with a steeper learning curve, so what?  They can learn the pieces of tidyverse needed for a particular library without learning all of R.  It's not like they'd even need to learn the whole library at once -- they can just look at examples and tutorials of how to use the particular functions they have need of as they come up.\n\nFWIW, I actually learned R first and when I started learning Python a bit later, I did find Pandas rather lacking in comparison, both in ease of use and readability of the syntax.  I do wish the designers of Pandas would've made it more like dplyr in the first place -- for a while, I was convinced that R was preferable for statistical coding or at least dataframe manipulations entirely because of how much nicer using dplyr with R's built-in dataframes is than using Pandas dataframes without a helper library; then I discovered the dfply Python library and started actually looking at more Python documentation and realized how much less *frustrating* it is to read it compared to so much of R's documentation.  \n\nUltimately though, I suppose it doesn't really matter all that much which language they start with -- what's most important at the beginning is that they get a feel for how statical coding works overall and they can do that with either language, and, once they learn the fundamentals of either language they can always switch to the other if they want."", ""I feel like there's been a switch to Python because of its use in machine learning (that is kind of anecdotal)\n\nEdit: but also, why not both? Rmarkdown you can use both and switch between them :3"", 'As Adverse Dodecahedron said, the ML ecosystem in python is larger. Also python is a more general programming language from my experience, which is nice if you want to do things other than stats.\n\nFor example my first project in python was making an reinforcement learning ai for a simple game, and coding up the simple game was something I dont think you could easily do in R.', ""This depends on what you're planning to use it for. Python wins hands down for data science type stuff, including machine learning (with a caveat). R wins hands down for statistics and econometrics, including if you're interested in the new causal machine learning stuff (causal forests etc). The reason is simply that these are the languages used in the respective fields, so for example new econometrics methods typically come to R first, and often not at all to Python."", ""\\> but also, why not both? Rmarkdown you can use both\n\nYeah being able to use both languages in one project is surprisingly easy since they are multiple prebuilt tools for doing so.  Haven't tried calling Python from R, but I know Jupyter notebooks in particular make it really easy to call R from Python and pass data between them.  It's nice to be able to do data cleaning and manipulation with dplyr in R and than pass the resulting dataframe to Python and use sklearn and the like to do data analysis on it and/or build ML models."", ""Thanks for the answer. I do know Python and am currently learning R but was more wondering about specialisation and libraries. I'm more in the machine learning space than pure stats so I guess Python may be a better choice but R is cool too. That's really cool that Rmarkdown supports both."", 'Im in a MS Econ program and so far different professors have had us use R, Matlab, stata, eviews, and Python.   Eviews seems to be the favorite for any sort of class doing time series analysis.  R was used in my two machine learning classes, getting the keras and tensorflow  wrappers running in R meant having Python running as well.', ""Stata still dominates among applied economists, but R dominates among econometricians. So if you want the latest methods when they're fresh, you're usually better off with R.""]"
Gage R&R - Help & Next Steps [Question],"Edit: Anybody care to let me know why this is getting downvoted?  Not sure what I did wrong here.  

[Seaborn BoxPlots](https://imgur.com/a/VfVddQw)

The above plot was generated with Seaborn using data from a field experiment at one of our sites.  I'm don't have a programming or statistics background, so bear with me.  Here's a breakdown of the data: 

* 14 operators total, 6 in Lab 1, 8 in Lab 2.
* 3 Samples each prepared and analyzed in triplicate
* Lab 1 used a single instrument and method
* Lab 2 used two instruments each using a separate method.

| Trial | Operator | Lab | Sample | Method | Instrument | Iteration | Value |
| ----- | -------- | --- | ------ | ------ | ---------- | --------- | ----- |
| 1     | 1        | 1   | A      | Auto   | 3          | 1         | Value |
| 2     | 1        | 1   | A      | Auto   | 3          | 2         | Value |
| 3     | 1        | 1   | A      | Auto   | 3          | 3         | Value |
| 4     | 1        | 1   | A      | Manual | 2          | 1         | Value |
| 5     | 1        | 1   | A      | Manual | 2          | 2         | Value |
| 6     | 1        | 1   | A      | Manual | 2          | 3         | Value |
| 7     | 1        | 1   | B      | Auto   | 3          | 1         | Value |
| 8     | 1        | 1   | B      | Auto   | 3          | 2         | Value |
| 9     | 1        | 1   | B      | Auto   | 3          | 3         | Value |
| 10    | 1        | 1   | B      | Manual | 2          | 1         | Value |
| 11    | 1        | 1   | B      | Manual | 2          | 2         | Value |
| 12    | 1        | 1   | B      | Manual | 2          | 3         | Value |
| 13    | 1        | 1   | C      | Auto   | 3          | 1         | Value |
| 14    | 1        | 1   | C      | Auto   | 3          | 2         | Value |
| 15    | 1        | 1   | C      | Auto   | 3          | 3         | Value |
| 16    | 1        | 1   | C      | Manual | 2          | 1         | Value |
| 17    | 1        | 1   | C      | Manual | 2          | 2         | Value |
| 18    | 1        | 1   | C      | Manual | 2          | 3         | Value |
| 19    |         | 1   |       |       |           |          |      |
| 145   | 9        | 1   | A      | Auto   | 1          | 1         | Value |
| 146   | 9        | 1   | A      | Auto   | 1          | 2         | Value |
| 147   | 9        | 1   | A      | Auto   | 1          | 3         | Value |
| 148   | 9        | 1   | B      | Auto   | 1          | 1         | Value |
| 149   | 9        | 1   | B      | Auto   | 1          | 2         | Value |
| 150   | 9        | 1   | B      | Auto   | 1          | 3         | Value |
| 151   | 9        | 1   | C      | Auto   | 1          | 1         | Value |
| 152   | 9        | 1   | C      | Auto   | 1          | 2         | Value |
| 153   | 9        | 1   | C      | Auto   | 1          | 3         | Value |
| 154   |         | 2   |       | Auto   | 1          |          |      |
| 190   | 14       | 2   | A      | Auto   | 1          | 1         | Value |
| 191   | 14       | 2   | A      | Auto   | 1          | 2         | Value |
| 192   | 14       | 2   | A      | Auto   | 1          | 3         | Value |
| 193   | 14       | 2   | B      | Auto   | 1          | 1         | Value |
| 194   | 14       | 2   | B      | Auto   | 1          | 2         | Value |
| 195   | 14       | 2   | B      | Auto   | 1          | 3         | Value |
| 196   | 14       | 2   | C      | Auto   | 1          | 1         | Value |
| 197   | 14       | 2   | C      | Auto   | 1          | 2         | Value |
| 198   | 14       | 2   | C      | Auto   | 1          | 3         | Value |

I'm trying to figure out how to properly analyze this stuff.  I don't really want a true repeatability and reproducibility test since it's all at the same site.  I want to be able to answer the following questions statistically: 

1. Which lab performed best?
2. Which method performed best?
3. Is there an inherent bias or greater variance between Instrument 1 and 3 (same test method)?  

I have the answers, more or less, from the graphed boxplots.  However, I'd like to also have the answers numerically.  I've reviewed ANOVA, but the setup for ANOVA confuses me and I'm not 100% clear what type of ANOVA test I'd be using or how to properly reshape the datatables. 

The answer to the above won't necessarily dictate which lab to perform the work in, however it will provide greater guidance.  Any help would be greatly appreciated.  Thanks!",1335jqh,STFUandLOVE,1682799357.0,1,0.55,"[""I don't think a gage r&r is the appropriate tool for the job.. its probably resulting in the down votes."", 'How do you define ""best""? Are there acceptance criteria to evaluate against?', 'Thanks. What would be the right tool?  \n\nGage R&R is industry standard for this type of analysis in the chemical engineering industry.  Its a chemical processing laboratory test method analysis to get repeatability (equipment) and reproducibility (operator) results. It helps an operator establish error bands on their data and determine if it is operator or instrument / environment related.  Which is exactly what I want to determine, statistical method of determining which instrument and which technicians are best suited to perform the analysis based on reducing the variance. \n\nNote there are many other factors to consider besides variance in the data but this is the first step. \n\nMaybe people arent familiar with Gage R&R study?  Because from its description and general application in the industry, its generally what Im wanting. In the chemicals industry, test methods need to be repeatable and reproducible. And this requires specific training on the methods. Theres often many standardized test methods that get the job done, but each comes with its own sources of error. And each requires a different level of training and accessory equipment to perform properly.', ""Typically for a gage r&r, you use the variance components of a random effects model and the mse to determine if the repeatability and reproducibility are sufficiently small relative to the tolerance (Precision to tolerance). \n\nYou're trying to compare multiple methods at multiple labs which isn't as typical. Its also not typical to do a comparison to determine which subgroup is best.\n\nWhat you're trying to do can certainly be done, but its going to require a lot more background knowledge than you're going to find on a reddit post. Consultants with this knowledge will probably charge ~300$ an hour.""]"
[Q]Mudlak model with categorical predictors,,1333i2k,3ducklings,1682794058.0,3,1.0,[]
[Q] Feature importance and contribution interview problem I can't solve.,"A few weeks ago, I had an interview for a data science job. Of all the questions they asked me, I was unable to solve the following one. I couldn't even attempt it because I didn't know anything about it. I have researched, but I can't find any book, paper, blog, etc. that shows the question.

So here's what I remember from the question:

The following table has the ""feature contribution"" of feature i with observation j. Additionally, each feature is divided into different categories. Category 1 has features 1, 2, and 3, Category 2 has features 4 and 5, and Category 3 has only feature 6.

||Feature 1/Category 1|Feature 2/ Category 1|Feature 3/Category 1|Feature 4/Category 2|Feature 5/ Category 2|Feature 6/Category 3|
|:-|:-|:-|:-|:-|:-|:-|
|Obvservation 1|\-4|4|1|2|\-2|0|
|Observation 2|2|1|\-1|0|4|2|
|Observation 3|1|4|5|3|3|1|
|Observation 4|0|2|1|1|4|\-3|
|Observation 5|\-3|\-2|0|\-2|2|1|

&#x200B;

If I recall correctly, they asked me to obtain the feature importance of each category. So the expected result was three numbers, one for each category.

During the interview, I attempted to calculate the means and sums and then means again, but the answer was never correct. Do any of you have any references on how to solve this problem?",132zfor,Jay31416,1682783892.0,3,0.8,"['Did you try to calc for each obsrvn percentage of category x from all categories? Then mean from all obsrvns?\nOr maybe sum of category x from all obsrvns, then percentage from all categories and all obsrvns?\nSounds right.']"
[Q] Publication reviewer made this comment about my use of factor analysis. How should I proceed?,"Hi all, 

I created a psychological questionnaire that I am now publishing. I subjected the questionnaire to various reliability and validity tests with excellent results, including a test of discriminant validity that looked like the following: I took my questionnaire and a pre-existing questionnaire that was deemed to be theoretically related but ultimately measured a distinct construct and ran an exploratory factor analysis on the pooled items. The idea here was that the items on each questionnaire should form their own distinct factor (with strong factor loadings), while remaining highly correlated, suggesting that the scales do in fact measure distinct, but related, constructs. The analyses were successful and my questionnaire (while being significantly correlated with the other questionnaire) diverged from the other questionnaire in the EFA in the way I described above, (i.e., items from each questionnaire forming their own clean, distinct factor, with strong factor loadings).

The reviewer wrote this comment: ""For the discriminant validity of your questionnaire, there is a risk that the EFA results could be partly attributed to the different likert-scale anchors used (your measure in terms of agreement and the other measure in terms of truth). 

My impression is to address this comment by making note of this in the discussion section by saying that while the results do provide strong evidence for the discriminant validity of this questionnaire, the results should be interpreted with some caution as the results could be partially due to the different anchor labels. Something like that, written better of course. 

I wanted to run this by you guys and see if you had any suggestions for addressing this beyond what I already plan on doing or if you had some suggestions for strengthening the plan I already have. And also how valid is the reviewer's comment? 

Thank you so much. If you need any information above what I have put, please feel free to ask.",132z0fc,TheEudaimonicSelf,1682782851.0,13,1.0,"['EFA is not usually used for discriminant-related validation', 'The reviewer is right, you could end up with distinct factors simply due to method. Traditionally discriminant validity has been evaluated with an MTMM matrix. Another alternative that is well regarded is the method developed by Westen and Rosenthal.\n\n\nWesten, D., & Rosenthal, R. (2003). Quantifying construct validity: two simple measures. Journal of personality and social psychology, 84(3), 608.', 'How did you determine how many factors to extract in the EFA? Parallel analysis?', 'Yes, that makes sense. My supervisor had recommended this method to me, and I trusted his judgment. I suppose Ill have to go a different route for discriminant validity.', 'Hmm, okay. This makes sense. Thank you. My supervisor recommended that I go the EFA approach, and I trusted his judgment. Im a bit pressed for time as it is due in 2 weeks, but do you recommend scraping the EFAs in favour of an MTMM matrix? Do you think Ill have enough time to make those changes (I know you dont know the full details, but just as an best guess estimate)?', 'Yes, I used parallel analysis', ""I will say, as an academic, I'm always a little wary that I'm wrong so take my advice with a grain of salt. It could be that your advisor knows something that I don't.  But I would just do an MTMM matrix. It's quick and easy and it is widely accepted i.e., a reviewer shouldn't give you grief about it (the methodology at least)."", ""Well, the comment seems to imply that they think there's a possibility that if you extract 3 factors, you might get a method loading and your items co-mingling?"", 'Fair enough, self-directed skepticism is always good practice and I appreciate you cautioning me. I will definitely heed your advice and make a thoughtful, careful decision. I also dont think it hurts to do an MTMM matrix if, like you say, its a relatively simple procedure. \n\nIs the MTMM matrix procedure possible via SPSS or R? If so, do you have any resources that you could point me to to help get me started? Perhaps the recommended R package to use or something like that? I know I might be asking for too much help from you, so feel free to set a boundary. The help youve already provided is much appreciated.', ""Actually, the more I think about it, MTMM would only be effective if you collected different measures AND used multiple methods (self-report, other-report, observation). There is a method-less version, but I'm not sure it's worth it. How comfortable are you with CFA in R?\n\nThis article provides an overview of all the possible methods as well as pros/cons and when to use them. It also provides tutorials for doing each method in R, Stata etc.\n\nRnkk, M., & Cho, E. (2022). An updated guideline for assessing discriminant validity. Organizational Research Methods, 25(1), 6-14.\n\nYou can find the online appendices with code walkthroughs here:\n\n[https://github.com/ormonline/DiscriminantValidity](https://github.com/ormonline/DiscriminantValidity)\n\nLong story short, the method they recommend the most is to run a CFA with standardized factor loadings (variance of latent factor set to 1) and then look at the factor correlations. If the confidence interval of the pairwise correlations are greater than .8 you have a problem.\n\nhappy to (hopefully) help."", 'This is how you would do it in R\n\n&#x200B;\n\n1. obtain covariance matrix (this is creating an example)\n\n`ECSICovData <- matrix(NA,11,11)`\n\n`rownames(ECSICovData) <-`\n\n`colnames(ECSICovData) <- c(""ACSI1"", ""ACSI2"", ""ACSI3"", ""CUEX1"", ""CUEX2"",`\n\n`""CUEX3"", ""PERQ1"", ""PERQ2"", ""PERQ3"", ""PERV1"",`\n\n`""PERV2"")`\n\n`ECSICovData[upper.tri(ECSICovData, diag = TRUE)] <-`\n\n`c(4.00,`\n\n`3.23, 4.41,`\n\n`2.66, 2.67, 3.61,`\n\n`1.81, 1.50, 1.56, 4.41,`\n\n`1.85, 1.62, 1.63, 2.63, 4.84,`\n\n`1.24, 1.16, 1.09, 1.55, 1.72, 5.29,`\n\n`3.04, 2.83, 2.35, 2.07, 1.96, 1.31, 3.61,`\n\n`2.81, 2.57, 2.19, 1.55, 1.74, 1.12, 2.67, 3.24,`\n\n`1.73, 1.64, 1.32, 0.89, 1.05, 1.41, 1.62, 1.62, 2.89,`\n\n`2.66, 2.49, 2.12, 1.40, 1.43, 0.95, 2.22, 2.01, 1.25, 3.24,`\n\n`2.99, 2.86, 2.42, 1.52, 1.50, 1.01, 2.34, 2.14, 1.31, 3.05, 4.84)`\n\n`ECSICovData[lower.tri(ECSICovData)] <-`\n\n`t(ECSICovData)[lower.tri(ECSICovData)]`\n\n2. set N\n\n`N <- 10417`\n\n3. Specify Model\n\n`CFAmodel <- ""ACSIPERQ =~ ACSI1 + ACSI2 + ACSI3 + PERQ1 + PERQ2 + PERQ3`\n\n`CUEX =~ CUEX1 + CUEX2 + CUEX3`\n\n`PERV =~ PERV1 + PERV2""`\n\n4. Fit model and get standardized estimates\n\n`CFAest <- cfa(CFAmodel, sample.cov = ECSICovData, sample.nobs = N,`\n\n`std.lv = TRUE)`\n\n5. Get 95% CI\'s.\n\n`summary(CFAest, ci = TRUE)`\n\n6. evaluate correlations\n\n`## Covariances:`\n\n`## Estimate Std.Err z-value P(>|z|) ci.lower ci.upper`\n\n`## ACSI ~~`\n\n`## CUEX 0.612 0.009 71.846 0.000 0.596 0.629`\n\n`## PERQ 0.957 0.002 384.934 0.000 0.952 0.962`\n\n`## PERV 0.875 0.004 226.986 0.000 0.867 0.882`\n\n`## CUEX ~~`\n\n`## PERQ 0.698 0.008 90.904 0.000 0.683 0.713`\n\n`## PERV 0.526 0.010 54.787 0.000 0.507 0.545`\n\n`## PERQ ~~`\n\n`## PERV 0.770 0.006 139.827 0.000 0.759 0.780`\n\n7. Compare to table of cutoffs\n\nsevere problem \\~ 1 <= upper limit of CI\n\nmoderate problem \\~ .9 <= upper limit CI < 1\n\nmarginal problem \\~ .8 <= Upper limit CI < .9\n\nNo problem \\~ Upper limit CI < .8', 'Thank you so very much for all your help, WeaponizedWhale. While getting this done on time is a daunting task, you have made it all the more easy and surmountable. Cheers from one academic to another!', 'No problem, I had some work to avoid']"
[Question] [Q] Z-scores for each item of a questionnaire?,I have to run correlations and regressions with all items from multiple questionnaires. Each questionnaire has its own scale. Should I normalize each item by using z-scores and then run the analysis? Does it make sense?,132sk9s,TonyLoo,1682773891.0,11,0.87,"[""I don't understand what you are doing or what you are asking. Going to need to provide more context/information."", 'you normalize to remove scaling effects. if everything is a 5 point likert then they are on the same scale - so no.', ""I have applied 5 questionnaires to 170 people. Each questionnaire has its own scale. I want to run correlations and regressions on all items from all questionnaires in order to find the right ones for a model. I haven't done this before. I m pretty new to statistics and I don't know if it makes sense to apply z-scores on each individual item of each questionnaire."", 'They all have different scales: 4-point Likert, 7-point Likert, 6-point-Likert etc with total scores that vary across different intervals (\\[-22, +22\\],\\[1, 7\\], \\[-72, +72\\], \\[0, 26\\])', 'Are the questions ""agree vs disagree style"" eg\n""I like stats""\n1) I strongly disagree \n2) I slightly disagree \n3) I neither agree nor disagree \n4) I slightly agree \n5) I strongly agree\n\nIf so, you can\'t standardise your data since the SD (and to a lesser extent the mean) are meaningless\n\nMore info about the data you\'re analysing would be useful', 'If the respondents answered different questionnaires, you cant do correlations for items across all questionnaires, regardless of scaling. Correlation basically measures whether respondents tended to answer two questions similarly, so you need respondents who answered both questions.', 'Yes, this is what they look like and each option is represented as an integer in my database. Questionnaires are about laterality, self-efficacy, and learning styles.\n\nThank you for your answer', 'You could convert those agreement-style answers into their corresponding integers.  Then a questionnaire may have a mean response of, say, 2.3 out of 5, instead of ""slightly more than slightly disagree.""', 'All respondents answered all the questionnaires.', 'Just to add, you can\'t z normalise the data but you can rescale your answers to have the same point values, from there you can still do statements like ""x% of people strongly agreed y thing"" etc\n\nHere\'s an [article](https://www.ibm.com/support/pages/transforming-different-likert-scales-common-scale) on rescaling from IBM\n\nNote however, [some people disagree when going from odd to even or vice versa](https://stats.stackexchange.com/questions/486957/rescaling-a-5-point-likert-scale-to-6-points) since it can lead to people acting differently if there\'s a ""neither agree nor disagree"" option or not, that\'s more up to you whether you think there\'s value to be gained in converting or not\n\nI\'d also add, if your questionnaires are previously published and you\'re reusing them, it\'s best not to change the point scale or option labels used as this alters the test from the original, if it\'s your own questions and scales though, then you can go right ahead\n\nHope this all helps, let us know if you have any more questions!', ""You _can_ but that type of questionnaire is a likert scale under the hood which is _trchnically_ ordinal data rather than interval data (ie the true distance between answer 1 and 2 isn't necessarily the same as between 2 and 3)\n\nThat said, most people do treat it as interval data even if it isn't strictly speaking, hence why I said you can't take the mean _to a lesser extent_\n\nThere's also been studies that show that the granularity of your scale can affect how people answer, since OP says they're working with questions on different scales, I'd stick to using the mode or summarised categorises for my analysis"", 'agreed, smart thinking!']"
[Q] Conditioning on other variables question,"This is purely a hypothetical question:


Let's say I want to regress salary on years of education in a random sample of the population. Then I want to control for age, because I think it could be the case that older people were less likely to go to college, and age may have an effect on salary.

If I were to limit my sample to-only people of the same age and run a regression, and then do this for ecery age in my sample, is there a way to aggregate those up to what the regression coefficient would be if I just ran a regression of salary on years of education and age?

The reason I am wondering is because my understanding of conditioning on age in the multiple regression is i am now making comparisons between people with the same age, but different levels of education. The latter example then is quite literally restricting the data so that you are only making comparisons between people of the same age, so
on a surface level it seems like they should coincide/bring the same coefficient?",132fphc,Whynvme,1682731762.0,7,0.89,"[""> If I were to limit my sample to-only people of the same age and run a regression, and then do this for ecery age in my sample, is there a way to aggregate those up to what the regression coefficient would be if I just ran a regression of salary on years of education and age?\n\nThis is precisely what happens if you treat age as a categorical variable, representing each distinct age value with dummy variables.\n\nThe negative is that you lose all context about nearby ages being similar.\n\nThe positive is that you gain a ton of flexibility in the modeling process, though the large flexibility mandates a reasonable sample size for each of the distinct age values, otherwise you're essentially overfitting."", 'I might be speaking bullshit buy maybe the question would be fully answered if you revisit how conditionals work in linear regression. Kutner book ftw.', 'Lets assume that the relationship between years of education and salary is a simple linear equation, y=a+b*x+e where y is salary and x is yrs of education, a is the y-intercept and b is the slope. Lets say you have data on 50 people that are 30-years old, 50 that are 40-years old and 50 that are 50-years old. If you fit (using least squares regression) the simple linear equation separately to the 30 year olds, 40 year olds, and 50 year olds, you will get three different equations with different estimates for a and b for each age category. The error variance will also differ for each age category and is estimated from the 50 data points in each subset. \nYou can get the same a and b model parameter estimates (but not the same error variance) by combining the three age group subsets into one data set and creating indicator variables z1, z2, and z3 such that z1 = 1 if age is 30, and z1=0 otherwise; z2=1 if age=40, and z2=0 otherwise; and z3=1 if age=50, and z3=0 otherwise. Then fit the model y = a1*z1 + a2*z2 + a3*z3 + b1*z1 + b2*z2 + b3*z3 + e\nTry writing this equation for each age group and you will see that the zs serve to create three equations, one for each age category, just like you had when you subsetted the data manually.\nThe error variance in this combined model with the zs, is now based on all 150 observations, so it will not be the same as the three estimates you got when fitting the age groups separately. \n\nThere are many ways to define a set of indicators variables (the zs). Some software creates them for you automatically using the first age category (30 yrs) as the reference category and some use the last category as the reference (50 yrs in our example). You are technically getting the same fit, but it wont look the same because the parameters for the other groups are in comparison to the chosen reference category. \nAnd one more way to build this model worth mentioning is instead of using indicator variables you could use q=age (that is when age is 30, q=30; when age is 40, q=40; and when age is 50, q=50.)\nNow when you fit y = a + b*x + c*q + e you will be fitting a surface; in this case the surface is a flat surface like a piece of plywood. If you fit y = a + b*x + c*q + d*x*q+e you will get a torqued surface where a slice at any point on the x-axis or on the q-axis is still a straight line, but the overall surface is twisted. You will not get the same fitted lines when age is parameterized as continuous (defined by q) as either of the previous models using z because using q implies that the change in salary from 30-yr olds to 40-yr olds is the same change as from 40-yr olds to 50-yr olds.\n(My apologies - I do not know why part of my answer is italicized. I am new at this.)', '>This is precisely what happens if you treat age as a categorical variable, representing each distinct age value with dummy variables.\n\nNot quite. The models that are split up by age are almost equivalent to treating age as a categorical variable and including the interactions between age and every other variable. \n\nI say almost because doing things this way also will estimate different standard deviation terms for each model (assuming linear regression), which may result in slight differences in inferences.', 'Not quite exactly the same, unless you separately fit the intercept term for the entire sample separately.', ""Sorry, you're right, I forgot to mention that it's only equivalent in the sense of including interaction terms.\n\nI'm probably just rusty, but I dont remember the reason that variance estimates are affected. Logically I would think they shouldn't be, since the calculations for each category can be performed entirely independent of one another in principle. Where does the difference come into play?"", 'It is exactly the same, but I left out the requirement that you need to include all interaction terms as well with the categorical variable (age in this case). The intercept term of the separate model for age\\_i is equivalent to original_intercept + beta\\_i, where beta\\_i is the coefficient for the age\\_i dummy variable.', ""\\> Logically I would think they shouldn't be, since the calculations for each category can be performed entirely independent of one another in principle. \n\n&#x200B;\n\nYou're right that this is easily done in principle, but a lot of regression software doesn't support doing this, so it's rarely done. Usually a single pooled variance is estimated using all the residuals at one time.""]"
Correlation matrix with numerical and categorical data [Q],"Im trying to make a correlation matrix with a dataset containing both numerical and categorical values, but Im not sure how to do it. Could I just change the categorical values to numbers (1, 0 etc) and run that?",13275lg,UndeadBlaze_LVT,1682709646.0,2,0.62,"[""correlation is not defined for categorical variables, in particular for those with more than 2 categories. So no you can't. Coding categories with numbers is ill advised most of the time. Something like factorial analysis of the mixed data (FAMD) may be helpful tho."", 'You can find the association between two nominal variables.  Cramer\'s *V* works for this.  But the interpretation of what may be a ""large"" association depends on the number of categories.\n\nIf your categorical variables are either nominal with two groups, or ordinal categories, there are different options.\n\nIf you have a nominal variable with more than two categories and a numeric variable, I don\'t know of a standard method for this.  But practically speaking, you could use a one-way anova, and take the square root of the r-squared value.', 'You could but rather than correlations with categorical variables, it is probably clearer to show the means and sds for each both levels of the variable.', 'You need to have strictly numerical variables for calculating a correlation', 'To do this you would need to dummy code your one categorical variable so that your values are dichotomized and run a point biserial correlation. \n\nBut in most cases you probably dont want to dichotomize your variables. Depending on your goal with your research question youd probably consider running a different test/making a different type of table with those variables.', 'WOE?', 'And yet basically all of the regression implementations in statistical programs turn the categories into 0s and 1s. Ill agree that its a bad idea to turn an unordered category into a numerical scale, but turning categorical variables into dummy variables (implemented as 0s and 1s) is standard practice. \n\nFor interpretation, theres no real difference between a change from category a to category b and a change from 0 to 1.', 'I think you are right and yet you are downvoted. They could just do ANOVA.', 'I am well aware of that. But that is for different purposes, other than correlation construction. In fact that is the way FAMD codes categorical variables to work with. Op was taking about coding 1, 2, 3 instead of a, b, c, which is not good most of the times', 'Thanks. There are lots of people, especially economists, who hate ANOVA and will downvote any analysis of categorical variables that doesnt involve dummy coding. I learned dummy coding and other coding methods such as contrast coding and effect coding many, many years ago  from a  Cohen and Cohen book. However, now that you can specify variables as factors, dummy coding seems so 20th century.']"
[Q] Estimation of covariance matrix from elevation data grid,"Hello all,

I have data in a grid format from elevations of a terrain. I want to estimate a covariance matrix from that data. For that, I have calculated a semivariogram, where I have fitted a Matern function to the empirical data. 

However, I have some doubts. From that empirical function, I have an estimated nugget value that is smaller than half the squared standard error in the measurements, which has also been given to me by the data source. From my understanding, this is not possible.

Does this mean that I did something wrong in the semivariogram estimation? Could I set the nugget value to match the standard error in some way? Would that be the same than rescaling the covariance matrix obtained?

Thanks in advance for providing any insight.",1324sf7,Pii-oner,1682704371.0,0,0.5,['It is certainly possible to have very small (or 0) values for the nugget. Its not inherently incorrect.']
[D] Discussion: Which Statistical Tests Would You Perform?,"# Problem

I want to know which ***type of investors*** and which ***combination of the type of investors*** (funding partner mix) are empirically predominant at each life-cycle stage of European startups that had a successful exit. The life cycle stage refers to which point in their life the startup is in. I have divided the life cycle stages into three: early, mid, and late stage. 

In other words, I want to find out which individual type of investor is predominant at each of these stages. So for example, lets say at the *early* stage I find that angel investors are the most predominant, followed by venture capital. 

Then, I also want to know if there is a combination of investors in that stage that is predominant (since most startups have more than one type of investor). For example, lets say I find that at that same early stage, the predominant ***combination*** of investors is accelerator + venture capital. The second most predominant combination is angel + venture capital + venture debt. And the third most predominant combination is solely angel investors (angel investor + nothing).

# Datapoints Available

I have the following columns in my sample dataset: startup name, startup founding date, startup exit date, investor name, date the investor invested in the startup, type of investor. 

I will determine the stage to which each investor belongs based on the difference between the startup founding date and the date the investor invested. Early (invested in 0-2 years since founding), Mid (3-5 years), late (6+ years).

**Note:** All of the startups in my sample dataset had a successful exit, I could not get my hands on data on startups that did not exit. That means I cannot perform any tests that may require startups that did not have an exit as a control.

# Question

With this information, **which statistical tests should I perform to find which** ***type of investors*** **and which** ***combination of investor types are empirically predominant at each of these three stages*****?** 

My statistical knowledge is very basic and I am using either BlueSky statistics software (GUI for R) or Excel to perform the necessary tests. Therefore, the preferable thing would be to find the simplest way to obtain what I want. However, I also appreciate suggestions for complementary tests I could perform to either strengthen my results or enrich my analysis.",1321wmw,yunnospllrait,1682701058.0,0,0.5,"['> All of the startups in my sample dataset had a successful exit, I could not get my hands on data on startups that did not exit. That means I cannot perform any tests that may require startups that did not have an exit as a control.\n\nWell, thats kind of a big deal.  I would think that your problem statement is inherently unanswerable without negative examples. And not cherry-picked examples like a lot of business books present (eg Good To Great), but either more comprehensive or a random sample.', ""I thought about it more and still think the negative data is necessary.  Different funds have different specializations, e.g. early stage, growth stage, late stage, private equity, and because of this the distribution will be different across stages.  Another factor is industry.   Your dataset might reflect that some fund/investors have better performance, but that might just be an industry bias.  Like if your data was from 2020-2021, you would think crypto funds were the path to success when we all know (now at least, and some of us before) that was an inevitable blip.  Or you may see one player in 10% of the exits, when the case is they are in 10% of all funding at that stage.  \n\nThere's so many factors you have to control for that you just can't do without the negative data."", ""Thank you for the input. I really appreciate it. Well, seems like I'm in a bit of a pickle then. For context, I'm analyzing startups that fall within the ICT sector, so although that would not fix the industry issue, it should at least alleviate it.\n\nOn the other hand, I see what you mean with the different distributions for different funds. That is also a big problem. My intention when beginning this research was to use a control sample with startups that did not have an exit. However, I had some technical limitations regarding it. I can still obtain a control sample, but I am unsure of how representative it may be. Let me explain.\n\nI used Crunchbase to extract the sample of startups that had an exit. About 2,700 startups approximately were extracted, and those were ALL of the startups with an exit that Crunchbase had in its databse. Now, in that same database there are around 80,000 startups that did not have an exit. The problem is I cannot extract ALL of them due to maximum extraction limits Crunchbase has. I could potentially extract around 3,000/80,000 startups without an exit. Therefore, if I was to extract startups without an exit to use as control, **the question would be how can I extract a representative sample of those 80,000 startups?** And could I use such a sample to compare it with the successful exit sample I already have?\n\nI would love to hear other opinions on this issue. What would be the best theoretical way to go about this? I may not be able to execute that theory in practice given some of Crunchbase's limitations, but maybe if we come up with a theoretical solution I can work around them and get a representative sample.""]"
[Q]How to work out confidence intervals for the following two scenarios? (please see description below)," I've  got a linear regression sas output. So, I know the standard error from  there. I need to work out the 95% confidence interval for the change in y  as x increase by 1, which is basically the confidence interval of the  slope I think. So, according to some research I have done, I need to  work out critial t value first. I have sample size of 31, so df=31-2=29,  and found the critical value for 95% to be 2.045. So I took the  parameter estimate of x +/- t-critical value(2.045)\*standard error of x.  parameter estimate and standard error taken from the aforementioned sas  output for regression. I am just not sure if I have done this correctly  or if I should take z-critical value, if so how?

2nd  scenario is chi-sq test. 3 treatments. recorded yes or no for disease. i  worked out the expected values this way. (total number of treatment 1 \*  total number of diseased) / total sample size, for expected number of  people who have disease with treatment 1 and so on. Please comment on if  I got this right. and then I worked out the chi-sq. And then, I need to  work out the 95% confidence interval for difference in rate of disease  incidence (yes-disease) between participents of treatment 2 and 3. I am  just completely at a loss here. i found thid potentially useful formula.  CI = sample mean +/- z\*(s/square-root of n). s is sample standard  deviation. but no idea what sample mean would be in chi-sq test or how  to find s or z.",131z6vj,Ngachate,1682698457.0,1,0.6,[]
[Q] Is linear regression suitable for discrete numeric dependent variable when responses are bunched?,"I am conducting regression analysis and my dependent variable is derived from dividing the number of candidates standing in an election by the number of vacancies. This gives me a discrete dependent numeric variable, which is throwing up issues in the diagnosis of my linear regression model.

Are there alternative types of regression I could use - or is it generally fine to use linear regression?



| cand/vac         | count        |
|------------------|--------------|
|     1            |      1098    |
|     1.3333333    |       159    |
|     1.5          |       659    |
|     1.6666667    |       316    |
|     2            |      7552    |
|     2.3333333    |       952    |
|     2.5          |      1933    |
|     2.6666667    |       902    |
|     3            |     14244    |
|     3.25         |         3    |
|     3.3333333    |      1291    |
|     3.5          |      1416    |
|     3.6666667    |       847    |
|     4            |     15528    |
|     4.3333333    |       458    |
|     4.5          |       445    |
|     4.6666667    |       222    |
|     5            |      7777    |
|     5.3333333    |        31    |
|     5.5          |        51    |
|     5.6666667    |        11    |
|     6            |      2048    |
|     6.3333333    |         2    |
|     6.5          |         3    |
|     7            |       336    |
|     7.5          |         1    |
|     8            |        35    |
|     9            |         3    |",131shky,Theogon,1682690161.0,4,0.75,"['1. What is the goal of the analysis?\n\n2. What exactly are your explanatory and response variables? The data you have included here is not something you would use a regression model for.\n\nAs a general answer to your question about discrete data, it can sometimes be appropriate to use linear regression for discrete responses, provided the residuals are approximately normal (in addition to the other assumptions of course). If normality is a concern, then other models should be considered (such as Poisson regression, which is common for count data).', ""I'd try a model suitable for count data such as a negative binomial regression with an offset. The outcome is the number of candidates whereas the (log) offset is the number of vacancies."", '\n\nIf your dependent variable is a ratio or a proportion, linear regression may not be the best option as it assumes the residuals to be normally distributed and to have constant variance, which may not hold for such variables. Instead, you may consider using regression models specifically designed for analyzing proportions, such as logistic regression or beta regression.\n\nLogistic regression is used when the response variable is binary, whereas beta regression is used when the response variable is continuous and bounded between 0 and 1. Both models allow for heteroscedasticity, and can handle data that have a non-normal distribution. \n\nBefore deciding on which model to use, it is important to understand the nature of your data, and the research question you are trying to answer. Consult with a statistician or an experienced data analyst to help you select the best model for your specific case.', 'Thanks Chatgpt.', 'You are welcome.']"
[Q] Best way to estimate a three-way interaction,"I am testing the effect of being female on chances of electoral success in German Federal Elections. I would like to find the interaction between female, a binary variable (female), electoral tier, a binary nested variable (estier), and political party, a categorical variable with 4 discrete categories (PartyID).

I would expect the estimated three-way interaction terms to closely approximate the actual differences I observe in the rate of female success between estier = 1 and estier = 0 across each of the 4 political parties.

&#x200B;

|Political Party|Proportion of Women Elected (estier = 1)|Proportion of Women Elected (estier = 0)|Difference (estier = 1 - estier = 0)|Expected Magnitude and Direction of three-way interaction|
|:-|:-|:-|:-|:-|
|CDU|0.370 (73 DF)|0.097 (217 DF)|0.273|Strongly Positive|
|SPD|0.331 (118 DF)|0.322 (152 DF)|\-0.008|Near-Zero|
|AfD|0.052 (38 DF)|0.375 (24 DF)|\-0.322|Strongly Negative|
|GREEN|0.0625 (144 DF)|0.285 (214 DF)|\-0.223|Strongly Negative|

Note: DF indicates number of women candidates in this case

The problem I am facing is that I can't find a model that replicates what I should expect from the table above.

After trying a few options, this particular model seems reasonable.

    lm(elected ~ female + PartyID + female:estier:PartyID))

From what I've read about nested variable, I exclude the 'estier' as an independent predictor variable, as it is only substantively meaningful when included in the interaction term. I also excluded the sub-interactions, such as female:estier and female:PartyID, as these would drastically increase the p-values of the three-way interaction terms, I think this is because of multicolinearity. Applying this model, I get the following results.

                               Estimate Std. Error t value Pr(>|t|)    
    (Intercept)                 0.19611    0.02073   9.458  < 2e-16 ***
    female                     -0.04054    0.02103  -1.928  0.05394 .  
    PartyIDCDU                  0.07192    0.02685   2.678  0.00744 ** 
    PartyIDGREEN                0.02366    0.02848   0.831  0.40606    
    PartyIDSPD                  0.16382    0.02860   5.729 1.13e-08 ***
    female:PartyIDAfD:estier   -0.10293    0.07439  -1.384  0.16657    
    female:PartyIDCDU:estier    0.14237    0.05405   2.634  0.00849 ** 
    female:PartyIDGREEN:estier -0.11673    0.04167  -2.801  0.00513 ** 
    female:PartyIDSPD:estier    0.01113    0.04574   0.243  0.80781    
    ---
    Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
    
    Residual standard error: 0.4232 on 2499 degrees of freedom
    Multiple R-squared:  0.03484,	Adjusted R-squared:  0.03175 
    F-statistic: 11.28 on 8 and 2499 DF,  p-value: 8.445e-16

So clearly the model is a decent approximation of the expectations from the table above. However, the coefficients are not really proportional to what we should expect. For instance, the AfD party should have a more negative coefficient than the Green party, which makes me believe that I'm forming the model incorrectly

I've also tried running the linear model with solely the three-way interaction, and no other predictors:

    lm(elected ~ female:estier:PartyID))

However, as I understand, this is methodologically improper. This model also results in three-way interactions that don't match expectations from the table.

So how can I best estimate the three-way interaction to match the expectations?

Note that I'm not really interested in the independent effect of female, estier or PartyID on election outcome, only on the three-way interaction.

\*\*UPDATE\*\*

I tried the following model.

    lm(elected ~ female * estier * PartyID)

At first I thought this model couldn't be right because the three-way interaction coefficients did not at all match what I expected from my table above. However, I used predict () to double check this and it does in fact result in the correct predicted values, so this is the right model.

Now, my question is, how do I substantively interpret the model's three-way coefficients? I'm not understanding how the three-way coefficient for CDU candidates could possibly be negative. Advice has been very helpful and appreciated so far!

                               Estimate Std. Error t value Pr(>|t|)    
    (Intercept)                 0.41727    0.03414  12.223  < 2e-16 ***
    female                     -0.04227    0.08897  -0.475 0.634782    
    estier                     -0.36236    0.04244  -8.539  < 2e-16 ***
    PartyIDCDU                 -0.26180    0.04297  -6.093 1.28e-09 ***
    PartyIDGREEN               -0.17165    0.04597  -3.734 0.000192 ***
    PartyIDSPD                 -0.16418    0.04653  -3.528 0.000426 ***
    female:estier               0.04000    0.11320   0.353 0.723876    
    female:PartyIDCDU          -0.01642    0.09666  -0.170 0.865106    
    female:PartyIDGREEN         0.08170    0.09808   0.833 0.404941    
    female:PartyIDSPD           0.11155    0.09991   1.117 0.264306    
    estier:PartyIDCDU           0.71792    0.05653  12.701  < 2e-16 ***
    estier:PartyIDGREEN         0.16191    0.06159   2.629 0.008620 ** 
    estier:PartyIDSPD           0.56232    0.06079   9.250  < 2e-16 ***
    female:estier:PartyIDCDU   -0.12246    0.13105  -0.934 0.350169    
    female:estier:PartyIDGREEN -0.06209    0.12918  -0.481 0.630822    
    female:estier:PartyIDSPD   -0.23181    0.13095  -1.770 0.076815 .  

&#x200B;

FINAL UPDATE

Thank you all for your responses, I understand now. The three-way interaction is heavily dependent upon the higher-order estier:PartyID coefficient, which provides most of the predictive power, alongside the other interaction coefficients. In other words, there is not a statistically significant difference between male and female outcomes across electoral tiers within the same political party. 

I suppose the takeaway is that I was trying to substantively interpret the three-way interactions without realizing they are model-specific, dependent upon the values of their constituent sub-interactions. This was probably obvious to many of you but it took me a bit to get on the same page. 

Funnily enough, now the statistical significance of my results has all but evaporated. Seems my discussion section will be dramatically shortened. At least I am doing it correctly lol.",131pv08,DementedFerret,1682683759.0,9,1.0,"[""> I also excluded the sub-interactions, such as female:estier and female:PartyID\n\nWell thats the problem, excluding the sub-interaction terms amount to assuming their coefficients are fixed at zero. If this assumption is not true, the rest of coefficients will have to change to account for the misfit. You can find more resources [here](https://stats.stackexchange.com/questions/27724/do-all-interactions-terms-need-their-individual-terms-in-regression-model#27726).\n\n\n> I exclude the 'estier' as an independent predictor variable, as it is only substantively meaningful when included in the interaction term\n\nIm not sure what you mean by this, since 'estier' is included in your three way interaction."", 'would be nice to see how data is structured, perhaps a `summary` on the data frame could be useful, I would like to see how is the `elected` variable behavior. Also I would like to see how the nested variable `estier` is also coded, this is very important. Can you explain further how is the nature of the ""nestedness"" of this variable? it is nested inside what? states, parties?', ""Ok. I did consider this.\n\nLet's try testing the model to determine the female:estier interaction across all political parties.\n\nlm(elected \\~ female + female:estier)\n\n|Intercept|0.259|<2e-16|\n|:-|:-|:-|\n|female|\\-0.02917|0.157|\n|female:estier|0.028|0.392|\n\nNo need to add PartyID in this regression. So female:estier itself is near zero and insignificant.\n\nBut now let's try including the sub-interactions within the three-way interaction model\n\nlm(elected \\~ female + female:estier + female:PartyID + female:estier:PartyID)\n\n`Coefficients:`\n\n`Estimate Std. Error t value Pr(>|t|)`\n\n`(Intercept) 0.18274 0.02108 8.670 < 2e-16 ***`\n\n`female 0.19226 0.08796 2.186 0.02893 *`\n\n`PartyIDCDU 0.14629 0.02865 5.107 3.53e-07 ***`\n\n`PartyIDGREEN -0.03243 0.03132 -1.035 0.30056`\n\n`PartyIDSPD 0.17586 0.03090 5.692 1.40e-08 ***`\n\n`female:estier -0.32237 0.10908 -2.955 0.00315 **`\n\n`female:PartyIDCDU -0.42452 0.09445 -4.495 7.28e-06 ***`\n\n`female:PartyIDGREEN -0.05752 0.09535 -0.603 0.54642`\n\n`female:PartyIDSPD -0.22849 0.09695 -2.357 0.01851 *`\n\n`female:PartyIDCDU:estier 0.59546 0.12290 4.845 1.34e-06 ***`\n\n`female:PartyIDGREEN:estier 0.09982 0.11804 0.846 0.39781`\n\n`female:PartyIDSPD:estier 0.33051 0.12056 2.741 0.00616 **`\n\n`---`\n\n`Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1   1`\n\nAs you can see, now the estimated three-way coefficients make even less sense than before. How can the female:PartyIDSPD:estier coefficient be 0.33 (statistically significant) when the rate of success of female candidates in the SPD party are almost exactly equal between both electoral tiers. It just doesn't make sense."", "">I exclude the 'estier' as an independent predictor variable, as it is only substantively meaningful when included in the interaction term\n\nI meant that I exclude 'estier' as an independent variable. Using the ':' instead of the '\\*' for interaction terms in R accomplishes this. estier is only included within the interaction term because it is a nested variable. I am not interested in the effect of estier itself on electoral chances, only when it is interacted with the other variables."", 'Here is better formatted version of the summary() with only the relevant variables.\n\n|elected|female|Party ID|estier|\n|:-|:-|:-|:-|\n|Min: 0.00|Min: 0.00|Length:2508|Min: 0.00|\n|1st Qu. 0.00|1st Qu. 0.00|Class: Character|1st Qu. 0.00|\n|Med: 0.00|Med: 0.00|Mode: character|Med: 0.00|\n|Mean: 0.245|Mean: 0.391||Mean: 0.475|\n|3rd Qu. 0.00|3rd Qu. 1.00||3rd Qu: 1.00|\n|Max: 1.00|Max: 1.00||Max: 1.00|\n\nTIL that Reddit does not like summary() output tables...', 'Yes, of course!\n\nHere is the summary.\n\nm               female             mw             elected        electedtier      PartyID               tier\n\nMin.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Length:2508        Min.   :1.000\n\n1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000   Class :character   1st Qu.:2.000\n\nMedian :0.00000   Median :0.0000   Median :0.00000   Median :0.0000   Median :0.000   Mode  :character   Median :3.000\n\nMean   :0.04585   Mean   :0.3907   Mean   :0.02951   Mean   :0.2448   Mean   :0.618                      Mean   :2.515\n\n3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.250                      3rd Qu.:3.000\n\nMax.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :2.000                      Max.   :3.000\n\nlistelected     conelected        estier             mm\n\nMin.   :0.00   Min.   :0.000   Min.   :0.0000   Min.   :0.00000\n\n1st Qu.:0.00   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.00000\n\nMedian :0.00   Median :0.000   Median :0.0000   Median :0.00000\n\nMean   :0.25   Mean   :0.118   Mean   :0.4749   Mean   :0.01635\n\n3rd Qu.:0.25   3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:0.00000\n\nMax.   :1.00   Max.   :1.000   Max.   :1.0000   Max.   :1.00000\n\nTo be specific, estier is a binary variable that equals 1 when a candidate is running in the Majoritarian (Single Member District) electoral tier. It equals 0 when a candidate is running in the Proportional Representation electoral tier. Hence, elected equals 1 only when the candidate is elected through their assigned electoral tier, and 0 otherwise.  \n\nEstier is therefore nested across all candidates, designating what tier the candidate is running in, not associated with political party.\n\nLet me know if this helps. I appreciate any assistance with this!\n\nEdit: for some reason, inserting the code block is glitching, sorry the summary output is a bit messy. Try to fix this', 'The coefficient for three way interaction is by itself not enough to compute the marginal difference between election types, you need to account for the lower level terms as well.\n\nRun the full model (including estier) and then compute difference between:\n\n\n    intercept + female + partyIDSPD + female:PartyIDSPD\n\nAnd\n\n\n    intercept + female + estier + partyIDSPD + female:PartyIDSPD + estier:partyIDSPD + female:estier + female:estier:partyIDSPD \n\n\nYou should see the same result as in the original table (hopefully I didnt forget about any term)', "">  I am not interested in the effect of estier itself on electoral chances, only when it is interacted with the other variables.\n\nThat doesn't mean you can exclude it as a lower level variable. Later interactions may not provide a meaningful interpretation of variance if you exclude it."", 'elected is dichotomous?  if so, `lm` is a bad choice of model.\n\nIn regards to `estier`, I understand now the nested nature of it and it is related to electoral tier, which is not included in the model (AFAIK). to have a saturated model were all the possible combinations of female, estier and PartyID have unrestricted means you have to especify a full interaction model: \n\n`lm(elected ~ female*estier*PartyID)`\n\nThis will specify all marginal, second and third order interaction terms. This will allow for any effects that you observed in the data.', 'Yes. I got here in the end. I added an update to my post. All makes sense. Thanks!', ""I did previously consider this option. Here are the results.\n\n`Coefficients:`\r  \n`Estimate Std. Error t value Pr(>|t|)`    \r  \n`(Intercept)                 0.41727    0.03414  12.223  < 2e-16 ***`\r  \n`female                     -0.04227    0.08897  -0.475 0.634782`    \r  \n`estier                     -0.36236    0.04244  -8.539  < 2e-16 ***`\r  \n`PartyIDCDU                 -0.26180    0.04297  -6.093 1.28e-09 ***`\r  \n`PartyIDGREEN               -0.17165    0.04597  -3.734 0.000192 ***`\r  \n`PartyIDSPD                 -0.16418    0.04653  -3.528 0.000426 ***`\r  \n`female:estier               0.04000    0.11320   0.353 0.723876`    \r  \n`female:PartyIDCDU          -0.01642    0.09666  -0.170 0.865106`    \r  \n`female:PartyIDGREEN         0.08170    0.09808   0.833 0.404941`    \r  \n`female:PartyIDSPD           0.11155    0.09991   1.117 0.264306`    \r  \n`estier:PartyIDCDU           0.71792    0.05653  12.701  < 2e-16 ***`\r  \n`estier:PartyIDGREEN         0.16191    0.06159   2.629 0.008620 **` \r  \n`estier:PartyIDSPD           0.56232    0.06079   9.250  < 2e-16 ***`\r  \n`female:estier:PartyIDCDU   -0.12246    0.13105  -0.934 0.350169`    \r  \n`female:estier:PartyIDGREEN -0.06209    0.12918  -0.481 0.630822`    \r  \n`female:estier:PartyIDSPD   -0.23181    0.13095  -1.770 0.076815 .`  \r  \n`---`\r  \n`Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1`\n\nNow, the three-way interactions are neither statistically significant, nor do they make sense with what we should expect. Logically, the three-way interactions should match the expectations I listed. \n\nNote: the reference category for the PartyID categorical variable in the model above is AfD Party (I assume) in case this is relevant. \n\nFinally, you are probably right, a linear model is not the best as I'm dealing with probability of binary outcome elected. I should use a logistic regression. However, this is for a dissertation assignment at undergraduate level, and my supervisor has recommended I use linear regression as I am more proficient in it, less so in logistic regression. Is it simply the case that I must switch to logistic regression to get accurate three-way interactions? Also, do three-way  interactions even work the same in logistic compared to linear regression?"", 'Its not necessarily the case that lm is a bad choice of model for a dichotomous DV. Its perfectly appropriate in a lot of cases.', '>Now, the three-way interactions are neither statistically significant, nor do they make sense with what we should expect. Logically, the three-way interactions should match the expectations I listed.\n\nThis is not correct, make sure you understand what is the meaning and how to interpret terms related to interactions. Hint: make predictions on certain combinations of covariates using your model. You will see that it works.\n\nsingle terms are associated with mean differences. second interaction terms are associated with differences of mean differences, and so on. All in all, when you specify a model with full interactions (when covariates are categorical) you end up with an unrestricted model which allows for different effects at each possible combination of all the categorical variables involved. this could be useful or not depending on the needs of the research, some people just consider this over fitting.\n\n>Also, do three-way interactions even work the same in logistic compared to linear regression?\n\nIn the appropriate scale yes. In lm we are working with mean differences, on a logistic model you end working with odds ratios.', ""I see what you mean now. I ran the predict() command and it resulted in logical outcomes for that model. I see now that the substantive meaning of the three-way interactions rests upon the values of all the constituent sub-interactions.\n\nI guess the confusing bit that remains is that the purpose of my research is to find the effect of estier, all other variables being equal. If I were to report the above three-way interactions, I would have to say that CDU female candidates do worse when estier = 1 compared to estier =0. Is there a way to actually measure my research aim. Sorry if I'm getting confused. If I report a negative coefficient in my paper despite the fact that a positive effect is observed between units where estier =1 and estier = 0 all things being equal, it seems very hard to explain in a substantively meaningful way.\n\nAlso: please see my update to my post above"", '>I guess the confusing bit that remains is that the purpose of my research is to find the effect of estier, all other variables being equal. \n\nOk. all in all what your results are pointing out is that effect of estier depends on the other covariates. note that triple interaction terms are not significant as well as the interaction between gender and party. these results are suggesting that the effect of estier depends only on party and not on gender.\n\nyou can reduce your model, you should try to remove the triple interaction term and maybe the interaction between gender and party since they are not significant.\n\nsomething like this:\n\n`lm(elected ~ female+estier*PartyID)`\n\nthis model should be more compact and should highlight the fact that the only thing determining the effect of estier is PartyID.\n\nfinally, you really must understand what are the meaning of the parameters in your model to make a good presentation and discussion of them. again those parameters are related to mean differences between groups defined by your covariates. The success of your analysis will rely on that.']"
[Q] Figuring out statistical test to use and feasibility of experiment,"I'm an undergrad games design student and doing a project trying to find how different variables affect the relationship between types of game rewards and player experience (an example being players with certain traits might prefer certain reward types).

&#x200B;

First off, I'm creating a test game where I will be able to put the specific reward types I want in (5 reward types in total), for the experiment I'm planning on having the 5 reward types in game and the participants will rate their experience with each on a score from 1-10. These scores on the reward types will be the dependent variables and I'll be looking at how player traits (5 scores for five player traits) effect the reward type scores. I'll also be gathering age and gender to look at how those could effect the scores on the reward types in a different test.

&#x200B;

I'm completely new to statistical analysis so I'm not exactly sure what test to use, I looked at some flowcharts and believe a multiple regression is the correct statistical test for this type of experiment but would like to know if that makes sense.

&#x200B;

If you guys know any good resources for the appropriate test or where to learn more about this sorta thing that would be greatly appreciated, and if you need some more info for the experiment just let me know.",131pa82,AlexDoubleU_,1682682208.0,0,0.5,"['You need repeated measures MANOVA or MANCOVA to analyze  all DVs in one model. You might also analyze each DV separately and run ANOVAs or OLS regressions.', ""I'll try look into these and see how to use them, appreciate it!""]"
[Q] Opinion on whether to trust correlation results when compared to graphical data,"Hi everyone, I asked a question a few days ago, but seems it didn't have the information needed to be clear enough so have put some graphs together. 

Basically I have an issue where a kendall's tau correlation is giving a highly significant (P=0.001) positive correlation for the dataset even though I cannot see any visual correlation on scatter plots for the raw data or ranked data. Basically this is ""in the wild"" (so not carefully measured lab conditions where I can get even results) data of a pollutant exposure on the x axis, compared to the measured health response on the y axis. (Kendall's tau is being used due to the type of data is is.)

Any suggestions? I'm at a loss as to whether to call this significant or overanalysed. Kinda frustrating as this data took a long time to gather but sort of seems like I can't report it either way? Thanks in advance!

Links for images of the raw data and ranked data plots (sorry I don't think I can attach them directly to this post for some reason.)

https://ibb.co/C8YR0Pb

https://ibb.co/jRn6kNd",131i7ko,jaciwriter,1682658364.0,12,0.93,"[""There's a huge stack of zeros in your first image.  Try doing it again without the zeros, it's probably throwing your calculations off.\n\nSame with the second one.  There are some outliers and your graph is too zoomed out.  The stack of zeros on the x axis probably not helping either."", 'You didnt report the actual tau.\n\nI would never judge a p value by visual means.', 'Not sure Id say throwing your calculations off. Id just say dig into whats going on a bit more. \n\nAlso a significant result for anything doesnt necessarily mean its a substantive large relationship. I have datasets with 75m observations where basically everything is significant. Doesnt mean theyre meaningful.', 'I agree that the zeros are probably resulting this to happen and should be removed to get a clearer picture. \n\nBut what if in other experiment where the inflated zeros are valid observations and could not be removed? Are there any tools to report or tackle with this phenomenon?', ""Excellent point. I think the graph is your best tool when looking for correlation and understanding the calculated correlation. With all those observations at x=0, you're going to get a high correlation, but it is meaningless. Use the graph to understand why your data looks like an amorphous cloud. Why are so many points at zero? I agree to remove them and see what happens."", 'Sorry thanks for the reply. The correlation coefficient was', ""You're right.  That was badly worded on my part."", ""There are two methods that I usually use.  I'm an econ guy so these might not apply to your field.\n\n1.  Only use the non zero parts and make inferences that are restricted to those only.  Ex: Only use full time workers and all the results are only valid for full time.\n\n2.  The data is truncated.  Ex: you can only see the wages of people that work.  Use Heckman correction, preferably with a good instrument."", ""Thank you that makes sense. Unfortunately they are results so can't really be removed (zero is a legit result in this sense and it's hard to justify removing them from the dataset), but that does help explain why it is that there is a correlation happening despite it not looking like one on the graph and could be functionally meaningless as a correlation.""]"
[Q] What do I use to find correlation between a nominal and ordinal data,"So I have 1 dummy variable: ""do you have this disorder?"" yes/no 

and I have socioeconomic status split into low, medium, high

I want to find out if the socioeconomic status correlates with whether or not people have this disorder.

Right now I went for the 2 test but I'm not sure thats right nor am I sure if there are more test I could do

I can't do whitney u because my dependent variable isn't ordinal

I can't do logistic regressions because I have high multicollinearity among the independent variables

and I cant do odds ratio since its a 2x3 contingency table",131exyh,I_WANT_PINEAPPLES,1682648534.0,2,0.76,"['What are the other independent variables and how high is the variance inflation factor (VIF) for each? Could you drop a couple of them and find a more parsimonious logistic regression model that fits reasonably well?', 'yes/no can be treated as ordinal\n\n> I can\'t do whitney u because my dependent variable isn\'t ordinal\n\nSure it is. ""Yes"" is further along the ""havingness of the disorder"" scale than ""No"" is.\n\n> I can\'t do logistic regressions because I have high multicollinearity among the independent variables\n\nPlease clarify; you mean the socioeconomic dummies have high multicollinearity? If that was strong enough to make a big difference, none of the analyses you\'re considering would be any good.\n\n> I cant do odds ratio since its a 2x3 contingency table\n\nWhy would that prevent calculating an odds ratio?', 'My recommendation would be to clarify what it is you are asking.\n\nIs the question just about the association between an ordinal variable and a dichotomous variable ?  Or is there some broader question about logistic regression ?\n\nIs one of your variables being treated as the dependent variable ?  If so, which one ? Or are you just looking for the association of two variables, neither one being treated as the dependent variable ?\n\nDo you want to treat the ordinal variable as ordinal ?  Or as a nominal variable ?  (As suggested by your suggestion of a chi-square test of association.)\n\nThere are relatively easy approaches to address any of these situations.  I think some of the confusion stems from not being clear --- to yourself --- about what you are trying to do.', 'It\'s only one more variable: ""smoked during pregnancy"" with yes/sometimes/no\n\nThe VIF is 5.5 for socioeconomic status and 3.6 for smoking\n\nI don\'t know what parsimonious logistic regression models are ', ""Thanks for the help I'll get to work\n\nI was taught that the odds ratio is only for 2x2 tables\n\nEdit: SPSS also gives me an error if I try to do calculate odds ratio on something other than 2x2"", ""To help the student along\n\nA binary variable is inherently ratio scale. Having the disorder versus not having it gives a metric space of 1 and 0. It has order property as noted above. It has equal interval property as there's only one interval. And it has absolute zero property as zero means none of the disorder, and 1 means not having none.\n\n2x3 table seems like a good solution to me. If smoking doesn't add new information, go with a model your client can understand. It will get you more referrals once you're done with school."", 'The question is just about the association\n\nMy dummy variable is the dependent one\n\nI want to treat the ordinal as whatever makes it easier to test the correlation \n\nI have the chi-square test and now I have also made a binary logistic regression which worked out fine', 'Oh a parsimonious model is essentially just a fancy way of saying a model with fewer independent variables. \n\nI think in this case logistic regression still makes the most sense. Those VIF numbers are a little elevated but Ive seen 10 proposed as a rule of thumb regarding when to be concerned about multicollinearity. Ive seen 2.5 and 5 proposed as well, but those are quite conservative thresholds. So I think youre okay! If the smoking status doesnt improve the model much you could also consider taking it out of the model.', '> It\'s only one more variable: ""smoked during pregnancy"" with yes/sometimes/no\n\nYou weren\'t including that in the chi-squared nor the Mann-Whitney, right? If you can leave it out of those other analyses, why are you including it in this one?', 'Thanks for the help! Does the chi squared test I already did make sense with the data I have?', 'That\'s fine.  \n\nThe chi-square test treats *SocioeconomicStatus* as nominal, which is fine.  Most software can also report ""ordinal chi-square"" or ""linear-by-linear test"", which will treat both variables as ordinal (which is fine for the dichotomous variable). \n\nHow the independent variable is treated in logistic regression depends on the software.  I would try to determine how your software is treating it.  Usually it depends on how it is coded.  It might be treated as numeric or as nominal.\n\nNote that what you are doing is a hypothesis test about the association of the two variables.  Not the degree of association.  (Analogous to the distinction between a correlation test and the value of *r* in Pearson correlation).', ""Aight I'll do that"", ""No I have it included there as well I just didn't mention it"", 'I think so, but your top post mentions some other predictors, too.\n\nThe chi-square test probably aligns with one of the hypotheses for the research project. It will take a short paragraph, not much to your word count. Anyone who reads it will say ""makes sense.""\n\nFollow on with analyses for the other hypotheses/predictors.', ""I'm using SPSS and I also got the Odds Ratio from ExpB in my logistic regression, that would be the degree of association right?"", ""Yea it's just a small part, thank you"", 'Yes, odds ratio could be considered a degree of association, or effect size statistic.']"
[E] Design Effect,"Hello all,

I am a long time mathematician / engineer but my statistics are somewhat weak. I recently was introduced to ""Design Effect"". Can anyone assist in suggesting (1) Books (2) Papers and/or (3) Videos to educate myself on the subject? Theory or practical applications are both great. Any help would be greatly appreciated. Thanks in advance.",131d9na,TheOneThatWeCallKurt,1682643920.0,1,1.0,"['I\'m not sure what you mean by just the phrase ""design effect"". Do you have some context?', ""Sampling: Design and Analysis by Lohr contains a short introduction. IIRC Lumley's Complex Surveys also discusses it."", 'Barely, but sure. I have serious stats guys who have never heard of it. Here are a couple links, Wikipedia is decent here I think.\n\n[https://en.wikipedia.org/wiki/Design\\_effect](https://en.wikipedia.org/wiki/Design_effect)\n\n[https://www.statisticshowto.com/design-effect/](https://www.statisticshowto.com/design-effect/)\n\n[https://www.sciencedirect.com/topics/mathematics/design-effect](https://www.sciencedirect.com/topics/mathematics/design-effect)']"
Modelling the revenue of a startup using a simple Patreo Distribution [Q],"Disclaimer

&#x200B;

\*\*It is generally not safe to assume that revenues for businesses in most industries can be modeled by the Pareto distribution. While the Pareto distribution is commonly used to model the distribution of income and wealth in a population, it is not a universal distribution that can be applied to all types of data.\*\*

&#x200B;

I am a mechanical engineer by trade, and it's been a while since taken a formal stats class, but I thought I'd give this a go!

I want to have a super rough ball park idea about my market penetration should I start a business that begins to perform in the bottom 25% or bottom 40% of this industry in terms of revenue.

&#x200B;

The industry:

&#x200B;

Total Revenue: $444.6 Million

&#x200B;

Number of businesses in my state: 343

&#x200B;

Average revenue: $ 1.296 Million

&#x200B;

Equations:

&#x200B;

f(x; ) = ( / x)\^( + 1)

&#x200B;

F(x; ) = 1 - (x / x\_min)\^-

&#x200B;

&#x200B;

&#x200B;

x\_min = $1,294,000

&#x200B;

x\_sum = $445,000,000

&#x200B;

n = 343

&#x200B;

 = 343 \* ($1,294,000 / $445,000,000)  1.00

&#x200B;

&#x200B;

&#x200B;

Distribution Function

&#x200B;

f(x; 1.00) = (1.00 / x)\^2

&#x200B;

&#x200B;

&#x200B;

Cumulative Probability Function

&#x200B;

F(x; 1.00) = 1 - ($1,294,000 / x)\^-1.00

&#x200B;

&#x200B;

&#x200B;

However,

&#x200B;

I feel like this is wrong because when I plot the x values for revenue, the probability function approaches zero and goes negative after x=$1.294M which intuitively does not seem correct.

There are surely businesses making more than $1M in this industry so my best guess would be that I need a more robust equation to model the maximum values of this distribution, but I am not sure.

What would be a better way to model this?

&#x200B;

Thank you!!",1314xeo,carn_hell,1682626279.0,9,0.77,"[""What you want to use for such things is a generalized pareto distribution rather than a simple pareto distribution. You can also try using the 2 parameter versions of pareto type II distribution (Lomax distribution).\n\nI think Lomax would be the most suitable and robust way to do it. It gives you a scaling parameter on top of the shape parameter (maximum you're using right now). This allows for you to have greater maxima and even lower minima."", ""Your computed F is wrong, that's why it doesn't behave correctly. \n\nIt doesn't have the same form as the general form of the Pareto F you gave above it."", 'Awesome! I revised the formula with the type 2 distribution and got $235,000 in revenue for the bottom 35 percentile. I used k as the shape parameter and made the maximum revenue no more than 10 times the average k= 1/(1-(1/M)) where M=10 thus k=1.11. \n\nThanks so much for the advice and guiding me in the right direction.\n\nThis is not a crazy accurate estimation I would assume but it gives me an idea of what to expect in general if I were to start this business or any other business.', ""I'm glad it worked for you. You can try getting some real world data and adjust max and average accordingly based on line of business for even better approximations.\n\nGood luck on your new business ventures!""]"
[Q]Converting data from non-normal to normal distribution,"Hey guys
  
I have data i want to convert it from non normal to normal distribution so i can apply control charts on the data
  
The data is so fucked up , i try every possible way to convert it and it didn't convert.
  
So I tried one last try on a method but I'm not sure about it , the method is by ranking data ascending at first , then apply this formula ( (R-1)/(N-1) ) Where R is the rank number and N is the number of data i have, finally we apply the z score by using the value calculated from the last equation.
  
The big question mark about it is If there are several measurements that you want to convert from the same product, the conversion results for both measurements will be the same, is that even okay? (I hope I explained it in the best way so that you can understand me).
  
Any solution or suggestion will help me.",130v794,aldroobi,1682613636.0,7,0.74,"['OP should post a histogram of said ""fucked up"" data.', ""I don't see how rank transformation will help you actually monitor actual control the process. Indeed I'm not at all convinced that any transformation will be much use for that.\n\nWhat do you mean by:\n\n> data is so fucked up\n\nexactly"", ""What is the current (non normal) distribution of your data? From what you are telling us it doesn't even look remotely close. \n\nJust to be clear, the fact that your response variable isn't centered and somewhat symmetric is kind of a good indication that you are NOT in presence of a stationnary (in control) process. \n\nMaybe the control charts (in the classical sense) aren't the best solution here. Can you find the source of these erratic variations?"", 'Perhaps not the most robust or statistically valid approach, but for non-normally distributed parameters, our procedure instructs us to fit a smooth curve distribution (we use JMP to do so) and establish control limits as certain percentile quantiles of that distribution. We used to use ARIMA for autocorrelated parameters but have moved away from that. \n\nTo be fair, those of us calculating control limits are not statisticians by training and our manufacturing data is rarely normally distributed. So the statistics group have moved to make it very easy for us, probably at the expense of statistical validity.', 'There are control charts you can use for non normal data.', ""One way if you can try to come up with sums of random data's you might trigger central limit theorem and data might converge to normal.\n\nTransformation of one distribution to another distribution might be meaningless and misleading.\n\nUse R or Matlab to fit the dist with Normal and notice the RMSE or IMSE to determine the goodness of fit."", 'https://www.spss-tutorials.com/normalizing-variable-transformations/', ""I mean by that the data is not linear like the p value is >.005 \nAnd when we try to make it linear it's still the same"", 'Actully im using minitab and tried Box-cox and johnson transformation (it succeed like twice from 10 trials)', 'Okay after i know from which distribution the data is , what should i do ?', ""You mean you did a QQ plot and some normality test? Forget the test, it's a waste of time.\n\nWhat are you measuring?""]"
[Q] What would be the ideal way to represent how much someone has progressed with their workout and their strength using percentages?," I  don't know if I am in the correct subreddit for this but I have a  question about how to calculate how much someone has improved their  strength over a period of a year.

I  am making a personal project website where users can input their  strongest lifts for a particular exercise and at the end get stats on  their strength increase in %.

So I  was wondering how do I calculate the strength increase? My current  process is that I get the first lift of the user and I grab the last  lift the user has made. I ignore the strongest lift because for example,  it could go like this:

January: 10kg

August: 75kg

October: 50kg

December: 60kg

And  in the above example, while the strongest lift is 75kg, the user can no  longer lift that weight therefore their progress has dropped. But if I  use the strongest and weakest lift to calculate the progress in %, then  it would be like the user has not experienced any loss of strength.

Here is my formula for how I get the %

    improvement = ((max - min) / min) * 100 

Sorry if this is the wrong sub for this.",130qpr0,adorkablegiant,1682608480.0,2,1.0,"['Are they only lifting one weight per month? Or do the months represent the strongest weight lifted that month?', 'It is the strongest weight lifted per each month. \n\nI am planning on including weekly and daily statistics later on but for now I am working with the users PB for each month in the span of a year.', 'You may want to use a moving average to start, try taking an average of last two months and you wont have to eliminate your maximum values. In your example the averages would be 42.5kg, 62.5kg, and 55kg. This may or may not help, but I hope Ive assisted in getting you closer :)']"
[Q] time intervals in regressions,"Hey gus,

must all variables have the time interval within a model? I have calculated the rolling 30 day volatility for Bitcoin, Gold and the US-Dollar. I want to include them to a model for the bitcoin volatiltiy but I also want to include other variables which may not be in dayli frequency like money supply or main refinancing interest rate. Model would look similar to this

    btc_vola = b0 + b1 * gold_vola + b2 * dollar_vola + b3 * money_supply + .... + bn * interest + ui 

So how do I handle different frequencies. Thanks guys. :)",130l7vb,LiberFriso,1682599983.0,4,1.0,"[""Well you can't have NAs for your money supply variable, so you'll have to do something to fill in the blanks. You could use last observation carried forward or some kind of smoothing technique like an average between the two most recent observations.\n\nAlternatively, you could rollup the daily variables to whatever frequency you have for money supply. That's what I would do."", 'Specific recommendations are going to on specific details of each variable you are interested in. What time periods are they measured on, are they co-occurent with other frequencies of variables you are interested in, are they measure me in the same frequencies for each of the time series, etc.\n\nThe measurement properties of each as well as your goals predictive and inferential goals will matter as well. Are you trying to predict future volatility, or do you want to assess impact of policies on volatility. For the measurement properties, are the variables measured at specific times or aggregated over time? Are they controlled and change at discrete times, or are they reported at the end of measurement periods?\n\n\nAppropriate modeling strategies are going to depend on answers to each of these questions for each variable separately.', 'As I understand it if I do a simple multivariate-linear OLS model I am looking at the impact of independent variables on the dependent variable. So the question I want to answer is what drives the volatility.\n\nIf I want to predict the volatility which models are used here?']"
[Q] Interrater-reliability for binary variable?,"Hey,

&#x200B;

I wanted to calculate cohen's kappa for binary variables (0= no, behavior is not shown, 1= yes behavior is shown). The issue is that for one variable, one rater always stated ""0"", which makes the variable a constant. Thus, I cannot calculate cohen's kappa, SPSS at least tell me so. I mean it's understandable given that there is no variation. What could I do instead?

Thanks!",130ena3,skippydi34,1682585533.0,7,1.0,"['I think you need to back up a step. Statistics is for making sense of things in the presence of variability. These data don\'t have that.\n\nMore concerning, it means your judges disagree on 100% of the ""Present"" observations and presumably some fraction of the ""absent"" observations. Even after getting a coefficient, how do you interpret it?', '[Here](https://stats.stackexchange.com/questions/523605/unable-to-calculate-kappa-or-weighted-kappa-when-1-raters-give-the-same-rating) and [here](https://stats.stackexchange.com/questions/29717/inter-rater-statistic-for-skewed-rankings) are some posts about your problem (frequently called the ""prevalence problem"").\n\nIn short, I recommend using Gwet\'s AC1 instead instead. This is not available in SPSS but very easily calculated in R using the `irrCAC` package.', 'I recommend checking out rogers tanimoto dissimilarity with the weights arranged according to the variance of all population per stimulus.', 'At the end of the day, all agreement statistics depend on variability between and within raters. If you have any rater which does not have any variability in their own scores (constant scores), then they effectively provide no information and this can cause estimation problems.', 'What is the application of your metric? How would you have used the kappa statistic? Would it lead to a particular policy, eg?', 'No they do not disagree on 100% percent of the observations. \n\n&#x200B;\n\n  \nAssume ""shows behavior"" is coded with 1 and ""does not show behavior"" is coded with 0. \n\nLet\'s also assume the both raters saw 20 individuals doing something, and during this process they should code whether they see the respective, specific behavior. \n\nOf all 20 individuals, rater 1 saw the behavior with one individual. Rater 2 never saw the behavior for any individual. Thus, they actually highly agree, but ratings of rater 2 are a constant. Overall, 0 is much more prevalent than 1. \n\nFor my research, this is not an issue. The scenario described above is fictional for easier understanding.\n\nIf I still get something wrong feel absolutely free to correct me...', ""Thank you so much, I could easily implement it in R. I struggle a little bit with the explanation. I study psychology so nobody is expecting that I will understand it to the fullest. However, I should at least understand the argumentation of people who know more than me and cite them (e.g. [https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-61](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-61))\n\nSo, is it impossible to calculate Cohens kappa in my case or is it an SPSS thing? How can I justify taking Gwet instead of cohen in my case? As far as I understand it, in my case, due to the high prevalence of one outcome, kohen's cappa is problematic (Kappa paradox). One advantage of Gwet is that it's more robust when change in prevalence happens."", ""are the ranges for interpretation the same? so far, I've found that both have a full range of \\[-1,1\\]"", 'It was about a rather seldom behavior and we went trough statements from participants if the thing they report is the behavior of interest. So two raters went trough these statements and I wanted to report the agreement between these two. They may later found a consensus trough discussion (as it is often the case when it comes to qualitative data) but I wanted to report initial agreement so the reader has a idea of how similar the ratings were in the first place', 'I said they disagree on all of the ""present"" obs, which you\'re calling 1. \nYour situation is one where Kappa is not appropriate. With a Bernoulli variable with a low probability of success, say p=.05, a completely ignorant judge who always returns fail, x=0, will have agreement with a perfect judge 95% of the time. \n\nIf you had a smoke detector that never went off, everyday for 10 years, would you consider it 99.9% accurate if it didn\'t alarm on the one day the building burned down?', ""No, you can't calculate Cohen's Kappa in your case, regardless of software. The paper you've linked is a good justification for using Gwet's AC1 instead of Cohen's Kappa. See also the [paper](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1348/000711006X126600?casa_token=fJruiNPy6yEAAAAA%3AuJ4Y3oWkdbSt5MezoPDrRggDRosW18OlkrJeZnXyNBCabnMNUTqYHeeDSGUsyXFDdJbctChQD45ht9c) by Gwet, which offers further arguments for the use of AC1. Generally, Cohen's Kappa is contraindicated due to the paradoxes associated with it."", 'Okay I kinda get it. In my case, we had statements of participants and wanted to classify participants based on their statements. This was done by two raters and I wanted to report their initial agreement for transparency.']"
[Q] Probability of Blocks,"There are 4 red, 4 blue & 4 white blocks that are placed randomly in 3 boxes of 4 blocks each. The probability that at least 2 of the boxes receive identical collection of blocks is x/y where x and y are relatively prime positive integers. Find x+y",130du61,Odd_Average_992,1682582774.0,0,0.17,"['We can start by counting the total number of ways the blocks can be distributed among the boxes. Since there are 12 blocks and 4 in each box, there are a total of (12 choose 4) \\* (8 choose 4) = 495 \\* 70 = 34650 ways to distribute the blocks.  \nNow, we need to count the number of ways in which no box has an identical collection of blocks as another box. We can do this by considering the number of ways to distribute the blocks such that no two boxes have the same collection of blocks.  \nThe first box can have any of the 34650 combinations of blocks. The second box can have any of the remaining 8 combinations that do not match the first box. Finally, the third box can have any of the remaining 4 combinations that do not match either of the first two boxes. Thus, there are a total of 34650 \\* 8 \\* 4 = 1108800 ways to distribute the blocks such that no two boxes have the same collection of blocks.  \nTherefore, the probability that at least 2 of the boxes receive identical collection of blocks is 1 - (1108800/34650) = 31/165.  \nThe sum of x+y is 31+165=196. Hence, the answer is 196.', 'Thanks a lottt']"
[R]Facing the Unknown Unknowns of Data Analysis,"[https://journals.sagepub.com/doi/full/10.1177/09637214231168565](https://journals.sagepub.com/doi/full/10.1177/09637214231168565)

Abstract

Empirical claims are inevitably associated with uncertainty, and a major goal of data analysis is therefore to quantify that uncertainty. Recent work has revealed that most uncertainty may lie not in what is usually reported (e.g., p value, confidence interval, or Bayes factor) but in what is left unreported (e.g., how the experiment was designed, whether the conclusion is robust under plausible alternative analysis protocols, and how credible the authors believe their hypothesis to be). This suggests that the rigorous evaluation of an empirical claim involves an assessment of the entire empirical cycle and that scientific progress benefits from radical transparency in planning, data management, inference, and reporting. We summarize recent methodological developments in this area and conclude that the focus on a single statistical analysis is myopic. Sound statistical analysis is important, but social scientists may gain more insight by taking a broad view on uncertainty and by working to reduce the unknown unknowns that still plague reporting practice.",130craf,m-heidegger,1682579078.0,26,1.0,"[""This is what I love about modeling for financial markets (specifically options). The immediate feedback lets you know very quickly whether your model is good or not. The quality and speed of the fresh out-of-sample data is wonderful for avoiding the absolute tar pit of trying to figure out if your model (or someone else's model) is valid."", 'Physicists have known this forever.  They try to quantify both random error (for which statistics helps) and systematic error (for which statistics is no help, you have to use theory).  So these social scientists are trying to get a clue about something physicists have been doing for centuries.  Maybe they should look at how the physicists do it (although, of course, there will have to be changes to work in social science).', 'This isnt new even in the context of social sciences, e.g. Gelman has been going on about garden of forking paths for like ten years by now. Most people just dont care.', 'No. ""Garden of forking paths"" is data snooping, which statisticians were woofing about before Gelman was born.  Systematic error is completely different, something statisticians (including Gelman) have nothing to say about.  Systematic error is the error that statistics says nothing about.']"
[Q] Regression variable interpretation (Excel),"I know the rules say no homework, but this is more of a comprehension issue. Not looking for answers to homework but comprehension on regression.

I am doing a university paper. I am comparing the satisfaction of two types of persons in excel (required). A) people who use banks and b) those who use credit unions. I have been directed to use regression and therefore using the number (1) for banks and (2) for credit unions. However I am unable to discover which population is responsible for any of my results. Should I sort them, even if I am directed to put them in the same column?",130bq89,RevolutionaryCut1159,1682575726.0,1,1.0,"['I would start by computing the means of both groups since that will tell you the direction of the effect. Regression is fine but a t-test is simpler (and identical). Moreover, if you have heterogeneous variance in the t-test analysis you can use the Welch test but the solution with heteroscedasticity in regression is more complex.', 'You should be using a dummy variable, not coding them as 1 and 2. Look at your notes again.', 'In fact, it does not matter. Coding 0-1 or 1-2 are both fine as long as there is a single unit of difference. The only thing it will change is the interpretation of the intercept, as it will not be interpetable with the 1-2 coding. The same stands for interactions and conditional simple effects.', 'Thank you for your response, I did that and got the same results, I believe 1 and 2 are dummy variables in the regression.  Thanks for your help anyway']"
[D] Handling Drastic One-Time Increase/Decrease In Sales Data,,130blwu,boredmonki,1682575339.0,6,1.0,"['It depends on why it happened and what you are using the data for. There isnt a single answer to that. You might treat 1,1,1,1,5,5,5,5 differently than 1,1,1,1,5,1,1,1.']"
[Q] Universal Significance of the Coefficients in the Simple Linear Regression,"  
Hello everyone. I have somewhat of a silly question about when we can actually say that our independent variables are significant. 

&#x200B;

I want to study whether the effect of Western financial aid on GDP growth is significant in the Gambia. I ran a regression with GDP as the dependent variable, Net financial aid received as the independent, and a couple of other independent variables for control. Suppose I get an insignificant p-value for the Net financial aid.

&#x200B;

My question is  can we truly say that financial aid is irrelevant to growth? I feel that it is a very simple model, and we cannot make such strong statements by doing a simple linear regression. There are hundreds of papers written studying the effects of financial aid on developing countries. If linear regression was sufficient, there would not be so much controversy over the topic. 

&#x200B;

I know about natural experiments in economics. I understand that comparing similar regions but that differ in a particular effect can make our results more rigorous and say confidently if the effect is significant. But then, when would having a linear regression be enough to say universally whether the effect is significant or insignificant?",130904r,srufach,1682567493.0,10,0.92,"[""1. Linear regression can only measure linear relationships (or the absence of such relationships). That is very far from declaring overall independence.\n\n2. Failing to reject the null (i.e. insignificant p-value) does not mean that the null is true. It simply means we cannot definitely determine that the alternative is true. This could mean that the null is true, or that our test is under-powered, or that our test is poorly specified (i.e. assumptions aren't met), or simply that we got unlucky with a non-representative sample (which happens a nonzero amount, to put it simply)."", 'Youre dealing with time series data (often at the year) thats strategic (aid is explicitly designed to increase gdp). Aid isnt randomly distributed.', '> with GDP as the dependent variable,\n\nGDP will be non-stationary.', '> Linear regression can only measure linear relationships (or the absence of such relationships).\n\nCould you explain what you mean by that? Because as written, it seems misleading to me. You absolutely can model nonlinear relationships using linear regression by using polynomials, splines or fractional polynomials, just to name a few. The ""linear"" in linear regression pertains to the linearity in parameters. I found [this](https://stats.stackexchange.com/questions/148638/how-to-tell-the-difference-between-linear-and-non-linear-regression-models/148713#148713) a great summary of the different meanings of ""linear"" in the context of linear regression.', 'Differencing will likely make it stationary.', 'Maybe to understand it best, you just need to back up to the first assumption of any statistical test: ""My model is correct.""\n\nIf you apply an incorrect model, all bets are off for the parameters.', 'Yes, the regression is linear in its parameters. This means that it can only tell you about the linear relationship between its inputs & output, allowing that those inputs can be nonlinear transformations of other inputs. But once the regression is constructed, it\'s only describing the linear relationship of those specific inputs that you gave it. In the context of OPs question, you can\'t look at a regression and conclude ""no general relationship exists here between these variables""; you can only say at most that ""there is no linear pattern from these inputs to the output"". You are of course free to apply nonlinear transformations and reattempt.', ""Not OP, but yes linear regression models linear sums of the parameters provided. You won't detect multiplicative effects between parameters, for example. If you want to find things like splines or power associations of your original variables etc, you need to explicitly transform your original data to create those parameters (or use other approaches like model selection to spit out transforms and find the best fit) - the model won't magic them up for you.""]"
[E] Need help on Adjusted R-squared decreasing,"Hi peepos, so I am working on my assignment which requires us to conduct regressions to identify independent variables affecting the dependent variables (exchange rate). I opt to use backward elimination with a confidence level of 95%, but there was a problem: After I eliminate the highest p-value variable (the largest p-value and also larger than the threshold of 0.05), the regression after that has a lower adjusted R-squared.   
I know that adjusted R-squared decreasing is kind of a signal that the accuracy of the regression is low, but I don't understand why. Can anyone explain it to me? Thanks!",1307oxy,noob_silverbot,1682563764.0,4,1.0,"[""> I know that adjusted R-squared decreasing is kind of a signal that the accuracy of the regression is low\n\nI don't think that's necessarily true at all."", 'When you get rid of a covariate, the R-squared will almost always decrease, even if that covariate is not significant. This is because the information provided by that covariate improved the fit to the training data (even if the improvement was only marginal).\n\nThe accuracy of the regression model, measured by R-squared, is often subjective. In some fields, an R-squared of 0.3 is considered great, while for other problems an R-squared of 0.8 is terrible. Hopefully this isnt the only metric of model fit that you are using.']"
[Q] what data do I need in order to set up a linear regression analysis on minitab?,"I'm not sure whether this question is very simple, or more complicated than I understand. For my statistics class I need to input data on minitab and do a linear regression for it. However, my issue is that I don't understand what data I need to use. I would like to do something in regards to level of education and salary earned, but I just don't know what data I need. I'm sorry if this question is either overly complicated or overly simple",13008os,Crican96,1682544508.0,1,1.0,"['One column of data with eduction level (n years?) and another with salary ($ per year)?\n\nThis looks like they have data\nhttps://www.kaggle.com/datasets/codebreaker619/salary-data-with-age-and-experience', '> what data do I need in order to set up a linear regression analysis on minitab?\n\nAt least two columns of data (pairs of values).\n\n> I would like to do something in regards to level of education and salary earned\n\nThen you would need to think about how you\'re measuring level of education and salary, and collect pairs of values (one of each number) from some set of subjects. That is, you need to collect both numbers from every subject.\n\nI\'d suggest using *years of education* instead, as it\'s more suited to a simple regression. ""Level of education"" is at best ordinal which will complicate things somewhat and complication is not something you should seek at this stage of understanding.\n\n(You might also consider whether the relationship might be nonlinear.)\n\nAlso consider that your subjects might consider this information rather personal, and may be disinclined to answer one or the other question (and may be less inclined for particular values than others; e.g. someone may not want to admit a low level of education). This non-random missingness is going to *bias your estimates*.\n\n(You would also need to worry about whether you\'re really getting some random sample from some distribution of interest, and if you\'re not, what the basis for inference would be, if you\'re doing any tests or confidence intervals.)', 'Thank you so much! I think I realized with the answers, but all I needed was a way to quantify education level. I didnt even think of just using number of years, I was thinking of the type of degree earned. Thank you so much again!', 'This makes perfect sense and explained everything! I didnt realize the only thing I really needed was a way to quantify the level of education with a number that I could calculate with. This explanation is perfect, thank you so much!', 'No worries!']"
[Q] Hello I have a problem where I have to determine the sample size with some given info.,"Hello, Iam pretty new to statistics and I have a problem. I have a mean of 297.5645, and I know that the width of the interval is maximum 5 and that the standard deviation squared is 200. Iam asked for a sample size needed to have the parametrs of the maximum width and the deviation. I am supposed to run a two-tail test with 99% confidence. I have found a formula that incorporates the width and deviation and got back 213 when I devided the width by 2 since it is a two-tailed test. Have I performed this correctly or does something change due to me knowing the mean? Tank you very much.",12zyktv,Routine-Space-4878,1682540576.0,0,0.25,[]
[Q] A priori G*power analysis for Mixed Anova Design,"For my research class we have to conduct a study. 
Our basic design: 
Two groups (treatment and control group), both have to fill out a questionnaire=pretest (t01), then the treatment group does their treatment/control group does nothing for a week, after that period of time both have to fill out the same questionnaire=posttest(t02). We assume that the treatment has some effect that lead to higher test scores in t02. 

To determine the sizes of both groups I need a priori power analysis, but Im a bit lost when it comes to ANOVAS and what those input parameters even mean. 

I assume its a mixed Anova design. 

I know our hypothesis is one tailed, alpha level at 0,05 and power at 0,8. Possible effect at may be 0,3 (middle sized). 

Can someone explain the other parameters to me or guide me to any help?",12zvicm,MissMags1234,1682536356.0,1,1.0,"['Boring Bayesian answer, just do a large number of simulations of the experiment at different sizes, either assuming one true effect size or a prior distribution of potential effect sizes and record the operating characteristics. Plot the relationship between size and operating characteristics and choose the smallest size that gets you acceptable results and acceptable number of times.\n\nIm assuming someone will come along with a better, closed form solution.', 'Your analysis is equal to an independent t test on posttest - pre test scores. It might be easier to view it that way for your power analysis. You could also do ANCOVA with pretest as the covariate but that might be a little too complicated for your research class.', 'This guide is really helpful: https://scholar.google.de/scholar_url?url=https://files.osf.io/v1/resources/pcfvj/providers/osfstorage/5dcea84d5b97bd000e57aba0%3Faction%3Ddownload%26version%3D1%26direct&hl=de&sa=X&ei=R7FKZJznBLeNy9YP4P6AwAg&scisig=AJ9-iYsL05m8bSyEtZ-YIMS2fnpC&oi=scholarr  \n\nAnd here is the app they recommend for simulation: https://shiny.ieis.tue.nl/anova_power/ \nThey recommend that on the basis that G*Power underestimates sample sizes for interactions (i.e. within-between interaction, or the interaction between group and time). Does your course require you to use G*Power? If so, regarding the other parameters I would probably leave non-sphericity correction at 1 (meaning not corrected for sphericity) and put in 2 groups, 2 measurements.', 'Chapter 13 of Doing Bayesian data Analysis is good if you do want to go this route.']"
[Q] Two-Way ANOVA results make no logical sense?,"I have a data set that is a time series with very different means. i.e. we have a drug concentration measured at different timepoints. The two-Way ANOVA claims only differences between drug concentrations at the last timepoint are significant, but the results look something like this:

[https://imgur.com/NGTrfFg](https://imgur.com/NGTrfFg)

Is there a more appropriate test for this than two-way ANOVA? I just cannot see how 100-fold difference with that small error bars can be not significant in the second group of columns.",12zumtl,Popular_Chemist_1247,1682535448.0,1,1.0,"['An ANOVA is not really just a set of pairwise comparisons. A two way ANOVA: drug(2) x time (3) with repeated measures on the latter factor is the most common way to analyze this design. Incidentally, repeated measures ANOVA is a special case of a mixed model because subjects is a random effect whereas the others are fixed. Repeated-measures designs are designed to allow the within-subject levels to be correlated.', 'If I understood correctly, u are interested in how the drug concentration changes over time in serum vs normal culture? \n\nIf you have repeated measures, you cannot use two-way anova cause your drug concentration measurements have to be independent.', ""Yes, but isn't it surprising that this effect size doesn't get significance? How come the middle two bars are not significantly higher than the leftwise two?"", 'There is also a distinction between repeated measures ANOVA and mixed ANOVA; conventional repeated measures ANOVA has no between-subject effects, so you cant use it to analyze two independent groups like here with serum 1 vs 2.', ""That is correct. The concentrations  are not independent - the drug concentrations will go up over time. What's the alternative test when I have two variables - time which is related to drug concentrations (Data 1, 2, 3 - timepoints) and status - serum starved vs non-starved?"", 'You would need to know what error terms were used.', 'This might be a matter of definition but in conventional usage, ANOVA can have any combination of between- and within-subject variables and any design with one or more within-subject variables is called a repeated-measures ANOVA. See [this page for details](https://onlinestatbook.com/2/analysis_of_variance/within-subjects.html). Technically, a one-way repeated-measures design is a mixed design because subjects is a random effect and trials is a fixed effect. Mixed does not mean a mixture of between- and within-subject variables but a combination of random and fixed effects.', 'Id recommend a linear mixed model; are u using R?', 'Thanks!\n\nI am using Prism, but I can figure out R if needed. If you can point me to some good reading it would help...', 'Yeah, in a linear mixed model you would basically model the trajectory of the drug concentration over time, and check whether this is different per serum status. The fixed effects are time and serum status, and the random part could be a random intercept for each culture? (Basically where the drug measurements are coming from). If u expect each culture to have different drug trajectories over time, u can also add random slopes for time.\n\nAka each culture has their own drug concentration starting point, and their own slope/trajectory of drug concentration over time. You can also plot the drug concentration over time ( I can help u later if u want)\n\nIn R, you can use the lmer function from the lme4 package. The code would look like: lmer(concentration~time+serum + (1|culture), data = data) or with random slope: lmer(concentration~time+serum + (1+time|culture), data = data) \n\nThe effect estimate (beta) of serum is the mean difference in average drug concentration over time of serum x vs reference serum.', 'And the more u add, let it be a random slope, or modelling time differently than linear makes the model more complex. And ur sample size really matters for this', 'https://rstudio-pubs-static.s3.amazonaws.com/63556_e35cc7e2dfb54a5bb551f3fa4b3ec4ae.html']"
[Q] Finding a paper/study that used a parametric test when it should have used a non parametric," I'm looking for something to illustrate the title of this post, and I can find many papers with mistakes and errors however finding something specifically that incorrectly used a parametric test when it should have used a non parametric has been much more difficult.

Have you guys found anything similar to this before?",12zstal,icybullet,1682533700.0,2,1.0,"['How would you determine when a parametric test should not be used?', ""In practice, all or almost all statistical models are wrong,  in the sense that they're not exactly true. So any model, parametric or nonparametric is an approximation. \n\nThere may be instances where the inaccuracy of parametric assumptions were leading to an  inaccurate significance level (not the alpha you wanted, which also impacts  p values) or to low power. In this case a different set of assumptions would have been more suitable but that doesn't automatically imply that they should have been nonparametric.\n\nI wonder if you're conflating parametric with 'assumes normality' there."", '[deleted]', 'That wont tell you.', 'How would that distinguish between the need for a different parametric assumption and no parametric  assumption?', '[deleted]', 'You seem to be following the OP\'s error in equating ""parametric"" with ""normal"" and that hence ""non-normal"" implies that one must not make any parametric assumption. As a result it looks like we\'re talking entirely at cross purposes.']"
"[D] Bonferroni corrections/adjustments. A must have statistical method or at best, unnecessary and, at worst, deleterious to sound statistical inference?","I wanted to start a discussion about what people here think about the use of Bonferroni corrections.

Looking to the literature. Perneger, (1998) provides part of the title with his statement that ""Bonferroni adjustments are, at best, unnecessary and, at worst, deleterious to sound statistical inference.""

A more balanced opinion comes from Rothman (1990) who states that ""A policy of not making adjustments for multiple comparisons is preferable because it will lead to fewer errors of interpretation when the data under evaluation are not random numbers but actual observations on nature."" aka sure mathematically Bonferroni corrections make sense but that does not apply to the real world. 

Armstrong (2014) looked at the use of Bonferroni corrections in Ophthalmic and Physiological Optics ( I know these are not true statisticians don't kill me. Give me better literature) but he found in this field most people don't use Bonferroni corrections critically and basically just use it because that's the thing that you do. Therefore they don't account for the increased risk of type 2 errors. Even when it was used critically, some authors looked at both the corrected and non corrected results which just complicated the interpretation of results. He states that when doing an exploratory study it is unwise to use Bonferroni corrections because of that increased risk of type 2 errors. 


So what do y'all think? Should you avoid using Bonferroni corrections because they are so conservative and increase type 2 errors or is it vital that you use them in every single analysis with more than two T-tests in it because of the risk of type 1 errors? 


-----------

Perneger, T. V. (1998). What's wrong with Bonferroni adjustments. Bmj, 316(7139), 1236-1238.

Rothman, K. J. (1990). No adjustments are needed for multiple comparisons. Epidemiology, 43-46.

Armstrong, R. A. (2014). When to use the B onferroni correction. Ophthalmic and Physiological Optics, 34(5), 502-508.",12zs90h,Bittersweetcharlatan,1682532599.0,45,0.92,"['A simple reason why the Bonferroni method should never be used is that the Holm-Bonferroni method is uniformly more powerful.\n\nThe question whether to use multiplicity adjustments at all or when to use them is a different question that has nothing to do with any specific method.', 'https://xkcd.com/882/', ""There are lots of alternatives to Bonferroni -- Bonferroni's main merit is being very simple, easy to calculate and easy to explain to non-technical people.\n\nBut doing *something* that lets you control the overall error rate of a batch of tests seems essential. There are plenty of people who don't like hypothesis tests at all; but I think there are very few people who believe in hypothesis testing, but don't believe in adjusting for multiple comparisons.\n\n(In my own work I have most often used Scheffe, rather than Bonferroni - with the idea of covering every possible combination of groups in one shot, rather than asking which particular combinations you want to test.)"", '""Sure mathematically, but does it make sense in the real world?""  and\n""Doesn\'t account for type 2 errors""...\n\nWhy bother with statistical inference at all?\n\nThe point is that since Neyman and Pearson, our attempt at discerning signal from noise is to choose statistical procedures to optimize power at a given threshold for science...e.g. alpha. All of this is designed for one test. If we do multiple tests, we have to do something about our method of protected inference (control type I errors in some way)  while doing something about power.\n\nBonferroni or Bonferrni-Holm protects the Family Wise error rate, e.g., prob that there is one of more false positives. Under independent test statistics, this probability is exactly alpha, otherwise its bounded by alpha. What about power? Average power, e.g., the one test power function at alpha/m, is the expected proportion of true signal tests  declared significant, e.g., true positive rate. So these two concepts, one type I-ish and the other, type II-ish are but one set of choices. There are many more. For multiple test power in a given circumstance we may wish to fix the probability that we declare a given portion or better  of the true signals significant, so called TPX power, and for controlled inference we may be happy to instead control the expected false positive rate (Benjamini Hochberg procedure), or we may prefer to control the probability that the false positive proportion exceeds a given value is kept small, so called FDX control (Lehman Romano procedure, or the procedure in my forthcoming publication). It\'s just foolish not to do anything. Anyone who says otherwise is praying that enough false positives equal tenure.  Have a look at my R package pwrFDR. Nice shiny preview at\n\nhttps://www.izmirlian.net/shiny-apps/pwrFDR/', ""No sound statistician uses Bonferroni nowadays. In basically every university stats class, they teach that it's too stringent to be practical, but they include it in the curriculum because it's an easy method to introduce students to the multiple testing problem\n\nThere are much more modern, optimal procedures. Like Benjamini-Hochberg. But even that's grown dusty"", 'Multiple comparisons without correction = garbage science.  No point in reading any paper that does this.  Does not, of course, have to be Bonferroni.', ""There are many, many other corrections. They're all more powerful. If you're forced for some reason to use Bonferroni, I'm very sorry."", 'Following!', 'A nice paper that discusses Bayesian approaches to the multiple comparison problem is the following:\n\n[Bayesian Data Analysis John Kruschke](https://wires.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wcs.72)', ""For a hypothesis-generating study with a fairly small number of comparisons (say, <10) I probably wouldn't use Bonferroni corrections. But for studies trying to get at causality, or with larger numbers of comparisons, you absolutely need some form of multiple comparisons adjustment, Bonferroni or otherwise. For example, a GWAS study with an alpha of 0.05 would be absolutely useless.\n\nSo there's no hard and fast rule, but the need for a multiple comparisons adjustment does depend on the context of the study."", 'Just another pathology of hypothesis testing.  Nothing to see here', ""As others stated there are other corrections that are uniformly more powerful and also control the family wise error rate.\n\nAs to whether to use corrections or not, it depends on what you want to archive with your study. For a study that is mostly concerned with generating knowledge not adjusting is ok, as long as you report all tests you applied. (In theory the reader can then apply a Holm correction themselves.) With multiple contrasts in a regression model it's harder, because one can not get the correlation structure from the coefficients usually reported. On the other hand correlation is also a reason to use other multiplicity corrections than Bonferroni. \n\nIf the aim of your study is to inform a policy decision (like registration of a medical product), then multiplicity correction is absolutely necessary. The simplest form is to declare one endpoint as primary (and base the policy decision mostly on this endpoint) and others as explorative and report them without correction. Other possibilities are hierarchical testing or graphical approaches, in which you test the first hypothesis with a greater power and the others only if you can reject in the first test. Having multiple equally important hypotheses is rare, but in this case you would need something like a Holm correction.\n\nIn other settings, other corrections are more appropriate, for example in genome wide association studies, procedures that control the family wise error rate are too conservative, but controlling the false discocvery rate is still important.\n\nOther settings where you need similar techniques are tests where you have multiple opportunities to reject for the same hypothesis, like group sequential designs.\n\nI think the Bonferroni correction is the most widely used technique of all corrections because it's easy to explain. And people think  sometimes justified, sometimes less so  that they need a multiplicity correction.\n\nAnd of course in a Bayesian setting you don't have to deal with all of this."", 'To add to this, there are even more powerful methods which accounts for the dependence between tests, such as Romano-Wolf. But these are more complicated to implement, and might require additional assumptions.\n\nBut as you say, it is *never* justified up use Bonferroni over Holm-Bonferroni.', 'So many papers I read use Bonferroni and I always wondered why not the Holm-Bonferroni? Any idea why?', '> The question whether to use multiplicity adjustments at all or when to use them is a different question that has nothing to do with any specific method.\n\nSo lets discuss that question. When do you think they should be used?', ""In practice, Holm-Bonferroni rarely gives any advantage over simple Bonferroni. The reason being that the first step of Holm-Bonferroni basically is a Bonferroni-corrected test on the smallest p value, and if that is not significant (which it usually isn't, Bonferroni being overly conservative), then the whole procedure crashes."", ""How about Holm-Bonferroni vs Sidak method? My supervisor always uses Sidak, and I'm hoping you could give me reasons for why you would use one over the other."", ""Not really adding much discussion to the discussion but hey I can't not upvote xkcd"", ""If Benjamini-Hochberg has grown dusty what is a more modern, optimal procedure that isn't dusty ?"", 'Could you name a few that you think are better than Bonferroni?', 'I\'m not sure why anyone wouldn\'t use a multiple-comparisons correction method (if that\'s what you\'re saying). Why even pretend to use concepts like ""alpha"" and ""p-values"" if you\'re going to ignore them? Even two or three uncorrected comparisons will inflate your familywise error rate. The desire to do a bunch of comparisons doesn\'t remove the consequences of doing so.', 'At what number of comparisons would you say that a study trying to get at causality would absolutely need some form of multiple comparisons adjustment? Any more than 2 comparisons or anymore than 5 for example. \n\nArmstrong (2014) discussed how in exploratory studies it may be best not to use multiple comparisons adjustments because of the increased type 2 error risk. Where would you say an exploratory study lands between hypothesis-generating studies and studies trying to get at causality? because although an exploratory study does hope to get at causality it is also quite similar to a hypothesis generating study in purpose.', ""I don't know why, but I read GWAS as Great White-Ass Shark. Would be an interesting study."", 'Could you expand on the nothing I am to see here?', '>For a study that is mostly concerned with generating knowledge not adjusting is ok, as long as you report all tests you applied.\n\nI deeply disagree, unless we stop using p-values at all. If we\'re going to use numbers that mean things, I think we need to be honest about them. Otherwise, it seems like saying, ""Yes, I do believe accurate dosage of this medication is crucial, and I\'m going to tell the patient the dose was accurate, but honestly I\'ll probably give them a bunch extra."" If you\'re going to do exploratory data analysis with multiple comparisons and not use familywise error rate corrections, just don\'t use p-values at all.\n\n*Edit*: I\'m open to other ideas, it\'s just that a p-value has a certain meaning, and I think if we abandon that we should honestly abandon it. If we keep it, we need to be honest about what\'s happening.', 'Oh, and for the pharma regulatory view, look at the EMA guideline on the topic, the guideline also states many cases in which no formal adjustment is needed:\n\nhttps://www.ema.europa.eu/en/documents/scientific-guideline/points-consider-multiplicity-issues-clinical-trials_en.pdf', ""I'm interested in your Bayesian comment - what is it about that approach that solves these issues with hypothesis testing?"", ""Interesting! Haven't heard of them. Could you tell me where I can learn more?"", ""When you say that  it is *never* justified to use Bonferroni over Holm-Bonferroni. I get that technically that is always the case however if you only have 4 tests and only one of them is remotely significant to begin with won't the result be the same regardless?\n\nAs to my understanding the most significant P value is basically treated the same as just using Bonferroni and the real strength of using Holm's method only pays off over multiple tests"", ""Pure speculation, but the Bonferroni method is extremely easy to apply: Simply multiply all *p*-values with the number of tests. The Holm-Bonferroni requires a bit more calculations (not much more).\n\nThe Bonferroni adjustment is also commonly taught and always works (contrary to what you'll sometimes read on the internet)."", 'For me, a seminal paper is that of [Rubin (2021)](https://link.springer.com/article/10.1007/s11229-021-03276-4).', '>Not really adding much discussion to the discussion \n\nIt does, actually.', 'Redditors just love sharing comics more than anything else, so much for the citations you posted :/', ""Oh, god, there are literally dozens. They are generally tailored to the type of test you're doing. For ANOVA there are two or three Tukey corrections. Scheffe, Hisak (sp?)... do some googling."", 'Again, I would say that there\'s no exact rule on this - I mentioned 10, but I wouldn\'t argue with a smaller (or larger) number.\n\nThe terms ""exploratory study"" and ""hypothesis-generating study"" tend to be used synonymously, as far as I\'ve seen (just throwing exposure/outcome variables in and seeing what sticks, with minimal regard for *a priori* evidence of links).  Keep in mind that there\'s no clear dichotomy between hypothesis-generating vs. causal studies, but rather more of a spectrum, with something like GWAS studies on one end and RCTs on the other.  Actually, the use of multiple comparisons adjustments themselves actually strengthen any causality-based arguments for a study, by changing the interpretation from ""look at these associations we found"" to ""our findings are highly unlikely to be spurious associations"".', ""When you report all the tests you did the same information is contained in the unadjusted p-values as is in the adjusted ones. I personally find reporting adjusted ones more useful, but it's not the most important thing. Reporting all tests is more important. \n\nIf you ran 1000 tests in your study, took the 20 with the smalles p-values, made a correction for 20 tests and published this it would in my opinion be way more dishonest than if one ran 20 tests and reported all their unadjusted p-values. \n\nAnd again, for policy decisions (like what dose of a medication to give a patient) adjustments are strictly necessary."", 'Thats not evidence that you dont need to do corrections. Thats evidence that the EU thinks you dont need to do corrections in the context of clinical trials.', ""Bayesian statistics just don't have the concept of testing at all. We only do model comparisons, and there are far less gotchas than testing."", ""Sure - here's the [Romano-Wolf paper](https://www.tandfonline.com/doi/abs/10.1198/016214504000000539). I also really like [this paper](https://www.tandfonline.com/doi/abs/10.1198/016214508000000841) on the topic - might be an easier introduction."", ""If they give the same answer, they give the same answer. In that case, it obviously doesn't matter. But you don't know that until you've tried them both, so then why not stick with Holm-Bonferroni?\n\n>As to my understanding the most significant P value is basically treated the same as just using Bonferroni and the real strength of using Holm's method only pays off over multiple tests\n\nYes. But you only use them when you have multiple tests."", 'If by discussion you mean the people who already agree with the comic get to smuggly laugh and everyone else just shrugs. \n\nWhich is quite well demonstrated by this comment having a bunch of upvotes because people agree with it but zero discussion about Bonferroni occurring because of it. This is a discussion post because I wanted to discuss the use of Bonferroni not just hear clever come backs', ""Not only this, with a useful prior Bayesian inference shrinks the estimates towards the null, around the null. \n\nThose two blog posts by Gelman are quite illustrative. I didn't read the response by Benjamini, but I'm certain the whole discussion is quite interesting.\n\nhttps://statmodeling.stat.columbia.edu/2016/08/22/bayesian-inference-completely-solves-the-multiple-comparisons-problem/\n\nhttps://statmodeling.stat.columbia.edu/2022/08/10/bayesian-inference-continues-to-completely-solve-the-multiple-comparisons-problem/"", 'Thank you@', 'Had you read *and understood* the comic, your question would be unnecessary.', 'Fascinating stuff, thank you! This has reignited my desire to learn statistics properly.', 'I find that surprising because the question is about Bonferroni and yet the comic is about why you must use multiplicity adjustments\n\nHad you *read* and *understood* the post, your comment would be unnecessary', 'Keep digging, Charlatan.']"
[Q] Annualized data vs 3m average for turnover,"Sorry if this is the wrong sub. Im trying to figure out the best calculation for employee turnover. Right now weve got an annualized turnover (avg # terms ytd)*12 / avg staff 

The problem with annualized is any operational changes arent really captured in the annualized calculation. 

The other metrics Im displaying next to turnover are all 3 month averages and Id like to do a 3 month average of turnover.

I dont know how to calculate that in a way that makes sense.",12zrygf,lady_picadilly,1682531960.0,4,1.0,['What do you mean by operational changes ?']
[Q] Can a statistic that is not complete achieve the CRLB?,"Can a statistic that is not complete have a variance that achieves the Cramer-Rao Lower bound?

eg: if X\_i are iid samples from N(0, sigma\^2), the sufficient statistic T = sum(X\_i\^2)\^(1/2) is not complete by Basu's theorem. Will its variance achieve the CRLB?",12zrjww,zebrapaad,1682531049.0,3,1.0,[]
[Q] Cronbach's alpha,wDoes anyone know if there is a way of converting Cronbach's alpha into a Pearson's correlation? I am interested in the reliability of a task but some people only report Cronbach's alpha for split-half reliability whilst I need the Pearson's correlation between odd and even trials.,12zq559,majorcatlover,1682528005.0,1,0.67,"['[just going to leave this here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2792363/)', ""AFAIK no. Cronbach's alpha can be seen as the mean of all possible split test reliabilities (if the number of items is even) and there is no way to extract result for a specific split."", 'Thanks so much!', 'Thank you!']"
Which statistical test would be most appropriate? [Q],I am trying to figure out if there is a positive correlation between breast milk intake (continuous variable) and whether the country is assumed to be developing or not (categorical). I also have age which effects milk intake but not the development status. I've been trying to figure out what to do for the last couple of days so I'd really appreciate the help!,12zk5ec,flickerflame13,1682521316.0,2,1.0,"[""I'd initially explore ANCOVA test and check if your dataset follows its assumptions. Milk intake is your response, development is your factor and age is your covariate."", ""I'd be worried about autocorrelation between age and breast milk intake. A country that has high breast milk intake when the babies are 3 months will also likely have a high rate at 6 months. This can cause problems. I'd choose one age and use that. Then check to see if your results meaningfully differ for different ages. I'd be somewhat surprised if they did. If your results do change based on which age you examine, then your analysis gets more complex.\n\nVisually, you can take the average breast milk intake for all developing countries at each age. Then the same for developed countries. Then plot a line graph showing the average breast milk intake as age change using one line for each type of country (developing/developed)."", 'A point biserial correlation is what you want.', 'Piggy backing off this:\n\n`model <- lm(milk_intake ~ country_status + age, data = your_data_frame)`  \n`summary(model)`\n\nI also suggest you explore a possible interaction between `country_status` and `age`. You can do that by changing the `+` to a `*`. Plot your data. And as Nirvana5b says, check if assumptions hold by running diagnostics on your residuals.', 'I second this, but Id not include age in the model since its not associated with development.']"
[Q] How to make non-stationary Time Series data stationary?,"I am currently working on building Time Series forecasting model to predict quarterly population volume of Ontario ([https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901](https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901)) based on the last 22+ years. The issue is that around in the last 10 quarters, the data becomes non-stationary. I took several differences, log transforms, and lags but non-stationarity in the most recent quarters still exists.

Thanks",12zjdzv,PythonEntusiast,1682519597.0,30,0.95,"['How do you arrive at the conclusion that the data is non-stationnary? If you used a unit root test, its pretty much worthless with such a low observation number.', 'You can try fitting a Fourier series to the data and specify a few important bases after doing an FFT on the data. Once you get a good fit, you can subtract the series from the original signal and train your model on the residual. \n\nThen when you are generating samples, you can add back in the Fourier series.', 'One additional option is to consider ARFIMA models (auto regressive fractional integration moving average). These extend standard ARIMA modes by allowing the differencing to be non-integer valued and can accommodate for long term dependence.', ""Seems like you are dealing with a break. I don't know the specific question of your research but you can always add a dummy =1 for t>T-10 and 0 otherwise"", 'How about fitting an appropriate state space model to your data? This would not require any transformations, because these models can model non-stationary series easily. Prediction is also straight-forward and you get an interpretable model for free.', 'The results of the stationarity test essentially are dependent on the method you are using. Usually, this is the augmented Dickey-Fuller method, which decides whether a series is predictable in an autoregressive manner. In your case, I suggest you use a nonlinear model or a sinusoidal one, as one of the comments before me said. Creating new features also works sometimes, you can use sliding statistics, which may produce interpretable new time series that are stationary.', 'Made some progress. Took cube root of the data and took two differences. PAC and PACF plots make more sense now.', '>The issue is that around in the last 10 quarters, the data becomes non-stationary.\n\nWhat has been different about the last 10 quarters, compared to the few dozen quarters prior?\n\nThere is in inherent assumption with a time series model: *The future looks an awful lot like the past.* If that assumption no longer holds, time series might not be an appropriate approach to your problem.', 'Forgot to mention, using SARIMA model.', 'Fractional differencing might be required. Sometimes a first difference will over-difference your data.', 'Used the following:  \n\n\n[https://machinelearningmastery.com/time-series-data-stationary-python/](https://machinelearningmastery.com/time-series-data-stationary-python/)  \n\n\nPlus, plot of the original data shows non-constant mean. The plot of the log of data also shows non-constant mean. The plot of the differenced data shows non-constant variance right when COVID-19 hit.', 'Am I correct in understanding that it would only work if the data was  seasonal or has some other periodicity phenomenon?', 'You could use ASINH(x\\_i \\* k) instead of cube root for the first step of the correction which will be much closer to LOG and its ability to pull back big values, since they are equal asymptotically. The ""k"" factor could be a relatively big number (1e18) so the vertical asymptotic part of the LOG would not disappear without considering it at some level. But the curve would still go through the origin, still making it possible to correct negative values as well.', 'Covid hit.', 'Differencing should remove any trend. Look in to garch models, which allow for non-constant variance', 'Yes, for my use cases I am often working with some kind of physical stochastic data like energy demand or wind speed.']"
[Q] AMOS 24 (Badly Need Help): Where do I find Cronbach's Alpha and AVE in AMOS 24? I also need the P Values of Direct and Indirect Effects,"I've been trying to look for these values but I cannot seem to find one, or maybe I just dont know what its name is in AMOS. Please help. Thank you so much.",12zippp,Zealousideal_Cold212,1682518087.0,0,0.33,[]
[Q] Do highly correlated variables affect feature importance in gradient boosted trees,"I want to ask this because I have not found very convincing / conclusive answers on the web. 

The question is: do highly correlated variables affect feature importance in gradient boosted (small) trees. And why. 

I know prediction accuracy measures don't get affected as much. 

I suppose we can limit the discussion to the implementations from sklearn and xgb.",12yx9or,porgy_y,1682459398.0,8,0.9,"['Yes they do. The simple answer is that when two variables are highly correlated no method can reliably distinguish between the influence of one vs the other. The information to distinguish between them simply is not there. This is only made worse by having highly flexible models like xgboost that can model complicated nonlinearities and accommodate for one or the other.\n\nThink of the extreme example where the two are perfectly correlated. Adding one or the other makes absolutely no difference as they can substitute directly for each other.\n\n\nYou didnt ask this, but this is also relevant to the common question of whether or not collinearity/correlation matters for prediction. The answer is yes, especially if you need to predict at all far from the support of your data. This is because of the same underlying problem, the model may misattribute the effects of one variable to another. If the effects are different in the regions of interpolation/extrapolation the predictions will be bad.', 'What measure of feature importance are you using? Information gain/entropy? Contribution to variance? \n\nIs your response binary?', ""You explained this without using any new vocabulary words I'd have to look up and it all made sense. Thank you!"", ""Thanks for the answer. I agree.\n\nThe part about prediction is really good. I haven't thought about it at all."", 'One thing I am not 100% certain is the direction of the feature importance change. \n\nIf I have two very important and highly correlated variables in XGBoost, would the feature importance for both variables be lower compared to if only one of them is in the model?\n\nThe link below has contradictory answers. (In fact, the top rated answer is the opposite of what we are saying here.) \nhttps://datascience.stackexchange.com/questions/12554/does-xgboost-handle-multicollinearity-by-itself\n\nIf two variables have exactly the same explanatory power in terms of their ability to split nodes or reduce impurity, is it up to the implementation of the algorithm to decide which one to pick? Randomly or first come first serve. I suppose I should ask this in an ML sub...', ""I don't see why they matter. But I can very well be wrong."", 'For most feature importance measures I expect that it will make the features each considered less important on average, but also be more variable. E.g. features may be rated as highly important, more so than they actually are, due to random variability. To what degree probably depends on which measure you use, but it can be said that the feature importances are simply less reliable in these case.\n\n\nFor that answer, I believe they are addressing that from a mode fitting perspective. For linear models for example severe collinearity can pose an issue in that software may not converge, provide errors, and you simply dont get a reasonable model, or sometime any model, at all. Tree based models are immune to this in that the algorithms will happily chug along and fit the model in spite of the correlation. The correlation is still an issue when you consider performance of the models though.\n\n\nFor your last question, yes the exact way it is handled can be implementation specific and may depend on tolerances, order of variables in the data, or even be random.', 'On a second thought, I think the importance reduces for both variables.\n\nIf the algorithm randomly selects one of them, then one of them shows up at a time. So the feature importance is lower because some of the appearances (made by one variable) in the trees could have been made by the other variable.\n\nIf the algorithm does first come first serve, then the one 2nd on the list has 0 feature importance since it is always superceded by the first one. In this case, order in the feature list matters.', ""I don't get why I am getting down voted for this. Can someone care to explain why the specific impurity measure matters? Same goes with classification vs numerical regression."", 'Your middle paragraph is where I suppose all the confusion might have come from. I am in an uphill battle to persuade folks who were taught in school that trees are immune to multicollinearity and now think that means feature importance should be immune too.\n\nThe corner case of ill-detected relationships among highly correlated variables affecting prediction accuracy on out of support data is great but unfortunately ""pedant"" to change minds in my line of work, due to other measures in place to limit out of support forecasts.\n\nAnyways, as someone mentioned, your explanation is really great without jargons that someone new to this needs to look up.', ""Because they didn't ask what your impurity measure is, they asked how you're defining feature importance. One feature importance measure is how much each feature reduces impurity, but it's not the only one, and the particular way you're calculating feature importance is of course critical to the question of how it will be affected by multicolinearity."", 'Can you please elaborate on the last part?\n\nFor example, how does information gain based feature importance get affected or not get affected by multicollinearity, as opposed to gini based feature importance?']"
[R] Is it still fixed effects IV to lag the independent variable?,"Hi everyone,

Hoping to get some advice at an undergraduate level. Working on an observational study using panel data - it's a development econ project.

Had a sit-down chat with my supervisor today where he told me I was doing the fixed effects instrumental variable (FE IV) method wrong as I wasn't lagging my dependent variable but actually my independent variable.

I've tried to do some reading on it and it seems that in summary, you should only lag your dependent variable if you believe the current value is heavily determined by its past value. I think this may be true in my case BUT I also think I was doing the right thing by lagging my main independent variable.

I hypothesised that there's an information lag effect between my dependent and independent variables. Essentially, economic agents are not responding to a situation contemporaneously, they are using past information to inform their current decisions. Therefore, any predicted values for the dependent variable would be reliant on the observed values of the independent variables from the past period. This would essentially be dealing with a reverse causality concern discussed in some political economy papers.

My questions then are -

1. Is it doing FE IV wrong to not use the lagged dependent variable as the instrument?
2. How can I include both the lagged versions of the dependent and independent variables in my model specification? Would I have to treat them as separate changes to my methodological approach or can I group together?

I hope I've asked these questions clearly enough but I can definitely clarify if not. Thanks in advance.",12yw7fa,temab1,1682457063.0,5,1.0,"['Is your prof up to date with the literature? We arent doing instruments. And two way fixed effects have a host of problems.', 'Wait, can you say more. Whats the newest research?\n\nEdit: \n[Holy shit.](https://web.mit.edu/insong/www/pdf/FEmatch-twoway.pdf)', 'Undergrad dissertation generally for internal grading only so I think theyre less concerned with it being up to date with the literature for the most part.']"
[Q] What statistical tests could I use to measure the similarity between rankings of two lists?," Say I have two lists, List A and List B, and both lists have 100 variables that are ranked 1-100. The variables are NOT quantitative. The two lists are independent of each other. What tests are available to measure the similarity between the rankings of the two lists?",12yw55s,bernful,1682456927.0,35,0.94,"['Spearman rank correlation is probably the thing to google my dude.', 'Wilcoxon too', 'What might be helpful for your decision is reading through these guidelines.   \n[https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf](https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf)  \nI know the application in this case is the ""Comparisons of Classifiers  \nover Multiple Data Sets"" but you should easily be able to transfer the tests and methods described there to your problem.', '> use to measure\n\nMeasurement is estimation, not testing.', 'Check out the Hubert-Arabie adjusted Rand Index. I think itll work for this case.', 'Yes, then maybe a null Hypothesis test (could also to right/left tail depending on your aim:\n\nMake an H\\_0: r = 0\n\nt = \\[r\\*sqrt(n-2)\\]/\\[sqrt(1-r\\^2)\\]\n\ncompare with t with t-critical\\_vale @ level of significant, df=n-2', 'Op says lists are independent, dont know how they know. And top comment is spearmans rank correlation coefficient.  I dont know Im confused']"
[Q] Question about holding variables constant logistic regression,"Hi :) 

I have a question about ""holding variables"" constant. For uni, I am supposed to visualise my binary logistic multiple regression with 3 predictor variables. They all use the same continouus scale. The Y-axis should visualise the likelihood of one outcome, and the X-axis should be the scale that all predictors use. We should visualise all predictors in one graph.

So far so good. I calculated the coefficients, I know how to calculate the likelihood. I know that I can visualise the predictors separetly by changing the X of one of them, and keeping the X of the others constant. However, how do I keep the other predictors constant?  The professor said that we should decide whether to  choose the median of each predictor, the middle of the scale or 0. 

Is one of them preferrable to the others? I have no clue- my first guess would have been to make X=0, because anyways the coefficiant of the predictor that I want to visualise is a result of the presence of the other variables. Does that make sense?  
Or am I on the wrong path here?  


I would appreciate any help :)",12yq9x7,Luluvaki98,1682444243.0,1,0.67,"[""I generally choose the mean or median. If there is a very strong mode (for example, if almost all value of x2 are 0), then I may use that modal value. It's useful to understand a couple key properties of marginal effects:\n\n1. The marginal effect of x1 is the change in y (kind of a slope) as x1 changes holding all other variable constant. This marginal effect/slope is the same regardless of whether x2 is 0, .5, 1, or any other value. \n2. The only thing about the marginal effect of x1 that changes when x2 is 0 vs 0.5 is that the \\*intercept\\* value for the marginal effect changes. \n\nIn short, the slope of the line you plot for the marginal effect of x1 will not change based on the value you choose for x2. The intercept will change, but we usually don't care about the intercept. Therefore, we don't usually care too much about what value x2 is set at when plotting the marginal effect of x1."", 'Of note, these principles only work if there are no interaction terms relevant to the variables you are plotting.', 'Thank you so much this is very helpful! I will read myself a bit more into marginal effects so to understand what you are saying 100% :) our professor did not really introduce the theory behind this, so I am sure that this will be useful :) \n\nHave a good day/night :)']"
[Q] Simulation of collider bias,"Hi, I could use some help diagnosing a problem with my simulation. I'm trying to convince myself of the effects of collider bias. I've simulated some data of two variable X1 (stroketx) and X2 (tumor)  that cause Y (death). X1 and X2 should be independent, but as you will see later there's reason to believe they aren't (?)

    set.seed(210423)
    
    sampsize <- 10^3
    simulns <- 10^3
    strokeeffect <- -8
    tumoreffect <- 4
    
    
    simul <- function(i) {
     stroketx = rbinom(n=sampsize, size = 1, 0.5)
     tumor = rbinom(n=sampsize, size = 1, 0.5)
    
     # the linear predictor
     lp = strokeeffect*stroketx + tumoreffect*tumor
    
     # Get probability based on linear predictor; assuming logit link
     vecp = 1/(1+exp(-lp))
    
     dth = rbinom(n=sampsize, size=1, prob = vecp)
    
     # X1 and X2 have no dependence (supposedly)
     marginal.x1x2 = summary(glm(stroketx ~ tumor, family=""binomial""))$coefficients[2,1]
     conditional.x1x2 = summary(glm(stroketx ~ tumor + dth, family=""binomial""))$coefficients[2,1]
     marginal.x1y = summary(glm(dth ~ stroketx, family=""binomial""))$coefficients[2,1]
     conditional.x1y = summary(glm(dth ~ stroketx + tumor, family=""binomial""))$coefficients[2,1]
    
     return(c(marginal.x1x2, conditional.x1x2, marginal.x1y, conditional.x1y))
    }
    
    
    v <- lapply(1:simulns, FUN=simul)
    
    
    # Get mean of unbiased association between X1 and X2
    mean(sapply(v,""[["",1))
    
    
    # Get mean of the biased association between X1 and X2 (included collider Y)
    mean(sapply(v,""[["",2))
    
    
    # Get marginal association between X1 and Y
    mean(sapply(v,""[["",3))
    
    
    # Get conditional association between X1 and Y (included X2 which is not a confounder)
    mean(sapply(v,""[["",4))

So while I do see that the association between X1 and X2 is biased because I've conditioned on a collider (Y). What troubles me is finding that:

1. My marginal association between X1 and X2 isn't close to 0 as I had hoped
2. My marginal association between X1 and Y is nowhere near the intended value of -8

&#x200B;

    > # Get mean of unbiased association between X1 and X2
    > mean(sapply(v,""[["",1))
    [1] -0.001861118
    > 
    > 
    > 
    > # Get mean of the biased association between X1 and X2 (included collider Y)
    > mean(sapply(v,""[["",2))
    [1] 3.563349
    > 
    > 
    > 
    > # Get marginal association between X1 and Y
    > mean(sapply(v,""[["",3))
    [1] -5.992347
    > 
    > 
    > 
    > # Get conditional association between X1 and Y (included X2 which is not a confounder)
    > mean(sapply(v,""[["",4))
    [1] -8.518688",12yogvh,lightsnooze,1682440374.0,2,1.0,"['For (1), the marginal association between X1 and X2 is -.0019, this is quite close to zero, and Im sure if you considered the confidence intervals you would have good coverage of zero.\n\nFor (2) logistic models are non-collapsible which means the marginal odds ratio and conditional odds ratios (as well as associated parameters), are not equal to one another. It should be expected that the marginal parameter will take a different value.', 'Cheers for the answer to (1). \n\nRegarding (2), this is new to me so thanks for making me aware. So non collapsability explains why my marginal and conditional coefs are different when modelling Y. But how does non collapsability of the OR explain the marginal association being different from the intended value of -8?', 'Your initial definition of lp includes both stroke effect and tumor effect, which means that the -8 association between stroke effect and Y is a conditional association (conditional on the covariate value for tumor effect). The marginal model you fit estimates the marginal association which is some number different from -8. This is usually closer to zero like the estimated marginal coefficient of -6ish.\n\nSo its different here because the intended value of -8 is a conditional, not marginal association. They should not be the same.', 'Thanks for continuing to help with this. I am not sure I fully understand what you\'re saying. To test your claims, I\'ve tweaked the value of stroketx to -12 instead. If it is true that the value of stroketx in my definition of the lp is indeed the conditional association, and if it is also true that the marginal coefficient should be closer to 0 than what I had specified, then we should observe:\n\n(a) the conditional association conditional.x1y should be closer to -12 than marginal.x1y; and\n\n(b) the marginal association of stroketx on death should be closer to 0 than -12\n\nBut this is not the case empirically for both claims\n\n>\\# Get mean of unbiased association between X1 and X2  \n>  \n>mean(sapply(v,""\\[\\["",1))  \n>  \n>\\[1\\] 0.008412075  \n>  \n>\\# Get mean of the biased association between X1 and X2 (included collider Y)  \n>  \n>mean(sapply(v,""\\[\\["",2))  \n>  \n>\\[1\\] 3.63549  \n>  \n>\\# Get marginal association between X1 and Y  \n>  \n>mean(sapply(v,""\\[\\["",3))  \n>  \n>\\[1\\] -19.76235  \n>  \n>\\# Get conditional association between X1 and Y (included X2 which is not a confounder)  \n>  \n>mean(sapply(v,""\\[\\["",4))  \n>  \n>\\[1\\] -22.79851', 'Two comments here:\n\nFirst, \n\n>(b) the marginal association of stroketx on death should be closer to 0 than -12\n\nNot quite. It should not be -12, and should be differ from -12 in the direction of 0, but it will likely be closer to -12 than to 0.\n\n&#x200B;\n\nSecond, what you are likely dealing with in this case is complete/quasi-complete separation in the regression models. A coefficient of -12 is an absolutely massive effect size for a logistic regression. As a result the model fitting process begins to diverge towards infinite values of the parameter rather than providing accurate estimation. That said -8 is quite a large effect size as well, but not so much you ran into issues here apparently. \n\nA more reasonable coefficient of -2 for example should confirm what I said in simulation.', 'Thanks, numbers are making sense to me now']"
[Q] why does the lognormal distribution tend to show up often where you wouldn't expect it?,"*My name is peperazzi74 and I am an addict - for cumulative density function calculation. /s*

Now for the serious part:

I've been doing some training on data science and statistics and have collected or scraped a variety of data sets. One of my current interests is Monte Carlo-type modelling, for which I need to have a PDF or CDF to generate the random samples.

Typically one would expect the lognormal distribution to appear for data that has a large spread over multiple orders of magnitude, but I found lognormal behavior in things like:

* the age at which US presidents come into office (spread \~ 40-80 years-old; 2x)
* HOA monthly spend ($10,000-$40,000; 4x)
* the tenure of HOA board members (spread \~3 months - 4 years; 10x)
* number of homes for sale simultaneously in my neighborhood (spread 2 - 25; 10x)
* outstanding delinquent sums in HOA ($3500 - $53,000; 15x)
* and some others that are similar in max/min ratio, but cannot share (work-related)

What makes this type of data so likely to have a lognormal distribution vs e.g. a normal distribution that we usually tend to use?  


  
*Note: a lot of data is related to my neighborhood/HOA, because they publish monthly and have done so for a long time, so lots of free and somewhat publicly available real data.*",12ymb7d,peperazzi74,1682435662.0,2,0.67,"['The log-normal distribution is a reasonably flexible distribution for strictly positive data, and can arise as the result of a central limit theorem where many independent factors have multiplicative effects on the outcome.\n\nHowever, you data very well may also not be log-normal and may be better described by other common distributions. What have you done to check log-normality and what have you compared against?', 'Lognormal distributions arise (among other ways) when many small random factors have multiplicative effect, just as normal distributions arise when many random factors are additive.\n\nA lot of financial processes have that kind of multiplicative effect, where your gain or loss this year is a percentage of how much money you had last year.\n\nIt is also very easy for a lognormal to look similar to anything with a power-law right tail, and possible for it to look similar to a Gamma or Weibull with shape parameter much less than 1.\n\nThat said, I would not expect the lognormal, or any other very thick-tailed distribution, to arise, except coincidentally, for variables like age or tenure. (The fact that some of these remain confined to narrow ranges feels like quite strong evidence you need something with thinner tails.)', ""In environmental statistics, it seems most things are lognormal-ish or at least rather skewed. Partly because things like concentrations can't be negative?"", 'Your HOA monthly spend and delinquent sums may be log normal, the others probably are not. The log normal often arises in connection to rates and time. It looks a lot like other distributions under certain parameter estimates. \n\nFV=PV*exp{r*t}. If r is normally distributed, then FV is log normal for a fixed PV. If PV itself varies a lot, then FV will have another distribution. \n\nI would point out that if you are doing a non-Bayesian regression, then as long as your dependent variable has finite variance, then the probability distribution of your variables does not matter. So stock market returns as a dependent variable wont work with ordinary least squares but HOA spending as a dependent would work. Median based regression would work when there is no finite variance. \n\nThe others are simply positive, skewed distributions. The illusion of a link comes from the flexibility of the log normal.', ""I dont get where you obtain  the 'many orders of magnitude' thought. In a lognormal, the typical spread (say the middle 95% for example) may have a very narrow range if sigma is small - even a tiny fraction of one order of magnitude\n\nThe distribution of president age won't be lognormal -\n\nhttps://en.m.wikipedia.org/wiki/Log-normal_distribution#Probability_density_function\n\nSince there's a high  minimum age requirement (relative to the median). \n\nSome of your other variables  may have similar issues"", ""Have you performed any statistical tests for lognormality? You said you've looked at QQ plots, but that can be misleading, especially for small samples\n\nHave you tested for other distributions taking values in positive reals? Do you know any?"", 'I think this question, along with the datasets you mentioned, would serve as a fine open-ended homework problem for students learning stats and data science. Nice', 'I use R, so mostly\n\n`ggplot(`*data*`, aes(sample = log(`*datapoints*`))) + geom_qq() + geom_qq_line()`\n\nor a comparison of the lognormal CDF vs the data points and visually comparing the points compared to the model.\n\nThe comparison that I made was versus the normal distribution.', 'I think this is probably the most reasonable answer to the question:  If the data are positive and right skewed, then a log-normal distribution is likely to be a reasonable fit.\n\nIn the natural world, there are measurements that are often relatively log-normal.  Things like the size of cracks in bedrock and the concentration of some compounds in water bodies.', 'This is an excellent answer.', 'Have you also tried comparing against a chi-squared and/or gamma distribution? Those also are flexible enough to look like a log-normal distribution.\n\nIMO, doing a Q-Q plot against a log normal distribution and seeing its somewhat close isnt enough to conclude the data is actually log normal. But this Q-Q plot comparison could be enough to conclude the log normal is a good model for your data (but it may be the case that some other positive distribution like a gamma could also be a good model as well.)', 'There are functions in some R packages to see how well different families of distributions fit a set of data.  They might be called *fitdist* or something similar.', 'Its not surprising at all that log-normal would be a better fit than normal. On the occasions that a normal *can even possibly provide a decent fit* there are log-normals that will have essentially equivalent fits. However if the distributions have any reasonable support near zero the normal will probably fit poorly.\n\nThere are a host of other possible distributions to consider for strictly positive values. To name a few worth considering weibull, exponential, gompertz, gamma, Pareto,and chi squared. \nIn terms of comparing model fits, AIC is a good choice. If you want an explicit test for comparison, the Vuong test would be a decent option.', ""I'll look into your proposed distributions.\n\nAs far as the QQ plot goes: most of the data I used as example followed the QQ-line pretty closely from z -2..2, so \\~95% of data (of course assuming normal behavior)."", 'If youre actually going to try and fit a gamma distribution to your data, you might have to be careful about how you do it because if I recall correctly, closed form MLEs/moment estimators dont exist for the parameters in this distribution so you have to use an iterative algorithm (so would be good to find some place online on how to do it). I guess what I was getting at was that theres probably a few distributions which will give you good looking matches on the Q-Q plot, which just means that a Q-Q plot alone is not sufficient to conclude that your data follows some very specific distribution like a log normal.']"
[E] Help with stats graduate programs,"Hi everyone,

Hoping to get some advice. Ive started working as a data analyst and have started to go through textbooks since Im incredibly passionate and excited about doing research work with data, and think that a statistics theory foundation would be very helpful. 

During undergrad, I didnt do so well in my initial math and stats courses (first year). I am planning on taking more advanced math courses this summer at a community college to get pre-requirements and hopefully show Im more serious about my studies. 

I also currently work, so have been trying to find programs that offer part-time / online programs. I live in Chicago and have been looking around but Id also like a strong program overall. 

Essentially:
- Is taking classes this summer a worthwhile investment?(not the pre-requirements but other advanced ones just to feel more comfortable and hopefully have it boost my application)
- What is the best balance between choosing a name school vs a program that matches my needs? (Im not sure how to weigh name-brand for programs so some overall tips on how to value program recognition and connections would help)
- Since it seems most Masters Stats programs are full-time and in-person, is it worth quitting my job to go back full time? (I know this ones personal and based on circumstances. Currently, I make a good amount so dont feel its smart to stop working right now, but I crave to learn and want to get promoted and do research work)

Thank you for your help!",12ylj5q,suckit_imin,1682433974.0,1,1.0,"['Is real analysis an option? Id suggest kicking ass at that if you want to take more advanced math and demonstrate how serious you ate', 'I would think about a few things:\n\n1. Most programs will require Calculus 1, Calculus 2 and Multivariable Calculus along with Linear Algebra to consider you. Probability Theory makes use of a lot of Calculus and working with multiple variables requires Linear Algebra. Getting these classes together should be the bare minimum starting point for you. Real Analysis along with any other math that shows competency will be a bonus (or requirement depending on the program)\n2. If you want to boost your application or see if you actually like stats, you can do a graduate certification to let potential schools know you are serious. I would not necessarily do this unless you have a serious gap since it is expensive and time consuming. Penn State has a program:[https://www.worldcampus.psu.edu/penn-state-online-applied-statistics-programs?cid=CPC39813&gad=1&gclid=CjwKCAjw9J2iBhBPEiwAErwpeVuCQRCege9JIL67kizRV30fGUelQI0hqKxm2jy\\_uWbLeRZEBf2dWhoCR\\_8QAvD\\_BwE](https://www.worldcampus.psu.edu/penn-state-online-applied-statistics-programs?cid=CPC39813&gad=1&gclid=CjwKCAjw9J2iBhBPEiwAErwpeVuCQRCege9JIL67kizRV30fGUelQI0hqKxm2jy_uWbLeRZEBf2dWhoCR_8QAvD_BwE)\n3. What do you want to do with it? Do you want to do research or are you planning on working mainly in a professional environment? It sounds like you may want to do Applied. Instead of the certification, you can do a full online Masters with Penn State']"
[Q] Is small sample size a problem with randomization besides power?,"Randomized leads to an unbiased estimator regardless of sample size, so besides power, is there a 'problem' to be aware of with small sample sizes? I was thinking maybe the sampling distribution - if we can't assume normality of the errors/rely on the central limit theorem, does this mean we can't do inference generally without making stronger distributional assumptions?",12yjksl,Whynvme,1682429587.0,16,0.94,"[""Yes. For CLT reasons but also because randomisation only balances groups *on average*, not for each and every individual trial. You're much more likely to end up with very imbalanced groups in a small trial and it is harder to compensate for this by using stratification or minimisation to force balance because the sample size is too small to do it well. And you'll also have too small a sample size to do a reasonable adjusted analysis to account for the imbalance after the fact.\n\nAnd low power is a very bad problem, of course. If I've got 40% power at 95% confidence in a reasonable real life scenario, I'll pick up about 40% of the 10% of hypotheses where there is a real difference (4%) to find and 5% of the 90% where there isn't (4.5%). That means that just over half of results with p<0.05 will be false positives. We're in coin flipping territory."", '> Is small sample size a problem with randomization besides power?\n\nWhat are you randomizing there? Assignment to treatment? Or are you talking about random selection from a population of interest?\n\n(I did wonder briefly whether you were asking about randomization tests, but it seems from the later part of the body of your question that this isn\'t what you mean.)\n\n> if we can\'t assume normality of the errors/rely on the central limit theorem, does this mean we can\'t do inference generally without making stronger distributional assumptions?\n\nI don\'t quite get what you mean by ""stronger"" there\n\n(a) You can use *weaker* assumptions perfectly easily (unless the sample size is extremely small, when other issues come into consideration, like limited available significance levels). For example, permutation tests or bootstrap tests.\n\n(b) other distributional assumptions still often end up with either an asymptotically normal or an asymptotically chi-squared distribution of standard test statistics. e.g. tests based off likelihood, including Wald tests, score tests and likelihood ratio tests. Consider inference in generalized linear models or survival models (both parametric and semiparametric)\n\n(b) outside of that, many tests -- including ones that *don\'t* have the CLT applying - nevertheless have asymptotic distributions (e.g. consider the asymptotic distribution for an ordinary Kolmogorov-Smirnov test; the distribution of Dn approaches the [Kolmogorov distribution](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution); the CLT doesn\'t apply because you\'re dealing with the largest difference there)', ""Think of the target image here: https://wiki.socr.umich.edu/index.php/File:SMHS_BIAS_Precision_Fig_1_cg_07282014.png particularly at the imprecise and unbiased target. \n\nUnbiased means that on average if you took that small sample size again and again, the mean of those small samples would be the same as the population. It says nothing about just seeing one small sample. You don't know where you are on the target."", ""No.  The problem is low power.  Except if the sample size is so low that discreteness is a big issue.  If the P-value isn't quasicontinuous, then you don't know how to interpret it."", "">\tWhat are you randomizing there? Assignment to treatment? Or are you talking about random selection from a population of interest?\n\nAssignment to treatment, I apologize for not clarifying that.\n\n>\tI dont quite get what you mean by stronger there\n\nMy thought process was with randomization to treatment, we have unbiasedness for any sample size. But then with a small sample size, i would have to impose an additional assumption like normality of errors in all sample sizes rather than rely on asymptotic normality for usual z/t tests and inference.\n\nSo permutation tests are fully distirbution free and work in finite samples? If the sanple wize is very small, is it still 'valid' (i don't know whT the correct terminology is, but I mean something along the lines of it it maintains sound statistical properties/is not theoretically wrong) but type 2 errors just become a large concern?"", ""> i would have to impose an additional assumption like normality of errors \n\nYou'd only need that\\* if you're using a test that assumed normality. \n\n\n> So permutation tests are fully distirbution free and work in finite samples? \n\nYes (given the assumptions hold for the specific one you're doing), and yes, given your specific alpha (chosen from the available levels\\*\\*), it will have the usual properties it's designed to have.\n\n> maintains sound statistical properties/is not theoretically wrong\n\nThe specific properties for a hypothesis test are the significance level (do you get the one you chose) and the power curve (e.g. is it reasonably high for effect sizes you care about at your sample size)\n\n>  If the sanple wize is very small, is it still 'valid' \n\nThe available significance levels may become limited if sample sizes are extremely small. For example n1=5 vs n2=3 has a smallest two-sided significance level of 0.0357 for any permutation test\\*\\*\\*  because there's only ^(8)C = 56 combinations of 8 distinct (i.e. all untied) values into two groups of 5 and 3, so if you wanted to conduct a test at the 0.5% level, you're out of luck; if you fix the sample sizes at 5 and 3 you really need some sort of continuous parametric distributional assumption there to get exact small significance levels, but they're only exact if the parametric assumption is true)\n\nIn general, don't use extremely small sample sizes if you don't want to make a parametric assumption that your significance level may have sensitivity to. You can either compound your low power problems by taking a much smaller alpha (either knowingly, with a permutation test, or unknowingly with a test under some parametric assumption) or you may risk inflating alpha (again, unknowingly). With something like a two sample t-test, the lower alpha is much more common, but more generally it depends on the test and the specific kind of violation\n\n---\n\n\\* (approximately, in some specific sense depending on the specific situation, where the degree and form of non-normality that you might choose to tolerate for its impact on your type I error rate and power would vary from test to test - and likely from situation to situation)\n\n\\*\\* By relying on permutations of the treatment labels (/group labels) there's only a finite number of permutations. Randomization to treatment should in normal circumstances satisfy the assumptions for this class of tests.\n\n\\*\\*\\* and nothing any closer to 5% either. Assuming you're doing the equivalent of doubling tail probability for the other tail it's 3.57% or it's 7.14% (in some cases the exact next available alpha could be higher depending on your specific permutation statistic)."", ""If you somehow know what the true population distribution is (rare), then you wouldn't necessarily need to rely on theorems like CLT, which is a phenomena that occurs regardless of what the population distribution is.  Of course, this theorem involves sampling, and the sample size has to be big enough for the centralizing, normal distribution behavior to occur.  Like throwing darts at a wall, assuming all throws happen under similar conditions.  You would need to throw it a certain number of times in order to see what target your throws are centralizing on.""]"
[Q] Is there a metric that measure inter-rater reliability for a set of rankings?,"I'm familiar with inter-annotator agreement (or inter-rater reliability) metrics for data that has categorical annotations, but what about a set of data samples that are ranked by several annotators? Would measures like Cohen's kappa still apply here?

The specific context that I'm talking about is within machine learning (text generation to be precise). For a set of items, several machine learning models generate explanations about what the items are in textual form. A set of human annotators then rank the explanations made by the different models.

There seems to be some subjectivity and disagreement among the annotators, but I'm wondering if there's a way to quantify that. Thanks.",12y2x5t,Seankala,1682384183.0,10,0.92,"[""I have a few work on this in the quality of experience for image/video domain. From what I understood from your explanations I think those might be applicable to your scenario. I can also share some state of the art from our domain that I think might work for your case. If you want to further discuss, I can give you my university email and we can chat further but below I will give a quick summary and some links to the works I am referring to.   \n\n\nLet me start by clarifying a few things to see If I understood your problem correctly.   \nYou have a tabular data, where each row represents a unique rater, and each row represents a unique stimulus. You know that you can use Cohen's kappa can be used to assess the reliability of raters (each row, cumulatively) but you are wondering if we can quantify the variance in stimulus ratings with similar metrics.  \n\n\nIf this is the case, we often refer these terms in Quality of Experience for image/video domain as follows:\n\n* subject inconsistency/ subject uncertainty (sometimes coupled together with a subject bias) / subject reliability : This terms refer to how reliable is each rater/participant in a subjective experiment.\n* content ambiguity: This term indicates how ambiguous or difficult to rate a stimulus is.   \n\n\nIn this context, I have a model (currently under peer review, so I prefer not to share the code publicly here) that can estimate both of the terms above. On another front, Netflix Sureal package has most of the state of art (ITU recommendations) implementation in its github page, including the one that they propose (often called MLE-CO and MLE). However, many of these models are not capable of estimating the content ambiguity term. I understand that this is what you are looking for. If true, the only option in Netflix Sureal package on the github (link below) is MLE. you can use this model by following the instructions. The model expects a json file, but should be straightforward to reformat your data into the required format.   \n\n\nNetflix Sureal repo: [https://github.com/Netflix/sureal](https://github.com/Netflix/sureal)  \n\n\nBest of luck,"", 'You can compare any two rankings using Kendall\'s Tau or Spearman\'s Rho. Let\'s compare the top eight rankings from the [MLB Power Rankings](https://www.mlb.com/news/mlb-power-rankings-for-2023-week-4) and [538\'s MLB rankings](https://projects.fivethirtyeight.com/2023-mlb-predictions/) as an example.\n\n`library(conflicted)`  \n`library(tidyverse)`  \n`dta <- tibble(`  \n  `teams = c(""Rays"", ""Braves"", ""Mets"", ""Brewers"", ""Yankees"", ""Astros"", ""Blue Jays"", ""Dodgers""),`  \n  `power_ranking = 1:8,`  \n  `fivethirtyeight = c(2, 1, 5, 7, 6, 4, 8, 3)`  \n`)`  \n`# Kendall\'s tau`  \n`kendalls_tau <- cor(dta$power_ranking, dta$fivethirtyeight, method = ""kendall"")`  \n`cat(""Kendall\'s tau:"", kendalls_tau, ""\\n"")`  \n`# Spearman\'s rho`  \n`spearmans_rho <- cor(dta$power_ranking, dta$fivethirtyeight, method = ""spearman"")`  \n`cat(""Spearman\'s rho:"", spearmans_rho, ""\\n"")`  \n\n\nYou could compute pairwise statistics to see how each one of your raters compares to the others. If you plotted the pairwise comparisons in a heatmap, you might find a rater who always seems to disagree with the others.\n\nIf you want everything organized in a single test, you could try Intraclass Correlation. I haven\'t used this one before, so YMMV.\n\n`dta <- tibble(`  \n  `teams = c(""Rays"", ""Braves"", ""Mets"", ""Brewers"", ""Yankees"", ""Astros"", ""Blue Jays"", ""Dodgers""),`  \n  `power_ranking = 1:8,`  \n  `fivethirtyeight = c(2, 1, 5, 7, 6, 4, 8, 3),`  \n  `power_ranking_reversed = 8:1`  \n`)`\n\n`library(irr)`  \n`icc_results <- icc(select(dta, -teams), model = ""twoway"", type = ""agreement"", unit = ""single"")`  \n`icc_results`']"
[Q] How to identify skewness form unique boxplots?,"I'm working on improving my knowledge in statistics for future careers of interest and am self-teaching myself. I read online through various resources that with boxplots, you can identify skewness with the location of the median and the whiskers.

Right skewed: If the median is closer to Q1, and the lower whisker is shorter than the higher.  OR if the box itself appears to be positioned more to the left of the overall graph (i.e. left whisker is smaller, right whisker is larger).

Left skewed: If the median is closer to Q3, and the higher whisker is shorter than the lower. OR if the box itself appears to be positioned more to the right of the overall graph (i.e. left whisker is larger, right whisker is small).

Normal: If the whiskers are mostly even, and the median is in the middle.

**My question is:** What if the median is closer to Q1, *but* the lower whisker is longer than the higher? And same with if the median is closer to Q3, and the higher whisker is longer than the lower?

Can the whisker length cause a distribution to ""level out"", making it more normal even though the median is closer to a Q1 or Q3?

I know QQ plots and histograms offer additional perspective, I'm seeing if there's anything I'm missing with identifying skewness with boxplots before I add those.",12xxibu,serpx,1682372376.0,0,0.25,"[""1. Not all asymmetric distributions are clearly left or right skew. Different parts may give different indications, there may not be a single descriptor that's clearly capturing it overall. In other words, it's quitepossible to have a population distribution with features like what you're describing for the boxplot.\n\n2. Boxplots are a fairly poor way to judge shape in general. \n\n   You can often get some idea, but -for example- it's perfectly possible for a clearly skewed sample to yield a boxplot that is either symmetric or even skewed the wrong way.\n\n  Its shape  information is very coarsely measured. \n\n3. A boxplot with contradictory skewness  indications is not necessarily telling you that the population distribution is  nearer to normal than one where they all point the same direction. Sometimes, sure, but it's  easy for it not to be the case.\n\nIf you want to judge shape, plots that show all the data are better choices. Boxplots are more valuable for showing many groups, where quick cross-sample comparisons of location and scale are useful. There's a decent summary of some of the issues raised in this answer over [here](https://stats.stackexchange.com/a/96556/805)."", 'Thank you for taking the time to throw in your thoughts.\n\nHonestly, what I feel that I\'m learning from boxplots is -- as you say -- they\'re not the best with identifying distribution.  It feels more subjective than a histogram and QQ plots, and that a boxplot with a ""unique"" shape could be anyone\'s guess in regards to what its distribution likely is.', 'I updated my earlier post by editing in a link at the end, which I think makes some useful points and has an explicit example of how a single boxplot could correspond to some very different shapes.']"
[E] What to Look for in MS Biostatistics Programs,"So I'm considering an MS in Biostatistics and I'm curious about what elements of programs I should be looking at. Right now I'm looking at some in-state universities which are decently ranked, and I'm not sure about what they offer and if I should look elsewhere. 

Some extra context: I'm in the US, I'm a traditional student - moving straight from undergrad to a masters, and I'm not currently interested in a PhD, though I'm also not ruling it out.

Here are my main questions:

1. **What's the deal with funding?** I've seen conflicting advice ranging from 'prioritize funding above all else' to 'expect to pay so go somewhere cheap' and I'm not sure what end of the spectrum is more realistic, and if funding is even something I should consider. \[The universities I'm currently considering are cheap, but offer no funding\]
2. **Are there key courses/topics I should be looking for in coursework?** If I'm getting a degree I'd want it to cover everything relevant, so are there certain topics or classes that are necessary/especially helpful (in your opinion) that not every university covers? (and does applied vs. non-applied matter?)
3. **Is there any other metric I should be judging programs by?** Faculty, networking/connection to opportunities, related degrees? 

I recently switched from looking at an MPH in Epidemiology to an MS in Biostatistics (if the extra math goes well), so I'm just trying to readjust my expectations and what I'm looking for. Thanks in advance!",12xvwqz,273owls,1682369316.0,1,1.0,"[""(1) Depends on your available resources and how debt-averse you are. I probably wouldn't borrow 200k to do an MS in Biostats, personally. If you get some kind of funding/scholarship then by all means, that's lovely. If you have a family member or spouse helping with tuition that's great too. \n\nOn the other hand I wouldn't fret if you don't go to a top 10 program or whatever. Once you are in the industry, people don't really care that much where you went to school. When I'm considering to hire analysts, I am much more interested in their industry experience and that they have the right educational background, not where they went to school per se. \n\n(2) Assuming you want to do an MS and go into industry, I'd try to take as diverse of a course schedule as I could. You aren't going to emerge from an MS program already knowing everything you need to know. As an example, your clinical trials class will teach you nothing about CDISC standards or dealing with questions from the EMA. Because the person teaching the course has never gotten a drug approved more than likely (they are an academic, not a drug developer).\n\nThat's ok, just be realistic that you aren't going to learn everything you need to know in school. \n\n(3) I would consider location. The classical Biotech hubs in the US are Boston, NYC, DC, Raleigh, Seattle, SF, and San Diego. Chicago/LA also have opportunities but less so. There are also some very specific employers in other areas not related to the above (for example, Eli Lilly is headquartered in Indianapolis). \n\nYou will have more networking opportunities closer to where your school is physically located. It doesn't mean you can't go to school on the East Coast and then find a job on the West Coast. But it does mean things like the senior leadership at a company like Seagen probably has at least 1 or 2 UW alumni that were probably PhD students of professors at UW."", 'I would ask programs how easy it is to get a graduate research assistant job for a MS student. Experience working with real world data, like for a clinical trial, is a big plus on job apps after you finish grad school.\n\nThe location comment above is a good one.\n\nAnother thing Id look for is what programming language the classes in the program focus on. Personally, Id look for programs that focus more on Python than R or SAS because I think that opens up more job opportunities since thats becoming more of the preferred programming language. Unless youre hell bent on working in pharma after grad school which is mainly SAS.']"
[Research] Prepping for ODSC (Data Science conference)….how?,"
I work as a data science but feel like I have some significant gaps in my knowledge of data science and what not. 

I am attending this conference in a few weeks and should have a few solid days to study a bit. **Anyone have tips on how to best prepare?** I really want to make the most out of this conference, learn things, and implement it/convey back to my team. 

They have a boot camp, but it is quite expensive (and I already had to get the conference tickets plus travel arrangements). Hence, Id like to keep it free to relatively cheap (Udemy cheap). If people have suggestions,please let me know. 

Something well rounded (a little of everything, but not everything in detail) might be the best way to go.",12xtgvb,DoctorQuinlan,1682364601.0,2,1.0,"['Brush up your basics and make short notes of all talks you attend(dont wait to go back and do it, you either wont or will not remember everything). The main goal here is one to network and two is to understand how this field is being used by professionals across industries.\n\nP.S - I am attending the event as well. Guess we can meet each other there.', 'Fair points. What did you think of day one? Maybe Ill see you there tomorrow', 'Ah it was grand! Getting to know how people solve issues we usually face is interesting.', 'Exactly! What were your biggest takeaways or the highlights of what you learned? Im curious too, did it unlock something in what you can bring back to your workplace?']"
[Research] Advice on Probabilistic forecasting for gridded data,"We have a time series dataset (spatiotemporal, but not an image/video). The dataset is in 3D, where each (x,y,t) coordinate has a numeric value (such as the sea temperature at that location and at that specific point in time). So we can think of it as a matrix with a temporal component. The dataset is similar to this but with just one channel:

&#x200B;

[https://i.stack.imgur.com/tP1Lz.png](https://i.stack.imgur.com/tP1Lz.png)

&#x200B;

We need to predict/forecast the future (next few time steps) values for the whole region (i.e., all x,y coordinates in the dataset) along with the uncertainty.

&#x200B;

Can you all suggest any architecture/approach that would suit my purpose well? Thanks!",12xpj5o,microlifecc,1682356389.0,40,0.99,"['You could use a space-time Gaussian process or spatiotemporal CAR model.', 'Not a straightforward problem. I know one example where this was successfully done \n\nSee https://icenet.ai', 'Do you know something about the spatial / temporal dynamics of your data? If so, maybe a state space model / partially observed Markov process might be something to consider.', 'Dumb dumb question - why not use a vector auto regression (VAR) model as a first-pass?\n\nEdit: when I said dumb dumb I meant *me* not OP, I have no idea what the best approach is or if VAR is even appropriate? Not sure why all the down votes here.', 'There are a number of ways to approach this problem, but you need to provide more details to help suggest an appropriate solution. Do you know something about the dynamics of the system, such that you could construct a simplified surrogate model? Is there a high level of auto-correlation? Is your data measured or modeled output? Can you perform data assimilation with observations? This is a very ""standard"" problem (think meteorology), but there\'s not enough detail provided to give a reasonable approach', ""I'm not an expert in this field, but I've lately been learning about Kalman Filters, which are used for tracking trajectories when their data are noisy. KFs are required to predict the position of an object, usually based on kinematic equations, then weigh it's predictions against the noisy measurements and decide how much of each to believe. What you have seems similar enough to a tracking problem that a KF should be able to handle it. As to how far into the future you want to predict, that's a whole new question, but a KF should be able to get you started, and reliably so. All the best with that!"", 'Came here to say the same, spatio-temporal GP. If youre talking a small area, Cartesian coordinates is probably ok, but large enough and youll want to model on the surface of a sphere.', 'Hi u/No-Requirement-8723, thanks for the response. I will look into this.', ""Hi u/Astheny, thanks for the response. I actually don't know much about the underlying dynamics. I just have the raster data and am trying to build a data driven pipeline."", 'Hi u/SearchAtlantis, thanks for the response. I will look into this.', 'Hi u/a6nkc7 and u/mikelwrnc, thanks for the response. Are you suggesting something like [this](https://github.com/AaltoML/spatio-temporal-GPs)?', ""That's the right class of model. My guess without reading it is that the paper you linked gives a fast approximate solution for that type of GP. If your data is not too large, you can just use off-the-shelf software like Stan or PyMC for fitting the GP."", 'Yup, a variational approximation. For a fast full-Bayes approximation-to-GP approach Ive been using with success in Stan, see [here](https://arxiv.org/abs/2004.11408) (and tutorial for the 1D case [here](https://avehtari.github.io/casestudies/Motorcycle/motorcycle_gpcourse.html))']"
[Q] Question about correlation between two time-series variables,"Hi guys! I am trying to measure the extent of the relationship between two time-series variables. These two variables are also categorized by country (8 countries in total). 

 I don't know what procedure or method I should use. So far I have made 8 line charts (representing each country) showing how these two variables moved over time. Is there any other statistical method or graphical representation to show that?",12xow3m,Rajsuomi,1682355072.0,1,1.0,"[""Dynamic Time Warping is a similarity measure that's specifically used to capture similarity between different time series.  Sort of like Euclidean distance, but better suited to capture sequential dependence.  \n\nIt's a common pre-processing step for other techniques in time series clustering and classification because it's a helpful way to transform time series data into something more meaningful.""]"
[D] Reverse Bait and Switch in Statistics,"This  post by an economist argues that people tend to change stastitical  questions to harder ones, when the easy questions give uncomfortable  answers. Thoughts?

[https://unreasonabledoubt.substack.com/p/the-gender-pay-gap-and-the-reverse](https://unreasonabledoubt.substack.com/p/the-gender-pay-gap-and-the-reverse)",12xomwf,heraklit8,1682354539.0,0,0.43,"['> the gender pay gap disappears when controlling for occupation, \n\nThe link is to a tweet of a picture that does not give the source, but from the research Ive read, this is not even close to true. The gap is reduced (e.g. a ratio of .9 v .7), but interpreting the 10% gap as the true gap and the 30% as biased ignores the fact that women are less likely to be hired for higher-paying positions even when their qualifications are equal (see the randomized resume experimental results). \n\nIn other words, these two statistics tell separate parts of the story. The first part (30% difference) could be considered the overall effect, while the second part (10% difference) could be considered the adjusted effect within profession. \n\nI disagree with the authors take on this. You should never stop at the first statistic that agrees with their opinion. Always continue to think about and test hypotheses on the meaning of the statistic.', ""Stats prof here.  If you throw in enough covariates you can... what? create a ridiculous model. The first assumption of any statistical test is the model is correct. \n\nIf I'm motivated to remove gender as a predictor, all I have to do is get a few covariates that correlate with gender. Same crappy approach that let people claim smoking doesn't cause cancer when you control for A, B, C, ... X, Y, Z. This isn't bait n switch, it isn't clever, it's just algebra and crappy misuse of the methods. \n\nI saw Jordan Peterson make this claim a while back and looked into it. Don't get me wrong, there's parts of his work that I think are great. Clean your room! But even if he's a good clinical psych (IDK), he's not a good data scientist if he fell for this amateur trick.\n\nI do believe we're on our way to gender equity because of the large advantage girls have over boys academically in K-12 and women have over men at university. We're not there yet. Whenever it happens, you have to know whoever is the president/prime minister will make a lot of political hay over it."", 'Completely agree on everything up to test hypothesises. Always good things to be done in just presenting posteriors or considering other decision rules.\n\nBut yeah, I agree, its like the simpson paradox example. Even in that story, yes its not that the departments prefer male candidates, but the school is clearly not providing enough funding to the female preferred departments. 2 statistics for two stories, both true and both being useful for differing decisions.']"
[Q] What to do if my descriptive statistics don’t support a hypothesis but the p value > .05 ?,,12xmr5i,meowmeowdilemma,1682351422.0,0,0.33,"['Huh? Can you clarify or provide more detail?', 'Please clarify (by editing the question text). It sounds as if you may be confused', 'It implies that your sample size was too small to validate the claim. \n\nP-value logic requires something to be quite extreme to overcome the role chance will ordinarily play in data when the null is true. \n\nThe most you can say when something is significant is that if the null is true, then the observed data is surprising, but quite possible due to chance. When it is not significant, then you cannot rule out the null as true as the result is not unexpected. \n\nYour results were not extreme enough.', 'This feels like a question from a multiple choice test, without the options being provided in the post.', 'You take it as the first step in your power analysis and use it to justify more funding/access to more samples so that you can prove your hypothesis or more confidently reject it. \n\nYou never prove the null, you just reject or fail to reject it.', 'p > .05 indicates no meaningful difference', ""P > 0.05 implies difference is not statistically significant. This also means do not reject the null hypothesis, ie there is not enough evidence against the null hypothesis. But this does not mean accept the null hypothesis ie you can't say the two groups are similar. Absence of evidence is not the same as evidence of absence."", 'Well, if you are seeing no difference between two groups in a graph but their means  are statistically different then you might need to look into your codesome kind of typo perhapsotherwise it makes no sense. \n\nFor a better answer we need more details', 'Fight another day', 'Right. so if I had to reject my hypothesis due to the results not supporting it, but the p value is > 0.05, does that mean I should disregard my rejection? does that make the hypothesis inconclusive because the data isnt statistically significant, or because of the small sample size?\n\nMight have to get you to dumb it down for me in baby terms, sorry about that.\n\nthanks for your help!', 'I\'ll always appreciate my stats prof for telling me ""If p is low, H0 gotta go ""', 'All you can know is that if you set your significance level at .05, you cannot reject the null hypothesis. You cannot distinguish between it being due to chance and the null is false and it being due to chance and the null is true. You can know how much statistical power your experiment had, but it is a bit problematic because it is after the experiment is complete. \n\nYou are facing the Ivermectin problem. When Covid began, doctors began trying everything in the pharmacopoeia to see if it had an effect. Ivermectin had some legitimate initial support. Its descriptive statistics looked good. It was also effective in a lab dish at ten times the lethal dose for humans. As the sample size became large and methodological controls were placed on it, it became obvious that the initial uncontrolled finding was spurious. However, until repeated experiments happened, people couldnt know that. Indeed, it could have gone the other way. With other substances, it did go the other way. \n\nLet us ground you a little better. First, you need to choose a set of rules to perform inference in. It turns out you can choose different rules as long as you choose them before you run the experiment. \n\nThe oldest of the p-value rules come to us from Ronald Fisher. He had a null hypothesis but no alternative hypothesis. Before you run the experiment, you decide up front what p-value you will use. If it is less than your value, it is said to be statistically significant. Otherwise, you fail to reject the null hypothesis and you go onto your next task in life.  You have discovered nothing. End of discussion. You report the p-value in the journal and each reader chooses the value at which they think it is significant. \n\nIn this construction, descriptive statistics are only interesting if they are significant to you. If they are, they matter. If they are not, then go on with life. A p-value is conceived as the weight of the evidence against the null. \n\nThe second rule would be Pearson and Neymans rules for inference. You choose a cutoff, called alpha, and set a sample size large enough for statistical power, and perform an experiment. If it is in the rejection region, then you treat the descriptive statistics as the best provisional estimate of the population parameters. If it is in the acceptance region, then you treat the null hypothesis as if true and treat the descriptive statistics as if a random error. You still report them for completeness but there is no strong reason to do anything with them. \n\nP-values do not matter here. If your p-value is p<.04 and your cutoff is p<.05, then they are of equal weight. The magnitude of the p-value does not matter, merely its location inside or outside the acceptance region. \n\nThere is a third rule. Imagine that you did not need to infer something, instead you needed to decide something. You still must use the Pearson and Neyman rules but they control your behavior. If it is in the rejection region, you behave as if the null is false. If it is in the acceptance region, you behave as if it is true. Inference and decisions are perfectly united except that the first one says here is information, while the second says do this.  \n\nIt is not significant, ignore your descriptive results. \n\nIf you believe the null really is false, determine the sample size that would be large enough to detect your effect and do the experiment again. Otherwise, follow Fisher, report the results and move on with life.', 'Your p-value being greater than .05 means you cannot reject the null. Im confused what youre saying by rejection my hypothesis due to results not supporting it. Your p value is how you determine whether you can reject the null\n\nIt sounds like you may be confused?', 'so if I had to reject a hypothesis based on the results not supporting it, but the p value was greater than 0.05, does that mean I disregard my initial rejection because the results werent significant? if so, does that leave my hypothesis inconclusive?', 'Your comments are kind of contradictory. You decide whether to reject the null or not based on whether the p value is above or below the critical value of .05', 'It means you fail to reject the null at the 5% significante level. It does not mean that the null is true or false, you just dont have statistical eveidence. It all depends on your sample size, and the size of you p-value. The p-value generally just indicates the probability of observing an equally extreme value (like you observe in your sample) in case the null hypothesis (and the assumptions about the distribution of the test statistic) is true. So is p = 0.1, you would expect to see equal or more extreme descriptive results 1 out of 10 times drawing a random sample, assuming that the null is true and you know its distribution.']"
[Q] Should I choose University of Toronto Statistics MSc over a cost-comparable American program?,"Hi, I'm a current American statistics senior debating between two master's programs, and I was interested in some input.

One is the University of Toronto Statistics MSc. It is 2 semesters, $21k USD in tuition, coursework only, 4 mandatory classes that cover material I have already taken, and 4 electives.

The second is an accelerated program at a mid-ranked large American public university. I have taken 2 graduate level theoretical courses and 2 graduate level applied courses already. It is 2 semesters, $19k in tuition, 3 electives per semester, in a low cost of living city.

I have already accepted the American program as a baseline. My specific question is: by your experience or knowledge, is the UofT Statistics MSc program high-quality or reputable enough to accept over the American program? I am largely interested in working in industry, though I also have a not insignificant interest in pursuing a PhD.",12xmawn,falanak,1682351035.0,2,0.67,"[""Canadian checking in. Some thoughts:\n\n* UoT is one of the best in Canada for graduate studies in general and also for Statistics. You could say it is THE best and few would argue. It's hard to truly define.\n* Toronto is one of the highest CoL in Canada. Though, you make freedom dollars so that might not be an issue.\n* Are you trying to get Canadian citizenship? It's WAY better for taxes to have Canadian (or literally any other country) citizenship than US citizenship if you plan to work abroad (not including the US).\n* If you plan to work in Canada, that's cool too, we have a great work/life balance. But you will be considerably underpaid in Toronto compared to many places in the US.\n* The pay gap is even larger if you want to enter tech or machine learning.\n* However, if you want to stay in academia, UoT is a solid place to do your MSc. Keeps plenty of academic doors open.\n\nOverall, the Stats will be great and you'll learn enough to be dangerous, not the perfect choice for optimizing earning potential, but not a bad one either."", 'You should follow your heart and listen to the wind.\n\n&#x200B;\n\nJust kidding. What are the placements from both universities? In terms of pure ROI, what is the tuition plus COL from one school to another? Will you be taking out loans?', ""U. Toronto has a pretty strong reputation, so, other things being equal, you might learn more / have better teachers / meet more interesting people while in school, and get somewhat better opportunities when you look for a job.\n\nI'm not too hung up on school status, but it sounds like the programs are comparable on other dimensions.""]"
[Q] Appropriate to use Fleiss' Kappa on data from 4 raters - answer doesn't make sense to me,"Hello, thanks for reading. I don't have a good stats background, so I appreciate the help.

**Background**: I have data from 4 raters who evaluated 9 different articles using 2 different quality assessments: [PEDro](https://pedro.org.au/english/resources/pedro-scale/) and Methodological index for non-randomized studies (MINORS). These assessments have questions related to the study that ask if certain things are found in the study (e.g. blinding). PEDro is 11 questions that are scored as 0 or 1  (criteria is absent or present). MINORS has 12 questions (we only used 1-8 due to the studies not being comparative) that are scored 0, 1, or 2 (criteria is absent, partially met, fully met). I am trying to assess the inter rater reliability of the scoring to provide a numerical representation of the degree of agreement between the 4 raters.

**Questions**: Is Fleiss' kappa appropriate for this? The scales of 0 or 1 and 0, 1, or 2 are categorical if I'm not mistaken (specifically, this would be ordinal data?). From the [wiki](https://en.wikipedia.org/wiki/Fleiss%27_kappa), Fleiss' is suitable due to the number of raters being >2; however, the following quote makes me question this because the raters were all the same:

>Fleiss' kappa specifically allows that although there are a fixed number  of raters (e.g., three), different items may be rated by different  individuals (Fleiss, 1971, p. 378). That is, Item 1 is rated by Raters  A, B, and C; but Item 2 could be rated by Raters D, E, and F. The  condition of random sampling among raters *makes Fleiss' kappa not suited  for cases where all raters rate all patients*

Does this mean I need a different measure?

**Follow up:** (this may be more appropriate for an R subreddit - happy to ask there)

I have tried to calculate Fleiss' kappa in R but the numbers don't make sense to me on a few of the cases. I'm wondering if I can't apply Fleiss' kappa to an assessment with only two categories.

I have files with the data laid out as so: (print out from R studio)

        Rater1 Rater2 Rater3 Rater4
    1       1    1      1    1
    2       1    1      1    1
    3       1    1      0    1
    4       1    1      1    1
    5       1    1      1    1
    6       1    1      1    1
    7       1    1      1    1
    8       1    1      1    1
    9       1    0      1    1
    10      1    1      1    1
    11      1    1      1    1

My code:

    mydata <- read.csv(""C:\\mydata.csv"")
    kappam.fleiss(mydata, detail = TRUE)

There are only two discrepancies between 4 raters yet the result is:

     Fleiss' Kappa for m Raters
    
     Subjects = 11 
       Raters = 4 
        Kappa = -0.0476 
    
            z = -0.387 
      p-value = 0.699 
    
       Kappa      z p.value
    0 -0.048 -0.387   0.699
    1 -0.048 -0.387   0.699

If kappa = 0 is the result of pure chance, how is data with only two different values almost the same as chance?

EDIT: MINORS scale questions:

    1. A clearly stated aim: the question addressed should be precise and relevant in the light of available literature
    2. Inclusion of consecutive patients: all patients potentially fit for inclusion (satisfying the criteria for inclusion) have been
    included in the study during the study period (no exclusion or details about the reasons for exclusion)
    3. Prospective collection of data: data were collected according to a protocol established before the beginning of the study
    4. Endpoints appropriate to the aim of the study: unambiguous explanation of the criteria used to evaluate the main outcome
    which should be in accordance with the question addressed by the study. Also, the endpoints should be assessed on an
    intention-to-treat basis.
    5. Unbiased assessment of the study endpoint: blind evaluation of objective endpoints and double-blind evaluation of subjective endpoints. Otherwise the reasons for not blinding should be stated
    6. Follow-up period appropriate to the aim of the study: the follow-up should be sufficiently long to allow the assessment of the main endpoint and possible adverse events
    7.Loss to follow up less than 5%: all patients should be included in the follow up. Otherwise, the proportion lost to follow up should not exceed the proportion experiencing the major endpoint
    8. Prospective calculation of the study size: information of the size of detectable difference of interest with a calculation of 95% confidence interval, according to the expected incidence of the outcome event, and information about the level for statistical significance and estimates of power when comparing the outcomes.

&#x200B;",12xiiwo,SleepingInMyF150,1682347757.0,4,0.84,"[""First, it's not immediately clear that this is an inter-rater reliability question. Whether or not certain things are found in these studies sounds like a matter of being correct or not.  In other words, there's no guesswork from what I understand. Additionally, are these raters independent? If not, the guesswork may be even less of a factor.\n\nThis is an important point because Fliess' Kappa adjusts for chance, which is meant to more specifically account for guessing among raters who are not sure but must input some score. Paradoxes can arise from the statistic, and those are well documented, particularly if assumptions aren't met from what I understand. Try looking through a few articles.\n\nAdditionally, I've seen a bit of pushback at this statistic as some suggest that simply assessing sensitivity and specificity is appropriate without adjusting for chance. This approach would make your analysis more akin to difference in proportion 'correct' per rater relative to accuracy or the overall accuracy of your review system."", ""OK so you're right about PEDro, there should be correct answers to the questions (e.g eligibility criteria were specified, subjects were randomly allocated to groups, allocation was concealed). The MINORS has a few more nuanced things but definitely most should be either correct or incorrect answers. I have posted the questions for MINORS in an edit above. I have no doubt that some of the answers are different because the raters did not understand the assessment that well (we're students with little research background). \n\nI was advised to do a weighted kappa on this data but IDK how closely that person understood the project. Is there something else I should do or should I just leave this out. We used the modes for reporting final assessment scores."", ""Maybe it's best to back up a little bit. What is the utility of this analysis? In other words, regardless of the person who told you to use kappa, what is the primary question and what actions will be taken based on its answer?"", ""We are performing a systematic review of 9 articles. We wanted to assess the methodological quality (or risk of bias) of each article so we picked these two assessments (PEDro and MINORS) and each person (4 total) filled out the assessment to the best of their ability (again, we don't have much experience). From the assessments, we compiled the data and made a table of the modes as the final score for each article, which we intend as a general indicator of the methodological soundness  of the research article. This was to evaluate the included articles to develop statements regarding our confidence in the research findings. My professor wanted us to provide some calculation of agreement in scoring between the raters for each article and mentioned a weighted kappa. Now it kind of seems like this isn't that useful...\n\nEDIT: in the original publication of MINORS they report a kappa in table 4: https://www.unisa.edu.au/contentassets/72bf75606a2b4abcaf7f17404af374ad/6f--minors1.pdf"", ""Thank you for that summary. It may be that you simply report the statistic that you've computed because your professor suggested it. You **might** include the caveat that the statistic has presented a bit of a paradox, which is relatively common for this statistic. Depending on where your professor wants to go with this conversation (how statsy), it could be a very valuable discussion. One point to address is that there are only two options per item (0/1) which increases the possibility that raters will agree by chance. That point is inherent in the mathematics of the statistic.\n\nInterrater reliability metrics like the one you've used are designed to account for chance agreement in contexts where a lot of subjectivity is at play. But here, there are only two potential answers per item, and whether or not they are correct doesn't seem to be subjective. \n\nIf there were 11 questions each about some abstract takeaway from the article that each rater was to score from 0 to 10, then you would find a more appropriate context for this statistic because chance agreement would be much lower.\n\nAnother option is to use a joint probability approach that doesn't account for chance agreement. That will be more aligned with the proportional difference between raters."", 'Your edit makes sense. There were many raters that lead to the results in those tables with larger scales (0-7, eg) which significantly lowers the probability of chance agreement.', ""OK, the paradox thing came up in my googling, but was thinking it more likely that I misunderstood the application rather than it being real. Thank you so much for your responses, I'm out of my depth here lol. I now feel confident enough to go to my professor and present the issue.""]"
[Q] Difference between time-varying variables and time-varying coefficients?,"Hi, Im working on a Cox PH model for a survival analysis and Im having a hard time understanding the difference between time-varying variables and time-varying coefficients. Can you provide me a brief explanation?",12xf66u,hawkeyeninefive,1682341616.0,11,0.88,"['I am also learning about it now. My understanding is that time varying variables are variables that change over time (for exmaple, you can have a variable that is ""suffered a heart attack"" that starts at 0 and then if the individual suffers one at time t becomes 1 for any time greater than t). If you are dealing with this only, the proportional hazards assumption stil holds and you can make the cox regression with the time dependent variables taken into account (don\'t put them as constant variables known from the beginning).\n\nThen, having time varying coefficients is different, because then the PH assumption does not hold any longer. This is not as clear to me yet, but this can be due to a number of reasons, and I think adding a frailty can help deal with that (since this can be caused by ommited variables). This last part maybe someone else is more knowledgeable with it, as I recently asked a question related to frailty related somewhat to this.', 'A lot of models at their center have a linear model, a sum of things that look like beta\\_i \\* x\\_i. (The cox model has some extra subscripts, but thats not important for the general idea here.) When we move to time-varying land, we have some choices. The most general being to make both beta\\_i and x\\_i functions of time, so the sum would have beta\\_i(t) \\* x\\_i(t) in it. But we could also assume one, or both, of those is constant in time. Consider the difference between beta\\_i \\* x\\_i(t) and beta\\_i(t) \\* x\\_i. One of these models a covariate ((independent) variable) which is constant but which has an effect that changes through time. The other models a covariate which changes through time, but where the per-unit effect of that covariate doesnt change.\n\nSay you know that the actual facility of treatment matters for patient outcomes, so you throw that into your Cox model. But, you also know that during the span of treatment, one of those facilities underwent a big policy change which could make a difference in that. The covariate is still a hospital ID, that hasnt changed, but the coefficient on it may now need to vary in time.\n\nOn the other hand, consider some sort of risk factor or comorbidity which can affect your patients outcome. The effect of this may not change over the course of the study (the coefficient could be constant), but if youre tracking whether a patient displays this you absolutely need to let the covariate vary in time (even if it is just an indicator variable for whether some condition is present).', 'Time varying variables: The numeric value of the variable changes over time. The effect does not change over time. Ex: A patients CD4 levels can change over time. The health implications of CD4 levels do not change over time; low CD4 levels in year 1 are just as bad as low CD4 levels in year 20.\n\nTime-varying coefficients: The values of a variable may or may not change over time. The effect of the variable changes over time. Ex: The the effect of CD4 levels diminish as a patient lives longer. Low CD4 levels are really bad in year 1, but dont make much of a different by year 20.\n\nI completely made up the medical example. That should give you an idea though.\n\nYou need to use the [counting process form](https://grodri.github.io/glms/r/recidivism) of survival models for both types of models. Its easier to think about like that anyway. Time varying modes have a single coefficient for a variable. Time varying coefficients are an interaction between the variable and time.', 'Thanks a lot', 'Thanks a lot', 'This was the clearest explanation, thank you!']"
[Research] Literature review articles: Where to submit them?,"Hello, 

Sorry if this sounds like publishing for the sake of publishing but as a phd student there are graduation requirements for me to fulfil. 

I am a phd student in statistics working on survival analysis and missing data. Over the last two years, I have written a lot of notes from papers I have read, some derivations and all that, literature review for quals. 

I am wondering if I were to compile it into a comprehensive literature review article, is it publishable / or can i submit it to a place like International Statistical Review? 

 Is there any other venues that accept review articles (I know I can possibly post it on arXiv) that you could recommend me? 

Thanks!",12xa4v8,vanhoutens,1682329527.0,13,0.92,"[""Without knowing your topic, it's tough to tell. However, there are so many opportunities - the classic is the BMJ stats notes. \n\nMost journals will take a lit review as long as it's reasonably rigorous."", 'Not sure if this would apply depending on what subfield youre in but thats what the APAs Psych Bulletin journal is, it publishes review papers in the field of psychology. Survival analysis is definitely used in the field and could be an appropriate topic for the journal. Not sure if they accept methods papers.   \n\nhttps://www.apa.org/pubs/journals/bul', 'Thank you for your suggestion! I was abit afraid since reviews are typically given by invitation to established people in the field!', 'Thank you so much! I will look into it, by any chance do you know if those review papers need to have a touch of psychology in them? While I might be okay for the stats, I definitely dont have the domain knowledge for the psychology aspect.', 'Im really not sure, but Im assuming if youre reviewing a type of statistical method it might have to be framed in the context of how its used in psych. But thats just my guess, Id look into the stats papers theyve published before and their submission requirements.']"
[Q] Is there an app for tracking random statistics over a long period of time?,"What Im exactly trying to do is, my grandma has a notebook and she writes down everything that happens every year. First day of snow, first day the snow is gone, first day the humming birds return, the first time the geese return from the south, etc. its pretty cool to see her have over 50 years of dates on this stuff. 
I would like to do the same using an app. 

Is there something out there that I could enter these types of things and then search it later? Say if 10 years I want to search first day I golfed each spring and then it would pull up all those dates. Rather than me having to flip through 10 years of calendars or notebooks.",12x5h9d,Kwild199,1682316571.0,0,0.5,"['Excel', 'MS Excel bru', 'Type it into an excel spreadsheet?', 'Every useful calendar software will let you search for events of a given type. A spreadsheet does that, too.', 'Google sheets']"
[Q] How can I conduct Cost-Effectiveness Analysis from Indirect Treatment Comparison?,"Hey everyone,

Thanks so much for checking out this post. There are a lot of elements at play, so I will try to ask my question as directly as possible.

Singh et al. previously published a network meta-analysis of randomized controlled trials (RCTs) that compared various intra-articular treatments for knee osteoarthritis (paper included in Google Drive link below). They analyzed 6-month follow-up pain and function improvements separately (via VAS and WOMAC scores, respectively, abbreviations below. WOMAC is an organ-specific system thats been developed and validated to assess knee function). However, they normalized their data and report it as a mean difference (MD) when compared to placebo. These results are summarized in Figures 2 and 3 from their publication, respectively. There are 4 treatments that they analyzed: PRP, PRGF, HA, and corticosteroid (abbreviations below).

Id like to know if and how I can use the MD values to perform an indirect treatment comparison (ITC) for these 4 treatments with a different type of treatment (treatment x). The common comparator would be placebo. There are a few RCTs for treatment x available currentlyfrom my understanding, I would need to perform a meta-analysis for treatment x in order to conduct ITC and subsequent CEA.

The ultimate goal would be to use the ITC results to then conduct a cost-effective analysis (CEA). From my understanding CEA requires EQ-5D data to calculate quality-adjusted life year (QALY). There is a mapping equation that has been developed by Bilbao et al. (paper included in Google Drive) that aims to convert WOMAC scores to EQ-5D specifically for the purpose of conducting CEA.

Here are my questions:

Is it possible for me to use the MD values from the Singh study in order to perform an ITC? 

If so, can I convert a derived value from MD to EQ-5D via the mapping equation in order to then perform a CEA? 

Is it correct that I would need to perform a meta-analysis for treatment x in order to conduct ITC and subsequent CEA?

Again, I really, really appreciate any help you can provide. I know these are very specific questions, and I can try to clarify any of these points as much as I can, given what I know. Thank you so much for taking the time to at least read this, you have been so helpful so far!!

Google Drive with publications referenced above:

[https://drive.google.com/drive/folders/1Lo\_IFVIvcD3W53k2YejfQPX2CtgrhwxY?usp=sharing](https://drive.google.com/drive/folders/1Lo_IFVIvcD3W53k2YejfQPX2CtgrhwxY?usp=sharing)

Abbreviations: 

VAS, visual analog scale

WOMAC, McMaster Universities Osteoarthritis Index

PRP, platelet-rich plasma

PRGF, plasma rich in growth factors

HA, hyaluronic acid",12x1hs0,pantaloonsss,1682306962.0,0,0.5,[]
[Q] Master of Applied Statistics vs Master of Data Science?,"Hello! Im really confused by the responses Ive read about this question, so Im hoping someone can help me? 

Ive read some people say that data science is a useless degree since apparently it can be a cash cow for a uni, but then others say that its more practical than a statistics degree so therefore more useful. On the flip side, Ive heard that a statistics degree will prepare you better for the theoretical and give you a leg up, but then others say its not really worth getting? IM CONFUSED! 

I really enjoy math and statistics  I find it very enriching. But I also love computers. I can program at an intermediate level in Java and python already, but I wouldnt mind exploring more of the technical side with data science. 

At the end of the day, I want to do the degree with the best job prospects and with the best  salary outlook. 

Which is more worthwhile? 

Based in Australia. Im transitioning from a different field, so itll be a transitionary post grad qualification into the masters.",12wqafg,incredibablybigspoon,1682284208.0,27,0.94,"['From my experience names can be misleading in these kind of programs. 2 ""masters in statistics"" from 2 different unis can give you very different preparation. Look into the program specifics, and also ask students if you can about how they feel about the program/what they want to do after the master to get a feeling for it. From what you say you\'d probably like a stats master that offers applied/programming courses as electives.\n\nI find that a strong theory and understanding of how things work will always take you longer. From data science students I\'ve met in some of my courses I get the feeling that they usually know many more methods and names, but then don\'t delve into why they work, so I\'d recommend stats master. Again, I\'m in stats myself so I am biased.', ""it maps to the professional world\n\ndata scientist is a newer and now larger career track ,  mostly applicable to medium/large Technology companies.  DS help product managers run experiments (A/B)  or they work to develop ML models for some specific domain  (eg fraud detection).  DS cirricullum is more coding,  more ML.\n\nApplied Statistician is an older professional specialization. They are more the people who biotech, pharma, and healthcare companies will call to help structure their trials and interpret results (and hack their p-values).   Also related to Applied Statistics is the work of Actuaries in insurance companies.  Applied statistics requires a stronger foundation in theory, experiments & bayesian statistics. It's likely less coding driven, although these days who knows."", 'The statistics degree will be far more worthwhile. If you can handle the math, this is the degree you should do.\n\nStatistics degrees are long established with curricula that are a known quantity. Data science degrees are... what exactly? 15 years ago data science meant ""statistics applied to e-commerce"", now it means anything from machine learning to business intelligence, and it\'s pretty cursory given the breadth of potential topics.\n\nYou won\'t learn to program in either degree, even if you do manipulate programming languages more in the data science one. Frankly, if you are even half as good at Java as you claim to be, I doubt you will learn anything about programming in either degree.\n\nI once made the mistake of showing interest in a data science master\'s from a top 10 US university. Even knowing nothing about me, they would not stop calling me. The emails were borderline predatory. Obviously a total cash cow and they were jumping on the hype train.\n\nData science as a descriptor feels like a race to the bottom. Only ten years ago it was the reserve of quantitative PhDs. Two years ago one of my undergraduate interns graduated into an analyst position as a ""data scientist"". Solid intern, but the job title has become meaningless because it can apply to a huge range of background skills and job descriptions, very few of which you (and me) will be actually qualified for.\n\nSource: too many years in Silicon Valley.', 'Our team is a data science team at my company. We are all called data scientists and we are under the chief science officer. My manager had me go through some resumes to fill out headcount and I was specifically told not to bother with those with MS analytics or MS data science on the grounds that their programs are usually not rigorous, designed to get people in and out, and they cannot pass our interviews. \n\nI dont agree with this viewpoint, but I think there is some merit to the statement as it is in fact true that data science students generally do not do well in our technical screening - which involves coding and statistical knowledge. \n\nSince you asked about job prospects, it would be reasonable to hear how a hiring manager thinks, even if you disagree with the sentiment.', 'For the record, I work as a data scientist but my degrees are in applied math/applied statistics and were very heavy on math, pretty light on programming.\n\nImo it\'s a lot harder to teach yourself theoretical statistics than it is to teach yourself Java and Python. If you can already program decently in those languages you\'re fine for data science, but the ""data science"" masters won\'t teach you the theoretical background for either programming or stats and I suspect isn\'t going to mean much to employers in a very short time imo. On the other hand, a masters in statistics will tell (decently competent) employers a lot more about what you do and don\'t know how to do. Plus there\'s a market in ""data science"" for a genuine statistician who knows what they\'re doing. Most of the data science degree holders I\'ve met in the wild are econ undergrad majors who took their one ML class in grad school and now come around telling me to try building a predictive model to answer a question that any halfway competent statistician knows u can solve with a series of basic 2-sample T tests. \n\nwhat\'s really gonna get you the bang for your buck in the long term though imo is data engineering. If you like programming that much and want to have a guaranteed paycheck for the rest of your days that\'s the field to be getting into. All these companies are swimming in raw data and someone has to set up databases and store it all for them, so genuine data engineers are in super high demand rn and I don\'t think the market\'s going to slow down barring some major world catastrophe.', 'The degree title, in my opinion, would do similar stuff in industry. I actually think the statistics title might give you more options to do more stats stuff. As others have said, look into what you actually learn in the programs.', 'I would evaluate each program on an individual basis. Way too many differences between schools and programs. Dont spend much more than $15k-$20k and try to get an employer to pay instead.', 'Applied stats more hardcore math, data science less hardcore math, more coding and soft skills.', "">useless degree since apparently it can be a cash cow for a uni\n\nA degree being a cash cow for a university does not necessarily imply that it is useless.  Actually, most cash cow degrees are very useful.  A degree is a cash cow if the school can make a profit on it without much effort, which usually implies that it is a popular degree that is valued in the market.  \n\nI'm more on the theoretical end of the spectrum, so my heart is in the stats degree where you dig more into the foundations.  But there is a big market for data science degree holders who have been technically trained in lot of useful things."", 'As someone who was educated in statistics and started working in data science, this very closely mirrors my experience.', '""the job title has become meaningless because it can apply to a huge range of background skills and job descriptions...""\n\nThis is exactly how I feel about it. I hope the title goes away at some point because it can mean literally anything.', 'This is very interesting, and not the first time Ive heard this. May I ask what background your team is generally compromised of?', 'Applied math, physics, statistics, CS, EE. We had both MS and PhDs. The vast majority were some in stats and physics. \n\nIt was a biotech company so we also worked closely with people who had bioinformatics, CS, or bio related phds.']"
[Q] R^2 from a multiple mediation analysis,"
Hi all, 

Im currently completing a study where I have carried out a parallel mediation analysis on the jamovi software.
I have the following:
1 independent variable 
2 mediators
1 dependent variable.

I am struggling on how to calculate an R square value for the two mediators.

If anyone knows anything about this I would greatly appreciate your help.

Thanks in advance!",12wq2m0,Daxelin,1682283773.0,1,1.0,"['R2 of mediators is from regressing the mediators (separately) on the IV', 'Thank you for the speedy response! Would I then have to report on each mediators R2 individually rather than a collective R2 figure for the mediation', 'for the DV, you report one, final R2']"
[Q] Adding regressions together for business revenue forecast,"Im attempting to introduce some basic regression analysis to improve how many company thinks about forecasting revenue. After some testing in excel I have created a set of variables that I think work somewhat well. Its about 8 products and each one has a renewal business regression and a new business regression, using about 48 months of data to predict next months revenue. Adding them up gets my total forecast and I can create confidence intervals on the total error.  Im wondering if my current approach could be sufficient for a starting point, where the R-squareds for the regressions are generally .65 -.93. In many cases the p-values for some of variables are high, some are <0.05, so Im not sure how to think about that? Once I become more confident in my own understanding I could take my datasets python to speed it up but wanted to make sure my approach was decent enough as a v1.",12whfwb,spreadsheet_jesus,1682267919.0,6,0.88,"[""It's probably best to bring the analysis into python or R now for the diagnostic side of the analysis, ie, check assumptions and fit, and outliers, etc. There are some libraries or base functions that can help you determine how your model is fitting and perhaps why it's fitting that way. \n\nMaybe more importantly, you have multiple models with varying degrees of accuracy. Interesting, but why? How do these results relate to outcomes you intend to predict?\n\nLastly, if you intend to operationalize the model(s), you should cross-validate and test their predictive accuracy. The cross validation may also give you somewhat different r-squared metrics, eg."", ""This sounds like times series forecasting, you may want to look up the assumptions required to perform regressions on time series data if you haven't already.""]"
[Question] The martingale system on roulette,"I know its gamblers fallacy but Im trying to wrap my head around it.

Its almost 50/50 for red or black no matter what the previous roll was, but the odds of continuously getting the same colour is low. 

Doesnt that mean that the martingale system technically increases your odds of getting the colour youre betting on increases with each play. Even though it is 50/50 regardless?",12w46ra,reapingsulls123,1682245983.0,16,0.9,"[""Correct. The Martingale system is flawless. If we were to play an imaginary game of roulette, with imaginary money, with me as the casino and you as the gambler, you would win every time.\n\nYou wouldn't even need to bet on red or black. You could bet on anything. As long as you kept sufficiently increasing your bets, you'd always win in the end.\n\nLose 100 bucks?, No problem just bet 1000 next time. Lose 1000? No problem, just bet 10,000 next time.\n\nEventually you'll win and cancel out all your previous losses and be a millionaire.\n\nOf course reality has two major differences to our hypothetical game.\n\nFirstly, I don't know your finances but I'll assume you don't have unlimited money. If your money runs out before you hit the big win, then your Martingale plan comes to an untimely end \n\nSecondly, even if you do have enough money, casinos have house limits. You might need to bet 20,000 on your next spin to break even, but they might not let you bet that much.\n\nWhat that means is that casinos are more than happy to let people try out the Martingale system. They know that most people will run out of money before they do.\n\nAnd if someone with unfathomably deep pockets does come in, they can just politely escort them from the building whenever they like."", '[removed]', '>  but the odds of continuously getting the same colour is low. \n\nIt\'s important to be clear. If you\'re standing there before the first spin, the probability that you\'ll get the same colour every time for a bunch of spins starting now is indeed low.\n\nHowever, if you have *already* observed a bunch of spins that were all the same colour, the chance of one more  \n***IS NOT LOW***.\n\nIf the spins are independent (which the casino tries very hard to make sure of, to as close as they can manage), then it\'s the same as it ever was; the recent past contains no information about the next spin.\n\n> Doesnt that mean that the martingale system technically increases your odds of getting the colour youre betting on increases with each play. \n\nI don\'t follow your reasoning but it sounds exactly like the same error of conflating probability of the whole collection (low chance of lots of reds) with conditional probability (almost 50-50 chance of the next one being red).\n\nWhich is to say, this sounds just like the usual gambler\'s fallacy.\n\nThere\'s *NOTHING* that compensates for an excess of one color. Nothing to balance it out. If you happen to get 100 red in a row, there\'s no magic force tweaking the chances for the next one. [If anything, you\'d bet on there being some imbalance in the table and bet red not black. But the casino will close the table and try to check that it\'s working properly long before that point.]\n\nThe law of large numbers does not come into that ""nothing compensates for the excess"" assertion; what happens is that as you keep sampling, the excess in proportion due to that early excess of reds eventually gets washed out -- eventually the conditional difference in proportions |#R-#B|/[#R+#B] given an initial excess of R does go to 0 because the denominator grows -- but the excess in expected count difference (conditional expectation of (#R-#B) given the initial string of Rs) does not \'wash out\'; on average it remains where it was after the string of reds.', ""The Martingale System has nothing to do with the odds of getting your colour changing as you go.\n\nIf I bet black every time, it's about 50% chance of winning every time. That never changes, it doesn't matter how many times I play.\n\nWhat changes is my bet. If I bet $1 and lose it, I'll bet $2 next time. Odds are the same and always will be but winning on $2 is enough to cancel my previous loss. \n\nIf I lose again, I'll bet $4. Still absolutely no change in odds but winning on $4 will get my lost $3 back.\n\nIf I lose again, I double again. My odds never change but my stake does so my prospective winnings go up."", "">Doesnt that mean that the martingale system technically increases your odds of getting the colour youre betting on increases with each play.Even though it is 50/50 regardless?\n\nIf you bet the same color every round then the probability you hit your color at least once will go up each round. But hitting your color at least once does not mean you make money.\n\nThe easiest way to understand why martingale doesn't work is by combining two facts:\n1. Each bet you make on roulette has negative expected value (loses money). \n2. Each roll is completely independent of each other roll (the outcome of one roll doesn't influence the outcome of future rolls).\n\nSo think of every roll as it's own small negative result not linked to any other result. There is no clever way to re-arrange these so you get a positive outcome."", 'https://www.youtube.com/watch?v=DntEoGG7RyY', ""Simulate it, in, say, Excel VBA. Simulate a few hundred turns on a given starting bankroll (or terminate when you're wiped out). Store your ending bankroll. Repeat a couple thousand times and do a histogram of your results."", 'I see what you mean with the house limits, but if youre doing like 2 dollar wins itll take a long time for you to reach the house limit, so if you have time cant you have enough goes that the odds become so low you eventually do win that 2 bucks and never reach the house limit?\n\n2 bucks will take 14 goes to finally get to 20k. Based off my dodgy engineering maths thats  0.0000610351% odds.', ""> Secondly, even if you do have enough money, casinos have house limits. You might need to bet 20,000 on your next spin to break even, but they might not let you bet that much.\n\n>And if someone with unfathomably deep pockets does come in, they can just politely escort them from the building whenever they like.\n\nhave you even stepped foot in a casino? Go to the high limit rooms, guarantee they'll accept your 20k wagers no questions.  You'll die of old age before the casino stops accepting your bets.  Even when playing a perfectly fair 50/50 game, you'll still get rekt by the house.\n\nThe problem with martingale is losing streaks are much more common than intuition.\nNumberphile had the best video on the topic: https://www.youtube.com/watch?v=zTsRGQj6VT4\n\nTLDR:  There is a 1/e probability of doubling your bankroll N using this system (which is actually not bad imo). \n\nMartingale guarantees the highest probability of winning something, but also guarantees the maximum loss in the shortest amount of time if you catch a bad swing."", 'Oh, nice, a free ad for the casino industry.', 'Thats where Im confused, yes its 50% no matter what, but every time its 50% meaning theres a lower chance of having a continuous colour and therefore increased chance of an eventual switch. No?', ""Yeah, but here's the rub: Let's say you want to beat the casino by $20k.  That is, you want to leave with $20,000 more in your pocket than you had when you came in.\n\nYou need to play the martingale system 10,000 times, successfully.  In a row.\n\nNow, the odds of a failure of the martingale system are actually 0.00610351% if you've got a $20k limit.  And that failure will cost you about $33k (that's the sum of 14 martingale bets).\n\nOver the course of 10,000 tries, you have about a 54% shot of getting one failure.  Every individual try is extremely likely to succeed, but 10,000 is a lot of tries.\n\nSo, that's the downside of the martingale system.  Success is common and cheap; failure is rare and expensive.  You need to go through a lot of tries to make any significant money, and doing so magnifies your failure exposure."", 'Its a mathematical certainty that you will eventually hit the bet limit.\n\nIf your bet size is small, it will take more tries, but you also will need to play more in order to make the same amount of money.', 'Sure but 2^14 is 16384, so at that point youre betting $16384 to win back all your previous losses and hopefully netting a full zero at 50% probability, or adding that $16384 to your exponentially increasing losses at 50% probability. All of that with a strategy thatll net you $2 at best.\n\nAnd dont forget that its not *really* 50% because of the 0 (sometimes 00 and even 000). So the casinos eating at the end of the day, and from its perspective youre great because youre a supercharged gambler: instead of coming in with $100 and when losing hoping to earn back what you lost over the course of an evening, youll quickly put those $100 on the table in one go (and as you saw, sometimes much more).', ""Yep, if you can find a table that will do this, then go for it. Most tables that allow 20k bets, don't start at 2 though. Although I suppose you could stay at the cheap tables and move up to the bigger ones as needed.\n\nThe other thing to remember is that if you start at 2 dollars, and are doubling bets on 50/50s, you are only going to be winning 2 dollars every round.\n\nYou do it long enough to win decent money, then you are going to have some very long runs of not winning. You don't really want to be risking thousands of dollars just to win your 2 dollars back.\n\nAnd if you do it long enough, then there'll definitely be a point where you max out (either your own funds or house limits) and you'll have lost everything."", ""Yes, that's right. Eventually you'll win.\n\nBut you'll have potentially risked a lot of money, simply to win a small amount.\n\nThe odds of you winning a small amount of money with this strategy are very good. Start with 2 dollar bets, you'll probably win 3 or 4 rounds without going higher than 32 or 64 dollar bets.\n\nBut then you've only won 6 or 8 dollars.\n\nYou'll have to play it a lot if you want to win something decent.\n\nAnd the longer you play, the higher the likelihood of a devastating loss.\n\nIf you lose 10 times in a row, which eventually will happen, you'll lose over 2 grand, which means you'll have to win another 1000+ two dollar Martingale rounds to break even, by which time you'll have another devastating loss.\n\nThe only way you can really win with it in the real world is to play a few low stakes rounds, and walk away with 20 bucks or so in winnings. The system means you'll likely win small in the short term, while losing in the long term."", ""There's a low chance of losing many times in a row which is why it is a viable system for any reasonable number of bets.\n\nBut once you've lost 4 times, it doesn't matter that losing 5 times is pretty rare. You're now in a place where your fifth loss has a 50% chance of happening."", 'It is very unlikely that you get 10 blacks in a row, but it is as unlikely that you get excactly any combination of red and black. It seems like one is less likely than the other because we seem to see a patern in the 10 blacks in a row, but any specific combination of colors has the same probability of happening (assuming 50/50 odds). \n\nIt might help you to think about a raffle winning number for example. If the winning number is 111111111111 you might think ""omg, how likely is that to happen?"", this is because there seems to be a pattern we recognise. However, if the winning number was 12489174717 (just a random number), it was as likely as the former to be picked. \n\nIn the martingale setting, you are talking of wins and loses of money, so you have multiply the prob of winning and losing by their respective gains or losses. This is why you will never realistically win, because even if you amount a big number of small winsz it takes just a few consecutive losses to lose it all as the losses grow rapidly.', 'Possible to lose 20 bets in a row. So what do you do?', ""Casino right by my house has $3 minimum on roulette table. I can sit at the table for hours utilizing the martingale. Even starting to bet after a color has come up twice. Eventually, without fail if I sit there long enough a color will come up 8x or more. On the final bet I'm usually betting around $600 to make up $3  The house advantage will get us every time.""]"
[D] Is there a concise textbook for statistics?,"Every book that I've came across is a 500+ page beast with 100x more words than math. I mean, we are just talking about one or several large vector of numbers right? How hard can it be to just describe some of the operations that can be performed on it?

Is there a concise (and credible/useful) textbook on statistics?

I'm specifically interested in learning about hypothesis testing (I have no idea what a hypothesis is in statistics).",12w15yg,fromnighttilldawn,1682238240.0,0,0.42,"['No theres no such a thing and thats because its a vast field that is more philosophical than most branches of mathematics.', "">How hard can it be to just describe some of the operations that can be performed on it?\n\nYou will find that descriptive statistics usually take up only one or two chapters of these books.\n\nSo, yes, an introduction to that can be had relatively concisely. But that's also not what statistics is about."", 'It depends on your familiarity with statistics.\n\nBack in the days in grad school, we used ""All of Statistics"" for reference and we dived deep into Asymptotic Statistics. \n\nBut if you were new to statistics, you would be drowning in terminologies. \n\nFor self-taught purposes, a good thorough massage with a beast textbook is a good starting point.\n\nIf you come from an engineering background, the book ""A Modern Introduction to  \nProbability and Statistics - Understanding Why and How"" might serve you well.', ""Statistics is maths. Every statistics textbook should be full of maths otherwise you're being short-changed!"", 'The first half of ""All of Statistics"" is worth a look.', 'Concise textbook on statistics which is such a vast field? The best recommendation for me would be something like Hogg and Tanis book which is used in introductory courses', 'Stats is a vast field. Any academic textbooks are going to be beasts. For a gentler intro to stats aimed at a popular audience, Id recommend David Spiegelhalters *The Art of Statistics*.', 'The thing isif you want to understand hypothesis testing you need to understand distribution functions. To understand distribution functions you need a first intro to probability and random variables. So there is no way to jump straight to hypothesis testing.\n\nSure, someone could just tell you that its basically one integralbut that wont give you any insights into the notion of randomness, sample variation and all the conceptual building blocks of statistics. I doubt you would go far without the conceptual understand of these concepts. \n\nOn the subject of concisenessyou can make statistics more concise by using more math and leaving intuition to the reader, but again, unless you are one of the wizz kids, I would prefer to combine the math with the intuition.\n\nI could suggest some books but I would need some more clarification about your actual needs.', 'Andy Fields texts are good for thos learning states through SPSS/R', '> Is there a concise textbook for [physics]? How hard can it be to just describe [the way things move]?', 'There are dozens of briefer texts out there. \n\nStart with: Statistics for Dummies, Statistics for people who think they hate statistics, Statistics for Absolute Beginners.', 'Almost all math is words, and stats has very little to do with vectors. Statistics is entirely about finding the optimal parameters for different families of probability distributions, which often involves a lot of complicated math. \n\nWhat are you interested in hypothesis tests for? They really arent useful for much, and nowadays there are probably more people who vocally oppose them than there are people using them.', 'Can you suggest the most in depth, detailed textbook for statistics?', 'Dear Indian guys Maths is not a word. Neither is Equipments.', 'You could try this: https://www.amazon.in/dp/B019N212NE?geniuslink=true', 'Youre not going to find a concise book on the entire topic. Its fucking huge. 500 pages is concise when you consider what didnt make it into the book.\nBut you may find concise work, particular stats papers, that touch on a specific method concisely.\nThis is a real life lesson in stats: know your denominator. Right now, it seems to be statistics and thats way too broad.\nRefine your search terms.', ""> Every book that I've came across is a 500+ page beast with 100x more words than math.\n\nBook publishers are built around a model of a new edition every year + extras, which leads to bloat. But to add to that, you're probably mostly looking at the books designed for students with low mathematics skills.\n\n> I'm specifically interested in learning about hypothesis testing (I have no idea what a hypothesis is in statistics).\n\nFewer words, with plenty on hypothesis testing. Sure -- *Statistical Inference* by Casella and Berger\n\nOr if you want something a little more recent, maybe Wasserman's *All of Statistics* (it isn't remotely all of statistics -- but it's a good title for people that think that it should all fit in one book.)\n\nOf course, you'll only get the mathematics, so when you find yourself with actual data to analyze you'll be back wondering why you know a bunch of theorems but have no idea how to solve actual problems, but one thing at a time I guess.\n\n> I mean, we are just talking about one or several large vector of numbers right? How hard can it be to just describe some of the operations that can be performed on it?\n\nLOL\n\nLook at this juxtaposition:\n\n> How hard can it be to just ...\n\n> I have no idea what a hypothesis is\n\n\nYou know ... that sounds a heck of a lot like ... https://xkcd.com/793/"", ""Have you thought about using google? It's amazing how many universities have their class slides posted that walk through statistical concepts quite nicely. For hypothesis testing, blog posts and youtube videos will also be your friend. \n\nAlso, hypothesis testing looks different by field. When I took stats theory and applied stats, I was shocked how different it was from the approach in biology. The approach I'd used in biology wasn't wrong, per say, just a bit different than that used in statistics. I left bio a decade ago so maybe that's changed."", 'I think statistics for dummies is a nice short skinny book for summary for statistics they have a statistics for dummies II as well if you want to continue learning', 'The closest I have found is The Simple and Infinite Joy of Mathematical Statistics by JN Corcoran, here https://www.amazon.com/Simple-Infinite-Joy-Mathematical-Statistics/dp/B0BD1YPQRN', ""[Book of statistical proofs](https://statproofbook.github.io/I/ToC) does what you're after"", 'There is no one textbook which covers statistics. However, if you want to start simple, you could look at final year high school statistics books? They will generally have more maths and exercises.', 'You will need multiple books starting from the very basic descriptive statistics then probability. \n\nIf you\'re into the applications more than the math formulas, I found a good book that introduces probability using R software, the book is called ""probability in R, an introduction with computer science applications by Jane M. Horgan"". In real world, we use programming to work on statistics, we don\'t use pen and paper so it could be a good start', 'Also Bertsekass book Introduction to Probability has two chapters that provide a fairly concise introduction to statistics.\n\nhttps://www.amazon.com/Introduction-Probability-2nd-Dimitri-Bertsekas/dp/188652923X', 'Hogg and Tanis is a 500+ beast.', 'Love Andy Field', 'They\'re complaining about too high a words-to-mathematics ratio; while they\'re not 500 pages, I don\'t think those titles really get at that ""more words than mathematics"" issue.', 'Thanks for the tip on hypothesis test :)\n\nWhich other parts of statistics do you think I should avoid learning about', 'what? Do you realize the UK, Australia, NZ, and others also use the word maths and its in the dictionary? Singling out India is a peculiar choice.', 'Or if you want something even smaller i recommend statistics a graphic guide by Eileen Magnello & Borin Van Loon', 'You can just look at hypothesis testing, it is a good book', ""I don't think he meant you should avoid learning about hypothesis testing. pvalues are still everywhere and it's important to understand what they represent, and for that one needs to understand the framework of hypothesis testing. It's an important topic even if just for the sake of being able to understand what people mean and why X or Y study is crap :D"", 'You shouldnt avoid learning anything, and if youre doing stats work, you must have a very strong grasp on hypothesis testing. Even if you never use them, other people will, and youll have to read other peoples work a lot. \n\nYou should start by learning the fundamentals of probability theory. People misuse or misunderstand statistics all the time, and its always due to not understanding probability theory.']"
[D]Chi-Square Goodness and other measures of association,Hello everyone! Do you know if there's an ebook or epub for the two topics? I need it for my report next week. Or any other related source that would help understand the topic is also okay.,12vzl3s,rizzielo,1682234201.0,2,1.0,"[""You might try this. Be warned: this is a big topic, and it's hard to get the basics right without much more than a week's study. Even trickier is the fact that different fields of study use different measures for very similar calculations. \n\n[https://us.sagepub.com/en-us/nam/book/correlation](https://us.sagepub.com/en-us/nam/book/correlation)"", 'What types of variables are you looking for measures of association for ?  (Nominal, ordinal, continuous...).', ""I think all of those 3 that you mentioned. I need to have an oral report for that topic. I'm looking for sources online and so far I got the Chi-Square Goodness of Fit test."", ""The chi-square goodness-of-fit test isn't a measure of association.  It's a hypothesis test.  ...  I'll try to give you some hints in another response."", ""I get it but I also need to report about the chi square that's why I included it on my response."", ""It's difficult to understand what you are looking for, but I'll try to give you some hints.\n\n There are many different measures of association.  The appropriate measure depends on the nature of the two variables compared.  That is, continuous x continuous, nominal x nominal, nominal x ordinal, and so on.\n\n These are distinct from hypothesis tests.\n\n Measures of association should be unaffected by sample size, whereas hypothesis tests are affected by sample size.  That is, the correlation between A = (1,2,3,4), B = (2,4,3,6) will be the same as A = (1,2,3,4, 1,2,3,4), B = (2,4,3,6, 2,3,4,6).  But the hypothesis tests for these correlations not being 0 will be different.\n\n In the continuous x continuous case, the common measure of association is Pearson correlation.  Spearman correlation is also common.\n\n In the nominal x nominal case --- where a chi-square test of association might be used --- the common measures of association are *phi* (in the 2 x 2) case, and Cramer's *V*.  I have other options included here: [https://rcompanion.org/handbook/H\\_10.html](https://rcompanion.org/handbook/H_10.html) .\n\n I have some measures of association for ordinal variables described here: [https://rcompanion.org/handbook/H\\_11.html](https://rcompanion.org/handbook/H_11.html) ."", ""Thank you so much! I'll study the reference!""]"
"[Q] R is posisitve in linear regression, what now?","Hello, Im writing a thesis and have a hypothesis, where higher score in one factor (negative emocionality) predicts lower score in other factor (attitudes toward autonomous vehicles). To test it I run linear correlation, and R came out positive (so positive relation?) at p was in significant range (.007). Everything seems as they are positively related, except for residual plot, where when score in one gets higher, the score in another one gets lower. Can anyone explain to me, why that plot is like that if the relation between them is positive?",12vzb3h,GoodeSVK,1682233487.0,0,0.33,"['In the table you have shared, the slope of regression line is negative (-0.402), so Im not sure why you think that the variables are positively related. R is positive simply because its a square root of R squared, it represents only magnitude if the effect, not direction. I.e. its not Pearson correlation coefficient per se.', 'Please provide the full linear regression calculation and plot if possible.  \n\nWhat is the intercept, coefficient, R squared values?', ""You are confusing several differences between correlation and linear regression.\n\nCorrelation: produces an r value.  There is no such thing as a residual plot or R.\n\nRegression: produces coefficients, standard errors, p-values, R2.  R2 is a measure of quality-of-fit and cannot be interpreted as directionality of effect.  Residual plot is a useful diagnosis, but also cannot be interpreted as directionality of an effect.  The coefficients and standard errors are for interpreting directionality.\n\nIt may seem pedantic to get so specific about all the terms, but I promise it's not.  Without precision in the terminology, it's impossible to communicate your conclusions or even to reach the correct conclusions."", ""The residual plot shows the difference between the predicted values and the actual values. If your model is appropriate, you should not see much of a pattern there. If this doesn't look like an undifferentiated blob, then you may not have a linear relationship between X and Y."", 'To anyone who wants to know, I run these variables in spss and it shower negative R. So thats that for me.', 'So what should I interpret to get them know that these two factors are negatively related. I personally thought that if R would be negative, that they would be negative related, and R squared would still be positive.', 'Yes, dependent variable is attitudes towards AV; covariate is negative emocionallity; R is .214, R squared is .046; p is .007; 95% CI lower is -0.692 and upper -0.112.\n\nLink to residual plot: https://imgur.com/1zHAOIu\nOn the right side is my dependent variable (attitudes toward AV). On the bottom is my independent variable (negative emocionality).\n\nLink to whole linear regresion table: https://imgur.com/upZFMxR', 'Yeah, im really bad in statistics. I thought I could do it for my thesis, but I really dont understand what this statistical program is giving me.', 'While squaring Pearson\'s correlation coefficient gives the amount of ""explained"" variance (R^2 ), taking a square root of R^2 doesnt necessarily gives Pearson\'s correlation coefficient. This simply because  R^2 is always positive, so there is no way to get the direction of the relationship from it. The software you are using is doing the later.\n\n\nSince you are using linear regression, you should be reporting regression coefficients to communicate the exact nature of the relationship.', 'Your CI should include R but doesnt?', 'Can you please elaborate more? I dont understand what you mean, I just checked that interval and that is what jasp gave me.', 'The CI is the range where you are 95% certain R is within. So R should be between the lower and upper bound of your CI. Generally its in the middle. So, either you are not seeing the - in front of R, or you are not estimating R and R is some other factor that the model spits out.', 'For clarity, the 95% CI in this case is for the unstandardized b-weight, not for R. I think thats where the confusion is.', 'I will try to convert my data to spss and try it there.', 'That should not be the issue. Are there any other numbers reported by JASP, or is this all? R is reported for many models and often doesnt mean correlation but explained variance.', 'I sent all information that I worked with.']"
[Q] Advice appreciated- What to do when the statistics don't appear to match the visual data trends,"Hi there. Wondering if anyone has some advice. I've run some statistics on a few different datasets (Kendal's Tau) and it's giving a strong positive correlation with the two variables (like P = 0.001 type ranges). However if you look at a scatter plot, if anything it appears to be a roughly negative association. I realise Kendal's Tau ranks data in a way that is less sensitive to outliers (of which my data does contain) but what do you do if you just can't see the trend the stats are saying are there? Do you assume it has been over analysed and bin it? Or something else? (Sorry I know probably hard without the data here to see. But it's been doing my head in trying to work out what is going on here. Data isn't suitable for parametric testing which is why Kendal's has been used. (And Kendals rather than Spearmans to deal with the outlier situation.) Thanks!",12vyn0n,jaciwriter,1682231776.0,3,1.0,"['My first advice would be to plot the data in the same way the test is interpreting them ( i.e. as ranks). I think in R this would be done with this code ""plot(rank(x), rank(y))"". Does the new visualisation better align with the result of Kendall\'s test?', 'Binning data is generally discouraged - its a waste of information. What sort of question are you trying to answer? Kendells Tau isnt really comparable to a trend line. How do you know it isnt suited for parametric tests?', 'Thanks for the suggestion! I did that and still cannot see any visual correlation. TBH I have no idea how the correlation has come up with the results it has.', 'I don\'t need a trend line per se, just a correlation (or lack of, one way or the other). It seems to fail various assumptions on every parametric test I\'ve run it through. Largely because there\'s clusters and outliers I think. I\'ve also got some (not all) datasets which have ordinal data against continuous which doesn\'t suit a lot of the stat tests out there too. They\'re ""messy"" datasets with a lot of uncontrollable variables which is pretty par for the course with the type of research this is.\n\nI\'ve got a few datasets that are in the ""I don\'t know what to do with this"" category where a scatterplot indicates one thing, and the a correlation test (usually Kendal\'s) says the complete opposite. \n\nI guess short answer is I want to know if one factor is influencing a second factor in a particular way. In this particular case you\'re looking at levels of a pollutant in a species of wildlife against various health perameters. Is there a direct correlation (ie more pollutant = sicker animal), threshold effects (ie over a certain level of pollutant, start to see sick animals) or no apparent correlation at the observed levels. \n\nMy issue is with some of the datasets, there appears to be a limited correlation that is probably negative in nature, but kendal\'s throws up positive correlations with p= 0.001 values which makes it confusing to decide how to report it. (Which is why I\'m wondering whether to just discard a whole heap of work due to this.)']"
[Q] Is there a way to find the threshold point of a dataset statistically?,"Hi there, I was wondering if anyone could advise on a test to figure out
where the threshold effect is on a dataset? For example say a test
group had been given different doses of a medication in mg and the
response recorded. Could you find at what point there appeared to be
getting a response to that medication in at least some members of the
test group?",12vygjd,jaciwriter,1682231317.0,3,1.0,"['How do you define ""response""?', ""If your response is binary (yes/no) then you're looking for a chi-square or logistic regression."", ""What are your research question and hypotheses? Without those it's virtually impossible to give useful advice."", '1. How granular is the dosage? You could discretize the dosage into some meaningful set of intervals (e.g, 1mg, 2mg, etc.). What to do next depends on how the response is operationalized.\n\n2. How is ""response"" operationalized? If the response is a continuous measure, you could estimate a linear or generalized linear model. Any global test for the model will address the question of whether there are any differences in the response across the dosage levels. The individual beta parameters will tell you whether the average response at that dose is distinguishable from the average response at the lowest dose.\n\nThis is assuming that the dosage is assigned at random.', 'Yes, this is known as dose-response modeling. Look into the EPA software called Benchmark Dose Software, it provides the tools to fit a variety of model forms for either continuous or dichotomous responses. You will specify the benchmark response which is the amount of response that is meaningful to you, and you are returned the benchmark dose which is a point on the curve and represents the dose associated with your benchmark response.', ""I'm a specialist in psychophysical threshold estimation, and the principles should be the same in your application.  \n\nPick a model of a psychometric function... This is mapping from dosage to the probability of the binary outcome.    I personally like ones of the form : \n\nP(x) = a + (b-a)*F(x)\n\nWhere *(a,b)* are upper and lower asymptotes, and *F(x)* is the CDF of a log-normal (there are many other choices besides log-normal that may be better for your application).  I like the log-normal because the location parameter is the threshold we seek.    If the asymptotes are known, then it's a two parameter model.  \n\nFit the model to your data.  Now in order to estimate your threshold for a certain performance rate *r*, you simply find the value of *x* which satisfies *P(x) = r*. In other words, you find the functional inverse of your model, and evaluate it at the threshold probability."", 'I\'m  trying to find a threshold point. You kind of have background noise where there is little to no difference between individuals, then you start getting ""outlier"" type responses past a certain point which show that a response has occurred in some individuals. It\'s kind of after which point, will you start seeing a possible effect? You can see it on a graph, I just need to back up where I\'m calling it with statistics. (It\'s not actually medication doses, it\'s just an easier example than trying to explain what\'s going on as it\'s the same general idea.)\n\nI\'ve uploaded an example graph. Basically I need to know where to draw the line (that I\'ve penciled in in yellow) to separate the definite responses on the right to the background ""non-responsive"" data on the left. Hope this kind of helps explain what I\'m trying to do?\n\nhttps://ibb.co/4s8vPxV', ""Regression won't help OP estimate the threshold dosage.   It would just tell OP the factors that covary with the response variable.       This is best done when the dosage is fluctuating around the threshold, assuming the threshold is the inflection point of the underlying function that maps dosage to outcome probability."", 'I think I\'m throwing everyone out with the medication example. (Sorry I was just trying to make it easy to explain). Trying not to get too complicated actual dataset is a pollutant level measured randomly in individuals from an area, with other ""health"" type measurements taken to see if they are affected. Given this, the data has a lot of uncontrolled variables (that are largely not being measured) as it is not a controlled lab set up causing some variations in the baselines and outliers that in some cases are, and sometimes aren\'t related to the focus of the study making the datasets a bit messy (not uncommon for this type of data though). Unfortunately for some of the datasets, the health measurement is in an ordinal scale.  The datasets I\'m having issues with, don\'t form a nice linear model. They seem to have more of a threshold effect.', ""Thanks, I looked at this site but my data fails it's assumptions it needs to run sadly."", ""It sounds like they're looking for a response (yes/no) based on discerete levels of an independent variable. That's within the domain of either chi-square or logistic regression AFAIK."", 'I can run something like a McNemars, the problem is I need to know at what point I should draw the line to separate it into the two groups', 'Yes you are correct!']"
[R] Linear Regression with ordinal DV and continuous IV,"Hi guys, I'm writing my thesis currently. In my thesis I want to see whether mental toughness can predict sport performace. Sadly in my questionnaire the only determinant to sport performance I used was the level of league in which athletes play (not the smartest option). After some research I've come to the conclusion that I have to use Ordinal Logistic Regression. I'm am using Jamovi. I'm not sure whether I can interpret McFadden's R\^2 as a I would interpret a typical R\^2. Can I interpert it as the typical R\^2 for variance? If you could see an option where I could also use another test, or have knowledge of how Ordinal Logistic Regression works any advice would be greatly appriciated. Thanks guys!",12vvpfl,MrMojjo,1682224487.0,10,0.92,"[""Here are some resources that explain how to interpret McFadden's pseudo R^(2):\n\n* https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/\n* https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation\n* https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/\n* https://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s\n\nIn short: No, you can't interpret it as you would an R^(2) in an ordinary linear regression (proportion explained variance). It is typical for R^(2) measures for logistic regressions to be quite low compared to the R^(2) for linear regressions. Personally, I would only use pseudo R^(2) for comparing two models but I wouldn't interpret the value for a single model."", 'Why cant you use linear regression?', 'Hi thanks for the response. I ended up using a regular Anova. I am no longer predicting however but just seeing whether the level of league a player plays in has an impact on their mental toughness.\n\nThanks for all the info!']"
[Q] What test can I use for my (large and confusing) dataset?,"I have collected data on cell survival for various radiation experiments.

The output/dependent variable is ""Percentage Survival Change"".

There are probably about 500 rows of data points from many different experiments. Each data point has multiple categorical variables attached to it. Independent variables include: Cell line, Sex of the cell line, Organ of the Cell Line, protein status (mutant, wild-type, or null), radiation source used, etc.

I'm looking to see if any of these independent variables have a statistically significant impact on the ""percentage survival change"" variable.

When looking at the distribution frequency of the ""Percentage Survival Change"" (values range from -99 to 140), it appears to be normally distributed.

When comparing ""protein status"" for example, the number of mutant, wild-type, and null cell lines are not equal (or even close to equal). All of these independent variables will be unbalanced.

I do not have a strong foundation in statistics and I'm struggling to find a test that will work for my particular case. I'm not sure how to approach this.

I appreciate any help or advice that you can give! Please let me know if you need any more information!",12vj95b,Ronnoc21,1682197481.0,1,1.0,"['One potential option could be a glm model. The link here is for running it using Stata, but other software packages can run similarly\n\nhttps://stats.oarc.ucla.edu/stata/faq/how-does-one-do-regression-when-the-dependent-variable-is-a-proportion/', ""The unequal sample sizes shouldn't cause problems with most common analyses.\n\nIt sound like you need to consult with someone.  Maybe there's someone at your company / university department you can talk to ?"", 'How does one have the training to conduct radiation experiments but have no idea how to analyze the data?', 'Ill look into this. Thank you very much!', 'Yes Ill have to check in with my supervisor this week. Thank you!']"
[Q] [E] Top Applied Stats Ph.D Programs for Academia positions?,"Ive read and heard, both on this subreddit and in my own math department, that if I expect to get a position in academia with an (applied) stats PhD, I need to be at a top 25 stats PhD R1 institution. This makes sense. However, what schools are considered to be in this upper echelon? I feel like its different depending on who you ask and want to know your thoughts!

EDIT: e.g. Northwestern has a smaller Stats department, some consider it to be highly ranked, but US News has it at 37 (outside the mystical top 25 moniker).",12vizbj,kurt_46,1682196927.0,5,0.67,"['I can\'t believe none of the posts so far have mentioned this. \n\nWho your advisor/post doc advisor is is a fuckton more important than ""what"" school you are from. \n\nA well known leader in their field from a ""lower"" school >>>>>>>>>>> some brand spanking new assistant professor from Harvard. \n\nGranted, Top 10 and Top 25 programs have a larger pool of well known leaders in their fields. But they aren\'t all in these institutions.', 'If you want to do applied stats work as a researcher and teach course work as a professor in a more applied flavor you will almost certainly benefit from being in a Bayesian department in my opinion. A very frequentist department are often times essentially applied math departments, they will want people who have very solid coursework at the PhD level in measure theory and probability which you may or may not get from an Applied Statistics PhD.\n\nNow why does this matter? Because you can dramatically improve your odds from going to a Bayesian department for your PhD that will allow you to stand out from other applicants and be able to bring something to the table that others dont. Additionally (as a PhD student at a very Bayesian program) the network you build as a Bayesian may be a bit closer knit than going to a normal or frequentist department because there are just a lot less of us in academia, everyone pretty much knows everyone. \n\nAdditionally, just practically speaking, the simulation and computational techniques you learn in the Bayesian framework is going to be very useful to pretty any applied problem you encounter.\n\nIn my opinion, if I were you I would strongly consider the well known Bayesian departments and connecting with the professors and getting a feel for whether you like it or not. Some do very theoretical work, and some do very applied work. \n\nHope this helps! Good luck on your journey! :)', 'I would use the US News ranking as a rough guide. Dont interpret it too literally, but Top 10 and Top 25 should be pretty accurate. A good advisor is the most important thing to set you up for success. There are good advisors at every school, but there are more options at better schools.\n\nStats is unique in that it evolved from agriculture departments as well as math departments. Stats departments at big name schools dont always match their reputation. Like you noted, Northwestern is an example of this. On the other hand, some big state schools have excellent departments (Iowa State, Penn State, Texas A&M, NC State).', 'I would look at the US News stuff, but not take it as the tell-all of things. (e.g., you may not want to travel cross-country)\n\nThere are three in NC that Id recommend the big three  UNC, NCSU, and that one in Durham who, for reasons pertaining to my undergrad affiliation, will remain nameless (jk its Duke) are great. No, they arent Harvard, Stanford, or UC Berkeley or UChicago or whatnot, but:\n- you may not like cold (Harvard)\n- Cali is expensive \n- Chicago is a BIG city\n\nI say all this to say that, while I only applied to 2 PhD programs, I chose mine because of the *culture* before the name.', 'https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings', 'You should look at the following resources:\n\nhttps://larremorelab.github.io/us-faculty/hiring-flows/Field/statistics/\n\nhttps://larremorelab.github.io/us-faculty/university-ranks/', 'Thats a fair point  keep in mind not everyone knows this a priori (e.g., me lol)', 'Its so hard as someone coming into the field trying to understand who is a good advisor or not and who is the best advisor for future success. \n\nWhat is your advice for finding a faculty member that can help you go far in a field you dont even know anything about (yet)?', 'Also who your advisor is in relation to your faculty can change a lot. I know three people within a year getting TT right after graduation because they had the department director as one of their advisors during their PhD. Nepotism is always more important than scientific contributions unless you are a science ""rockstar"".', 'This is really helpful! Which universities have the well known Bayesian programs?', 'I feel like Bayesian vs Frequentist is an old way of looking at statistics departments. Good schools will have a mix of Bayesian and Frequentist professors. People jump back and forth between the two paradigms depending on the project.', 'Im actually in Chicago so Im trying to stay in a city!! Sucks that UChicago and Harvard are in big cities and are very competitive', ""I'm not an academic (I went to industry after my PhD) so take anything I say with a slight grain of salt but...\n\n1. Does the person have a track record of graduating students (check their CV)\n2. Do they have a long history of funding and high impact publications (total citations, h index, etc.)"", 'Duke, UT Austin, UCONN, UC Santa Cruz, UC Irvine, Columbia. Obviously Stanford, Berkeley and UW have amazing Bayesian work too, but I was trying to give you schools outside of the Top 5.', 'All of them will be very competitive.', 'Ah gotcha! You might be more accustomed to it, then (I am NOT lol).\n\nI jokingly say that I wouldve tried Stanford for undergrad bc they sent me a packet of info, but it was cross-country lol', 'Thank you! UCSC was my dream school for undergrad. Columbia and UW are also programs Im going for. How do you think writing an SoP with the intent of studying mostly Bayesian stuff would work?', 'Sorry, let me rephrase: UChicago and Harvard are 2 of the highest ranked and best universities in the world and it would be awesome if there were other options in these same cities', 'On your first comment, how did you discover a culture? All the info from faculty seems so spoon-fed and never fully reality', 'You can DM me if you want, I can help and let you know how I approached it.', 'Im entering this fall, so I havent gotten the FULL exposure; but I could tell that many of the faculty werent trying to *sound* elitist or say the right things.\n\nIt also helps that its still a school close to my home (and undergrad for that matter), and so I generally *know* the types of people thatd choose it as a school, root for it, etc.', 'Can I dm you too?']"
[Q] [E] Question about finding the appropriate lower boundary for what is considered a famous tiktok book,"Hi!

  
For my master's thesis, I am writing an algorithm that can predict which books will become popular on Tiktok. Right now I am working on getting the right labels for the dataset, based on the viewcounts of the books on Tiktok. The dataset I have by far doesn't include all the currently popular tiktok books, so for now, I just have a list of 19 books that have done well on Tiktok, with their viewcounts (there are probably more, but I had to make a list myself of what I know for sure are famous books). I want to use their viewcounts to set a baseline for what is considered popular on tiktok and what is not considered popular. However, the viewcounts all lie pretty far apart. They are the following numbers:  
66089172, 909551, 14159253, 5771561,  68456152, 20982050, 6767132, 61012995, 39505320, 1299157, 27307,  38193455, 34048345, 9830311, 87600000, 37921810, 88484025, 55764970,  108154.  


I have considered using the mean or median, but since the numbers lie pretty far apart and they aren't normally distributed, I don't think I can use those. I then considered using the mean - 3 times the standard deviation, but this gave me a lower bound of zero, meaning that all books would be considered tiktok famous. I also tried using the 25 percentile - 1.5 times the interquartile range, but he same thing happened. 

Right now I am thinking I could just use the lowest number of the list, since I know that one is Tiktok famous so ones with even slightly more views will be considered famous as well, but this feels like it's very wrong, statistically speaking, so I was wondering on your opinion on this, and if you had any advice or recommendations?  


Thank you so much in advance!",12vhy7o,Romcom1398,1682194810.0,1,1.0,"['I would try log-transforming your data, calculate statistics on the log domain, and then back transform when you need to see the linear scale.    That will avoid your zero errors, and also help stabilize the variance in your data.\n\nJustification for a log-transform:  the [change in] popularity of a book will depend on how many social media users have posted about it.    This is the key trait that means growth is exponential.', ""It seems like you have already decided what counts as 'popular' by picking these 19 books. Unless you want to come up with a more agnostic criteria and gather more data to see what meets these criteria, just use the smallest on the list that you have already generated. You still need to justify why these 19 are what should count as popular, but doing more math to the 19 data points you have won't help you there."", 'Thank you so much, that actually makes so much sense, I will try doing that!']"
[Q] Conducting a lot of multiple regressions on the same data set,"Working with a social science student who has a data set that includes a 25 potential predictors. These can conceptually be grouped into three basic types - demographics, social factors and digital device usage, with approximately 7 to 8 predictors per group. It is not though possible to sum any of these to create a single measure. There is a single outcome variable, which for the sake of this question I will say is amount of money spent (continuous). Sample size is 350 participants.

His plan is to conduct three regressions - one with the demographic predictors, one with the social predictors, and one with the digital device predictors. Same outcome variable in each case. My concern is that by doing so he is effectively just doing one single regression model, except that by splitting it into three models he will not pick up on potential issues such as multicollinearity between predictors (e.g. if they are in different models). I know there are probably more complex analysis he could do that would be more appropriate than a regression, but there is a limit on how complex an analysis he can be expected to do at this stage in his studies. It is also a good learning experience for him to understand the pros and cons of using regression for this data. However my Yoda like attempts to guide him to his own realisations are being hampered by my own lack of understanding, as I have never encountered a situation where someone has so many predictors for a single outcome. I inherited this student from someone else who has even less stats knowledge. Any advice?",12vf3fw,J8766557,1682188865.0,23,0.96,"['> It is not though possible to sum any of these to create a single measure\n\nYou can always run dimensionality reduction with PCA or FA. If I remember correctly running PCA on the predictors and then doing the regression as `outcome ~ component1 + component2 + ... + componentN` (where the PCA selected N components) is a discrete approximation to L2 regularised (Ridge) regression.\n\nIncidentally the recommendation elsewhere in the comments for Ridge|LASSO|elastic-net regularised regression is a good one, and technically better. The PCA approach is conceptually simpler and on the way to what you want.', ""Why exactly do they not want to run a single regression with 25 predictors? Are they worried that 25 is too many for your sample size or for interpretability? If that's the case, then they could use some kind of penalized regression like LASSO, ridge, elastic net, etc. that will likely shrink some regression coefficients to 0 or near 0 which may help with interpretability."", 'In addition to other answers, you could also just fit all the scenarios youre concerned about. Y ~ all_predictors, Y ~ demographics, Y ~ social, Y ~ device\n\nAssess the models as normal. Look at the Adjusted R squared to see which had the best fit. You can take it a step further and compare the models with Anova testing. The best fitting model with have the lowest AIC score in that case', 'If he has three non-overlapping questions and uses three non-overlapping sets of variables, he\'s not committing a mortal sin -- as long as he realizes that the answer to ""what affects Y most?"" is not just the sum of ""what demographics affect Y most"" and ""what social factors affect Y most."" What you *realllly* don\'t want to do is run model after model after model with similar combinations and pick the one you like best.\n\nYou do better to either fit one model via a previously-chosen variable selection scheme (LASSO is possibly most popular and best understood), or to do something called \'multimodel inference\', fitting all 2^(25) possible models and taking a weighted average of them (usually weighted by something like exp(AIC) so that the best-fitting models dominate the results.) Before LASSO took off, multimodel inference was just starting to get going, but is has sort of been overtaken and less computationally intense methods get preferred now.', 'Depending on his research question.. does theory or ex ante reasoning help narrow down the subset of variables under each category that are most relevant? Or is this project merely a matter of predicting/seeing how much variation in the dependent variable he can explain by these factors? \n\nIf the latter, he could first add the demographic predictors to see how much of the variation they explain, then add in the social predictors with the demographic ones, and then in a third model add the Digital device predictors (ie all 25 variables). If some of these measures are highly multicollinear, he could easily get inflated standard errors on some of the individual coefficients, which would make individual coefficients hard to interpret', 'If this is a learning exercise, the best thing to do is to fit several regressions and force yourself to come up with theories as to why the coefficients change when further variables are included. Eg a common pattern in social science data sets is that some ""interesting"" predictive relationship goes away once you add demographics or socieconomic variables to the regression. Why is that?\n\nie, think about the science that gives rise to the patterns in the data, which are summarised by regressions.\n\nI\'d say it is unlikely someone will get much real utility out of hierarchical regressions until they thoroughly understand good old linear regression and how things change with additional predictors.', 'What is his end goal? I mean say he decides on a model. What does he want to do with it? Prediction, Inference?\n\nI am not sure how one compares three models with three different set of predictors? A model that starts with the 25 predictors can be refined to (by eliminating predictors) to one of his three models. So, he is throwing away information that for no reason.', 'I am confuse. He basically has three datasets? Is he trying to answer a single research question or multiple? Why cant he combine the datasets? Is this a prediction or statistical significance model?', 'He could look at the effect of each set of variables controlling for the other sets. For demographic variables, he could find the difference in R^2 between the full model and a model excluding the demographic variables. This avoids potential problems of potentially high correlations within a set.', 'Have you considered structural equation modeling? You could add all of your variables into one model, where the 25 measures are your *observed variables* split between three group types, which are your *latent variables*, and then use that to predict the outcome variable.', 'Unless you want to be able to interpret your predictor variable with any degree of reliability. PCA needs, well, PCs.', 'I would add to this CART', 'The theoretical basis  is strong for the demographics, reasonable for the social factors and non-existent for the digital device usage. My thought was that he could add each of the three groups as steps of a hierarchical regression. Would that be the same as what you suggest? I think part of his confusion (and probably also mine) is uncertainty over whether a step in a hierarchical regression model is all part of the same model, or a different model. \n\nUltimately this is primarily a learning experience rather a piece of research that will be published, so it isnt too much of an issue if there are limitations to how reliable the results are, provided that he can identify these limitations.', 'A single dataset with a single outcome variable. I think his rationale for doing three separate regressions is that it will help him better understand how important each of the three groups of the predictors are compared to the other groups. My initial reaction was that he could just do a single hierarchical regression that would allow him to i) identify which predictors are significant and ii) determine how these significant ones compare to each other in predictive strength. Not sure if that is feasible though.', "">A single dataset with a single outcome variable. I think his rationale for doing three separate regressions is that it will help him better understand how important each of the three groups of the predictors are compared to the other groups. \n\nHow does that work? If I have a model $y = a + bx + e$ and $y = c + dz + e' $ how can I compare the effects of x and z (if they are not comparable)?""]"
[Q] Show sum of exponentials is chi-square,"\*\*Sorry, I can't change the title. I am looking to show that 2 times the sum(X_i - X_{(1)} is Chi-Square(df = 2n - 2) \*\*

\---

if *X1, . . . , Xn* are iid with pdf

*f(x|s, t) = s*^(-1)*exp(- s****^(-1)****(x - t))* ***1***\*(x > t)\* , (exponential with scale *s* and location *t*)

How do I show that *2s****^(-1)****\[ ( X1 - X(1) ) + . . . + ( Xn - X(1) ) \] \~ Chi****^(2)****(df = 2n-2)* ?

where X(1) = min(X1, . . . , Xn) and ***1***(x > t) is the indicator function which is 1 when *x > t* and zero otherwise.

\-----

My first thought was to decompose the terms in the sum into *( Xi - t )/s - ( X(1) - t )/s,*

which would then give *2( n\*(Y1 + . . . + Yn) + Exp(V) )*, where Yi  are iid \~ *Exp(1)* and *V \~ Exp(1)*. But *Yi* is not independent of V.

Someone suggested showing that *sum( Xi - X(1) )/s* are independent of *( X(1) - t)/s* using the joint pdf of the order statistics. But after this I am not sure how to proceed beyond this.

I feel like I'm missing something obvious",12vcnk9,zebrapaad,1682183778.0,2,0.76,"['**Hint:** show that exponential spacings are independent and remain exponential random variables (possibly with different rate parameters). This can be done by looking at the order statistics of iid exponentials and applying a change of variables. You can then write the sum you are interested in as a linear combination of the spacings. You can take t = 0 and s = 1 WLOG.', 'Sorry for the mediocre formatting of the math. Reddit seems to change things when I edit.', 'I think it should be asymptotically distributed as, not distributed as.  Then you can use Slutskys theorem for the first order statistic.', ""The chi-squared distribution arises from sums of squared unit normal random variables.  \n\nI've never heard of a Genesis from exponential random variables....\n\nEditing to say that after some more thought, I recall the chi-squared and exponential are both special cases of the gamma distribution.  This implies there may be a scale transformation that you are looking for."", ""A sum of exponentials is not chi-square in general. (per your title)\n\nThe quantity you're asking about is not merely a sum of them"", 'Sums of independent chi square RVs are chi square... sums of exponentials follow a gamma distribution. \n\nThis will be easiest to show with moment generating functions. The moment generating function of the sum is the production of the individual moment generating functions.', 'by ""exponential spacings"" you are referring to $X\\_i - X\\_(1)$?\n\nedit: I just googled it.\n\nspacing = $X\\_(i) - X\\_(i-1) for i /ge 2$ ?', ""Reddit uses a version of Markdown. So if you use things that correspond to Markdown formatting, that's how they'll be interpreted."", ""No they're dealing with an exact finite sample result\n\nAn (unshifted/ one-parameter) standard exponential, when doubled, is chi^(2)(2)"", 'I apologize. I was trying to be concise.', 'Yes. Each X_(i) - X_(1) is a sum of spacings.', ""aah I didn't know this trick. Thank you!!""]"
[Q] Master's Degree: Statistics or Computational Statistics and Machine Learning?,"Hi all, I am choosing between 2 postgrad programmes, Statistics or Computational Statistics and Machine Learning in one of the universities in the UK. My future goal is to be a quant/trader or a data scientist/machine learning engineer (but not interested in being a 'sql monkey' or doing dashboard and visualization stuff, but more on modelling stuff).

I am torn between these two programs but one is seen as more academical and traditional and the other may be seen as more modern and practical (but some may think it's just buzzwords and just a cash cow programme).

I am keen in both traditional statistics, but also interested in coding. I have a bachelor's degree in Data Science (which again, may not be seen as too academical), which involves already involves both statistics and coding . I know this is a STATISTICS subreddit, but would love to hear opinions from everyone!

What would my best choice be?",12vb5vh,No_Canary_5299,1682180867.0,8,0.71,"["">I am keen in both traditional statistics, but also interested in coding.\n\nIn my experience it's far easier to learn traditional stats formally and coding on the side than the other way around."", 'I think the selection of modules you can take from CSML are more varied and more applicable to different job fields', 'Depends what you want. Example, if you plan on just trading or working with ml models then CSML is a great option. If you want to work with ml models and have a deep understanding of them then statistics. Now if you want to create your own ml models get a PhD stats.', 'In my opinion, I would get a stats MS. If you want to go the quant or industry researcher route, you will need to be very strong in your maths. In my opinion, as a Stats PhD student in a very Bayesian department, you should look for a program that is going to make you learn some good statistical foundations AND modern simulation techniques/methods. In my opinion, you cant really do modern statistics without being very competent in your coding and simulation techniques. Obviously I am biased, but I would look for a place that has a Bayesian slant so you will get a good mix of classical statistical asymptotic inference/methods and the more computational simulation techniques leveraged in most Bayesian research or course work.\n\nYou would be surprised how many departments simply dont teach you any coding or simulation techniques in favor of things like mixed effect models, ANCOVA and measure theory. There is 100% a time and place and need for those things, but not for the types of things it sounds like you are interested in.', 'Hi, thank you for your suggestions. Unfortunately looking at the course structure, it seems the two programmes do not focus on both statistical foundations AND modern simulation techniques/methods. It is either or. Would you be able help me take a look at the curriculum and suggest which would be better?']"
[Q] How would you determine if a new measurement is different from a series of previous measurements?,"
There seem to be many approaches to this type of question for different fields. What do you view to be the most appropriate analysis method to determine if a new measurement within an individual is significantly different from the previous series of measurements within that same individual?

 I am purposely trying to not use any leading jargon here to see the widest variety of answers.",12uugb9,thebigmotorunit,1682137430.0,15,0.9,"[""One approach would be to treat the value of previous measurements as a distribution. For a new measurement, the number of standard deviations away from the distribution's mean would inform how different the new value is from previous values. \n\nNaturally there are caveats, exceptions, ways it can fail, and thresholds to determine significance. This may or may not be appropriate for a given data set but works in a wide variety of contexts."", 'It very much depends on what you want to assume and what alternatives you seek power against.', 'Depends on the measurement.', 'As a civil engineer with a background in empirical modeling of systems, this problem comes up often. Before you design your test (whatever that ends up being) you need to understand what system the data is coming from, and what is being measured. Ex.: is the data coming from a mechanical system, a biological system etc., is the data captured by a sensor with a given frequency or is it more stochastic, etc.? Basically, you try to assess what you know about the system and the data you are looking at before you try to make any models. If it is a mechanical system, is it possible to describe the system mathematically (ex.: as a state-space equation), or is the system  highly complex or stochastic in some way (ex.: where simply estimating the system with a linear regression or other data-driven model can provide a good enough description)? Your model design is important, and honestly as much of an artform as it is a science; experience and domain knowledge of the system you are trying to model helps inform choices here. Poor modeling choices; over simplification or (in the other direction) overfitting needs to be avoided if you want to say anything about new data points this is why models of complex or unfamiliar systems are typically only developed with a subset (ex.: the first half) of the overall dataset, and the remaining data (second half) is used exclusively for validating that the model performs well. With a good model of the data set in hand, it is not only possible to understand if the new data point belongs to that system, you can often say why it belongs (or not) this is one of the reason I frequently eye-roll when younger data scientists come back to me with a ML model that was trained on the whole data set, has amazing performance and fit and then breaks down completely when new data is introduced, or the model is used to make some prediction outside of the bounds of the training data. Without understanding what you are modeling and taking precautions to validate the model, there is a risk that you choose a poor model type to approximate the system, and end up overfitting without knowing why. Im all for ML btw especially now that it is so cheap to create amazing models that capture so many system features but I want engineers to know when it makes sense to use it, and most importantly understand WHY they use it when they do but if a rant, but to summarize my answer to your question (1) figure out what you know about the data and the system it comes from (2) make an informed choice in model section (3) validate the model with an unusedportion of the data set (4) use the model as a way to test if the new data point belongs or not.\n\nEdit: continuation \n\nThe exact test you use to determine if the new data point belongs depends on the model if the systems outputs (your data set) can be described by a simple normal distribution, you can measure how many standard deviations away from the mean it is and pass judgement that way even non-normally distributed data can be tested with a variety of statistical methods. Personally, as an engineer, I find this somehow unsatisfying as it doesnt necessarily tell me much other than how likely a data point may or may not belong to the overall system I typically want to understand why the system (or the model used to describe it) might produce such a data point (under what conditions that data point might occur, in addition to how likely it is to occur) if the system can be described by a first principle (ex.: physics or thermodynamic, etc.) equation(s) then you can literally plug in the data point and understand if the system can behave in that way, or under what conditions it might, Or, if the system is stochastic or otherwise complex you can try to localize which part of the model might produce the data point you are analyzing (ex.: it could be noise from the model, or some system behavior that is not well captured by the model, etc.) \n\nGood question! Would be curious to see how others might approach this!', 'You can check if the new observation is outside the range of previously observed values.\n\nIf you can make assumptions about the distribution, you can estimate parameters of that distribution from historical data, then see where the new observation falls on the CDF.', ""This is a good approach if we don't expect a trend. If there can be a trend (e.g. following weight of the individual) then we should probably remove that trend first."", 'AKA control chart', 'Look up time series change point detection class of algorithms', 'Do you have some examples that would require different analyses?', 'How would your approach change depending on the data?', 'You say, ""within an individual"". How many individuals are there? How are they related? What are the expected patterns within individual as suggested by the theory of your specific context?\n\nThe issue is that the approaches vary so widely that it\'s nearly impossible to answer this question without context and more detail. \n\nFor example, if the suspected change is at the end of a series of values then you might use statistical process control. If not you might use standardised residuals. But these are not so much approaches as they are entire fields of statistics.']"
"[Question] How to calculate the probabilities of certain outcomes in Magic: the Gathering tournaments? (This is an interesting question, I swear!)","*Edit: This has been* ***solved****! Thank you to* u/WhosaWhatsa *for pointing me towards Markov chains. Now I'm off to go figure out a good, free way of modelling those...*

Earlier I was trying to calculate the expectation value of some events in Magic: Arena. They generally have the structure of ""Play games until you win X or lose Y"". For instance, one event might continue until a player has *either* 7 wins *or* 3 losses. I wanted to calculate the EV of the rewards for these events, factoring in my winrate in the game.

I've taken some stats classes, and gotten As in them. This seemed like a pretty approachable problem, so I  googled it, found [an existing thread](https://www.reddit.com/r/statistics/comments/11wzl9f/q_odds_for_magic_the_gathering/) that offered some pointers, opened up Excel and R, and started messing around with the negative binomial distribution. I can't bend my head around how to do this.

So my end goal is to find the expectation value of an event in-game, where the rewards are based on the number of wins achieved (up to 7) before reaching 3 losses. If I'm understanding it correctly, this means the probability space is {0-3, 1-3, 2-3, 3-3, 4-3, 5-3, 6-3, 7-3, 7-2, 7-1, 7-0}. The problem is that these are obviously not equally weighted--a 7-0 record is less likely than a 3-3 record, and the chances of each depend on an individuals winrate.

As far as I can tell, the Negative Binomial Distribution is *should* to being able to address this, but doesn't actually. To my understanding, it gives us the number of *attempts* we must make before seeing a certain number of failures, which we can extrapolate a probability from. This seems like it should work--I set up a spreadsheet that you punch a winrate into, and then it tells you the probability that you'll end with any given win-loss record in that probability space. But I keep ending up with total probabilities that aren't even close to 1, particularly for very low winrates. ([Here](https://docs.google.com/spreadsheets/d/1hSKoi3-Mbs3UbpVwZCSqO0nw_tiIRUci/edit?usp=sharing&ouid=109837159073213104462&rtpof=true&sd=true) is a link to the spreadsheet I was working on--it doesn't look like much, but it's as close as I could get to an actual solution. I also spent a decent amount of time in R, and didn't get far with that either.)

This has driven me completely insane. I think part of the issue is that not every string of possibilities is of equal length. (ie, 1-3 vs. 7-0.) Also, not every permutation of wins and losses is permissable. (WWWLLL is allowed, LLLWWW is not.) Finally, not every string ends in an L. (0-3 through 6-3 must all necessarily end in losses, but any 7-win sequence ends in a win.)

How the h\*ck do I calculate this? Is my approach fundamentally flawed? Am I totally misunderstanding how the negative binomial distribution works? Or am I missing something else totally obvious? At this point, I'm pretty well beaten by this problem, and I don't have any clue how else to proceed. Can someone give me some pointers?

Thanks y'all.",12usw0n,JacenVane,1682133444.0,25,0.94,"['It may be helpful to consider these probabilities as part of a markov chain. In other words, the probability of winning or losing a match in-game may be more dependent on the present status of the win/loss situation at that time.', "">If I'm understanding it correctly, this means the probability space is {0-3, 1-3, 2-3, 3-3, 4-3, 5-3, 6-3, 7-3, 7-2, 7-1, 7-0}\n\nI don't think 7-3 is possible, since you'd need to be either 6-3 or 7-2 beforehand (unless Arena counts a draw as both a win and a loss?)\n\n>0-3 through 6-3 must all necessarily end in losses, but any 7-win sequence ends in a win\n\nWe can solve this by using two negative binomials, the number of wins before hitting 3 losses A(x)=nbinom(3, 1-winrate)(x) [x from 0 to 6] for the probability of going x-3 and the number of losses before hitting 7 wins B(x)=nbinom(7, winrate)(x) [x from 0 to 2] for the probability of going 7-x.     \nWe don't even need to renormalize, since the tail of one distribution is the body of the other (i.e. you see 7+ wins before hitting 3 losses if and only if you see 2- losses before hitting 7 wins, so [sum A(x) from 0 to 6] + [sum B(x) from 0 to 2] = [sum A(x) from 0 to 6] + [sum A(x) from 7 to infinity] = 1)"", ""I did exactly that not long ago, I'll check if I still have the notebook in which I redacted the formula and how to get it.  \n\nEdit: I didn't explain everything, that's a bummer.\n\nThe idea is that in order to win a bo5, one needs to get the third win. So the probability of winning a bo5 is the probability to win a match * the probability to get a score of 2 to x with x = 0, 1, 2. with p the probability to win a single match:  \n\nP(3-0) = p * P(2-0) = p * comb(2, 0) * p^2  \nP(3-1) = p * P(2-1) = p * comb(2, 1) * p^2 * (1-p)  \nP(3-2) = p * P(2-2) = p * comb(2, 2) * p^2 * (1-p)^2  \n\nP(X-Y) if about all the ways to reach this end, so a realization of a binomial (X+Y, p), hence the formula with the 2 choose k above.  \n\nAnd from there:  \nP(win boX) = sum {from i = 0 to (X-1)/2} comb((X+1)/2, i) * p^{(X+1)/2} * (1-p)^i"", 'did you just censor the word heck?', ""For this specific case, it sounds like you're not interested in solving the general problem but just want to know your EV for the current game as it exists. That's some simple probability, which can be easily done in a spreadsheet program like Google Sheets. The only assumption I make here is that the probability of winning a certain match is constant throughout the tournament, which in reality is not the case. I assume you have the same probability of winning a game when you're 0-2 as when you're 7-1, which probably is not correct: as you progress in the tournament, you'll face stronger opponents and your win percentage will drop. But if we ignore that, you can just fill in your win percentage per game and see what your EV is.\n\n\nTo clarify your problem with calculating the probability of each outcome: split the probability space in events where you first reach 3 losses and events where you first reach 3 wins. In the case of 3 losses, the final match is a loss, so you only calculate combinations for the first n-1 matches. In the case of 7 wins, the final match is a win so you only calculate combinations for the first n-1 matches. \n\n\nIf you have a win percentage of 50%, your EV is 820 Gems and 2.5 Packs. With 55% this increases to 1000 Gems and 2.9 Packs, with 60% even to 1190 Gems and 3.33 Packs. \n\n\nHave fun playing around with the sheet (you can download it and adjust Win% and prizes in case they ever change these). And yes, I'm a mathemagician :)\n\n\nhttps://docs.google.com/spreadsheets/d/16RMVOrih2VakzrpyYo8OE4pYkUh7NQaxVvMm-MTrGos/edit?usp=sharing"", ""I kept reading the comments and I saw that you were interested in the drafts. I did a computation about MTGA drafts even a bit earlier. I did it in colab so I can just share the notebook. You can change values inside and it displays curves and shit so that you can see when and where which stuff has the higher EV:\n\nhttps://colab.research.google.com/drive/1GO_elVdGf66yZF5duo66ciokW44ZUXWz?usp=share_link  \n\nI did that quite some time ago and maybe the reward structure changed in the meantime, but hopefully everything is transparent in the code and you can change the values you're interested in.  \n\nFun fact: I didn't get back into MTG in the end lol."", ""It's interesting that you should mention this problem. I tackled it 2+ years ago. You might find my Jupyter notebook helpful (although it's written in Python and not R): https://nbviewer.org/github/multimeric/MTGA-Gem-Rewards/blob/master/MTGA%20Limited%20Format%20Rewards.ipynb\n\nThe gist of what I did (although I forget if it's correct now!) is to map the outcomes of a negative binomial random variable x onto a new random variable whose values are the gems you receive. The outcomes and their values are determined from x using this table:\n* x == 0 => 50\n* x == 1 => 100\n* x == 2 => 250\n* x == 3 => 1000\n* x == 4 => 1400\n* x == 5 => 1600\n* x == 6 => 1800\n* x >= 7 => 2200\n\nThis resolves the issue of x having an infinite range, which I guess is what you are having trouble with? You can kind of see me doing the mapping in this function which I've replicated from the above notebook:\n\n    def premier_ev(win_rate = 0.5, pack_value=200):\n        rewards = [\n            50 + 1 * pack_value,\n            100 + 1 * pack_value,\n            250 + 2 * pack_value,\n            1000 + 2 * pack_value,\n            1400 + 3 * pack_value,\n            1600 + 4 * pack_value,\n            1800 + 5 * pack_value,\n            2200 + 6 * pack_value\n        ]\n        # games counts the number of games until we get 3 losses\n        games = nbinom(n=3, p=1-win_rate)\n        ev = games.expect(\n            ub=7, # We can only win at most 7 games\n            func=lambda x: rewards,\n        ) + (games.sf(7)) * rewards[-1]\n        # We have to add the probability of losing less than 3 times, still counts as 7 wins, hence this last term.\n        # This saves us having to sum over the possibly infinite range of the RV\n        cost = 1500\n        return ev - cost"", ""I haven't worked with Markov chains before, but they look like they may be applicable? Do you have any recommendations for accessable resources for learning about them?"", "">I don't think 7-3 is possible, since you'd need to be either 6-3 or 7-2 beforehand (unless Arena counts a draw as both a win and a loss?)\n\nYou are correct, that was a typo on my part. Sorry.\n\n>We can solve this by using two negative binomials,\r\n\nSo that's what I was trying to do, but the probabilities I was getting out of it were not adding up to 1. Is this to be expected?"", 'Yes, because I thought it would be funny.', "">it sounds like you're not interested in solving the general problem but just want to know your EV for the current game as it exists.\n\nMy original intent was to build something more general, but that turns out to be significantly harder than I expected.\n\n>The only assumptionI make here is that the probability of winninga certain match is constant throughout the tournament, which in reality is not the case. I assume you have the same probability of winning a game\n\nI actually think this is a defensible assumption, if for no other reason than plugging in your winrate from the ladder is the best metric you have for trying to gauge EV. But also--and this is confusing as hell to me--I haven't actually seen a marked change in winrates across a pretty wide variety of ranks. (If you play, I'm talking about like a ~3% variation between Platinum 4 and Mythic 500 or so.) I'm not sure quite what to make of that, but it does seem to support the idea that just plugging in a winrate is a decent enough number for EV calculations.\n\nSeriously, thanks--this abstraction was actually the next one I was  trying to tackle, but couldn't quite make it work. (My approach was essentially trying to figure out the odds of the 3rd loss or 7th win occurring on the nth game. It was... Not pretty.) I really appreciate the link! Do you mind if I do some 'poking around under the hood' after I download it to try and figure out if I was on the right track?"", ""Thank you, I'll dig into that! I don't know Python (I mean, I don't wanna oversell my R either lol) but it seems like something that I might be able to bash together haha.\n\n>This resolves the issue of x having an infinite range, which guess is what you are having trouble with?\n\nOne of the issues, yeah. I don't think it was the only (or even the biggest issue) but it definitely was."", 'This article may be a helpful start as it also addresses markov processes in the context of another discrete time period based competition in tennis. Not entirely sure if it lines up, but do Magic games have points-based damage counts eventually leading to a win or loss? If so, it seems similar to tennis scoring, as well as martial arts competitions. \n\nhttps://towardsdatascience.com/markov-chain-models-in-sports-7cb907a6c52f', 'Oh boy youre in for an awful weekend depending on how complicated this gets. Markov chains/processes are real math', ""They should add to 1. It looks like you have the x-3 distribution wrong. Since we want the sequence to end on the 3rd loss, losses are the successes, wins are the failures, and the probability of success is the probability of losing, so if I'm not misunderstanding NEGBINOM.DIST's syntax D3 should be NEGBINOM.DIST(B3,C3,1-A3,FALSE) rather than NEGBINOM.DIST(C3,B3,A3,FALSE)"", 'oh ok, it was funny ', ""> Do you mind if I do some 'poking around under the hood' after I download it to try and figure out if I was on the right track?\n\nNot at all, that's why I built the sheet with as few hard variables as possible, so you can indeed play around with the numbers and see what happens! You can use this sheet as a basis for any W/L structure other than 7/3 as well, just add or delete a few rows. Curious to hear what you find, please send me a DM if you have any interesting results you want to share."", ""I guess if I had to translate this into R I'd do something like this (assuming the parametrisation of nbinom is the same in R which it may not be)\n\n    > win_rate <- 0.5\n    > max_failures <- 3\n    > # The probabilities of each outcome of the transformed RV\n    > # These are the same as nbinom densities except for x > 6\n    > probs <- c(\n    +   dnbinom(0:6, size=max_failures, prob=win_rate),\n    +   pnbinom(6, size=max_failures, prob=win_rate, lower.tail=FALSE)\n    + )\n    > probs\n    [1] 0.12500000 0.18750000 0.18750000 0.15625000 0.11718750 0.08203125 0.05468750\n    [8] 0.08984375\n    > # Note that the probabilities sum to 1!\n    > sum(probs)\n    [1] 1\n    > outcomes = c(50, 100, 250, 1000, 1400, 1600, 1800, 2200)\n    > # Calculate EV\n    > sum(outcomes * probs)\n    [1] 819.5313"", ""They do, they aren't the *only* way to win a game, or even strongly predictive of who will. (For instance, certain strategies might deliberately sacrifice life totals.) That makes me a little hesitant to use them as a metric.\n\nThank you for the link--I'll read up on that!"", ""Fortunately I think that it's going to pan out in a relatively simple way. Each state's transition matrix only has two nonzero transition probabilities, and I think I can get away with modelling those as each being a constant derived from a players winrate. (Which I think I can get away with modelling as unchanging, for reasons that have more to do with gameplay than math.)\n\nActual complex Markov chains look intimidating as hell, but this is a little baby one haha."", ""Fuuuuuck that makes sense. Thank you! That's only for X-3 sequences though, right? The 7-X ones should be fine as-is?"", 'Can you explain how/why you use each negative binomial function?', 'I see. If accounting for all of those strategies is infeasible, as well as accounting for all of the other many variables that could influence point gain/loss, then the markov chain may be a good option. Happy hunting.', 'They can be super simple too thankfully! Wait till you get to a continuous time problem, I gave up lol', 'Yeah, the 7-x are right', ""`dnbinom` is the PMF of the negative binomial (ie `P(X = x)`), which we use for the first 7 outcomes because they're the same for our transformed random variable. `pnbinom` is the CDF, and `pnbinom(6, ..., lower.tail=FALSE)` is `P(X > 6)` for a negative binomial, which is the probability of the outcome that you get 2200 gems, because even if you were going to win 10 games, you still only get 2200 gold because the event finishes. I concat together those two sets of probabilities with `c()` to make a vector of all outcome probabilities."", ""OK, as an update on this, I found a very crappy tool intended to teach college students markov chains, and it is in fact the correct way to model this process. Thank you very much! (A markov chain wouldn't be suitable for predicting the outcome of an individual game like in Tennis, but the *tournament structure*, which is what I was looking to model, is very, very similar. Seriously, thank you so much for helping me make that connection!)\n\nOf course, now I need to go and find some actually good tools for modelling markov chains..."", ""Yeah, those do not look fun. Fortunately (if I'm modelling the problem right) time is both discrete and actually finite here, which is much easier haha."", 'Hey thanks for the update. Really glad that you found it helpful']"
Can you help me articulate the error with the approach?[Q],"Id like to determine the average ratio of CEO to worker pay in the USA. Someone would like to take the average CEO salary and divide it by the average worker salary. I think this wouldnt be a statistically valuable approach, but Im having trouble explaining why. Can you help? Or explain why Im wrong and it is a good approach?",12ulhq8,BenevelotCeasar,1682117378.0,2,1.0,"['The first thing to be clear on is that the ratio of average and the average of ratios  (average of workplace ratios) will be different, so you need to be sure you have the one you want. \n\nAnother thing to beware of is that CEOs often have substantial benefits that are not cash salaries. These often get missed in such comparisons', 'This sounds like you have a narrative you\'d like to support and are looking for ""statistics"" to back it up.\n\nI would expect the CEO pay is heavily right skewed as CEOs of large corporations make significantly more than CEOs of small corporations. This means the [average](https://en.wikipedia.org/wiki/Arithmetic_mean) is less than meaningful for CEO pay. There also appears to be a false dichotomy, in that there are either ""CEOs"" or ""workers"", apparently ignoring the rest of the c-suite and professional workers who can easily pull down large pay checks as well.', "">Id like to determine the average ratio of CEO to worker pay in the USA. Someone would like to take the average CEO salary and divide it by the average worker salary.  I think this wouldnt be a statistically valuable approach, but Im having trouble explaining why\n\nYou could take each firm i and calculate the average worker pay, x\\_i. Divide the CEO's pay, y\\_i by x\\_i.\n\nThen you calculate the average ratio over all firms, sum (y\\_i/x\\_i) \\* k\\_i where k\\_i = 1/n\n\nAlternatively, you could take the average CEO pay, sum y\\_i / n and divide by the average of firm-average worker pay, sum x\\_i /n. Or just sum y\\_i / sum x\\_i.\n\nThat latter quantity can be written as  sum (y\\_i/x\\_i) \\* k\\_i where k\\_i = x\\_i/ sum x\\_i.\n\nDo you see the problem with that weight k\\_i=x\\_i/ sum x\\_i, compared to the first approach, which has k\\_i = 1/n? That's one issue."", ""Others have discussed the various trickeries in defining this quantity. I'd like to point out that it isn't necessarily very useful.\n\nHedge funds, for example, have extremely high incomes whether you use the mean or the median. They contract out their low wage roles, like cleaning and security. Labour intensive areas like agriculture, food processing and retail have a lot of very low wage workers (but the senior executives are still making out like bandits). It's very hard to see how you could make a useful comparison between sectors or what a pooling of ratios would mean because it would be so heavily dependent on the mix that went into the calculation (making comparisons between states, and points in time, especially difficult).\n\nI think it's more useful to compare top incomes to the minimum wage because every large organisation is using minimum wage workers somewhere along the line, directly or indirectly. It should probably be federal minimum wage too, as most of them will be sourcing at least some input labour from the lowest wage states. That still ignores some complexities, like prison labour and tipped minimum wages, but it's a start.\n\nAverages hide a lot. The whole distribution is more informative, and using the bottom of it usually more meaningful in this sort of context. Obviously it depends a bit on what you're trying to achieve, there are oodles of analytical choices and they all subtly alter the nature of the question you're asking and thus how the answer can be interpreted.\n\nWhatever you're trying to achieve, find some useful ways to plot the data as well as, or instead of, trying to come up with a one number summary."", 'I understand its a political subject. Im not asking to support a stance. Im no statistician but I understand enough to know if presented with the question what is the average ratio of CEO to corporately employed employee  within the United States in 2021\n\nThere are many methodologies you could take, some useful, some not, but the useful arent wrong per say, just some approaches work best for some situations or problems. \n\nIf that approach provides valuable insight great, but Id like the rational of why it either is or isnt and if it isnt', 'Im sorry, I follow mostly what youre saying, but you totally lost me at the end. I know youre asking me if I see the issue between something to do with weight and population sample I think but I dont really understand?', "">I understand its a political subject.\n\nIt's not political at all. It just sounds like you're starting with a conclusion and want to find an analysis that will provide results that support that conclusion. That's not how science is done.\n\nFor that, and other reasons pointed out by u/jentron128, you're practicing bad statistics."", ""Both ways of calculating (ratio of averages, average of ratio) are weighted averages of the firm-specific ratios.\n\nDo you think it's a good idea that instead of giving every firm the same weight, so 1/n where n is the number of firms, we give them the weight x\\_i / sum x\\_i, ie, how much share of the total worker pay that firm has among the firms you are analysing?\n\nThis would give more weight to the CEO-worker pay gap, measured by the ratio of CEO pay to the average pay workers get at that firm, of those companies that pay their workers a lot.\n\nEg if you have three firms in your sample, two minimum wage shops and google, the CEO to worker ratio of google will contribute much more than 1/3 to the overall estimate if you choose to use the ratio of averages. It would contribute 1/3  if you choose to use the average of ratios. To the extent that you're unhappy with such weighting, the ratio of averages is a bad choice.\n\nOn the other hand, perhaps such a weighting, or something similar (eg, by number of employees, or total assets, or total revenue) is actually what we want."", 'As I explained to him, we cant event get to whether theres a stance if we dont agree on the quality of the metric in the discussion.\n\nIs the methodology described an approach that will provide insight or is there a reason the data would be skewed in some way and another approach should be used? \n\n\nIt seems to me that both of you decided bc of the political nature of the any discussion around CEO pay that I have a foregone conclusion. Im no statistician but I do work with data and actuaries daily and do enough analysis to have a hunch something isnt right with the approach but Im not advanced enough to follow my hunch to either validate or invalidate. I had a hypothesis I dont know how to test. So I came to a subreddit and stated where the question arose, and the methodology in question. \n\nCan you please give me your thoughts on the method to approach the question ?', 'This really helps! I do understand theres issues no matter how you measure, the metric itself is of questionable value and presents a loaded perspective, but its difficult to have any convo if we cant at least agree to measure the same thing - while also understand limitations of the thing were measuring. \n\nReally appreciate the follow up I know my reply was kind of starkly I was frustrated with my inability to articulate what I wanted to say.', ""One approach asks about the ratio of the compensation of average CEO compared to the average non-CEO, across all companies, which isn't very useful at all.\n\nThe other approach asks about the ratio of compensation of CEO to average non-CEO, by company, which is only barely more useful.\n\nPartitioning workers as CEOs or non-CEOs is an extreme oversimplification, and will likely lead to results that offer little insight."", 'I agree with you OP; theres nothing political about your question.  Youre asking a methodical question.  This website looks at it on a company by company basis: https://aflcio.org/paywatch/company-pay-ratios']"
[Q] Paid Ads Probability of Never Converting,"Hello All - appreciate any help or insights on this question:

I am the paid ads manager for my company. I am running ad campaigns where we bid a specific amount on individual keywords related to a product. There are sometimes that the keywords are distantly related to the product and never drive a conversion. I am wondering how many clicks (or views) on a specific keyword we need to be sure that the keyword is x% likely to never deliver a sale. I will give data as a specific example.

The Product is a ""Knee Brace"" in general the knee brace converts 10% of all visitors. Across all traffic to the listing.

We are bidding on the term ""arthritis products"" we have had 100 visitors to our knee brace product and had 0 conversions.

Is there a way to give a % to the certainty that the term ""arthritis products"" will never deliver a sale for the knee brace?",12uhaa4,JAF805,1682109066.0,2,1.0,"[""I might be totally misunderstanding the problem, but here's a suggestion.\n\nLet's say there are n clicks for a particular keyword, each with probability p of converting to a sale. Assuming each click is independent, the number of conversions x has a binomial distribution x \\~ Bin(n,p). You know x and n, but not p.\n\nYou're interested in p, the probability of conversion. In particular, you want to know the chance that p is 0, or very small, if x is 0.\n\nOne approach is Bayesian - the [beta-binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution) distribution. This is where x has the binomial distribution described above, and uncertainty in p has a beta distribution. The distribution for p is updated after you observe x (there's a formula on wiki). You can then calculate the probability of p being less than 1% given that x=0, for example, by integrating the updated distribution.\n\nThe other approach is frequentist. Basically, you consider the hypothesis test with the null hypothesis p > 0. Neyman-Pearson theory can tell you the n (how many clicks) you need for the hypothesis test to reject the null hypothesis. I'm not too familiar with this approach, but might give you an idea to start looking. Note that this doesn't give the chance that p is 0, but rather says the data are unlikely if p is not 0."", 'Do you have data on other keywords / conversion rates?  Are others about 10% also?', 'I think you are understanding it clearly.  And I appreciate your suggestions.  In short I trying to understand if I want to continue bidding on a particular keyword.  \n\nSo, I believe the conversion rate of each keyword needs to be treated independently.  My goal would be to determine the likelihood that the conversion rate is low enough that I am not willing to bid on it.  To give further detail on the knee brace example:\n\nTerms:\n\n""Knee Brace"" - hyper relevant - 10% conversion rate\n\n""Help my knee arthritis"" - moderately relevant - 2% conversion rate\n\n""Honda Accord"" - not relevant - 0% conversion rate\n\nI am trying to understand, how many samples do we need to be sure that the conversion rate of the ""Honda Accord"" term is 0% or statistically significant that it is below .1% lets say.  Its obvious with the human eye that ""Honda Accord"" is not relevant, but I am trying to build an algorthm that does not require that human eye', 'I do and would have data on all keywords and their conversion rates.  For the sake of this exercise I made this data up.  But basically my goal is to understand when I should stop bidding on this keyword and I can be sure it is not likely to drive a purchase.', 'Ok so if you have some kind of baseline data, you could use that to estimate the probability of x # of failures before the first success using the negative binomial distribution.  \n\nIf we used the 10% baseline the formula in excel would be:\n\n= 1 - (NEGBINOM.DIST(100,1,0.1,TRUE)) = .0000239\n\nThis is the probability of 100 or more failures before the first success if the probability of success on each trial is .10 (10%).']"
[Q] why do we opt to test the null hypothesis instead of testing our alternative hypothesis instead? Is it because we don’t have enough data to make the alternative hypothesis specific enough yet? Or because multiple alternative hypotheses could yield similar data?,,12ufg3c,Ddawn_123,1682105442.0,15,0.94,"['Because the sampling distribution of the test statistic is known under the null hypothesis, that is assuming that the null is true.', 'Usually the alternative hypothesis encompasses a range of parameter values, not just a single one. This makes it impossible to calculate how the test statistic will be distributed under the alternative without knowing *which specific* value under the alternative it will take, which in turn makes it impossible to develop a test for.\n\nIn case where the alternative is also a point hypothesis I guess it\'s possible to test it. E.g. H0:  = 50 vs H1:  = 60. But then it\'s mostly semantics which one is the null since you can flip them around at will.\n\nAlso for hypotheses that are not about parameter values of a model, this same idea of null = one case, alternative = many cases still holds. For example in normality tests you have one hypothesis that says normality and another that says non-normality. There\'s an infinite amount of non-normal distributions, hence you cannot calculate how the test statistic will behave in general under ""non-normality"". So in normality tests, the null is always normality.', ""I think it's also important to appreciate the historical reasons for this approach.  Once they were developed and disseminated, simple tests in the *null hypothesis significance test* paradigm were relatively easy to compute, with just a table of critical values and, later, a hand calculator. \n\nIt might be argued that now, with everyone having computers, there are are different paradigms maybe we should teach as the default approach, perhaps Bayesian inference."", 'Null is what one considers to be an established fact whereas alternative is the possibility that such an established fact is in fact not a fact. Philosophically speaking there are infinite number of possible states of this world and you just cannot believe something while also believing in another which disproves said belief.', 'Its convention because to test a hypothesis you need to start with a hypothesis that can be rejected (null).\n\nThe alternative hypothesis isnt designed to be rejected - its there to show the null hypothesis is incorrect.', 'You are describing an equivalent test. \n[TOST](https://en.m.wikipedia.org/wiki/Equivalence_test)', 'Cos how else U gonna confuse the f out of people with double negatives in every other sentence?\n\n:)', 'I think you can do that though. Just depends on what your research question is. Usually for null hypothesis, we design a test to reject such potential relationship.', ""Think about it. If you want to test a simple alternative, then it goes without saying that a positive finding would correspond to accepting the alternative. We wouldn't actually accept the alternative, we would do a noninferiority test, e.g, the effect is with x% of the alternative, which in turn provides useful information only if x is relatively small. This requires a much larger sample size than a corresponding test of the null."", 'I am pretty sure other people can explain it better than me but I will give a go.\n\nLets take for example normality test. Not rejecting the null hypothesis means that you sample is likely to follow normal distribution. Rejecting the null hypothesis means that it doesnt follow normal distribution and therefore could follow any distribution under the sun. Accepting the alternative hypothesis means that you accept that your sample could be any other distribution that normal which adds no information at all. So I guess technically you can accept the alternative but in practice you accept nothing.\n\nYou can instead of normal distribution test another distribution lets say gamma but thats a new test where the null hypotheses now that your sample follows gamma.\n\nSame things happens with any testing.\n\n\nEdit: apparently someone had answered already', 'Ok, so basically we dont have enough data about the alternative hypothesis yet ?', 'Ok thats what I was thinking, thanks', 'Not exactly. It\'s more about the structure of hypothesis testing and how it works. \n\nIn null hypothesis testing, we usually assume a point null, that is, a null hypothesis that is a single point. This null is specific enough that we can describe the distribution the test statistic would have if the null were true. Then we look at where the test statistic calculated from our data lies within that distribution. If it\'s very extreme (indicated by the critical value or a small p value) then we reject the null hypothesis because the test statistic we got from our data would have been unlikely if the null were true. The alternative hypothesis in this setting is really just ""not the null"".\n\nThere are other kinds of hypothesis tests that compare two point nulls, (e.g. mu=0 vs mu=1) but are performed using different methods.', 'Ok, but say theres a weak relationship between two variables and your p value causes you to incorrectly determine that you shouldnt reject the null. But some other hypothesis would more likely result in your sample.. is that why ppl favor Bayesian approach to this ?']"
[Q] Help replicating power analysis from a published article,"Hi all, I hope you're all doing well!

I'm a student assisting a MD with a research project, and I have been tasked with trying to replicate a power analysis from a published paper. Specifically, I am trying to replicate the steps the authors completed to generate the results found in Table 1 of this article: DOI: [10.1177/23259671221110851](https://doi.org/10.1177/23259671221110851)  but for my own data. 

In their methods, the authors wrote the following: ""The statistical power to detect a difference equivalent to the lowest reported MCID between tenotomy and tenodesis groups was calculated using G\*Power with alpha = 0.05.""

I have a very little experience using G\*Power, and I have not used the MCID in power analysis previously. If possible, I would appreciate if anyone can help me to understand how to use the MCID in the power analysis and what exactly the authors' entry in G\*Power looked like to get these results. I would also appreciate any tips, general advice, or recommended readings/videos regarding power analysis, as I feel that my grasp on this is very weak. 

I am happy to answer any questions or provide any more info as needed. Thanks",12ucxre,lostinthesauce248,1682100447.0,4,1.0,"[""Not to get you in trouble with the person you are assisting, but computing power after collecting your data is a poor statistical practice. You might ask the MD to consider these articles:\n\nHoenig, John M. and Heisey, Dennis M. (2001), The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis The American Statistician 55:19-24. DOI:10.1198/000313001300339897\n\nLevine M, Ensom MH (2001) Post hoc power analysis: an idea whose time has passed? Pharmacotherapy 21:405-409 DOI: 10.1592/phco.21.5.405.34503\n\nGoodman SN, Berlin JA. (1994) \nThe use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. Ann Intern Med 121:200-6. doi:10.7326/0003-4819-121-3-199408010-00008 (Erratum in: Ann Intern Med 122:478. doi:10.7326/0003-4819-122-6-199503150-00029)\n\nThomas L (1997) Retrospective power analysis. Conservation Biology 11:276-280. DOI: 10.1046/j.1523-1739.1997.96102.x\n\nYuan K-H, Maxwell S (2005) On the post hoc power in testing mean differences. Journal of Educational and Behavioral Statistics 30:141-167. DOI:10.3102/10769986030002141\n\nWalters SJ (2008) Consultants' forum: should post hoc sample size calculations be done? Pharm Stat 8:163-169 DOI: 10.1002/pst.334\n\nMiller, J. (n.d). What is the probability of replicating a statistically significant effect?. Psychonomic Bulletin & Review, 16(4), 617-640.\n\nGreenland, S. (2012). Nonsignificance plus high power does not imply support for the null over the alternative. Annals of Epidemiology, 22(5), 364368.\n\nWilkinson, L., & Task Force on Statistical Inference, American Psychological Association, Science Directorate. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594604. https://doi.org/10.1037/0003-066X.54.8.594\n\nHow post-hoc power calculation is like a shit sandwich | Statistical Modeling, Causal Inference, and Social Science\n\nZhang Y, Hedo R, Rivera A, et al Post hoc power analysis: is it an informative and meaningful analysis? General Psychiatry 2019;32:e100069. doi: 10.1136/gpsych-2019-100069"", '>  ""The statistical power to detect a difference equivalent to the lowest reported MCID ...\n\nWait... they\'re computing *post hoc* power?!?\n\nYikes. And you want to help someone commit the same error?\n\nPlease, if it\'s at all within your power, dissuade them from this misguided endeavour\n\nEdit: was about to come back and edit to offer a reference, but /u/dmlane has you covered', '> How post-hoc power calculation is like a shit sandwich\n\nYou seem to have lost the link on the second-last item there\n\nI presume you were looking for :\n\nhttps://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/', 'Thats the one, thanks.']"
[E] Hello! I'm learning EDA statistics from scratch and wanted to know whether this is a good source for it,"Source I'm learning from: [https://www.itl.nist.gov/div898/handbook/](https://www.itl.nist.gov/div898/handbook/)

&#x200B;

I started going through it and it hit me with a lot of words that I do  not understand (I looked most of them up, and didn't find definitions  for things like ""flat graph""). Apparently some of them will be handled  afterwards, still, I'm wondering if its a good starting point for  someone who wants to apply EDA Statistical methods on quality standard  related projects",12uai0o,Molger,1682095676.0,16,1.0,['Seems reasonable.']
[Q] Why run a T-test instead of an one-way ANOVA?,"I am sure there must be a very obvious thing that I am missing but I can't seem to see why people would choose to run a T-Test over an ANOVA other than the T-test just being more simple and easier. 

Ross et al., 2017 states that you can run an ANOVA to compare the means of two or more groups. So you can run an one-way ANOVA instead of a T-test. 

So that would just make them interchangeable but since ANOVA's can also control for increased Type 1 error probability caused by multiple comparisons. (Kim, 2017) Doesn't that just make one-way ANOVA's entirely preferable? 

If I have two groups and four dependent variables why would I ever run a T-test and then go through the hassle of running an bonferroni's correction in order to just get the exact same result from just running a One-way ANOVA?",12u4zgj,Bittersweetcharlatan,1682088608.0,42,0.91,"['If youre using software to run the analysis then a one way ANOVA on two independent samples *is* a T-test. Theres no difference in the results.', 'Keep in mind that ANOVA uses an F-test which allows for testing of multiple factors/factor levels at once. Basically, it checks it theres a change in means of the dependent variable due to at least one factor level but it doesnt tell you which factor or factor level is significant.\n\nA t-test can then be used to look at which specific factors and factor levels are significant. There are ways to correct for multiple comparisons such as a Bonferroni Correction, Scheffes method, and Tukeys HSD. \n\nWhen there is only one factor with two factor levels (two groups youre comparing) then ANOVA is equivalent to a t-test. In fact, if you square the t value from a t-test, youll get the F value from a one-way ANOVA with two factor levels.\n\nAlso, MANOVA is only going to be used if you expect covariance/correlation between multiple dependent variables. Otherwise it operates (mostly) the same as ANOVA.', ""The question is more the other way around: there is very rarely a reason to do an ANOVA because it just answers a question nobody cares about\n\n>Are there any differences anywhere here? I don't care where!\n\nhttps://quantitudepod.org/s2e09-manova-must-die/"", 'OP can you give the complete references of Ross et al., 2017 and Kim, 2017?', ""Perhaps another way to look at this is from a pedagogical point of view.  Often when teaching analysis of experiments, we start with simple tests that are easy for a student to visualize. (For example, *t*\\-test).  And build up to one-way anova, two-way anova, ancova, and so on.\n\nAnother approach would be to teach the more general approach first.  In this case a general linear model includes all of *t*\\-test,  one-way anova, two-way anova, ancova, and so on.\n\nIn a sense, the latter approach is more straightforward, because you would only need to learn one example in software, and this example would apply to many situations.  But it's a lot to throw at students just beginning to learn these concepts."", 'Often ANOVAs are followed up with pairwise comparisons (like t-tests) or other types of comparisons to see how the means actually differ.  Usually these have some sort of family-wise error control.', ""I think your looking at this the wrong way:\n\nA t-test can be shown to be a special case of ANOVA with a single categorical variable of two levels. If you write out the entire Y=X'B+e and derive the test of the coefficient, you will end up with a t-test."", ""There's only one comparison with two groups. Multiple comparisons is a non-issue with two groups. So no gain there.\n\nT-tests have an advantage  over the corresponding one-way ANOVA; the ability to handle one-tailed tests. It's also easier to do equivalence tests (*et c.*)\n\n>  I have two groups and four dependent variables\n\nIf you have a multivariate response you will generally want to deal with that properly as a multivariate response, not as a bunch of univariate t-tests"", 'Because a one way Anova with two factor levels defaults to a two sample t test. Same test.', 'There is no need for type one error correction in a t-test, because only one comparison is made. If you ran it as an ANOVA with bonferroni correction, you would get the same exact results.', '1940s statisticians are shook.', 'Mic drop', 'Many software packages dont assume constant variance for the t-test, in which case the results would differ from a standard ANOVA.', ""Oh... So I'm guessing in that case it also doesn't control for increased Type 1 error caused by multiple comparisons?\n\nSo I would still need to apply a bonferroni's correction?\n\nWould the same be true for a MANOVA ?"", 'Thank you for your answer. You along with /u/Filosifee have rolled away my uncertainty', 'It is very useful for mixed models or for designs with multiple fixed effects\nDo you speak of MANOVAs?', '> Analysis of variance (ANOVA) is one of the most frequently used statistical methods in medical research. The need for ANOVA arises from the error of alpha level inflation, which increases Type 1 error probability (false positive) and is caused by multiple comparisons. \n\nKim, T. K. (2017). Understanding one-way ANOVA using conceptual figures. Korean journal of anesthesiology, 70(1), 22-26.\n\n\n\n\n\n> A one-way ANOVA (analysis of variance) compares the means of two or more groups for one dependent variable. A one-way ANOVA is required when the study includes more than two groups. (In other words, a t-test cannot be used.)\n\n\nRoss, A., Willson, V. L., Ross, A., & Willson, V. L. (2017). One-way anova. Basic and Advanced Statistical Tests: Writing Results Sections and Creating Tables and Figures, 21-24.', 'Incorrect.', 'Some software test for normality by group, then depending on normality do a Levene or Bartlett equal variance, then depending on equal variance do a Welch or Classical t test.', 'Both one-way ANOVA and two-sample T test can be seen as having the formula:\n\n     Y ~ X\n\nIn two-sample T test, the `X` is a binary variable (e.g., male/female, young/old).\n\nIn the one-way ANOVA case, the `X` can be of *any number* of categories (e.g., male/female, low/mid/high, red/green/blue/purple/yellow/...).\n\nSo two-sample T test is essentially a special case of one-way ANOVA. In ANOVA you need to consider multiple comparisons *because multiple comparisons are possible*:\n\n\n1. low vs. mid\n1. mid vs. high\n\nBut if you take the two-category case (e.g., male vs. female), there is only 1 possible comparison you can do.', 'When comparing two populations, there are no multiple comparisons to worry about.\n\nWhen comparing more than two populations, that concern would be valid, although they aim at different things...', 'It would not control for increased type 1 errors. What exactly are you trying to compare that you feel a MANOVA would be better than ANOVA?', ""> Kim et al\n\nEdit: excuse me while I rant. If you don't like rants, don't read it.\n\nOFFS, not this bilge again. Why does that travesty of a paper keep getting brought up here? How is this drivel regarded as gospel? I cannot comprehend how it pops up over and over, like a coked-up mole. Who the hell is promoting it to all ends of the earth?\n\nIt's written by anaesthesiologists, not statisticians, and the referee and editors were not statisticians as well, *and it shows*.\n\nImagine you had a question about anaesthesiology, and I pointed to a paper, written by a bunch of statisticians, published in a statistical journal, where none of whom had any formal training in anaesthesiology (at least not from anaesthesiologists, though maybe a couple of them had once read a book written by ... yet another statistician. Who in turn had never read a book written by an anaesthesiologists, but had huffed NO2 a few times, and had read an account of anaesthesiology written by a chemical engineer because he had some interest in it).\n\nWould that paper be likely to be ... filled with great advice and not contain a bunch of errors and nonsense? Well, you might be lucky, but how confident of it would you be? Would you want your health in the hands of someone whose understanding of the topic was based on such a paper? Or would you want thoughts of actual anaesthetists involved at some point?\n\nApply a similar level of doubt the other way around.\n\nIs everything in it wrong? No. Should you have a high level of doubt about what it says? Yes.\n\n---\n\nI'm glad you gave a full reference for Ross, I thought you were referring to something by Sheldon Ross (who is an actual statistcian). The author I could find easily (the last one) is an educational psychologist. I expect the others are from similar disciplines. Again, exercise a relevant level of caution. I don't know that book (so I won't call it names), but unless you've had an expert look at it to identify its most egregious errors, *don't damn well trust it*.\n\nIf I want to learn physics, *I read a book by phsyicists*. If I want to learn about psychology, I read something by psychologists. If I want to learn about biology, I'll read a book by biologists. Why is stats the exception to getting information from people who have expertise in the subject?"", ""I'm comparing two groups across four measures. \n\nI'm not sure which one of those I fit in to"", 'Why not criticize the arguments rather than the authors? Plenty of good articles about statistics were written by non-stastisticians. Statistics is an applied discipline and the people who use it in practice often have interesting and useful things to say about it.', 'Ah, Kim et al  that mythical beast of a paper, the one that just won\'t quit. It\'s like some kind of unstoppable academic zombie, feasting on the brains of the unsuspecting! Who in their right mind would worship this unholy grimoire as if it were gospel truth? It\'s as if the paper\'s authors have some sort of secret, globe-spanning fan club, hellbent on promoting it at every opportunity.\n\nNow, don\'t get me wrong, I appreciate a good anesthesiologist as much as the next person, but when did they become the ultimate authority on statistics? It\'s as if someone decided to mix up the ""who\'s who"" of academic disciplines, and now we\'ve got anesthesiologists doing the hokey pokey with statistical analysis, while the editors and referees are just along for the ride.\n\nPicture this: you\'ve got a burning question about anesthesiology, and lo and behold, the ""expert"" source is a gaggle of statisticians who\'ve dabbled in the field, perhaps after reading a book on the subject written by  you guessed it  another statistician. This statistician, in turn, once inhaled some nitrous oxide and read a thrilling account of anesthesiology by a chemical engineer with a passing fancy for the topic. Is that the kind of paper you\'d trust with your life? Or would you, like any rational human being, prefer a good old-fashioned anesthesiologist to weigh in on matters of anesthesia?\n\nSo, let\'s all take a step back and apply that same healthy skepticism to the world of statistics. Sure, Kim et al might not be 100% balderdash, but it\'s worth approaching it with a hearty serving of doubt. \n\nAnd as for Ross  I\'m relieved you provided a proper reference, as I was worried you\'d stumbled upon the musings of Sheldon Ross, a bona fide statistician. Instead, we find ourselves with an educational psychologist at the helm, likely joined by a merry band of similarly qualified individuals. I implore you, dear reader, to approach this book with the same level of caution as you would a paper on anesthesiology written by statisticians.\n\nAfter all, when we want to learn about a subject, we turn to experts in that field  be it physics, psychology, or biology. Why should statistics be any different? Let\'s put our trust in the experts and leave the satire to the comedians.', 'I think you have 4 ""response"" measurements and want to compare across two groups.\n\nFor example, you may want to compare height, weight, IQ and pulmonary capacity between men and women.\n\nIn this case, you would have two perform either 4 t-tests, one for each measurement, or 4 ANOVAs, one or each measurement again.\n\nThis will rise multiple testing considerations, of course.']"
[Q] Collapsability vs Non collapsability,"I've been learning recently about the Cox regression models, and frailty theory. Basically, I've been taught why a Cox model that does not include all relevant covariates will have biased estimators for the regression coefficients. This already breaks my mind a little (having a model that could do perfect survival predictions but have very off coefficients), but then theoretical explanation was that this model, and logistic regression as well, are no collapsible.

I get the intuition as of why bias would be introduced, but then I wonder: why do other models like linear regression not suffer from this? What is the mathematical proof behind and the theoretical definition of collapsability?

Also, on an end note, learning about frailty and non collapsability made it seem like Cox regression can be much less ideal than I first thought (it seems hard to draw conclusions for experiments using Cox regression due to this potential bias of the coefficients). So, why could this models still be preferred over collapsible ones?

It was maybe a lot for one post, but the questions all came from the collapsability concept.",12twrbt,Chus717,1682071405.0,11,0.93,"['Fraility is (to my knowledge) the same thing as random effects variables in linear models. Maybe making that parallel can help you get a better picture of what it means? \n\nI surprisingly never heard of collapsability, but I assume that it means that if you ""collapse"" the different random effects levels (by that I mean ignoring them entirely) , the model becomes less significant and/or viable. Therefore you HAVE to consider the random effects variables in order to get good results.', 'I cant comment on Cox models, but as far as logistic regression goes, the root of noncollapsibility is unobserved heterogeneity. We can think of the binary dependent variable as a proxy for latent/unobserved continuous variable. For example, binary ""survived/died"" variable is a proxy for ""propensity to survival"". Because the latent variable is unobserved, we dont know what its mean or variance is. Instead, we fix to some constants, say 0 for mean and 3.29 for variance. So far so good. \n\nThe problem appears when we start adding predictors into our models. Each predictor increases the amount of explained heterogeneity, as it successfully predicts some amount of the dependent variable\'s variance. But remember that the amount of unobserved heterogeneity stays the same, as the variance of the latent variable is fixed to a constant! This essentially means that as you add more predictors, the total variance of the latent variable changes, even if you are using the same data. This means that regression coefficients from different models are on different scales and cannot be easily compared to each other. Also note that this is not a problem just for comparing coefficients across models, but also across sub populations (e.g. men and women).\n\n\nThe reason why linear regression doesnt have this problem is that there is no latent variable and hence no unobserved heterogeneity. All the variance of the dependent variable is observed and the total variance stays the same across models (we just keep shuffling it from ""unexplained to ""explained"").\n\nThe easiest solution (for logistic regression) is compute average marginal effects on probability scale, which dont suffer from this problem.\n\nFor more details, you can read this paper: (https://www.su.se/polopoly_fs/1.341161.1501927873!/menu/standard/file/Eur%20Sociol%20Rev-2010-Mood-67-82%20(1).pdf', 'I may have misunderstood your question, but linear regression with omitted variables does have biased estimates: https://en.wikipedia.org/wiki/Omitted-variable_bias', 'We shouldnt be relying on coefficients anyways. I dont know about how to do it for the Cox model but for Logistic regression you can use G computation (marginal effects) and get a risk difference or risk ratio from the predictions and these are collapsible. \n\nImo its a better way anyways, and it can be used on black box models. This whole non collapsibility and G computation thing kind of shows how the whole logistic reg is interpretable because of coefficients, ML models arent is such a vast oversimplification and is not right. In the case of GLMs though you get the uncertainty via delta method. R has a marginaleffects package for this. \n\nThe non collapsibility is just a result of the link function (logit) and working out the math. Link functions like identity and log are collapsible', ""Wow, Cox regression and frailty theory can definitely be mind-bending concepts to learn about. To answer your first question, linear regression models typically do not suffer from non-collapsibility because they do not involve survival analysis or time-to-event data. Non-collapsibility occurs in survival analysis when conditioning on certain covariates changes the effect estimate of another covariate, which is not observed in linear regression. \n\nIn terms of mathematical proof and theoretical definition of collapsibility, it refers to the property of a statistical model where the effect estimate of a covariate is the same regardless of whether or not other covariates are adjusted for. A model that is collapsible is one where conditioning on a covariate does not change the effect estimate of another covariate. \n\nIt's true that non-collapsibility can introduce bias in Cox regression models, but they are still preferred over collapsible models in certain situations because they are more flexible and can account for time-varying covariates and non-proportional hazards. It's important to carefully consider the potential for bias in any statistical model and interpret results with caution. Great questions!"", ""yes, I guess frailty can be seen as a random effect, that helps a little.\n\n I might have missunderstood what you said, but what I know of collapsability so far is that if a model is collapsable you can still get unbiased estimates of the regression coefficients even if you ommit some predictors, when in non collpasible models if you don't consider all the relevant predictors you can no longer rely on the estimators of the coefficients to interpret effects because they will be biased. I just do not understand what intrinsic difference makes that happen, or how it is proven. \n\nIt is less about if the model is less significant and more about the properties of the regression estimators (I hope I did not say something incredibly wrong because I am still in need of more understanding)"", 'Okay this explanation with latent variables and observed vs unobserved helped me! And I will check the paper for sure, appreciated!', ""If I'm reading correctly, one condition for the bias to be there is that the unobserved variables are correlated with other regressors, and as far as I know, this does not need to happen for non collapsible models.\nAnyways, I haven't thought of when this bias appears in linear regression too much, so thanks for the link!"", ""Thank you for the answer! I will look more into those G computations. In the Cox model survival analysis context those regression coefficients become important if you are doing randomized control trials and want to measure effect of treatment, and so far I've not seen papers going too deep in other alternatives to these regression coefficients"", 'Thanks a lot! Your reply was really useful! I have 2 questions following your answer:\n\nLogistic regression still suffers from non collapsability, but it does not deal (necessarily) with time to event data, so then why do they still have non collapsability?\n\nCox model still needs the proportional hazards assumption to work right? At least that the cox model I have learned so far, and one reason we need to add a frailty is because this proportional hazard assumption is not met with the covariates we have access to (or so is my understanding).', 'Perfect zero correlation is unlikely, though.', 'In RCTs when theres no covariates the non collapsibility issue doesnt come up. Non collapsibility relates to the big difference in the estimated hazard/odds ratio with and without a covariate even when the covariate is independent of treatment (so no confounding). \n\nBut otherwise at least for the logistic and other  GLMs you can do G comp by fitting the model, creating 2 datasets with setting treatment  to 0 and 1 for everyone, making a probability prediction, averaging it across the dataset and taking the difference. Uncertainty is more complicated cause it needs delta method.\n\nFor cox the thing im not sure about is because there isnt just one probability but the whole survival curve and also needing the baseline hazard.', 'Fair, we might be thinking in different contexts. When I am asking this collapsability questions, I am thinking of survival analysis. For example, you could have a binary variable to account for treatment given, and a biomarker variable that impacts your survival probability to a disease. This biomarker and the treatment given are independent (assumption, but it is a realistic one). In this case the Cox model will still have biased estimators if the biomarker is excluded']"
[Q] Regression advice!,"Hey all! 

Im a psych student and am running a hierarchical regression to see if test anxiety predicts performance on a numeracy task. 

I have to control for two variables. Age is one which Ive dummy coded. 

Im back and forth between controlling for age and education attainment but my question is in several parts: 

1. I find it more interesting to dummy code the scaled age variable as opposed to keeping it as a scale. 
A) When I use 65+ as my reference group all 3 other groups have significant interactions.
B) when I use 16-24 as my reference group, only 65+ group is significant 
So, Im just trying to decide which comparison is more interesting. Either being 65+ compared to 16-24 predicted poorer performance, or being 65+ compared to all other groups predicted poorer performance. 

2. Is this even of interest since it is my control variable would it be better to keep my control variable as a simple scale and not get bogged down? 

3. If I did education level I will probably dummy code it into 3 groups, so same story as above really. 

4. Bonus question! For some reason more of my variance is explained (diff of around 2%) when I dummy code age than when I keep it as a scale, even though descriptives are the same. Just interested as to why this is? 

TIA!",12twhuu,hot4halloumi,1682070652.0,3,1.0,"[""The choice of reference group doesn't matter for the model itself (it's the same model), the only difference is in which comparisons are easy to read directly from the coefficients.\n\nFor the last question, this is probably because the relationship with age is nonlinear, but using it as a continuous measure imposes linearity. Breaking it into categories functions as a crude nonparametric way of allowing for a nonlinear relationship."", 'I would definitely keep your age as a continuous variable rather than artificially make it a factor. What is the benefit of making age a factor? Do you think of age as a category or is it continuous? Probably the latter.', 'Yes, I definitely will keep it continuous! Im just learning and was trying to get my head around it all last night. Thank you! :)', "">What is the benefit of making age a factor?\n\nAs I mentioned in another comment, it's a quick-and-dirty way of allowing for non-linear effects in a way that's easy to interpret. I don't think it's necessarily a bad approach.""]"
[Q]Type I and Type II errors,"I have been reading about these two types of errors and a part of the definition called my atttention.

It is said that the type 1 error is rejecting the null when this is true, and this is the power of the a test when the null is true, and later it says that the error type II is accepting the null when this is false and that is also 1-the power of the test when the null is false.

So following that it is incorrect to say that 1-probability of type 2 error = probability of type 1 error, but when I look at the graph of different power functions, it is defined like that, the lower part like probability of error type 1 and the upper as 1-probability of type 2 error,

So, what part am I mistaking? Is it possible to have one equation that connects the probability of error type 1 and the probability of error type 2?",12tqhv9,Unhappy_Passion9866,1682051929.0,13,0.89,"['Your alpha isnt related to power. As you point out its the probability that you are willing to accept rejecting the null when it is true (type 1 error). Your confidence of this inference is 1 - your predefined willingness threshold.\n\nBeta relates to power since the goal of your statistical test is to reject the null when it is false. Accepting this null is beta and your power is the ability to reject i.e 1-beta. \n\nThere is a direct relationship between alpha,beta and power (increasing your willingness to reject a true null i.e alpha, decreases your probability of retaining a false null i.e beta, and hence increases power) but this comes at the price of lower confidence in your test. There is no equation that links both alpha and beta because theyre answering slightly different questions', '> So, what part am I mistaking? Is it possible to have one equation that connects the probability of error type 1 and the probability of error type 2?\n\nYes: I think the one ingredient you are missing is that you have one kind of Type I error -- the null is true and you don\'t think so --- but Type II errors come in all shapes and sizes, and as a practical matter, the null is essentially never true, except in constructed experiments. P(Reject null hypothesis | parameter value) is usually a smooth curve, with a minimum (for a 2-sided test) at the point where the null is true.\n\nSay you want to find people\'s average income, and test whether it is $100,000 or not. You build yourself a test that has a type I error rate of 5%. Great.\n\nNow suppose you apply this test to a population where the average income is $100,000.01. You have about a 5.0000001% chance of ""correctly"" concluding their income is not $100,000, and a 94.999999% chance of committing a Type II error. You may have a 6% chance of detecting that $99,000 is different from $100,000, and a 50% chance of detecting that $90,000 is different from $100,000.', ""I would suggest you ask yourself some questions. Can you have a false positive (type one error) when the null is false? Can you have a false negative (type two error) when the null is true? Broadly, there are four possible combinations of two separate pairs that you can consider. There's whether or not we reject the null (do we *decide* there's a positive result or not) and then there's the truth (is it positive/negative). And remember, it's very very important to keep track of what is being declared or assumed to be true when dealing with significance testing."", 'Would [this page from PSU](https://online.stat.psu.edu/stat415/lesson/25/25.2) help?', '> and this is the power of the a test when the null is true,\n\nI think considerably more commonly (though perhaps not all sources, some are weird) it\'s the *rejection rate* or  *type I error rate* when the null is true; but normally the rate of rejection is only called power when it\'s correct to reject. (Nonetheless a *power function* will typically fill in the null case; when that\'s done it\'s strictly a rejection-rate function, but power function is the term people tend to use.)\n\nConsider: It would be truly bizarre to call a test with a high chance of incorrectly rejecting the null ""powerful"" in that situation.']"
[Q] Is it possible to calculate approximate wage range for a given position with two to three years of wage numbers,"I know I may have to make some assumptions but for example if I made $50,000 and I know that that wage is 107% of the median.  The following year with a raise I have $55,000 and maybe that penetration is 105% and the next year I'm making $60,000 and the wage penetration is 104%.
I guess I can calculate the median pay fairly easily since I know I was making 7% above 5% above than 4% above. And I also realized that median pay is also probably increasing throughout the years. Is there a way to back calculate maybe assuming a bell curve for pay, maybe what percentage wise the +/-20% of employees are making around me.  Or to get some other kind of relevant range of what I can expect The range of my possible pay to be at my current position?

Or is getting any kind of meaningful possible pay range data not really possible from this limited data set?",12tgvqv,locke1718,1682028591.0,1,1.0,"[""You have 2 pieces of information across 3 years: what the median is and how far above it you are. With this you know that you make more than 50% of the people with this position, and unless there are a total of 3 or 4 people with this position, there isn't much else that can be inferred.\n\nYou do not have an average (mean), a standard deviation, a count of the number of people with the position, or the knowledge that the pay distribution for the position can be appropriately approximated by a bell curve. If you look at certain industries with winner-take-all effects, there's a good chance it won't look normal (distribution wise).\n\nSo what you have mentioned is not sufficient to calculate what the employees around you are making, and you would need more data."", ""It's a fairly large company and I think it would be a fairly ok assumption (bell curve shaped pay distribution) for my end goal of just wondering realistic pay ranges.  I know the total number of employees in my job category can probably be approximately to ~300.  Is there specific other information I would need to get a rough approximation of the pay range?"", 'If in the USA, check job postings for similar positions, some states require that they post the ranges. A lot of companies comply with that just to make things easier, while some just refuse to hire in those particular states.']"
[Q] How would you go forward calculating the greatest score separator of a golf hole,Say you have data from a tournament of strokes per hole for every player on every hole. How would you go forward ranking every hole based on how much separation you get in score from the hole? Is it simply by calculating the variance of every hole? What would the value of the variance then mean?,12tg6i7,WiseAfro27,1682027029.0,0,0.33,"[""It depends on how you want to define 'separation'. \n\nVariance would be one criterion you could use to compare\\* holes -- though consider that the variance for a par 5 would naturally have a bigger spread than a par 3, so you'd pretty much always just be picking out the par-5 holes. \n\nThis seems to suggest that maybe you want to consider some form of par-adjusted measure. There's no obvious way, since it's not necessarily a given that the spread will simply scale in direct proportion with the par-value. It's probably a decent first approximation, but I imagine a simple scaling for that (dividing spread - say sd - by par) probably overcorrects and just picks out par 3's instead. It might require some model -- perhaps a regression-like model to find something suitable. I imagine that (say) generalized linear models have already been used for per-hole golf scores, and that would be a starting point to try to pick out the holes that seem to have more variation that you might otherwise expect.\n\nIf you're trying to find a hole that specifically tends to sort the sheep from the goats (in effect pick out the very good players from the casuals), and you have external information on who they are (like *handicap*), then you'd take a different approach.\n\n---\n\n\\* If you want to *interpret* the value rather than just compare values, however, convert back to a standard deviation, which will be easier to think about."", 'I\'m not sure what you mean by, ""based on how much separation you get in score from the hole""... Might need to explain what you mean a little bit more.']"
[Q] Is there a null model in conditional logistic regression?,"I am modelling site selection of an animal using an m:n case-control design. My understanding of regression is that null models are typically models the assumption that all betas except the intercept term equal zero. Since the conditioning cancels out the intercept term we can't use the intercept term as a null. Is there another way to make a null model?

I am using package Survival in R

I have searched everywhere I can think an no one explicitly answers this. There are some people that claim we can just do something like  clogit(response ~ 1 + strata(stratification_variable)). I think they are mistaken and this does not work using the clogit package. 

Does anyone have any insight on this or a reference paper that I can look through? This is driving me insane. Thanks!",12tbpxu,AzranDan,1682017525.0,2,1.0,"['It depends on what your goals are here. What are you trying to use the null model for? You can directly use clogit, removing the strata term and all covariates from the formula. This should give the totally null model that conditions out a single intercept with no individual effects and no covariate effects. This will likely take a very long time to fit, but will give you a model object that you can use for testing against the null model.\n\nIf you are looking at testing whether or not their are significant covariate effects, it likely makes sense to consider the null model the model that includes subject specific intercepts, i.e. the model with the included strata term. This may make more sense than the single intercept null model because the conditional logit model is often motivated by study design.', ""I'm using AICc to determine the top model (and model average). In other regressions I would typically include a null model in this set. I didn't in conditional since I didn't think there was one.\n\nAre you suggesting clogit(Response ~ 1)?\n\nClogit doesn't run with that the same way other regressions do.""]"
Possible to use COVID vaccine data to determine exactly how many undocumented immigrants currently reside in the United States? [Q],"I was thinking about the use of total vaccinations, and if it was possible to determine if there are more people here that are undocumented then the estimates suggest.

Do you think it's possible to use any data from the lockdown to determine if there are millions more warm bodies here than are on record?",12tb4af,TommyJeffs1776,1682016339.0,0,0.19,"['No, that cannot tell you exactly, per your title. Indeed such an attempt would require implausible assumptions. \n\nFrankly this sounds like an attempt to indulge in hefty doses of confirmation bias.', 'How would you even go about doing this? Say a county has 1000 vaccinated. Does that mean there are more or less undocumented immigrants than you previously suspected?', 'Please leave this subreddit and go back to playing with your sex robots.', 'No. Because of the volume, most state IIS systems have more unique people in them than there are people in the state. And people used fake names and addresses out of fear other people would know that got a Covid shot. The is no way to merge Robert Smith with Bob Smith unless they have tons of matching pieces if information. \n\nVaccinators did not check to see if patients were already in the system so every vaccination created a new person.', ""We did have a census, just a few months before the vaccines came out... a census that tried quite hard to reach everyone, citizen or not, vs. the vaccines that a lot of folks opted out of. Is there a reason you prefer the less-complete data set over the more-complete data set? (Neither asked explicitly about citizenship... but I think it's a safe bet that the census bureau can do a better job estimating how many undocumented immigrates we have than anyone else will.)"", 'You gotta admit if you remove all the confirmed people from the list, you would be left with both made up names and potentially undocumented people. That might be something to be cross referenced somewhere with some other datasets to get some estimates to then cross ref census data', 'There must be some way to measure it using this data in combo with death / border crossings out / birth records etc', 'I create those robots and you bet your bottom I will', 'Is there a list of people whos names dont match citizens, who also were not visiting?', 'Was thinking it would be more accurate to cross reference this data since the census is in my opinion not accurate at all. How does the census do a better job estimating undocumented workers? As it turns out I think there is federal law that prohibits its sharing without consent https://www.npr.org/2020/01/04/793325772/to-produce-citizenship-data-homeland-security-to-share-records-with-census', 'Also since travel was prohibited with non citizens without vaccine you might see a larger outbreak quicker in areas with more undocumented workers who werent vaccinated or gave fake information to get it, thereby leaving a paper trail within the vaccine data that doesnt correspond to a documented person and where there was quicker outbreaks locally', 'None of this is a problem statistics can solve.', 'Lol. Theres no good data on illegal border crossings', 'No. The only goal of the vaccinator is to get shots in the arm. Nobody cares if the street really exists or the name is real. There is no check to see if someone resides in the locality, state or nation of the vaccinator. \n\nIt is very common in the US for people to provide fake information to hospitals and health care providers to avoid a $50,000 ER bill.', 'The cost to treat an unvaccinated person for tetanus can easily reach $1,000,000. It is a $25 shot. The savings on the recommended vaccinations, per person, are enormous.', 'Have you ever done any work estimating hard to measure data?', 'There is bad data though, if they did get a vivid vaccination there would be bad data to use', 'If thats true you might see some weird data where people dont wanna give accurate info', 'No but there are some emerging technologies that are really good at it. Why do you ask?', 'Because it sounds like you havent done a lot of this type of work.', 'I have not done any of this type of work. Im a creative person asking questions', 'This subreddit is generally geared toward the study of statistics. Hence the name.', 'What exactly were we talking about all this time because I personally was talking about the collection, organization, analysis, and interpretation of data. Im a fucking moron I guess hey']"
[S] Significance differences between groups on SPSS,"
Im working with 3 different samples. Each sample is treated with 10 methods. Then I calculate concentration.

I want to create a bars graphic with concentration for each treatment, comparing signicance differences between all 30 treatment.

I have standard desviation for all of them. I just want to know if A is different enough from B or if C is different enough of A and B or just from B.

I have tried with t-student, Tukey and Anova but It doesnt seem to work :c
My variables are Run (1-10, nominal) which is determined by Time and Amplitud (Both continuous, isnt it?). 

Im working with SPSS and excel. TIA",12t9vkk,dalvi5,1682013885.0,2,1.0,"["">Im working with 3 different samples. \n\nSo you have n=3. As in, three independent samples.\n\n>Each sample is treated with 10 methods\n\nWhat does that mean? All at once? One at a time? Are there interaction effects?\n\n>I want to create a bars graphic with concentration for each treatment, comparing signicance differences between all 30 treatment.\n\nWhat treatment are you referring to?\n\nYou need to explain your design better, what you're writing doesn't make sense to me."", 'Well, they are solutions with 3 different weights of solid fruit on oil (the objective is to extract chemicals from them (calculate concentration)). \n\nEach of them has 10 different treatments (different times and amplitudes of Ultrasounds) (so 30 different tubes). Lets call them A1, A2...A10, B1, B2...B10, C1, C2...C10. And finally each one has been made 2 times. \n\nI wanna know if treatment 1 is signif. different from 5 for example. (With all of them). to give a letter to significally different groups:\n\n- A1: a\n- A2: b\n- A5: ab \n\nWould mean that 1 and 2 are different but 5 isnt with any of them', ""This is still very unclear to me. You don't need to necessarily describe the specific variables or methods, but you need to be able to define what your dependent and independent variables and minimum experimental units are. Be clear and explicit. \n\n>Well, they are solutions with 3 different weights of solid fruit on oil (the objective is to extract chemicals from them (calculate concentration)). \n\nWhat are you measuring here exactly? Is it the concentration of an extracted chemical (solute) in an oil solvent from these mixtures? \n\n>Each of them has 10 different treatments (different times and amplitudes of Ultrasounds) (so 30 different tubes). Lets call them A1, A2...A10, B1, B2...B10, C1, C2...C10. And finally each one has been made 2 times. \n\nWhat does A, B, and C represent here? Again, you're not being explicit and clear. If you're going to ask for help don't make people guess."", 'Yes, concentration of solute in oil solvent. A, B and C refers to different quantities of solute on the solvent. 1-10 refers to time+amplitude combinations. So I have 3 quantities of solute treated with 10 different combinations of time (10-50minutes)+amplitude(10-70%) (30 tubes in total)\n\nFor example: A1 and A2 have same quantity of solute but got 2 different times and amplitude. While A1 and B1 have different quantity of solute but share time and amplitude.', ""Thanks for the clarification. \n\nIt's still a bit fuzzy, but from what I can tell your design looks like this:\n\nDependent Variable:\n\n* Concentration\n \nIndependent Variables:\n\n* Fruit Weight: 3 levels\n\n* Time_Amplitude: 10 levels\n\nYou have two independent subjects per combo of Fruit_Weight and Time_Amplitude, for a total n=60 (i.e. experimental units).\n\nIf you were to test for differences in concentration between groups, you would use a mixed ANOVA design that looks like this:\n\nconcentration ~ Fruit_weight*Time_amplitude \n\nThis tests for differences in concentration for Fruit_weight, Time_Amplitude, and the interaction between them Fruit_weight*Time_amplitude.\n\nHowever, there's a problem - you lack replication. You have only two actual samples per level of the independent variable. It means that there isn't enough replication of the experiment to provide any statistical inference. You need a minimum of three per group to make any inference, and even then, that's often not enough if the data have any noise in them.\n\nYou can still calculate the mean for each group combo (i.e. averaging the two measurements) and create a bar graph figure showing the concentration for each group (e.g. A1, C8, etc). But any error bars won't be valid since there are only 2 measurements per group."", 'Thanks for clarification!!\n\nI did a graph with average concentration of each combo as you said showing standard desviation too.\n\nSo I would need 3 or more measurements of each one to reach a conclusion, right?? \n\nIll think on it tomorrow, too late right now here. Thanks again!!', 'Yes, 3 or more. Be aware that the more groups you compare the more replication you generally need. And you have a lot of groups. \n\nThe standard deviations you have won\'t be very useful because there\'s only one ""deviation"" per mean (i.e. the difference between the two concentrations). But if they\'re very low it is a good indicator that you might not need a lot of replication.\n\nAnother consideration is the source of the fruit. Are you mixing a large number of fruits together and sampling this for each extract? Or are you using a different fruit for each? Consider that there is variation between fruits, so that can affect your results. Ideally you\'d want to homogenise a large amount together.']"
[Q] Very small scale pilot experiment for a project for University.,"So Ive been tasked to run a pilot experiment, it needs to basically consist of a response variable and 1 or 2 treatment factor/s and/or blocking factors. My group has suggested plant growth in different water, or the average rating of the same drink but dyed with different food coloring. Does anyone have any suggestions for a different experiment to run?",12t6y9t,PathSecret,1682008101.0,3,1.0,[]
"[Q] If the introduction of external information is not desirable for Bayesian analysis, why not treat the likelihood function as posterior distribution?","Let me clarify some things:

1. I am aware that there is not a pdf/cdf since it does not obey Kolmogorov's axioms.
2. The likelihood function does not obey the measure theoretic triple, hence it is not truly probabilistic to begin with.
3. Sometimes the likelihood function is not even measurable, hence it can lead to some fringe conclusions.

However, I am also aware that:

1. Once we have sufficient amounts of data, the prior does not play a major role (Savage's results show that).
2. Being s times more likely sounds awfully similar to being s times more probable (even though it is used in an everyday sense, the two not being mathematically equivalent).
3. The likelihoodist view on statistics treats the likelihood function very similarly to how a Bayesian would treat the posterior, and the conclusions one would draw using a Jeffreys prior would be very similar to the ones one can draw from treating the likelihood as a posterior.

Are there any arguments for or against what I am saying? I am not in any way making claims that I am correct here, just curious about the topic.",12t4327,MinLikelihood,1682004107.0,0,0.5,"['> If the introduction of external information is not desirable for Bayesian analysis\n\nwhy do you claim this?', 'p(x|y) = p(y|x) p(x) / p(y) \n\nYou want x, but good luck sampling that from p(y|x) without p(x). The likelihood still has lots of other uses but once you actually want samples from p(x|y) you need the prior.', 'I think you are referring to something called ""fiducial inference"". I\'ll be honest, as a PhD I\'ve only vaguely heard this referred to and I\'m a little surprised I even remembered what is was called. \n\n[https://en.wikipedia.org/wiki/Fiducial\\_inference](https://en.wikipedia.org/wiki/Fiducial_inference)\n\nI do know that it is of limited historical or practical applications.', 'Let me give you three potential objections. \n\n1) If you have external information and elect not to use it, then a non-Bayesian method would minimize the maximum amount of risk, which is a highly desirable property without a prior. The prior acts as context to minimize average loss and purposefully ignoring information interferes with that advantage. When you minimize the maximum risk, you dont need the context because it is context free. \n\n2) In problems with three or more dimensions, the posterior is not assured to integrate to unity with a flat prior distribution. In common real world problems it often does not. \n\n3) In finance, such a solution can be assured of the existence of a Dutch Book and a clever opponent can use your price or prices to force you into a game of heads I win, tails you lose, granting you a sure loss regardless of the outcome of the random event. \n\nIf you have prior knowledge and elect to ignore it, it begs the question of why use a Bayesian solution at all when credible alternatives exist.', ""I think it's important to inform people that:\n\nWith Bayesian analysis you actively add external information, weak or strong but with frequentistic statistics you make so many assumptions (of normality) which is essentially the same as adding external information.\n\nSomeone once told me this, I took it to heart. But it was so long ago that I doubt it now, any input welcome :)"", 'As I pointed out, I am aware of all of that. The point is that with lots of data, does the prior even make a difference?', 'My point is that there is nothing wrong with external information.', 'Yes, it can make a huge difference. Depends on the problem. If you have lots of data then a Bayesian method might not even be needed. In my experience, Bayesian methods are most useful in data-poor problems where you need to use intuition (aka a prior) to guide your answer.', 'With lots of data relative to parameters']"
Interpretation of an simplified GLMM [Q],"I am currently working on my master thesis in psychology and I was asked by my supervisor to utilize and GLMM, which i am not too comfortable with due to me not receiving too much formal education on this. However, I believe I understood the basics, successfully run my power analysis, fitted my model and simplified it. Now, i just want to ensure I drew the correct conclusions from it. 

What I did was conducting a 2 (factor 1, binary) x 2 (factor 2, binary) within-subject experiment with repeated measures for every factor combination. My outcome variable was also binary. My expectation were that the behvaiour i was looking at would be reduced from condition A.1 to condition A.2 and that this effect would interact with factor b. Furthermore, I had a questionnaire (C) and expected it's individual score to moderate this behaviour reduction. Now I constructed am GLMM like this: 

    fitted_model <- glmer(response ~  A * B * C + (1|Subject) + (1|Stimuli))

Furthermore, I also came up with three reduced models by stepwise simplification or theory insights:

    reduced_model2 <- glmer(response ~  A * B + (1|Subject) ) #based on theoretical insights dropping C, which is non-significant in my saturated_model and appears to have no influence on the outcome based on a visual inspection of the plotted data. 
    
    reduced_model4 <- glmer(response ~  A + (1|Subject) + (1|Stimuli)) #Most parsimonious model preferred by AIC; also preferred by LogLink and deviance compared to red_mod3
    
    reduced_model5 <- glmer(response ~   (1|Subject) + (1|Stimuli))) #Random effects only, preferred by BIC

Based on my visual inspection I would expect a main effect for A and potentially an interaction between A and B.  In my [fitted\_model](https://imgur.com/zs3XdqR) there are no effects. In my [reduced\_model2](https://imgur.com/3fxDjZU)I have significant effects for A and A\*B. In my [reduced\_model4](https://imgur.com/Pk17s6g) I have a significant effect for A. And these are the results of [reduced\_model5](https://imgur.com/ByHkaRW). 

So, based on my [anova() Comparison Result](https://imgur.com/8XqBidW) and the summaries of my models I would drwa these conclusions:

* Factor A appears to have a significant main effect, as this remains in the most parsimonious model.
* Factor C appears to be of no relevance for the outcome variable
* Factor B is a litte more complicated. When removing the random effect for stimuli from the model, it does lead to a significant effect for the interaction between those two factors indicating that B is relevant, too. This would be in line with previous studies and the literature. However, this is not clear and needs to be taken with a massive grain of salt.

Are those conclusions fair play, did I miss something or did I draw things from the data that are not in there?",12sx1hy,paulschal,1681993713.0,4,0.84,"[""You have a three way interaction in your fitted which is damn near impossible to interpret. My first question is why did you model stimuli a random effect? I would suggest that you think what exactly your hypothesis is, and fit a model to address that specific question. It's better than fitting multiple models. If you need to compare models for significance, I recommend comparing two models: one with and one without your main effect of interest, then do an f-test. Also, try to drop as many interactions as possible to improve interpretability. You may also run into power issues if your sample size isn't large enough."", ""> You have a three way interaction in your fitted which is damn near impossible to interpret.\n\nWhile sometimes complex, there's nothing inherently uninterpretable about a 3-way interaction. Or do you just mean it's not suitable here because there's no significant response?\n\n>Also, try to drop as many interactions as possible to improve interpretability. \n\nI agree they should frame the model based on testing explicit hypotheses but I'd be very cautious about this approach. You don't want to just exclude non-significant results and this can produce misleading interaction effects. Especially important to note is that you cannot drop lower-order interactions and keep higher order ones. I'm mentioning this for OPs sake.""]"
[D] I’ve just ordered Drunkard’s Walk. Can you recommend any other entertaining statistics/probability books?,Finishing up a minor in Statistics and really enjoyed all my statistics and probability classes. Ive enjoyed Martin Gardners math puzzle books in the past too. Would love more recommendations for entertaining books covering probability or statistics concepts.,12slcf6,AllAlike,1681963434.0,108,0.99,"['I found the Seven Pillars of Statistical Wisdom really enlightening and full of fun history. \n\nWeapons of Math Destruction is a classic warning tale. \n\nHow to Lie with Statistics is a book of short anecdotes of real life misuses. Prob my favorite of the three. \n\nFor a paper,.I get happy reading Cohen\'s ""The Earth is Round (p<.05)""', 'The lady drinking tea is an entertaining collection of nuggets from the history of statistics.\n\nTh theory that would not die is in the same style for the checkered history of Bayesian statistics.', 'Ian Stewart has a series almost as long as Martin Gardner\'s now. I think ""Game, Set, and Math"" (about how to calculate the probability of a tennis match, given the probability of winning a single point) was the first in the series, or at least the first I saw for sale, in the late 90s. They aren\'t all stat/probability-related. Some pure math, some physics... one about what kind of alien life forms might exist on other planets.', 'Have you read ""How to Lie with Statistics""?\n\nI might still have this one somewhere:  \n\n""Fifty Challenging Problems in Probability with Solutions""', 'The Flaw of Averages by Sam Savage is entertaining and thoughtful.', 'Andy field discovering statistics using R is very funny.', 'Super crunchers was entertaining, though not super solid from a statistical perspective.  \nNaked Statistics is good too, though Naked Economics was better. \nSignal and the Noise was very entertaining and good imo. \nCurrently reading The Book of Why. Not very engaging but it is very solid.', 'How Not To Be Wrong by Jordan Ellenberg is not just about statistics and probability but is hugely engaging and often laugh out loud funny', 'Weapons on Math Destruction by ONeil is a bit polemical but has some interesting contemporary examples of probabilistic math gone wrong.', 'The Theory That Wouldnt Die is a fun history of Bayesian Statistics', 'If youre looking perhaps for something with some programming exercises, Digital Dice is a fun way to learn simulation/Monte Carlo methods.', 'Fred Lords On the statistical treatment of football numbers in American Psychologist is a classic', 'The congressional report on psychic research by the former ASA president is pretty entertaining imho, especially the rejoinders. The Tukey analysis of the Kinsey report is older but also interesting.', 'Not sure if I would call it entertaining, but certainly interesting and enlightening: The Improbability Principle by David Hand.\n\nAlso in this genre is Knock on Wood by Jeffrey Rosenthal.', '""Against the Gods"" has an interesting history of statistics.', 'Nate Silvers (the 538 guy) _The Signal and the Noise._', 'Jesus. This is the same story for the last 20 years. Learn SQL if you want to get paid. Excel too. VBA but never admit you know VBA. My CV checks. Trust. Great math does not lead to great sales.', ""*Innumeracy* by John Allen Paulos is great. It's what got me interested in statistics in high school."", 'This is an excellent [list](https://www.stat.berkeley.edu/~aldous/157/index.html) by David Aldhous of books relating to probability.  His course at Berkeley is STAT157 Probability and the real world.', ""With an apology for suggesting my own book, and another apology because it is not published yet, I think you'll like *Probably Overthinking It*!\n\nhttps://www.amazon.com/Probably-Overthinking-Questions-Statistical-Decisions/dp/0226822583"", 'I have read Weapons of Math Destruction, my major is cybersecurity so it was perfect for my interests. The others look great too, love learning the history behind math concepts. Thanks!', ""I'm half way through The Lady Drinking Tea and it's been an absolute pleasure."", 'Thanks! Just added Do Dice Play God? to my cart.', 'Fifty Challenging Problems sounds fun, thank you. Added it to the cart. Lots of good stuff in there now!', 'Love the picture with the guy drowning in water that is 3 foot deep on average.', 'Thank you! Who knew there were this many funny stats books?', 'I just downloaded GNU Octave so this might be good to play around with, thanks!', 'easy on the drink there bud', ""Do you already have Stephen Stigler's History of Statistics? Lots of stuff about how physicists and astronomers found best-fitting curves before modern statistics was invented, about Karl Pearson's collecting and analyzing his racist brain-volume data, and the time in between. Lots of stuff about guys like Udny Yule who don't get much press anymore."", 'This one looks fascinating, and Amazon has a used copy for cheap, perfect.']"
[D] How to Compare Regression Models?,"Hello everyone!

I am having confusion on how to evaluate and compare the quality of different regression models. 

- For example, I understand that classification models are more straightforward to compare and evaluate as metrics such as F-Score, AUC/ROC, Confusion Matrix are all bounded between 0 and 1 .

- However, in regression models, comparison metrics such as RMSE, Cross Validation Error, AIC and BIC are all unbounded - if several regression models are compared, the model with the lowest RMSE, AIC, BIC still might be an overall bad model even though its better than all the other models! (e.g. a turtle is faster than a snail but both animals are still slow!) 

This being said, is there any general advice on how to compare different regression models fit on the same dataset?

Thanks!",12sjg84,SQL_beginner,1681959014.0,2,1.0,"['R^2 is bounded between 0 and 1. Maybe thats what youre looking for. Of course if youre comparing models that are predicting the same thing youd want to use adjusted R^2 instead.', 'You always have to know and understand the context of the data and the purpose of the model. There is no general indicator in the sense of a number that pops up and if it\'s above or below whatever the model is ""good"".', 'It\'s good practice to use the same cross validation partitions for all of your models, then you can compare their performance on identical test sets.\n\nRemember that any missing value imputation or feature engineering/transformation that requires a sample statistic in the formula (e.g., calculating the Z-scores for a feature), it should be done inside the cross validation loop.\n\nAlso remember the wisdom of George Box: **All models are wrong; some are useful.** He elaborated:\n\n*""Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = nRT relating pressure P, volume V and temperature T of an \'ideal\' gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question \'Is the model true?\'. If \'truth\' is to be the \'whole truth\' the answer must be \'No\'. The only question of interest is \'Is the model illuminating and useful?\'.""*', 'Things like RMSE and MAPE can obviously tell you something about the absolute and relative quality of your models if you know the context of the data generating process youre modeling. An RMSE of 100kg would probably not be very good for a model that models (normal) human weights (well, actually mass). Conversely, a model fitted on the same data with an RMSE of a couple of kilograms would likely be useful depending on the context, circumstances and the models use cases. It would clearly be more useful than the former.']"
[Q] Question about Autocorrelation and lags,"So I have been trying to decorrelate some data which are autocorrelated, but one thing I am quite unsure of is how lags works. So say that for example I have taken a autocorrelation test of a variable that goes from week to week and get some autocorrelation values back. So in lag 1, that means the autocorrelation from 1 week to the next week as I have understood it, but does it only take my first two values and looks at that, or does it look at every value in my variable and see them in a week for week, then averages a autocorrelation value. Or is it that every autocorrelation value is the same for lag 1? I am wondering the same things about 2 lags and 3 lags etc.",12seda5,SaudSaudesen,1681947075.0,2,1.0,"['for weekly data you look at all the values, shifted one week. \n\nIf you have 120 values and lag the data by 1, you get 119 values -- the original set of values shifted along one time unit.', 'Does that mean with a lag 1 that it has calculated the autocorrelation between each one week difference all the way down for all data and then given a average, and same for 2 lags etc?']"
[Q] Sub-Gaussian Random Variables,"Besides fast tail decay, what are other desirable properties that can be extended to sub-gaussian random variables? Why is fast tail decay so important and how does this apply to statistical research?

The more statistics I'm exposed to, the less I feel like I understand it all. If I were to speculate, I would think that fast enough tail decay is important for hypothesis testing, in order to always be able to define a critical region. Is this right?",12sczci,zebrapaad,1681944069.0,1,1.0,"[""Sub-Guassian random variables are desirable since it's gives us a way to guarantee large deviations from the center of the distribution are rare. Ideas like these are used to show concentration results. In turn those concentrations can be used to show insights on how we can approach problems."", ""I don't see why fast tail decay would be important to hypothesis testing. If my test statistic were to have say a Cauchy distribution or a Pareto distribution under H0, there's no difficulty in using that in a hypothesis test, defining critical regions using it, etc.\n\nYou're going to have to explain what difficulty you see there.\n\nOnce you clarify what difficulty would arise, consider t tests and F tests. Are t's or F's  sub-Gaussian? How do they work?"", 'can you expand a bit on what you mean by ""insights on how we can approach problems?"" I am often asked to show concentration & large deviation results, without knowing why they are relevant.', ""I see what you're saying. In that case, why do we care about fast tail decay or large deviation results?"", 'Maybe unrelated to OP, but I have seen a sub-gaussian requirement for certain moderate-deviation bounds, which can be important for validity of testing in certain scenarios.\n\nMore specifically, in general we do not know the exact distribution of our test statistic and only its asymptotic distribution (using some type of CLT). In some instances we actually care about looking quite far into the tails (eg. in genomics/stat-gen when you have multiplicity to worry about). In those cases results like Berry-Esseen does not really justify use of p-values obtained from a gaussian:\n\nBerry Esseen gives us a result like (for Z\\_n a scaled, centered, sample mean of iid obs)\n\nP(Z\\_n <= t) = Phi(t) + O(n\\^{-1/2})\n\nWe generally want a \\[moderate deviation\\] result like\n\nP(Z\\_n <= t) = Phi(t) (1 + o(1))\n\nand it needs to hold uniformly(ish) in t\n\nIn the case that you use the true standard deviation, you generally require subgaussian tails for the original variables to get a moderate deviation bound, something like (I believe):\n\nP(Z\\_n <= t) = Phi(t) (1 + O(t\\^3n\\^{-1/2}))\n\nIt turns out if you use a t-statistic (so you standardize by the empirical standard error), then you only need an absolute third moment condition.', 'Yeah, I was sticking to exact distribution of test statistics, not asymptotics; you run into a couple of issues when going into the tails (which is more about problems with approximation than problems with hypothesis testing)']"
[Q] How to determine whether chi2 from one dataset is the same or different from another,"Hello,

The question is the title. Lets say your doing an OLS to solve for some values given some data, you have multiple different sets of data that give similar chi2 with different chi2 values

    dataset1-->OLS-->chi2
    dataset2-->OLS-->chi2
    dataset3-->OLS-->chi2
    #maybe you do some global fits and combine datasets
    dataset1,2-->OLS-->chi2
    dataset1,2,3-->OLS-->chi2
    #where chi2=(sum((data-model)/error)**2)/N) where N is the size of the dataset

How can you determine the correlation of these chi2 values? For example one gives you a chi2=2, another chi2=4? Is 2 really that much better than 4, or is it the same thing?

I feel like the chi2 distribution test is the answer here, but I'm completely confused how to use it. Do I look at the distribution of the chi2 before summing for each dataset?

    chi2 distribution = ((data-model)/error)**2) 
    dataset1-->OLS-->chi2 distribution-->sum-->chi2
    dataset2-->OLS-->chi2 distribution-->sum-->chi2

And there are so many different tests for looking at different things such as Pearsons test or CDF. So just looking for some guidance to filter out all the noise and find what applies to the question I'm asking.",12s923o,DrBobHope,1681936179.0,5,0.86,['[Ratio of 2 independent chi square random variables divided by their degrees of freedom is F distribution](https://www.danli.org/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/) This page has the derivation but you can safely take their word for it.']
[Q] How to speed up model fitting with R & Stan?,"I frequently need to fit Bayesian regression models, with just R or in some cases using packages that rely on Stan in the backend, such as brms. I usually have a number of models to fit (e.g. a variety of model specifications and outcome variables to predict). Right now, I'm just running everything on my MacBook and each model has to be fitted before moving on to the next. This means the whole process takes a long time. I am sure there is a much better way to set up my workflow, I'm just not sure exactly what it is. 

How can I change my workflow to speed up the process and run my code more efficiently? And what are some resources for learning to do this (e.g. online classes, books,, documentation). 

Some of the specific questions I have are:
- How could I transfer my work from my computer to AWS? What AWS Services should I use to set up a good workflow? 
- How can I fit multiple models at the same time?
- Should I learn a tool like Spark?

I don't know what I don't know, so if there are other tools or strategies I am missing, please share your ideas!",12s374j,yaasqueen,1681924309.0,4,0.84,"['There are a few things you can try to speed this up. Firstly, if youre not already you can likely use multiple cores to run separate chains from your model. Depending on how many cores your laptop has this can speed things up dramatically.\n\nSecond, you may be able to optimize your Stan program. Use vectorization instead of loops when possible, and check if you are using the most efficient functions for what you are doing. For example Stan provides normal_id_glm which is usually more efficient than manually writing out your regression with the direct normal pdf.\n\nThird, Stan also supports variational inference which should be much faster at the cost of providing only approximate posteriors. Im not sure if brms directly supports this, but you should be able to write out the Stan code from brms and then run variational inference with that code.', 'Do your models take a long time to fit individually, or is it that you have a lot of models to fit ?', 'Parallelize what you can; multithread when you can; have more cores. Use vectorization instead of loops, especially for likelihoods and priors. Use the ""O1"" stanc compiler optimization option when using brms or cmdstanr.\n\nOther things you could do: if you have lots of data, one strategy is to create small bins for covariate combinations... e.g., if you have categorical predictors only, then aggregate to sufficient statistics and use weights; this can massively speed up your model. If you have continuous vars, then, depending on your model, you could round them to some bin value, and again aggregate data and use weighted regression. It\'s identical in the former case, nearly identical in the latter case, and you drastically decrease the number of likelihood evaluations. In general, you can use sufficient stats to decrease the amount of likelihood evaluations required. For example, you don\'t need to have 1000 rows that look like: x1=0, x2 = 0, x3 = 1, outcome = 1; you need \\*one\\* row of x1 = 0, x2 = 0, x3 = 1, outcome = 1, with n = 1000. Then just use brms\'s weights(n) functionality to add 1000 contributions from that row to the likelihood; in stan, this is the same as multiplying the log-likelihood for that row by 1000. \n\nUse non-uniform priors; use some semi-informative priors.\n\nBrms stamps out reasonably optimized or optimizable stan code, but if you\'re writing stan code yourself, definitely use vectorized lupdfs and lupmfs; definitely precompute constants; precompute anything you\'ll need repeatedly; use specialty functions when possible (e.g., diag\\_pre\\_multiply instead of doing matrix\\*diag(matrix); use inv\\_logit instead of 1/(1+exp(-x)), etc).\n\nStan is very sample-efficient, so you may only need a total of 500-1000 samples per chain.\n\n&#x200B;\n\nI would use ADVI as an absolute \\*last\\* resort, because Stan\'s ADVI is, honestly, terrible right now. There are improvements planned, but it\'s currently not good.', 'Im going to echo folks and say that some more information on both what is taking time and how many things you need to run is going to be very important. The answer could easily be dont recompile models unnecessarily, shave off some iterations or burnin, optimize your Stan code, optimize the models parameterization, or exploit parallel capacity before you hit something like AWS.', 'I would suggest you look into netflix metaflow\n\nIt basically allows you to transparently run r code on aws and parallelise\n\n( I have not used it on r, but with python)\n\nhttps://docs.metaflow.org/v/r/getting-started/tutorials/season-2-scaling-out-and-up/episode05\n\n( Just to mention that it uses aws batch under the hood,\nAnd you should look into specifying spot instances.. which are cheaper because occasionally aws takes the machines back in periods of high demand)', '[deleted]', ""To add, can you write your models into one big model so that you don't have to do MCMC multiple times?  You would have to increase the number of iterations to compensate to ensure convergence, but I think it would still result in a net decrease in computation time."", ""Stan is a PPL that's independent from R. Your model is compiled to C++, and supports parallelization of chains over multiple CPU threads/cores, and GPU acceleration. It's very fast and well optimized.\n\nIt has interfaces in R, Python and Julia. Whichever interface you use won't change your model's sampling speed.""]"
[Q] Why doesn't flipping the axes of a scatterplot give an inverse trendline?,"Sorry if this is a basic question, but I'm trying to find a best fit equation to map two datasets, let's say they're student scores on Test A and Test B.  If I have their scores on Test A as the x-axis, and their scores on Test B as the y-axis, the best fit equation I get is .46x+39.  My understanding is this best fit equation is essentially saying ""If a student scores x on Test A, their score on Test B will likely be .46x+39.""  So, for example, a 90 on Test A would predict an 80.4 on Test B.

My assumption would be that you could invert this -- for y=.46x+39, you get x=(y-39)/.46, or y=2.2x-84.8.  However if I flip the axes on my chart, with the x-axis being their score on Test B and the y-axis being their score on Test A, I get a best fit equation of .389x+76.9.  This would mean getting an 80.4 on Test B would predict a score of 108.2 on Test A.  I understand best fit equations aren't exact, but I'm wondering why flipping the axes doesn't just produce an inverse of the best fit equation?",12s21os,m0nkeybl1tz,1681922137.0,2,1.0,"['This is illustrated on the cover of David Freedman\'s [""Statistical models: theory and practice""](https://www.amazon.com/Statistical-Models-Practice-David-Freedman/dp/0521743850)\n\nUsing that cover as an illustration, and imagining the ellipse as representing a level set of a bivariate gaussian distribution (ie, a rough outline of a cloud of points), the solid line represents the regression of y onto x.  If you did a ""pencil test"", ie, lay a pencil parallel with the y axis onto the graph and slide it across the cloud from left to right, half of the dots (or half of the shaded portion of the ellipse) should be above the solid line, and half below.  This line minimizes the sum of the squared y-distance of each point from the line.\n\nThat is a different line than the dotted line, which seems to go through the long axis of the ellipse.  This line minimizes the sum of squared distances in any direction of points from the line.  The optimal direction to minimize distance is perpendicular to this line.\n\nI think the property you are thinking of, that is, if x and y were swapped, would the new best-fit line have 1/slope of the old line, would be satisfied by the dotted line.', ""Consider the extreme case of two *unrelated* variables (neither has any relationship to the other, they're independent). When predicting Y from X, where's the population average of Y at each x-value? (i.e. given X=x)  \n\n .... It's just the population mean of Y.  Your population regression line of Y on X is horizontal.\n\nNow interchange the roles of Y and X (predict X). The answer doesn't change, only the roles -- the prediction of X at each value of Y=y is the population mean of X. \n\nThat is, you get two lines that are at right angles.\n\nYou're asking why isn't the second line vertical, which seems to forget what the exercise is (predicting X, not Y).\n \nAs the variables become more linearly related, the mean of Y at each given x will become less horizontal, and the two lines move from being at right angles to having some smaller angle between them.\n\nAs the  (population) correlation approaches 1 -- no noise, the lines coincide (the angle becomes 0). That's the case you were expecting for all possible relationships. \n\n(Similarly as the correlation approaching -1..., the two different lines approach each other)"", 'Wow thats a very clear illustration, thanks! So in my example, is there one that would make a more accurate prediction? If my end goal was to take a Test B score and predict their Test A score, would I use Test B as the independent variable, or should I use Test A then invert it?', 'Thanks thats an interesting way of thinking about it. Just to clarify, why isnt the second line vertical? If you were analyzing the points x values in terms of y, that would be a vertical line, no?', 'It\'s customary to put the independent variable on the x-axis. If you have ""Test B"" scores and want to predict ""Test A"", then you would do a linear regression of A onto B with matched pairs of Test A and Test B for your training data.\n\nThe resultant equation will be the best linear unbiased predictor of ""Test A"" using ""Test B"" as input.\n\nIt might be easier to predict ""Test A"" from ""Test B"" than the other way around, it just depends on the data.  If everyone got exactly the same score on Test A but Test B had a range of scores, it would be easy to predict Test A from B (it\'s all the same score!) and hard to predict Test B from Test A-- because Test A basically provides no information.']"
[Q] Is R squared truly ineffective for nonlinear regressions?,"Basically every article I've found says that R squared does not work for nonlinear regressions, but there are so many online calculators and programs that display R squared values for nonlinear regressions

Some examples:

[https://stats.blue/Stats\_Suite/exponential\_regression\_calculator.html](https://stats.blue/Stats_Suite/exponential_regression_calculator.html)

[https://www.easycalculation.com/statistics/exponential-regression-least-square-fittings.php](https://www.easycalculation.com/statistics/exponential-regression-least-square-fittings.php)

Even Excel shows R squared for nonlinear regressions in charts

Are they all wrong or can R squared be used for nonlinear regressions?",12s1dng,427987526743,1681921334.0,31,0.97,"['There are pseudo r-squared measures that can be used.  The question is to determine what statistic the software is actually reporting and calling ""r-squared"".', '> Are they all wrong\n\nYes: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892436/', ""The problem isn't the nonlinearity of the fitted function. The issue is how you choose to measure the size of errors. Garden variety r^2 works in a world where squared distance in the y-direction is how we measure errors, and what we minimize to fit a model. If you do that, r^2 works the way you expect it to.\n\nIf you do quasi-linear regression (usually we fit y=a exp(bx) by fitting log y = log a + bx with simple linear regression), you will get a different measure of goodness of fit if you do the measurement in the transformed space than in the untransformed space. (Usually we use the measurement in the transformed space; if we log-transform data it is because our errors are proportional to our observations and after we take logs, we have uniform errors again.)\n\nIf you are using some more exotic model where the notion of squared distance doesn't apply, you need to find yourself a new distance metric, and find a new goodness of fit measure that applies to your new distance metric: if you're doing something likelogistic regression, don't expect your pseudo-r^(2) to behave anything like the r^2 you are used to using."", 'The difficulties are multiple. Here are some issues off the top of my head. \n\nIn linear regression, equivalent ways to calculate R^2 and consequently, the ways to interpret it (proportion of variance, closeness of fit to the data, deviation from a horizontal relationship, etc etc) rely on properties of linear regression.\n\nAs soon as you move away from that, you no longer have these connections -- various ways to write R^2 are no longer the same, and the various interpretations relate to different things. \n\nFor example, typically R^2 will no longer be constrained to be between 0 and 1; if you manage to find or construct a definition that obeys that restriction, you\'ll then certainly lose another property you\'ll want to rely on.\n\nSo your first problem when you have something that calculates some ""R^(2)"" is figuring out *exactly what they calculated*.\n\nYour second problem is figuring out what *that* ""R^(2)"" actually tells you about the fit.\n\nYour third problem is avoiding you or anyone else thinking any other property of R^2 applies.', ""It's not that it's useless, but just that the interpretation and how you go about using it might be different.\n\nFor example if you do linear regression to fit a non-linear curve by say fitting in log space or something like that, you might be measuring the R^2 in the log space and not the real space meaning that a distance of 0.1 is not actually a distance of 0.1 in real space.  There's been times I've fit a linear model in function space where I actually don't take the most minimized parameters because they mispredict key features in the real space. \n\nThe second problem is when you move into models with a very large number of parameters, the R-square can be misleading if you don't couple it with other metrics.   It's always possible to get a perfect R-square with a model that's bigger than the fitting data, but of course it will be complete garbage when you go to deploy it because it's over-fit.  You always deal with aspects of the curse of dimensionality when you deal with more complex models.  Primarily that many metrics have different interpretations in high dimensions than they do in low dimensions and model complexity is one way to increase the dimensionality of your problem."", 'Has anyone experience using Mean Absolute Error (MAE) as an alternative?', 'Seriously, if you have the option just use a friggin information criteria.', 'In what way are they ""wrong"" ?  \n\nA pseudo r-squared is a calculated statistic.  It may be useful or not useful in a given situation.  Can you say that calculating a mean is ""wrong"" ?\n\nBTW, maybe it\'s just me, but I didn\'t find that article at all convincing.  First, they are addressing only one potential pseudo r-square measure, which they just call R^(2).  But more importantly, all the models they are comparing have an r-squared of 0.99 or greater.  What good is that ?  What the paper really concludes is that Akaike weights, AICc and, BIC were more useful in selecting models in this case, which I think is just good advice: don\'t use r-squared to select among model, and consider measures like AIC or BIC.', ""> with the 'true' model having up to 500 times more strength of evidence based on Akaike weights, this was only reflected in the third to fifth decimal place of R2. In addition, even the bias-corrected R2adj exhibited an extreme bias to higher parametrized models. The bias-corrected AICc and also BIC performed significantly better in this respect.\n\n> Researchers and reviewers should be aware that R2 is inappropriate when used for demonstrating the performance or validity of a certain nonlinear model. It should ideally be removed from scientific literature dealing with nonlinear model fitting or at least be supplemented with other methods such as AIC or BIC\n\nThat says R^2 is not good"", ""MAE and other common forms of accuracy measures (RMSE, and so on) are based on the differences between the observed values and the predicted values.  They are similar in this approach to what I call Efron's pseudo r-squared.  (The calculation as depicted here: [https://www.datasciencecentral.com/wp-content/uploads/2021/10/2853465446.png](https://www.datasciencecentral.com/wp-content/uploads/2021/10/2853465446.png) ).\n\nI would say if you have problems with that form of pseudo r-squared, you would have problems with those measures of accuracy.\n\nBut as always, understand the statistic you are using and if it helpful for your situation and purpose."", 'MAE is pretty common in machine learning applications.  Like any distance metrics they all tell you something, but you just have to understand what it is telling you can vary from problem to problem.\n\nA golden rule is that the more complicated the data set, the less likely one metric is going to tell you everything unless you luck out and get a data set that is really something like a mono-variable problem hidden in a complicated space.', '>all the models they are comparing have an r-squared of 0.99 or greater. What good is that ?\n\nIn some domain spaces, an R^(2) of 0.5 would be considered pretty good. In other spaces, an R^(2) value of 0.990 would indicate rather poor performance.', 'What do you mean by ""good"" ?\n\nI don\'t want to write a book about this paper, but I really don\'t think they\'re results support what they think their conclusions are.  It\'s a little rich to compare models with r-squares from 0.99 to 0.9999 and then critique the r-square measure for changing only in the third decimal place.\n\nReally, look at Figure 3 in the article, and see if you agree that AICc performed better than adjusted r-square.\n\nThat being said, I agree with the basic conclusion of the article, that r-squared or adjusted r-squared shouldn\'t be used to choose the best model.  \n\nBut that doesn\'t mean that pseudo r-squared values aren\'t useful for non-linear models.', ""u/Chris-in-PNW , Agreed.  ... And to some extent that further undermines the point of the paper.  They have pseudo r-squares ranging from, say, 0.99 to 0.9999, suggesting that there's some use to the measure across these models. ... My point was just that if I was going to write a paper about r-square measures for nonlinear models, I would have some models where the psuedo r-squares range from, say, 0.5 to 0.99.  The problem with that is that it would undermine their point, because the reader would be able to easily see the difference between the fit of the models across that range.""]"
[Discussion] Paper on discrimination of NY police officers,"Hi, I am writing a paper looking at discrimination of who NY police officers have chosen to frisk, and whether or not they were carrying something. I have used a heckman sample selection model to account for the selection bias, and I was wondering if you have any suggestions to other things I could try? 

Furthermore The rho term in the sample selection is not significant, which is a bit worrying. They have stopped quite a few more black people than white, and Im not sure if I should control for that somehow

Thank you",12rz5en,andr521a,1681919320.0,2,1.0,"['Look at the work of mummolo and Knox. You need to know how many encounters police have with whites and blacks in order to make inferences', 'I have just read up on that actually, but he/they also say that how often racial groups encounter police is largely unknown. I only have data on people who where stopped and then wether or not they were frisked/searched', 'Write out your dag and be very careful with what inferences and assumptions you make']"
[D] How often do the Linear Regression model-based assumptions truly hold in social science research?,"In Econometrics (and other social science research fields), researchers often have to make some model-based assumptions because their main tool of analysis is linear regression. An important assumption is that the functional relationship between the dependent variable and independent variables is *additive*.

To me, this assumption seems somewhat implausible unless there is a reason that the linear model is commonly found in the natural world. But I think this must be true, otherwise, why would so many papers rely on this assumption? So why does a linear functional form hold for many economics and social science research questions? Does this have to do with the CLT?",12rvy7b,Voth98,1681916155.0,7,1.0,"[""Linear regression always estimates the *best linear and additive approximation* to the true conditional mean function. This can be reassuring because this means our models will estimate something well defined and hopefully useful even when our models aren't quite correct. If the linear model is not correct, but only slightly so, it also means we will be correctly estimating a model that is only slightly incorrect. This can be very useful in practice if the truth is only slightly non-linear and slightly non-additive. \n\nWe can also often justify linearity and additivity if we are looking at a sufficiently localized version of some phenomenon. This is because Taylor series let us approximate most continuous functions as linear and additive functions in small neighborhoods.\n\nAdditionally, linear models can easily be extended to be more flexible. The linear component of the model can be replaced with a polynomial or spline type term, allowing non-linearities. Interactions can also be added that allow the model to accommodate for non-additivity."", 'Its quite common to use generalized linear models or apply transformations to explanatory variables that alter the additive property. Interaction effects also make it so that while the explanatory variables are no longer individually  linearly combined, there is still a linear combination of combinations of explanatory variables. \n\nIn short, there are all sorts of somewhat nonlinear models.\n\nThat said, the less linear the model, the less well behaved the model is. Poorly behaved models are extremely difficult/impossible to work with in a mathematical sense. Classical statistics relies on some sort of linearizable combination of explanatory variables to estimate coefficients in the model.\n\nFinally, allowing too much flexibility in the model leads quite quickly to overfitting. Its one of the reasons you should be extremely skeptical of excessive bendiness in your model, such as using cubic or quadratic models. So we like some kind of constraints on the model to avoid overfitting, and the linearizable constraint is generally seen as useful. \n\nThe central limit theorem certainly smooths out major fluctuations in our parameters. I suppose this could lead to the linearizable constraint being more applicable, but I havent thought about it that way before.', ""Lots of great answers here already, but I want to add some further commentary on justifying the linear model. There are two major reasons why we work with linear regression; the first is a bit tautological and the second perhaps is a bit more interesting.\n\nFirstly, given two random variables y and X, I can always write the model y = X beta + e, where E\\[Xe\\] = 0. In this case, beta is always identified via the OLS formula and thus I can estimate it via linear least squares and recover what some (in particular the econometricians) the average predictive effect of X on y. Whether this beta has the interpretation that you want depends on the setup of your data and the assumptions you are willing to make about it. Next I'll discuss perhaps a more interesting motivation for the linear model.\n\nConsider the standard setting a running a randomized controlled trial (RCT) where we flip a coin and use it to determine treatment. In this setting, the difference in means estimator to identify causal effects (i.e. taking an average from control and subtracting it from the average across treatment) is exactly the same as running linear regression where we regress the outcome on the treatment variables with a constant covariate. We aren't making any assumptions about the linearity of the outcome w.r.t. to the treatment, but when we write the linear model it happens that the coefficient **identifies** the causal effect. \n\nNow this setting may seem a bit contrived, as you could easily just do a difference in averages to estimate the causal effect. However the picture gets quite a bit more complicated when we either (a) block or (b) have additional covariates. I'll focus on the latter since that's what I'm most familiar with but the two settings have quite a bit of similarities. What happens when we have additional covariates is that we may not be particularly interested in them, but we want to use them to improve the precision/reduce the variance in our estimate of the causal effect. One way to do this is just to add them as additional covariates in linear model and then run OLS on our data, and then this will give us better estimates of the causal effect, all without requiring the linear model to be valid (its a lot more complicated but this is roughly the idea, see Freedman 2008 and Lin 2013 for further details on this topic).\n\nTL;DR: There are settings in which we can run OLS without necessarily believing that the outcome is a linear additive combination of our covariates, and the results will still be an estimate of what we are interested in."", 'In psychology, interactions are typical and often the most important result. Therefore, additivity is rarely assumed.', 'To add, we should be particularly concerned abt the necessary assumptions for causal inference being satisfied. I think more often than not, things like SUTVA or parallel trends are not actually satisfied', '>So why does a linear functional form hold for many economics and social science research questions?\n\nThe goal is to answer some kind of research question, not necessarily find the true functional form. Finding that form may be intractable, and a linear model is often a good enough approximation to be useful.', 'Often if you relax this assumption the main results stay the same. Often researchers will do different model specifications (in the appendix) that are more complicated. But they present OLS since its easily interpreted. Has nothing to do with the CLT.', 'All models are wrong - some models are useful', 'If you mean ""hold exactly"", in all likelihood, literally never, outside fake data examples. Indeed, in many cases it\'s likely that *none* of the assumptions exactly hold. (This is a non-issue; it\'s a model - an approximation - not a fact)\n\nIf you mean ""hold close enough that the p-values under H0 would be close enough to be correct for my purposes"", then probably reasonably often, like maybe something in the ballpark of half the time. (*Testing* assumptions may be a considerably less than ideal way to approach this in general, especially given you can almost always assert the falsity of the assumption)\n\n> Does this have to do with the CLT?\n\nOnly in a slightly indirect sense that for the assumptions relating to normality, in large samples, the p-values should get closer to being correct under H0.\n\n\nOther assumptions (which are almost always more important than normality) don\'t tend to relate at all to the CLT. In particular, why would *additivity* have anything to do with CLT?\n\nFor some forms of non-additivity, you can literally expand your model class and avoid assuming their absence, but you will tend to need large sample sizes unless the form of the non-additivity is only going to take  a few d.f. In the cases where this makes sense to do, you can then see how much difference it makes to your conclusions.', 'From Gelman and Hill 2007\n\n```\nWe list the assumptions of the regression model in decreasing order of importance.\n\r\n1. Validity. Most importantly, the data you are analyzing should map to the research question you are trying to answer.\n\n2. Additivity and linearity. The most important mathematical assumption of the\rregression model is that its deterministic component is a linear function of the\rseparate predictors\n\n3. Independence of errors.\n\n4. Equal variance of errors. If the variance of the regression errors are unequal, estimation is more efficiently performed using weighted least squares, where each\rpoint is weighted inversely proportional to its variance. In most\rcases, however, this issue is minor. \n\r\n5. Normality of errors. The regression assumption that is generally least important is that the errors are normally distributed.\n\n```\n\nWith the right transformations, the answer to your question is ""very often"" and / or ""very often, enough that these models are very very useful"".\n\nAnd of course there are extensions to models that help them better represent reality. In the book quoted above, for example, they offer""multilevel models""', 'Nobody ever remembers SUTVA ', 'But to be good enough means the true functional form likely isnt extremely different. So this goes back to my question, why are most natural phenomena close enough to the linear model? Also, what is the evidence that the approximation is good enough for most cases? I think this second question might help answer my initial question.', 'Thank you for the response. Regarding your point about why would additivity have anything to do with CLT, I thought that when some phenomena is a product of many additive effects, than it tends towards the normal distribution. So specifying a linear model with an additive relationship between a bunch of parameters presupposes that the outcome of interest will be influenced by the CLT. Perhaps my reasoning is flawed?', '>But to be good enough means the true functional form likely isnt extremely different.\n\nThat\'s the thing, though - statistical models are not mechanistic models, and they are not necessarily being assessed on how closely they represent the natural laws governing whatever relationship is being modeled. Depending on what you\'re trying to do, it might actually be a negative thing for a statistical model to closely represent the true functional form.\n\n>Also, what is the evidence that the approximation is good enough for most cases?\n\nWhen I say ""good enough"", I\'m referring to if the approximation is appropriate for answering the question at hand. If all you want to do is figure out the average height of some population of interest, for example, you could represent it as a linear regression with just the intercept.']"
[Q] Multiple comparisons,"Hello,

My study investigates the effect of a particular variable (3 comparisons) across 2 outcomes (6 total comparisons). In calculating the number of pairwise comparisons for Bonferroni, do we use the total number (across all the outcomes) of comparisons, or simply the number of comparisons within an outcome? Are both methods acceptable?",12rs5e6,ConstantCheesecake37,1681908497.0,1,0.67,"[""That statement are you trying to make (i.e. what do you want to try to reject or don't reject)?"", 'By *outcomes*, you mean different dependent variables ?', 'Survival and disease recurrence outcomes', 'Okay, but are they different dependent variables in different models ?', 'By dependent variables do you mean covariables?', 'No.  By dependent variable I mean the variable on the left side of a model.  In your question with 6 comparisons, are they all from one model ?  Like,  *Height = Age + Gender*, *Height* is the dependent variable, and *Age* and *Gender* are the independent variables.  (The term ""outcome"" in your question is confusing.)', 'I understand now, no there are three comparisons with two separate models. Between the two models there are six comparisons total', 'So, the number of comparisons to use for a Bonferroni correction is the number of *p*\\-values in the ""family"" of comparisons.  What you consider a ""family"" may depend on how you see things.  \n\nWith two separate models, I would consider the comparisons for each model a family.  (That is, each has three tests).\n\nBut you could interpret the situation as being a family of six tests.']"
[Q] SPSS Q Merging Variables,"SPSS question. There is definitely a better way to do this but I have two variables (var1 and var2) that both look at size (the variables just differ in the year they were created in a large dataset (var 1 are cases from before 2016 and var 2 after) so theoretically each subject should have only one datapoint in either var1 or var2 but not both. I wanted to merge these two variables into one variable that has the size for all patients in the dataset. I couldn't figure this out so I decided to compute a new variable=sum.1(var1,var2). knowing that it would be okay to add them b/c each case should only have a datapoint for one of the variables. however, some cases put the size in both var 1 and var 2 so adding them doubles the actual size. It is a huge dataset so I started by going through and trying to delete the duplicates by hand but its taking too long. I then decided to select for cases if var1>0 AND var2>0 and copied those cases to a new dataset (over 1100 cases). I then recoded var2 (ELSE=0) to set all var2=0 so I could use my compute merged variable appropriately but now I do not know how to get these cases back into the original dataset and replace the duplicated cases only. There has got to be a better and more efficient way to do this lol but just learned stats and spss recently. can anyone help?",12rryuq,oliversacks7,1681908109.0,0,0.5,"[""I don't know SPSS but in other languages, this is called *coalesce*"", 'You just need to use a little syntax:\n\nDo if not(missing(var1)).  \nCompute NewVar = var1.  \nElse.  \nCompute NewVar = var2.  \nEnd if.  \nExecute.', 'I am not sure I completely understand what you did...To clarify: You have two datasets that include the same variable, and you want to merge them into a single vector?', 'Thank u for replying itll help me with searching how to do it if I have a term. Ill check it out!', 'i knew there must be some syntax for this so thank you! i will give this a try!', 'OMG THANK YOU THANK YOU THANK YOU!!!! you just saved me hours and also from not banging my head into a wall lol. just curious how did you best learn more advanced syntax? Do you have a resource you think is good to consult? Thank you so much again for your response and any suggestions on how to improve my syntax skills would be amazing!', 'No I have one dataset but selected for cases that had data in both var1 and var2 bc all cases should just have data in one or the other but not both and then filtered these out to a diff dataset so I could randomly choose var1 or var2 and make the entire column = 0 to remove the duplicate data. I cant just delete one of the variables though bc if the person input the data correctly the info should go into one or the other and not both and I want to make one new variable that has info on size for all cases instead of having 2 variables with size info', ""I'm so glad I could help.  I love SPSS syntax.  And I mostly learned it from reading the Command Syntax Reference guide which is under the help menu.  You might like the sections on DO IF, DO REPEAT, and DEFINE-!ENDDEFINE.  DM me anytime if you get stuck."", 'I see. The other response you got about coalescing is right, but you cant do that without cleaning up the data first, because, as you note some people accidentally got size values on both var 1 and 2.  If it were me, I would use Excel for a task like this (and this is coming from a frequent SPSS user).  You can run a macro in Excel to look for duplicates, which will help you clean the data much quicker.', 'you are a statistical angel!', ""thank you so much for answering! usually with this dataset I can't export to excel b/c its too large (in the hundreds of thousands). i have cut it down some by excluding certain things etc. do you know if having around 55k cases could be exported to excel?"", 'Do you have a sense for how many people have duplicate values? I am wondering if it might easier to filter those cases out, so that you have a smaller dataset to work with.', ""I do. its roughly 1400. I filtered them and exported those cases to a new dataset and then was going to set all those people's var2=sysmis or 0 but then can't figure out how to add them back to the original dataset with these new values instead of the duplicate ones.""]"
[Q] Ranked data and hypothesis testing,"I have a question regarding ranked data and hypothesis testing.

Hypothesis: Car drivers are more likely to consider the car as the most beneficial transportation method for mental health than other transport method users'.

Respondents were asked to rank 4 transport methods from least (1) to most (4) beneficial to mental health. Responses are in the table below.

|Outcome|Drivers|Non-drivers|
|:-|:-|:-|
|Car 1st|10|3|
|Car not 1st|35|82|

A Fishers Exact Test test gives me a p=0.001.  The Fisher's test shows that there is a significant relationship between the variables, however, that would prove a hypothesis such as: 'Mode of transport influences the perception on which transport provides the most mental health benefits.' However, I want to know if car drivers are more likely to think this way.  What test should I do here? Thank you for your help.

Kind regards,",12rossl,csira_allapot_5876,1681900618.0,15,1.0,"[""Given the caveats in other comments (which are important),  you are correct that the Fisher's Exact Test tells you the relationship between your variables (for your sample) is significantly different from 0. \n\nIf you want to understand the direction and magnitude of that effect, you might look into using an Odds Ratio.\n\nHere's a post that is specifically on how the Odds Ratio relates to Fisher's Exact Test\nhttps://stats.stackexchange.com/questions/211487/interpretation-of-the-fisher-exact-test"", 'Did respondents rank order the transportation types from 1 to 4, or were they asked to provide a score out of 4 for each transportation type separately?', 'It\'s not correct that the test you did ""proves"" one variable influences another.\n\nFirst, in statistics we basically never prove any hypothesis. We only show that data are unlikely to show a given relationship if the null is true. Even with a very low p value the null can be true.\n\nAnd this tests the null of independence across variables, which does not test casual relationships. They may be correlated and both driven by some other unobserved variable.\n\nFor analyzing ranked data, check out the awesomely named exploding logit model.', 'Perhaps you could start by doing some ANOVAs and Mann-Whitney U test?? Monotonic regression.', 'Take your 4 by 2 data and put it into POLR (proportional odds logistic regression, R function polr in R package MASS, but also available in other statistics software).  Or you can learn about and use some other method for ordered categorical data.  But if you have that low a P-value for your data when dichotomized, the strong statistical significance will probably get even stronger when not dichotomized.  But you never know until you actually do the analysis.', ""They were asked to rank the order from 1 to 4. I didn't know it would make a difference. Could you explain, please?"", 'Thank you for your response. You are right in both points and also my phrasing was indeed wrong. I will check the model you mentioned. Thanks', 'Sorry, but I am quite sure ANOVAs are not useful in this case....can someone else confirm, please?', ""See Kendall's W to assess if the two groups agree in how they rank order. Otherwise, it sounds like you've already answered your research question if I'm understanding correctly... ie drivers were more likely to rank the car first.""]"
[Q] Goodman and Kruskal's Gamma Resources,Does anyone know good academic sources (or textbooks preferably) that explain and show how to compute a Goodman and Kruskal's Gamma correlation?,12rjnt4,MadPotato61,1681886285.0,7,0.9,"[""It's discussed in Agresti, A. 2007. *An Introduction to Categorical Data Analysis*, 2nd. Wiley. , but *very briefly*.\n\nYou might use that for a reference, but for examples of the calculations, maybe see something like:\n\n[https://www.statisticshowto.com/gamma-coefficient-goodman-kruskal/](https://www.statisticshowto.com/gamma-coefficient-goodman-kruskal/)""]"
[Q] How to define/find this accuracy in data,"Pardon my misuse of terms; I am an engineer but I dont deal with stats very often.

I have 2 sets of data which are both estimations of a 3rd, unknown, set of data. I need to determine 1. How close the 2 known sets are and 2. How accurate the second one is... the problem is in how to define that accuracy

As an example:

There is a picture with 9 elephants. A human counts 8, and a software counts 10. The first dataset is built from the human counts, 8 in example, of a bunch of pictures. The second set is built from the softwares counts of the same data, 10 in example. The third, unknown set, would be the actual count, 9 in the example. 

Ideally, we would like to know how accurate the software is in comparison to the true counts, IE the count was 9 but we got 10 from the software. However, we dont know the true counts. The idea then is to assume that the human counts are off by some percent on average, and to see if the software counts are accurate enough to that.

The problem is I do not know what measures to be using for this so that I can say this:

The software is ____% accurate compared to human counts, which we estimate are within ____% of the true counts on average. This means that the software is ____% accurate to the true counts.

Any and all help appreciated, I am more than happy to discuss more in the comments!",12rd9er,ineedausername95,1681870604.0,4,1.0,"['If we never know the true counts then we would need to make some pretty drastic assumptions. Is all the data we have is the faulty human and software counts then we would need to assume some further structure in the data to go off of. Do you have any more info about how the faulty counts could be generated?', 'So we can find the true counts for a portion of the data, but certainly not all and only with images that have a low number of objects just due to the difficulty in verifying.\n\nThe faulty counts on the human side are generated more regularly on images with high counts. For example, an image which has 20 objects is likely to be counted as 20, but once we reach the hundreds there is likely to be missed counts.\n\nAlso, I do have some formulas for estimating counts that I know the humans use. For example, in a 10x10 image with a lot of objects, the human count is generated by counting in a 1x1 grid square and multiplying by 100, which inherently has error.\n\nIs there any specific info you would like to know? I may be able to provide more details if it helps']"
[Q] Interpretation of interaction terms,"Hey yall, hope this is not a dumb question. Not a native english speaker, so I might've missed already answered questions on this.

I try to estimate if a variable, lets call it ""impact"", has an effect on another variable, ""outcome"", after event X, which I denote by a dummy, ""post"", which is 1 if observations in my panel are after event X. For this, I did the following two regressions:

(1) outcome \~ post + error

(2) outcome \~ post + post\*impact + error

Now, in (1), post is significant, which I interpret as event X having an effect on outcome. In (2), post\*impact is significant, but post isn't.

Can I just disregard post not being significant in (2) and solely focus on post\*impact as my explanatory variable, or is there anything I need to address regarding post? I vaguely remember something from my stat classes about variables being hard to interpret when they're also in an interaction term in the regression, but that feels like ages ago and I can't find anything in my notes (and sadly can't read up in Stock and Watson for a few more days).

Any help is greatly appreciated!",12rcb9l,IndeterminateYogurt,1681868510.0,1,1.0,"['model 2 is telling you that there is no difference based on time except in the group that experienced the impact (that is, the impact has a measurable effect). model 1 is not really relevant to your question, and you need not interpret our report it. Incidentally, your design is called a before-after-control-impact (BACI) design', 'Thanks for the answer! That helps a lot, and especially thanks for the name of that design, now I can read up a lot more.', 's forgot to mention, you should also add the main effect of impact to ensure that there werent intrinsic differences between the groups:\noutcome ~ post +impact + post *impact + error', 'Opps, forgot to include that in the post. Have it in the model tho. Is it an issue if that one is significant, or does that simply mean theres an additional time effect of impact on top of post\\*impact?', 'OP if youre doing this in R just including the interaction terms will also include the uninterested variable.', 'if the main effect of post is significant, there a time effect that is not impact-related (i.e. it occurred for both groups). if the main effect of impact is significant, the two groups have intrinsic differences that were detectable before the impact occurred. \n\nthat outcome weakens any inference you might draw about the interaction effect because it raises the question: if the two groups are intrinsically different to begin with, might they not have intrinsically different time-based behaviors? so a little more caution is necessary when reporting the interaction', '>the two groups have intrinsic differences that were detectable before the impact occurred.\n\nSo theres an effect not captured by the controls then?\n\nBasically I\'m trying to evaluate if ""impact"" has an impact on voters likelihood to disclose their decision to vote yes on a political issue. As I\'m trying to figure that out, it shouldn\'t be an issue if impact stays significant, as theres likely other differences between those who remain ""dishonest"" about their intention and those who ""reveal"" them by flipping their stated decision, right? I\'m trying to isolate the effect of impact on that likelyhood to reveal their preferences', 'Answer to first question is yes. Beyond that, youre into the a little more caution part of my answer above, and this is where domain expertise comes in. Are the other differences possibly associated with a propensity to flip a stated decision in the absence of an impact? Here you either enter the discussion with your best rationale, or figure out how to set up the next experiment.', 'Thank you so much! Yeah, I got another dataset to test this (results are contradicting, but no result is also a result i guess) and some test suggested by my supervisor.']"
[Q] Real estate: estimator for average residence time based on limited data,"I've been collecting real estate transactions in my neighborhood for a while and have data about resales since 2017. The neighborhood was established in 2005, so much longer than what I have data for.

I'm looking to estimate average residence time (time between sales of a single home) in two ways:

1. (# of total homes) / (# sales in the last 12 months): This is more or less a proxy estimate since it doesn't include an actual sale of a single home twice, as if it were a longitudinal sample. The average residence time that I get out of this is very comparable to the expected (10-12 years).

2. actual longitudinal tracking of sales: This method (for now) is severely flawed because the length of the dataset is much shorter than the age of the neighborhood. I cannot seriously state that homes are being sold every 22 months because a small fraction of homes that I have longitudinal data for (only have ~50 months of data, so this do not count people who would increase the average because they havent sold within the last 4ish years).

Besides collecting data for the next 20 years or so, is there a way to get an estimate out of the data I already have?

crosspost from stats.stackexchange where there were no answers so far.
https://stats.stackexchange.com/questions/613218/real-estate-estimating-average-residence-time-based-on-limited-time-date",12r9uqr,peperazzi74,1681863110.0,11,0.92,"['Maybe look into the methods used by quality control & medicine, survival analysis. I dont know that world well myself but have a feeling Ive seen work there on reliable estimation of time-to-event data like this where the observation period is much shorter than the typical time-to-event period.', 'The residence time is a so-called ""censored"" variable -- for people still in their homes, what you know is that the residence time is greater than what has already elapsed. Survival analysis deals with problems like these. You can squeeze some juice out of the censored data, but the flip side is that censored data contribute less to pinning down parameter values than uncensored data -- you can get something out of each censored datum, but you could get more if it were uncensored. I guess that\'s fair.']"
[Q] Applying Different Statistical Methods to Certain Areas of The Feature Space," Hi [r/statistics](https://www.reddit.com/r/statistics/)

I'm trying to design a method to evaluate the price of an asset given certain features. I have lots of data to work with, so the # of observations is not a real constraint.

Based on my conceptual knowledge of the features, I expect most of them to have a linear/semi-linear relationship with the predicted value except for 2. For these 2 features, I expect the predicted value to have more of a clustering/radial relationship.

I can understand how to model each of the two feature-types and their relationship to the predicted variable separately, but how could I ensure that the interaction between them is captured as well?",12r74rk,ryan_s007,1681857341.0,2,1.0,['> how could I ensure that the interaction between them is captured as well?\n\nOne way - perhaps the most usual way - introducing interaction terms as features']
[Q] .25 to .34 after mediation?,"Mediation Help? .25 to .34 with mediator

Im not sure how to really interpret this mediation, as Ive only ever seen a a decrease.does this mean (in laymans terms) that the addition of my mediation variable makes the relationship between variable 1 and variable 2 weaker?

For context, with variables..

The relationship between conscientiousness and GPA was .25. 

The relationship between a specific learning style and conscientiousness is .34.

The relationship between a specific learning style in GPA is  -.15.

After running the mediation with the specific learning style as the mediator, the relationship between conscientiousness and GPA increased to .34.",12r2s1d,Beer_ReviewedArticle,1681848881.0,8,0.89,"['The relationship between the part of conscientious that is independent of learning style appears to be more strongly related to GPA than is the relationship between all of  conscientious and gpa. Learning style suppresses an irrelevant portion of conscientiousness.', 'Hm okayso then is it appropriate to say that being conscientious diminishes academic performance when utilizing the specific learning style?', 'I wouldnt say that. More like partialling learning style out of conscientious increases its relationship with GPA. I would explore it graphically. First predict conscientiousness from learning style and save the residuals. Then compare a scatterplot of those residuals on the X-axis with GPA (on the Y-axis) to a scatterplot of the full conscientiousness variable and GPA.']"
"[Q] Ph.D students and graduates, what was your first few months of your Ph.D program like?",,12r2q4r,vandeu12,1681848780.0,2,0.67,"[""At least at my school the first couple months are really focused on classes-honestly it's really expected in your 1st semester that you mostly focus on that, while also just getting a feel for the type of research areas you want to explore."", 'PhD in the Netherlands. My first 6 or so months was all about formalising my research proposal and coming up with a time plan and doing the preparatory reading and such for the first chapter of the thesis.', ""U.S. PhD student here, just about to finish my first year. Since this is a pretty generic question I'd like to talk about content and the other parts as well.\n\nI came directly from undergrad with no previous graduate school experience (that is, no graduate level coursework in my undergrad). The first semester (first few months) was completely focused on the coursework. There was no expectation to think about research or potential advisors until the first semester was almost done. The coursework in my program is very theoretical during the first year, so it is mostly theorem/proof style lectures with similar homework and exams. This was a bit of an adjustment since my stats undergrad did not have the same focus. Therefore, I needed to adapt to this style of learning and grasping the material, which took several months and one terrible midterm.\n\nRegarding the other aspects, I found that working with other people in your cohort is essential and they can become very good friends. Additionally, expect long hours studying (especially if you didn't do this in your previous academic experience) with people in your cohort and courses. Also, don't forget to take breaks with your study group to talk about things that aren't math/stats. The other experience is that of an emotional roller coaster, which was pretty universal in my cohort. For example, you might feel confident about your knowledge on a topic one day then after the next lecture you can have imposter syndrome setting in. I don't think this feeling goes away but it just becomes the background cycle of the PhD experience.\n\nI hope I answered your question and please reach out if you have any other questions."", 'Fun. Then the changes happened', 'Im doing my PhD in Sweden. The first 3 months or so are reserved for you to catch up on literature and write a detailed plan about what your PhD project is going to be. Obviously it will be more detailed for the beginning than the end. After that we jump around between courses, lab, method development or field work. Depending on what is planned for you. In the beginning, everyone is very relaxed about the amount of time you work so you can get settled in the new city or country. This time is also used to socialise in your group or department. The last 8 months are then meant to be spend on writing your thesis.', 'What school in the netherlands did you do your PhD in? Was it Statistics PhD or Data Science PhD?', 'Thank you for your detailed response!']"
"[Q] To those who are actively involved in statistical research and have received research grants, what is the money being used for?",,12r0l3i,Ok-Birthday2928,1681844879.0,3,0.71,"['Wages + a largish server + travel + workshops.', ""Mainly wages (and the Uni's cut), but also publishing fees, travel and presentation costs, computers, maybe access to specific data, that kind of stuff.  Mainly wages... Why are you asking, are you going to write a grant?"", 'Almost entirely wages, either my own over the summer or for GRA positions. The overhead taken by universities is also substantial. Funding just one student requires university indirect (charged at 50% or so) plus tuition and fringe benefits before even getting to salary, so even a fairly large NSF grant doesnt go all that far on its own.', 'Food and babies']"
[Q] Something like a paired t test when you have unequal sample sizes?,"I've got survey data from the same population across two points in time, one before an intervention and one after. 

I want to test to see if the mean score has changed, so thought I should use a t test. But then, I don't think an unpaired t test is appropriate because the two samples aren't independent because they're the same population and some people are in both.

 But I also can't do a paired t test because the two samples aren't the same size: some people are in the first but not the second and vice versa.

What should I do here? I don't want to reduce the data set to only people who answered both times to force a paired test, as then I'm throwing out about half of each sample. Am I misunderstanding something about the independence requirements of a normal unpaired test and that's the appropriate one?",12qzon0,yoyomangi,1681843192.0,1,0.67,"['This is about as straightforward as paired data gets. I dont think you have a choice except to exclude unpaired data points.', 'It might be that imputation is another approach that can be useful in your situation as well (e.g., imputation of the mean). Depending on the underlying mechanism behind the missing data, i.e., missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) it might work for your case. Imputation methods work best when the missing data are either MCAR or MAR. If the missing data are MNAR, imputation may introduce bias in the analysis.If you decide to use imputation in your case, multiple imputations could be a way forward as it accounts for the uncertainty associated with imputed values. After imputing the missing data, I think you can perform a paired t-test on the complete dataset.', ""If you can identify the scores of the people who are in both groups you might look at an overlapping samples test\n\nOn the other hand, if you've don't have the actual pairings, you only have one of the independent  samples t tests ... \n\nOne thing to worry about is whether the missing values were missing at random or not and if so whether it was completely at random. See the missing data article on wikipedia\n\nhttps://en.wikipedia.org/wiki/Missing_data\n\nIf most of the data are paired and the missingness is completely at random (or even just at random I think), you could drop cases that aren't paired without bias, but it would cause  some loss of power"", 'Thank you']"
[q] would working in a bioengineering lab on machine learning make me more competitive for statistics grad programs,"Hi everyone. I'm a bit stumped on what I want to do with my life. I'm currently an undergrad majoring in applied mathematics and I know that I want to pursue a graduate degree but I can't decide if I want to pursue statistics or bioengineering. I'm interested in working for a pharmaceutical company in the future, but also I really enjoy statistics and can see myself doing statistics work in the future. I have the opportunity to start research in a bioengineering lab this fall working on machine learning and modeling. Would doing this type of research be good for statistics programs?",12qyqsz,maie123,1681841423.0,4,0.7,['Yes. Any relevant work will be better than some random job.']
[q] [r] How to control age in my social science survey data?,"I'm working with survey data for college students' food choices. We want to calculate the rate of campus food security, but we find that it varies a lot by age (freshmen, who have access to the dining hall, are way more food secure than sophomores/juniors/seniors; people who have been in college 4+ years have the highest rates of food insecurity). Respondents to our survey opted in to a survey, so our 2000+ responses are not a perfect reflection of our campus. In particular, freshmen are overrepresented in our survey. As a result, we believe that we are over-reporting food security on campus (freshmen are the most secure). If we controlled for age, and had a sample that reflected the age break down of campus, we think we would have a much more realistic picture of my campus's food security.

Is there a way for me to do this ethically, without fudging the data? What sort of language and tests are done to do so? And how do I justify the fact that I chose to control by age -- as opposed to by income, or race, or any other variable?

Thank you!",12qvnvf,o-oops,1681836403.0,2,0.58,"[""You need to do post-stratification weighting using student year in school. The  freshman weight would be the proportion of the school population that are freshmen divided by the proportion of respondents who are freshmen. Also calculate the sophomore, junior, senior weights the same way. Then apply this weight in whatever stats program you're using; they all have procedures for using weights."", ""In addition to just asking for age, you can also add a weight variable to each respondents.  If you know what the population demographic is supposed to be, you can back it out from a biased sample.  Ex: if there are not enough seniors in your sample compared to your population, then each sampled senior would represent more people compared to freshman.  \n\nYou should control for other variables as well, so long as you can get the data.  Income for example is especially important one to consider.  Using binned income/other variables to preserve some anonymity.  \n\nAlso, I'd suggest you look up how other surveys are designed.  \n\nHere's a link of a book by Statistics Canada on how do surveys.\n\nhttps://www150.statcan.gc.ca/n1/pub/12-587-x/12-587-x2003001-eng.pdf\n\nCh 6,7,8 and 12,13 seems to be the most relavent"", ""Can't you just add one more point in the survey asking what year are they in? That would be the best solution."", 'Assuming you can\'t rerun your survey, then this seems like a reasonable thing to do. You seem to have justifiable extra-statistical reasoning for why you believe that class/age has a significant effect on whether or not a student has food security. \n\nThis is a commonly done problem in public opinion surveys and a common approach is ""multilevel regression and poststratification"", where you fit a model to estimate effects according to certain demographic traits, e.g. age, and the with a known distribution of those traits within the overall population, you can project those effects down onto the population and estimate a population level effect.\n\nThe logic is that you know your survey sample is not representative of the overall population, but you have data on your responses to estimate how biased the survey is compared to your population. Combining those two pieces of information allows you to generate population estimates.', ""Find the average food security for each class year, and present that data. You'll likely bucket together all students who have been enrolled 4+ years. This approach isn't a test, merely a way to effectively summarize and present your data.\n\nFor statistical tests, we'd need a lot more information about what kinds of analyses you want to run."", ""1. This is an opt-in survey, so the results will pretty much be garbage whatever you do.\n\n2. Your motivation for re-examining the analysis techniques is that the survey didn't give you the results you wanted. That is a very bad way of doing science.\n\n3. Your specific problem is the same one political pollsters deal with. Look up poll weighting techniques if you want to continue working on this monstrosity."", 'i agree', '[deleted]', 'How would you describe a survey which has an obvious, unquantifiable, and uncorrectable bias?']"
[E] Best graduate statistics and probability books?,"
Im looking for grad level books on statistics. Im already learned in measure theory and econometrics and want to round myself out.",12qqudq,Dragonix975,1681830780.0,40,0.99,"['Brush-up: All of Statistics\n\nIn-depth: Hogg Tanis and Rao (intermediate), Casella and Berger (advanced)', ""It's not clear from your post what level of graduate statistics you want -- since you have a measure theory background, you may want to consider an advanced treatment: _Theory of Statistics_ by Schervish or maybe something like _Asymptotic Statistics_ by van der Vaart. The suggestions in the other comments are more appropriate for more introductory graduate material, i.e. no measure theory."", 'Bayesian Data Analysis by Gelman is a sorta the Bayesian Bible. Doesnt sound like youd need it but in case others see, I believe Mathematical Statistics with Applications by Wackerly is the go to for math stats is many non-measurement theory classes', 'Statistical Inference, Duxbury Press Davidson, J. (1994).\n Stochastic Limit Theory, Oxford Univ. Press \nDoob, J.L. (1994). Measure Theory, Spring-Verlag Dudley, R.M. (2002).\n Real Analysis and Probability, Cambridge Univ. Press\n Fristedt, B. and G. Gray (1997). A Modern Approach to Probability Theory,\nBikhuser Kallenberg, O. (1997). Foundations of Modern Probability, Springer\n Shao, J. (2003). Mathematical Statistics, Springer', ""It's been mentioned elsewhere here, but van der vaart is a classic in asymptotic stats and gets my full recommendation.  A lot of chapters, but they're all short, yet just as rigorous as necessary to make very general claims."", 'Well, it\'s not a technical book, but as for a set of concepts to frame the technical stuff, I recommend E.T. Jaynes, ""Probability Theory: the Logic of Science"", which is an exposition of Bayesian inference. It\'s marred a little by screeds against Ronald Fisher; oh well.', 'Andy field discovering statistic using R', 'Applied Linear Statistical Models by Kutner et al.', 'If by ""round yourself"" you mean broadening your scope, I recommend intermediate-to-advanced quantitative methods books from applied disciplines (other than economics, since you already got that one checked). It gives you a feel for what other methods are out there and in what contexts there are useful. Learning statistics bottom-up (from application to, eventually, theory behind it) is more enjoyable than learning it top-down (starting at the most general and abstract level).\n\nFor a few recommendations:\n\n* McElreath: [*Statistical Rethinking*](https://xcelab.net/rm/statistical-rethinking/)\n* Rothman, Greenland, Lash: *Modern Epidemiology*\n* Woodworth: *Biostatistics.*\n* Buck, Cavanagh, Litton: *Bayesian Approach to Interpreting Archaeological Data*', 'ESL, maybe BDA if you want to go bayes.', 'I strongly second this!', 'Yeah these are great texts', 'Van der Vaart... a classic but not my friend', 'I second that asymptotic statistics is excellent --- but it\'s not one to easily read ""cover-to-cover"". Honestly, even just going through the first half of that book would be excellent foundation.', 'So currently my graduate metrics course is taught with measure theory. Im looking for both stats and probability theory books that cover the subject from the measure-theoretic pov.', '[deleted]', 'Ehh, I hate the applied approach to learning.', 'ESL?', 'How come? I had an asymptotic stats course based on that book and found it really interesting and it felt like I was learning a lot about what conditions are needed and why to get nice asymptotic properties', 'u/Maisie_Millaa is a bot using ChatGPT to generate comments', ""Suit yourself. I guess there's always learning for the sake of learning."", 'https://hastie.su.domains/Papers/ESLII.pdf', 'Love that for you but my experience w asymptotic stats has been a little less fun lol']"
[E] Is real analysis overkill?,"Would real analysis be overkill for getting into a applied stats / data science masters or career from undergrad? Would I get any ""use"" out of it in those fields? I could spend the time taking another course or picking up different skills.",12qqmoq,seriesspirit,1681830562.0,4,0.75,"[""Can't speak for data science, but real analysis is quite possibly the best preparation you can have for math stat, which will form the foundation of your stats education and will almost surely be needed in any stats program.\n\nThat being said, it is overkill in the sense that it is often not required for admission. But if you take it, understand it, and do well in it, you will have a huge leg up when it comes to understanding the mathematics behind probability, statistics, modelling, etc. Which in turn will make your stat program easier and less stressful.\n\nA worthwhile investment in my opinion."", ""What you can get out of real analysis is practice in proving things for yourself. This is a skill which is useful across all fields.\n\nIt turns out that in order to look something up, you have to have about half an answer in mind, so that you can recognize the rest of it. It's useful to have some experience with getting the half an answer -- which requires being able to state the question (itself a huge part of any problem) and scope out a solution -- and also doing the first half, and then just doing the rest yourself."", '[deleted]', ""No, it's not. It's a standard module for anyone who's involved in advanced statistical research & development. \n\nIt can boost your journal reading experience to the next level."", 'Plus, it would prepare you for a course in measure theory, which would really give you a leg up in any stat or data science track you may pursue', 'For doing data science work (as contrasted with mathematical statistics) it is overkill. There are a lot of other topics and skills that will be more valuable.', 'If you want to understand the finer points of probability theory, then there is no getting around some rudimentary real analysis. Likewise for optimization', ""I took real analysis after finishing a masters in applied stats and most definitely could have used it. I did well in the program, but wouldve gotten a lot more out of the theory courses. It took every ounce of my energy to just keep up with the material and not fall behind, so I didn't have much of an opportunity to explore any given topic before we moved on to the next. I'm not entirely comfortable with my understanding of math stats/probability theory as a result."", 'I have an econ phd from one of the best unisnin the world. I spent 90% of my time writing sql in my data science job.', 'You dont need it. But you might find it fun! And thats worth a lot', 'IMHO, also for getting admitted.  It both develops and demonstrates your math skills.', 'Was measure theoretic prob easier or harder than real analysis']"
[R] [Q] Question about multiple comparisons (neuroscience data),"Hi everybody,

I am performing EEG analyses. Our outcome is a time-amplitude progression. In the end we receive a characteristic curve, consisting of certain components (e.g. N100 -> negative deflection at the timepoint 100; P300 -> positive deflection at the timepoint 300, etc.).

We have 4 such components of interest. Furthermore, we have 4 conditions, across which these components can differ. We also have 64 channels distributed across the scalp where everything has been measured. 

In our main analysis we performed a Kruskal-Wallis test to compare each condition with each other for one channel. We did this for every component separately (i.e. component i of conditions w,x,y,z of electrode A).  In the post-hoc analysis we corrected for multiple testing, as we had 4 conditions in every test. The components are related to each other directly and are therefore looked at separately.

Now we want to compare each component separately within the same condition between 2 different channels, likely using a Wilcoxon test (component i of condition z between electrodes A and B) . Do we still need to correct for multiple testing, even though we basically only compare 2 distributions with each other?

Does all of this seem sound to you? Thanks for any replies!

Alf",12qqhxz,AlfCarison,1681830421.0,2,1.0,['Yes. Source: dead salmon study']
[Q] What are the general expectations for a stats PhD applicant? Do I need a stats degree prior to applying? Please help.,"Hi all, I have an undergrad degree in CS, but I wanted to get into stats, so I applied to many stats program, but got rejected from them all, owing to my weak GPA and less research experience.

But I eventually ended up getting into a predictive analytics course at UIUC, which has ~60% stats coursework and some finance/risk management coursework.

If I want to get into a top PhD program eventually, could this be a good stepping stone or I should wait 1 more year and try to get into a core Stats program?

I am really looking for some help here as I feel stuck, as I really want to get into a top PhD program eventually and I want to give myself a best chance to achieve that.

Even some information only slightly related would be very helpful.",12qq7h4,ineed_somelove,1681830123.0,4,0.75,"['Have you tried asking your professor for your predictive analytics course for advice? Or go talk to them about this during office hours', ""That's right, I'm going to do that."", 'There are three axis\' that PhD apps hinge on:\n\n1. Coursework/ Grades: Has the student taken the courses we require (calculus, linear algebra, mathematical statistics, probability theory and then maybe a regression class and real analysis) and demonstrated that they can do well in them such that classes won\'t be a problem in their PhD.\n\n2. Research Experience- Has the student demonstrated that they can start and complete research? Do they have experience tackling open ended and hard problems and making some progress with them? Have they been around people who have done this and know the vibe?\n\n3. Letters of Rec- does the student have people that have intimately worked with them and can speak to 1 and 2. Do these people know what a PhD is? Do they advise graduate students/ have deep research experience and can they speak to if the student in question would be a good grad student? \n\nIn order to have a compelling, effective application you generally need two out of those three solidly met with the third being sufficient. If you are going for ""a top"" PhD program all three need to be excellent. If you really want to do a PhD in stats, you need to make decisions that maximize the likelihood of demonstrating some of those axis\'.', ""Hey I got the gist of it, can I send you a dm? \nI've also emailed my professors.""]"
"[Q] Does the assumption of sphericity in repeated ANOVA matter, where the only test of relevance is the between-subjects test","I know that an assumption of Repeated ANOVA is the sphericity assumption however to my knowledge this primarily concerns analysing the within subjects tests. 

If you were to only be interested in the outcome of the between subjects test would the  sphericity assumption still be important? 

If you could point me to academic papers which discuss this I would be most grateful.",12qn7aj,Bittersweetcharlatan,1681824707.0,1,1.0,"['Sphericity applies only to within-subject effects. A between-subjects effect is based on only one score per subject, the mean across all levels of within-subject variables. Therefore, assumptions about a covariance matrix dont apply. Also, sphericity is automatically satisfied for within-subject variables with 2 levels. Incidentally, I wrote [this tutorial](https://www.tqmp.org/RegularArticles/vol12-2/p114/p114.pdf) on the assumption of sphericity.', '""Discovering Statistics Using R"" by Field et al has a chapter on this method and it\'s assumptions including examples and literature references. I\'d give that a try. (there\'s also an SPSS version if you prefer)', 'Thank you very much. This was just what I was looking for.\n\nSo would you say that when looking at between-subjects differences in repeated or mixed ANOVAs it is more important that to have Equality of Error Variances ?', 'With unequal n homogeneity of variance is important. The Welch test can be used if assuming homogeneity of variance cant be justified.']"
[R] [Q] Modified Rolling Regression with variable time block size,"Hello, I am trying to fit a model that follows generally: ln(Y) = n ln(time) + b which is straight forward enough. I am most interested in the value of n and it varies anywhere from \~0.3 - 2 (Always positive) and my dat has two distinct regions of n where you sort of have a fast rise (large n), then a flatter region where you've reached steady state. Generally people fit one half of the data then the other. The cross over region between the two is somewhat hand-wavy and I am trying to more quantitatively determine that point and compare it between samples and the difference is on the order of 10% of the x-axis so fairly wide given my number of repeats.

It seems like a decent method would be something like a modified rolling regression where I say - look at the first 10 minutes, calculate the a linear regression R1. Look at the next 10 minutes and calculate R2. Look at the total 20 minutes, calculate regression R3. If R1 and R2 fall within R3, keep R3, add 10minutes, repeat. If not, slope has changed, start over from t2. Might be even better if it calculates forward and backward to avoid overfitting either side if the change in slope is close. From the core science side I can even give the change in n some reasonable bounds like n=3.001 and 3.002 might end up as statistically different, but realistically changes <0.1 are within reason to be ""the same""

Does something like this already exist? I can't seem to find something more similar than rolling regressions. It's almost like fitting a regression model and looking at the residuals and backing off the data range until the residuals come into line but automated and not as problematic as ""it looks right to me""",12qkmk8,MJP_UA,1681818884.0,1,1.0,"['Do you want only the parameter n to vary over the window, or do you want the entire model to vary?\n\nEither way, youre looking for regression methods with adaptive windows. Adaptive kernel regression is one option that is widely implemented in software.\n\nIf you only want one the n parameter to vary while keeping b fixed, additive models are a good option. For example in the mgcv R package, you can fit a time varying coefficient model for the coefficient n. With most methods this is roughly equivalent to a fixed window, but mgcv also allows for an adaptive penalty which is equivalent to an adaptive window.\n\nFor both types of methods, I caution that adaptive selection can be quite data hungry and may be unstable without large amounts of data.', ""Thank you for the input! I will take a look into adaptive kernel regression.\n\nI believe b should vary with the window also. For more context my reaction rate is expressed as: m\\^n = Kt -> n\\*ln(m) = ln(t) + ln(K) -> ln(m) = (1/n) ln(t) + ln(K)/n\n\n&#x200B;\n\nSo I'm plotting ln(t) vs ln(m) and yes I realize I could plot the other way and get n directly instead of 1/n but convention is the other way for some reason. Either way the intercept is a function of K / n so it should vary with n. \n\n&#x200B;\n\nFrom the reaction side, K traditionally varies with n but that may only be because we force it to. Ie this reaction is parabolic fit m\\^2 vs t and the slope is K. Or this reaction is linear fit m vs t. K constantly varies but maybe more fundamentally it shouldn't, were just forcing it to vary to account for the imprecise n value w fix at 0.5, 1, 2, 3 but that's a discussion for my PI lol""]"
[Research] Is it better to use Cummulative abnormal return or daily stock return for an event study?,"I am doing some regression for my bachelor thesis related to regression, regression is not my strong suit. I am attempting to measure the impact of Quantative easing announcements on stock price for reference. As a bonus question, I am also wondering if it is nessesary to use control variables when one is doing 1 variable regression. Tyvm for any guidance :).",12qjymx,Simoncuddlebear,1681817027.0,1,1.0,[]
[Software] Bayesian Networks in >PyMC4,"I am trying to write a simple BN in PyMC for a research project. I found this discussion on the pymc discourse here about how to write a BN in PyMC3 [https://discourse.pymc.io/t/bayes-nets-belief-networks-and-pymc/5150/2](https://discourse.pymc.io/t/bayes-nets-belief-networks-and-pymc/5150/2?u=i_love_bn) . But I am confused about how to do this in PyMC4, because the theano.shared function does not exist in PyMC4. Can someone help me out with this?

I would also like to know if there is an easy way to create a BN where there are 10 input nodes and one output node because I do not want to create a function with 10 arguments like the reply above above.",12qak6r,Worried-Title8760,1681792568.0,4,0.84,['I have a similar question. Any help will be appreciated. Thanks!']
[E] ISL Python edition coming up!,"Good news for ISL fans who are Python users. Apparently it is the same book with labs worked out in Python.

https://www.statlearning.com/",12q6gir,shashvata,1681783870.0,60,0.96,"['Thank God I have been procrastinating for months from reading that book because of R.', 'Rest assured I will keep all my tutorial content in R :D', 'Cant wait for this.', 'haha still a good book though', 'me too, im keeping both books']"
[Q] Would you trust repeated measures or a greater sample size more?,"Hi everyone,

Hoping to understand something my PI asked me to do. They previously did a repeated-measures structure analysis with 30 people for a total of 120 sessions (a session every 3 months). They now have 50 people for the baseline and want to re-analyze the data of just the baseline. They are curious to see if the results vary. If they were to vary, which analysis would you trust and why?

Thanks so much, looking forward to learning more.",12q63n6,houshaseniippani,1681783150.0,2,1.0,"[""It's hard to help with so little info. What's the outcome of interest? Do you mean 4 sessions per person?\n\nMy gut feeling based on almost no context is that your PI wants you to conduct a transversal sensitivity analysis to see if including more people will impact the baseline level of whatever it is you're measuring.\n\nIt's impossible to tell you anything about modelling and hypothesis testing with what you have provided."", 'What\'s getting measured?  With some things measuring the same person 4 times you end up with changes because of how the measure is done. For example with a lot of biological assays there\'s a lot of variability due to instrumentation - small changes in the reagents, temperature, etc. That ""bench variability"" may make it where the measures are effective for seeing differences between groups, but not stable values over time.\n\nOther types of measures, say if it\'s a math test, you would expect taking the test 4 times would result in improvements. Practice makes perfect, right? Seasonal variations can exist.\n\nGenerally speaking, which analysis I would trust really depends on the thing getting measured. In the presence of stable differences between the sampled items, repeated measures will be much more sensitive. If the items are fairly homogeneous and the measures are not stable over time then not so much.', 'What is being tested are cognitive exams for patients while they are undergoing weekly therapy that is suspected of having adverse side effects. And yes, 4 sessions per person, each session is 3 months from the last.\n\nThe hypothesis is this therapy may be associated with worsening cognition.', '>What is being tested are cognitive exams for patients while they are undergoing weekly therapy that is suspected of having adverse side effects. And yes, 4 sessions per person, each session is 3 months from the last.  \n>  \n>The hypothesis is this therapy may be associated with worsening cognition', 'What are the variable types?', 'numerical, not ordinal']"
[E] Theory of Statistics online videos,Anyone know of some good videos to self learn theory of statistics. Some people might also call it Mathematical Statistics 2 or just the sequel to probability theory. I know of a few good books but I find that when I just read the book I have no clue whats going on and just end up memorizing how to use the formulas.,12pyjtb,Mvota711,1681768677.0,15,1.0,"[""From your description I'm not entirely clear what actual topics you want covered, at what level of mathematical sophistication. \n\nDo you have a list of topics you're hoping for coverage of? (If you're using a book presumably there's a table of contents with named subsections; the ones giving you the most trouble shouldn't be hard to identify)\n\nAre you looking for something that assumes you have measure theory? Something that assumes calculus but not measure theory? something else?\n\n> I have no clue whats going on and just end up memorizing how to use the formulas.\n\nNot sure videos would make a lot of difference to this unless you're also doing some other things\n\nHow many exercises are you doing on a given topic? Do you look for ways to apply what you're learning?"", 'Pretty standard, at least in the US. Basically the second half of C&B.', 'The book Ive used is Probability & Statistics, 4th edition (Chapters 7-11). No measure theory but it requires knowledge of multi-variable calculus.\n\nThe reason I think I understand things so much better from videos is because they explain the notation. In books Ive noticed its all just mathematical notation which I tend to have difficulty actually understanding what it means (I sort of just see it as a formula).\n\nI try to do a as many practice problems for each topic until I feel I understand the theory and could apply it to an arbitrary problem. OR Ill try to find the mathematical way to do something (meaning Ill know how to compute something if given numbers but generally dont rely on the theory).', ""> Probability & Statistics\n\nThere's multiple books with that title; do you mean the book by DeGroot and Schervish?\n\n>  In books Ive noticed its all just mathematical notation\n\nThe notation is usually defined in the books, unless it's completely standard stuff you'd know from earlier work like a calculus class or something (and often even then).\n\nHowever I think I am getting some sense what you mean.\n\nOffhand (assuming you do mean something at the level of DeGroot and Schervish) I don't have a direct suggestion right now but it might help someone else come up with something. \n\nOn the probability side (sorry, I don't remember what chapters 7-11 cover in that book but I assume that's past the probability stuff so this may be useless) there are youtube lectures for the harvard first course on probability, stat110, but failing that, I'd try searching by specific topics""]"
[Q] Sample size calculation for study evaluating AUROC of new diagnostic method,"Is there any method that has been put out? I cant find anything in literature. I have expected AUROC and prevalence of disease, but cant find an equation to plug them into.",12pycpy,The_noble_milkman,1681768324.0,3,1.0,['There should be resources online for this.  I did a quick Google and found this:  https://search.r-project.org/CRAN/refmans/pROC/html/power.roc.test.html']
[Q] Multiple clustering variables for linear mixed models?,"Is it common practice to have multiple clustering variables? For example, if individuals (ID) are being recorded multiple times (one clustering variable), but individuals are grouped by a different type of test. Would it make sense for the type of test to be another clustering variable for random effects?",12px0g9,TwentySevenSeconds,1681765922.0,1,1.0,"['Sure', 'Yuppers', 'Sweet']"
[Q] What does this notation in linear regression mean?,"||y-X(theta)||^(2) 

with a subscript 2 as well, but i can't type the subscript in this post.",12pvxji,Wonderful-Ad-7200,1681763964.0,0,0.33,"[""It's the notation of vector norms; the subscript means the L2 norm."", 'Double bars refers to the norm, kind of the length, of a vector. And I suspect that the formula within is calculating the difference between y and the prediction based on X. In other words, this is the way you represent the residual in linear algebra.', 'https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm']"
[R] How to compare the impact of different marketing campaigns on customer acquisition.,"I have been working on this project on and off for about a year and am stuck. I am hoping someone might be able to point me to existing research on the topic.

I am trying to measure the impact of different marketing campaigns on customer acquisition. I have used regression up to this point. The dependent variable is the volume of new customers by month, each dependent variable represents a different type of campaign (TV/Radio, Storewide sale, etc) and the observation is the number of occurrences of each campaign type in the given time period.  
My first attempt was to use monthly data, but I did not get any significant results. I have refined my measurements to weekly but repeatedly fail the linearity assumption. I tried box cox transformations to overcome this, to no avail.",12pr9w1,Old-Bus-8084,1681755061.0,1,1.0,[]
[Q] Bayesian inference using MCMC: why?,"I  needed to simulate a posterior distribution for a simple discrete model, and I've gone through the process of learning the metropolis  algorithm.  Everything looked fine, but then I tried to do the same  using Bayes'  rule directly, and naturally,  the computation was not  only more precise but much faster.

My  question is:  what are the real-world cases where MCMC is used instead  of directly using Bayes' formula? I thought the issue was that  integrating to compute the Bayes' denominator takes time, but since I  have to compute the numerator for every value of the prior, why not add  up all of these numerators and use the sum as the denominator? If I can  do that, why would I  use MCMC? Even if the distribution is continuous,  couldn't I just sample many values, compute Bayes' rule for each, and  add them up to integrate?",12pkthp,an_mo,1681744563.0,21,0.84,"['> My question is: what are the real-world cases where MCMC is used instead of directly using Bayes\' formula\n\nAlmost every ""real-world"" case which uses anything more complex than a simple toy model uses some kind of numerical approximation scheme, such as MCMC. It is generally impossible to derive the posterior analytically in anything by the simplest models.\n\n> since I have to compute the numerator for every value of the prior, why not add up all of these numerators and use the sum as the denominator?\n\nThere are infinitely many ""values"" of the prior in general, such as when the prior is continuous. In that case, the procedure you are describing is a simple approach to numerical integration, which is computationally infeasible in anything more than a few dimensions (see [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)).', 'There are a handful of problems for which you can do pencil and paper (symbolic) computation of Bayes rule.  For most non-toy problems you cannot.  Hence MCMC.\n\nI never understand the ""compute the Bayes\' denominator takes time"" stuff.  MCMC samples the posterior and allows Monte Carlo calculation of posterior probabilities and expectations.  Those are the integrals you cannot do symbolically.  You could use ordinary Monte Carlo instead, but there are no OMC methods for most multivariate models (only normal, multinomial, Dirichlet, and uniform on boxes, balls, and spheres).  So what your last sentence is suggesting, is exactly what no one knows how to do for most applications.  OMC works better than MCMC when it can be done.  But it usually can\'t be done.', ""Outside of conjugate priors, which involve the selection of a very specific assumption for the parametric distribution of the data-generating process AND very specific parametric assumption about your prior, there are very few posterior distributions which can be written down with an analytic solution. That's not to discount their use-- there are an endless array of situations where this is a very reasonable set of assumptions. But they are mostly constrained to univariate analysis. \n\nHence, there are lots of problems requiring a numeric solution. Hence, MCMC. \n\nOne key piece of intuition that is often lost is that MCMC is just a numerical integration technique. It has useful applications for evaluating integrals numerically that have absolutely nothing to do with probability."", 'It would probably be helpful if you described your ""simple discrete model"" a bit more. My suspicion is that you were simply lucky having a conjugate prior-likelihood available.', '> but since I have to compute the numerator for every value of the prior, why not add up all of these numerators and use the sum as the denominator? \n\nRelatively easy for a simple case on a discrete distribution with finite support. But that\'s not how the world works in general.\n\n> Even if the distribution is continuous, couldn\'t I just sample many values, compute Bayes\' rule for each, and add them up to integrate?\n\nHow are you going to ""just sample many values"" in general? You\'re trying to integrate the joint posterior.\n\nIf you\'re talking about evaluating the constant in the denominator, MCMC (or indeed most other numerical approaches to Bayesian statistics) is designed to avoid the necessity of doing that. If you could sample the joint posterior, you can skip the normalizing constant altogether. The tricky part is ... how to sample that?', ""What you've described is a grid algorithm, and it's an excellent idea (not sure why some other responses are trying to talk you out of it).\n\nIf there are 3 or fewer parameters in your model, it is often practical to enumerate a grid of possible parameters and estimate a posterior distribution without sampling or integrating. I used grid algorithms extensively in Think Bayes and found that I could solve many interesting problems.\n\n[https://allendowney.github.io/ThinkBayes2/](https://allendowney.github.io/ThinkBayes2/)\n\nIf you can say more about your model, I might be able to make some suggestions."", 'Because most posteriors are not analytical', ""AFAIK the numerical methods (i.e. integration) to compute the denominator do not perform well.  \nIt's very easy to have high-dimensional integrals, and the integrands tend to be very peaked since they are products of two terms that can be very small except for a small region.\nIdeally you would refine the grid based on this information, but \nI'm not even sure if there are good methods to do that with many dimensions.\n\nOn the other hand, sampling is super easy, it always works and it is trivial to parallelize. It's just not that efficient."", 'I use MCMC all the time for a model which has a discrete component. In the past week, for the smallest problem I examined, that discrete state could take over 2x10^76 values. The next smallest had one was over one googol squared. If each value required a mere nanosecond, the small example there would take almost 10^60 years to iterate over and compute the posterior numerically. To say nothing of the required space just to store that all. It just cant be done.\n\nMy point is: brute force doesnt work when the dimensionality of the problem gets large. Are there places you can? Sure. But there are far more places you cant. Check out the [Ising model](https://en.m.wikipedia.org/wiki/Ising\\_model) for one such example.', "">Even if the distribution is continuous, couldn't I just sample many values, compute Bayes' rule for each, and add them up to integrate?\n\nWhat you are describing there is basically importance sampling/sequential Monte Carlo - sample from the prior, weight the samples by the likelihood and compute posterior quantities as weighted averages of the samples. \n\nThis works great in simple and low dimensional problems. In high-dimensional problems this is very inefficient, as the volume of space grows exponentially with dimension, so if you have 100 samples to cover a one dimensional space, you need 10\\^20 to cover a 10 dimensional space as well. Assuming your data is informative at all, this also means that you waste A LOT of your samples because they will have very small weights, so contribute almost nothing to those posterior weighted averages.\n\nMCMC works by taking a guided random walk to find region of parameter space with high posterior mass, then wander around in it to map out the posterior distribution. This is much more sample efficient and does not suffer as badly with dimension. Rather than exploring the exponentially growing full space, it can focus on only the region of high probability mass (only the places that would have non-trivial weights in the previous case)."", ""I don't think it is usually necessary to apply stuff like MCMC when simply applying Bayes' rule, though maybe there are some counterexamples. I think the main applications of MCMC techniques are to Bayesian networks or other hierarchical models, where the computations can't really be done by hand anymore."", 'Use stan', 'I have this gripe too. I dont know what the alternative is. But when I want to do Bayesian statistics, I dont want to do some numerical MCMC thing - I cant explain the theory to leaders or colleagues that dont understand it. The different methods for MCMC are very advanced and require special software and are not intuitive. Seems like busting out a bazooka for a regression problem. I dont know what the alternative is, but I do not like MCMC. Maybe its because I learned it late in my masters, but gradient descent on a black box neural network feels much more scientific than MCMC, while Bayes theorem is a sound thing', ""Riddle me this. Suppose I have a Poisson likelihood associated with my observations, and the rate parameter I for whatever reason decide to parameterize as a function of exp(a cos(bt + k)), where I assign a joint prior on p(a,b,k) that has some analytical form. (This is used in nonhomogeneous Poisson processes)  \n\nSure you can use Bayes rule to find the posterior. Your integral in the denominator is an integral over a continuous space (a,b,k) which is also non-analytical. How would you sample from it? If you didn't need to sample from it, how would you would find the expectation given that the posterior is already a function of an intractable integral? How would you find credible sets without an analytical density.\n\nYou see, knowing the posterior density via Bayes rule is only a small part of Bayesian inference, and finding the posterior is arguably one of the least interesting things in the context of bayesian **inference.** Being able to come up with credible sets and means and variances are crucial in inference, and when you have these intractable posteriors, MCMC is used to provide you a sample, in which you find quantiles, and can invoke LLN to find posterior means to name a few."", 'Thanks. So if I understand correctly, the difference is that I could in principle randomly draw a set of points (using some distribution, i.e. uniform in case of a finite domain) and add them up, but with MCMC instead, the draws are performed more efficiently rather than using an arbitrary distribution.', ""Thanks but my question had nothing to do with the ability to compute the posterior distribution analytically. If you can do MCMC you can also computationally evaluate Bayes' formula.  It seems to me, from the answers, that \\*even if\\* you had the analytical formula, you'd still want to use MCMC in some circumstances."", 'Hi, interesting. Can you parallelize MCMC though? Seems like the sequence is crucial, unless you want to compute multiple chains and then aggregate.', 'But MCMC won\'t give you the posterior for all 2x10\\^76 values anyway so that can\'t be main issue. You\'re still sampling; from the answers received so far I think the correct one must be  MCMC gives you a more efficient sampling when the space is very large. In you\'re case it\'s nearly impossible to pick the ""right"" points using any sampling distribution.', "">but gradient descent on a black box neural network feels much more scientific than MCMC,\n\nYou had me until here - are you really claiming that these algorithms are easy to analyze?\n\n(More broadly: the complexity of some particular explanation of an algorithm is not very related to how well-understood the algorithm's performance is)."", ""In the equation p(|x) = p(x|)p()/p(x), p(|x) and p(x) don't depend on each other. So for the sake of convenience we often just estimate p(|x)  p(x|)p(), i.e., we ignore the normalizing factor p(x) and work with the relative magnitudes of the posterior probabilities. In other words, the denominator p(x) often isn't that important in practice, and it certainly isn't the main reason that MCMC methods are used.\n\nIn principle, if we could do something like a grid search over the support of  and calculate p(x|) and p() at each point to get the (unnormalized) posterior probabilities. But if   R^(k), the number of points you need grows exponentially with k - or equivalently, if you have a given amount of compute available, the distance between the points you can test grows exponentially as k increases. At the extreme, an exhaustive search of the values that can be represented in 64-bit floating point would require something like 2^64^k computations of p(x|) and p(), which obviously isn't tractable in practice even for relatively small k. And at almost all of those points, the product p(x|)p() would be very close to 0, since at least one of the factors would be very close to 0.\n\nThe value of MCMC methods (and especially of of modern MCMC methods like Hamiltonian Monte Carlo) is that they can efficiently focus their computation on the subset of R^k where p(x|)p() >> 0, which puts them at a huge advantage over the naive strategy."", 'What do you mean by and add them up?', "">what are the real-world cases where MCMC is used instead  of directly using Bayes' formula?\n\nUhhhhhhhhh this question?\n\nBayes' Formula= computing the solution to a complicated integral in the denominator, either analytically or numerically. Regardless if you arrive at that solution either analytically or numerically, it's the same problem.\n\nI explained to you why there are some cases where you may attempt to do it analytically, and why you may need to do it numerically when that fails."", ""Computing multiple chains is very common -- in, fact, it's almost always done. The posterior is the stationary distribution of the chain, so comparing the distributions of multiple chains is generally done to ensure that each chain has actually converged."", ""It is possible to parallelize MCMC, in the sense that it's possible to run multiple chains in the same amount of time it would take to run one.\n\nHowever, you can't parallelize the simulation of a single chain. It's a Markov chain, the probability distribution of the (n + 1)st state depends on the nth state. So you have to simulate the states in sequence, one after the other.\n\nThis is why there is no easy way to shorten MCMC runtimes, so a lot of research effort is directed towards finding MCMC algorithms that are guaranteed to converge quickly to the stationary distribution."", 'Youre quite right that MCMC wont sample from that entire space. Most of that space has negligible posterior mass, so it has negligible effect on the things we want to know about. The neighborhood I need to actually sample from, while still quite large, is much, much, much more tractable. \n\nKeep in mind that MCMC is not an approach to enumerate anything, it is a way to draw samples from the posterior distribution. With those samples, we can say things about the posterior. Youll hear some people say something a bit more precise, which is that it is numerical approach that allows us to evaluate integrals and sums. And most things we want to know about posterior distributions can be phrased as one of those. \n\nMCMC allows you to make a series of local perturbations which add up to a more global exploration of the distribution. If most of your discrete states with non-negligible posterior mass are in the same vague region of parameter space, then you can move around that quite efficiently. And since we always accept moves which increase the posterior mass, when we start our chain elsewhere, it will move towards this region of space on its own.\n\nBrute-force enumeration could also be done without touching every state, but then you start having to ask how to locate the region you need to be working on. Which requires testing a lot of regions of parameter space, and that quickly becomes intractable.', 'Yes, anyone understands a line of best fit. While the gradient descent algorithm does seem complicated with derivatives, moving the line slightly as to minimize errors each time - makes sense theoretically and to a leader this is elementary.\n\nFor metropolis, its not clear intuitively why that would work at all. \n\nApples to oranges comparisons, but hopefully Ive illustrated my point', ""exactly what you do in the denominator of Bayes' rule. An integral is a sum."", 'Of course, the question of what ""seems intuitive"" is quite personal, so there are no wrong answers there. But the math for solving the basic problem seems pretty elementary in both cases (both solving the least-squares problem and checking the implications of detailed balance are 2-3 lines of high-school algebra), and checking convergence is much easier for MH than gradient descent.', 'In many applications, the marginalizing constant is not necessary to make a statistical inference; since a density estimated via MCMC is  *proportional* to the posterior probability whether or not it has been normalized, relative posterior probability can still be assessed. This is also true in applications that rely on information criteria: the marginal likelihood can be disregarded in calculation of DIC or WAIC, since information-criterion-based model comparison requires a common set of observations, for which the common constant term can be disregarded when making comparisons.\n\nIf a precise value for the marginal likelihood is required, MCMC can only ever provide an approximation, and convergence of a good approximation can be quite inefficient in high dimensions and with heavy-tailed distributions. However, given that many posterior distributions arising from real-world problems lack a closed-form expression, direct integration is often impossible anyway and *some* approximation strategy is required regardless.']"
[Q] Book or textbook for statistical modeling (biology),"Hi, I am an European PhD student in population genetics. I have attendend some courses (42h in tiotal) on mixed models, geneal and genralized mixed model but i get very little out of them: may it be for the very concentrated lessons, issues with the language which is neither my native one or English, or simply I am not a peak in, well, wathever.

I want to dig better and more on these topics, as they can be very useful in the near future - *storms*, I am dealing with a glm with a logit as a link and I walk into the dark.

I don't like the idea of getting piece by piece from online dedicated page to single topics, so I am looking for siomething more comprehensive. In example, I found of great help in dire times *Statistical thinking from scratch* by M. D. Edge.

Is there something which could help me? I'd be very grateful if someone can point me a book - even better, *exegi ei monumentum aere perennius.*",12pkmkb,moranindex,1681744320.0,10,0.87,"[""I've been plugging away at [Modern Statistics for Modern Biology](https://www.huber.embl.de/msmb/). Weird that it's English only, I'm pretty sure one of the authors is German."", 'I like Rosner\'s ""Fundamentals of Biostatistics"" and Daniel\'s ""Biostatistics"" (https://archive.org/details/BIOSTATISTICS) as introductory textbooks', 'Thanks, that\'s a goldenful piece of information!\n\n(although, ""goldenful"" doesn\'t exist, but let\'s play with words)', ""Came to say this! \n\nI've been working through this book and I like how it jumps right into statistical concepts with relatable application problems.""]"
[question] Hypothesis Testing exercise,"Lets assume that someone has asked you to test the follow hypothesis:

Does the name of males affect their blood pressure?

Also lets assume that these males live in England.

Finally, lets assume that each sample point/ sampling has a cost.

I.e. you could measure everyones blood pressure and name but this would be very expensive.

Whats the most cost effective way you would do that hypothesis testing in order to be able to generalise the outcome for are males living in England?

Cheers


Edit: its not a uni exercise its a real question bothering me. I would appreciate an answer :)


Edit 2: this is a mental exercise Im just interested in the approach you would take. If you can please tell how you would do this hypothesis testing if you had all the resources in the world. And if you cant why you cant. 

One approach for example would be to measure the whole population. Whats the next best using stats?",12pe0qf,Emergency-Agreeable,1681734155.0,1,0.56,"[""so you're taking a course on sampling eh?"", 'Your main worry here is omitted variable bias.', 'You need at least 20 observations with the same name or the same type of name to begin to make reasonable inferences.\n\nYou need a lot of additional theory to guide you in selecting names or in categorizing names.\n\nThere are a lot of restrictions on collecting medical information about people, especially with identifying information about that person.\n\nI doubt a true random sample is possible given privacy concerns, not just financial concerns. Imperfectly random samples are plausible, but as you note, probably expensive.', 'Spot on. Do you have a suggestion for the whole journey not just the sampling?', 'Hi thanks for your response. We are getting somewhere. \n\nThis is hypothetical problem so dont worry about GDPR. Im just interested in the approach. The cost element is to discourage people to use the whole population int their solution and take a statistical approach.\n\nWhat makes you think I need additional theory to select names. If so can you indicate a direction to that theory.\n\nAlso why at least 20 observations, where is that coming from?\n\nAnd finally still you havent talked about the approach to the actual testing.', 'hmm can we not use t tests? the max number of observations required for performing a t test is 30, so as long as we have a maximum of 30 males (data/info), we should be able to perform a hypothesis test? (altho with a very poor confidence score but hey..a hypothesis test nonetheless haha )', ""Omitted variable bias and the issues I raised need to be considered before tackling  the sampling approach. The answers to your questions are beyond the scope of what I can communicate in a Reddit comment. I'd suggest taking more courses in statistics and sampling.""]"
[Q] Ordering items from a list,"In a survey, respondents are asked to drag some items in a list in order of decreasing importance. Say the items are A, B, C, D, E and a respondent's answer might be D>A>B>E>C. What statistical analyses can I perform to analyse these data? Which test can I use to determine if the mean ranks of items differ? Are there any other commonly used techniques or tests to analyse this type of survey question? Thanks in advance!",12p9hua,DJ-Amsterdam,1681723615.0,9,0.91,"['You could treat their rank as a straight ordinal measure and regress against other respondent characteristics?\n\nThere may be a fancier technique more geared towards this type of rank analysis; just mentioning what I would try first.', 'Analysis depends on what questions you want answered. Not something to do for the sake of it.\n\nFor instance if you want to segment respondents based on their preferences then clustering can be done. If you want to match how close a respondent is to a reference response rank correlation can be calculated. If you want to identify collective preference then various voting approaches can be used (rank choice, for example).', 'You could look at some Markov approaches too and quantify the journeys/ transition probabilities thru the different states. Maybe theres a hidden Markov application too where some hidden state determines the respondents journeys', 'Have a look at exploded logit (plackett-luce) models. These models infer a latent ""worth"" for each option based on the ranks. You can then compare these worths (and their uncertainty) \n\nThey\'re used a lot in stuff like sports analytics (race results) and economics/psych (market research type thing). That should give you loads of flexibility also to include predictors and do all sorts of fancy stuff with your ranks!', 'Make your data have a columb for 1. Survey respondent ID 2. Rank 3. Option ID\n\nRegress option ID on rank with error clustered by respondent ID with ordered logistic regression.', 'Excellent approach!  Not the type of analysis I originally had in mind, but definitely an interesting idea that I can use. Thanks!', ""Thanks for aksing the proper questions that help me specify what I actually want to analyse! I want to know which trait (A/B/C/D/E) is the most important. Because the ranks for the 5 traits are obviously not independent measures, the Friedman test didn't seem appropriate. How can I do this, preferably in SPSS? I don't know exactly how to do a rank choice analysis."", ""Thanks for the idea! To be honest: this is too complicated for my current knowledge. I can do some SPSS, but Markov chains are way too hard for me. Good to know there's more depth to this type of survey question than I can (currently) handle."", ""Thanks for the advice! I'll look into this and see if I can wrap my brain around it. Sounds like a very interesting topic to delve into more, even beyond the actual survey I'm about to analyse right now."", 'https://en.m.wikipedia.org/wiki/Electoral_system\n\nEssentially what you are asking is of 5 candidates A-E which will win the election. For this, you will have to learn about voting systems. ""Important""  is defined in lots of ways as you will see in the link shared.', 'I think this is not exactly what I\'m looking for. I want to know (based on the current ""election"") if the winner is actually more popular than the other candidates and if they will likely win again in the next election, or that the winner won by chance and a different candidate is as likely to win the next election. I want to test for a significant difference in importance/popularity, not merely see who wins in this election. Or am I overcomplicating things now? I do appreciate this translation of the problem, it makes my question more tangible, but it also confuses me on how/what to test in my data set.', 'You can test chance of observed distribution against reference distribution using Chi Sqr test, but you need to decide reference distribution. If A gets 25% votes, B gets 19%, C gets 15%, and so on, but you expect all of them to get 20% each then you can check that.', 'This is a simple and elegant solution to compare top picks. Thanks, this is a good addition to the other analyses to run and get a feel for most important trait!']"
[S] JASP is deleting rows and columns.,"Hello, I have a problem with Jasp 0.17.1 in which I was doing descriptives and testing my hypothesis for my thesis. Does anyone encounter deleting rows and columns after saving data? For example in saved data I dont have column ""Gender"", it is completely gone even when I had it in descriptive statistics. Deleting rows can be seem in ""Age"" where now I have only 28 valid and 0 missing, instead of 158 valid and 0 missing.

Does anyone encountered problem like this?",12p4rft,GoodeSVK,1681712111.0,4,0.84,"[""It's unlikely that a release version would have a bug like that, and you would need to provide enough detail for a reproducible result - we can't see what you're dealing with (what do you mean by 'saving data'?). Screenshots arranged in a workflow, with annotation/description would be needed, I think. \n\nIt might be that you've got missing data in some columns and not others, and so when you do an analysis, JASP automatically drops incomplete cases. So that would seem like you've got more useable data in your descriptives than you do in a main analysis. Alternatively, your data might be imported in a weird format, so it's trying to drop rows or entries that are formatted incorrectly? I don't really use JASP, so I'm not sure about its default functionality.\n\nJASP has a bug reporting portal on its website, where you can get support too, so that should likely be your first port of call.""]"
"[Q] How to I write a function of how much longer a popular twitch streamer will stay online, based on how many hours they have already been streaming?","> ***Foreword:*** *Popular twitch streamers have bots in their chat that you can type commands such as `%online` and it will tell you how many hours that the twitch streamer has currently been streaming on any given session*

Here are my variables:

**** -- The mean amount of hours any popular twitch streamer will stream on any particular session

**** -- The standard deviation

What I need help with is the formula and fact-checking my guesswork. My guess is that the average popular twitch streamer streams 5 hours with a standard deviation of 2 hours. Does my guess for standard deviation make sense? Or should I use a much smaller or larger value such as 1 hour or 3 hours?

From what I remember from undergrad, the SD can be guessed by taking the range of which 95% of all streaming sessions occur for a popular streamer (5h plus or minus 2 SD's) giving a range of 1-9 hours using 5 for  and 2 for . 

~ 

#Second question:

One of my favorite concepts from statistics/probability is ""survivor bias"". Since twitch won't show you streamers who are already done streaming for the day, this will make my data skew toward the longer tail of the bell curve, right? (a random streamer who streams for 9 hours is 9x more likely to be visible on my watch-list than a streamer who streams for only 1 hour on that particular day, right?)

This is purely for my own curiosity and I mainly want fact-checking on my mathematical reasoning, plus I want the formula if it is super easy to compile! 

**TL;DR:** How to I write a function of how much longer a popular twitch streamer will stay online, based on how many hours they have already been streaming?",12oxcs8,CroationChipmunk,1681696137.0,0,0.5,"['> Does my guess for standard deviation make sense? Or should I use a much smaller or larger value such as 1 hour or 3 hours?\n\n1. You should use data, not a guess\n\n2. You should not expect streaming durations to be anything like normally distributed, and doubly so if you\'re looking across streamers rather than within. (Within will be skew, across will be more skew)\n\n3. ""Since twitch won\'t show you streamers who are already done streaming for the day, this will make my data skew toward the longer tail of the bell curve, right"" -- it\'s NOT a bell curve but yes, if you\'re getting on at some random moment the chance you hit a given streamer will be proportional to both stream frequency and stream length (the second of which is the very thing you\'re estimating ... so yes, a naive estimate of the duration distribution will be biased). Its the same kind of bias you get if you try to estimate number of children per household by randomly sampling people and asking them how many children were in their childhood family (them + their siblings). You\'re much more likely to sample people from large families than small and you miss the households that had 0 children completely.\n\n   If it were possible to do, it\'s better to sample everyone over an interval of time, and to take account of the censoring if you\'re trying to estimate the distribution of complete duration (as with survival  analysis)\n\nDoing this stuff well is a nontrivial task', ""The Twitch algorithm might be more likely to show streams that are not expected to end soon.\n\nLet's assume we get a random streamer, we learn they are currently online, and we cannot use any information about that particular streamer except for how long they have been online. Find the stream length distribution on Twitch. This will not be a Gaussian distribution. Cut the part that's shorter than %online, then normalize the remaining distribution, and you should get a distribution for the remaining time of the stream you joined.\n\nIf we get a random *online* streamer then people with longer streams will be overrepresented and we should adjust the length distribution. That's a non-trivial task."", 'Sounds like a Poisson distribution', 'Look into the exponential distribution, or gamma for more flexibility. A lot of people use this to model time until events so you could use this for time until logging off. Exponential has the memory less property tho, so make sure you check the assumption - if a streamer is on for t hours, maybe it doesnt affect how much longer you expect them to go? Or maybe gamma will generalize better. Maybe the parameters of a gamma will give a shape that matches the data, or maybe not', 'The distribution would be closer to poisson, no?', '> Let\'s assume we get a random streamer, we learn they are currently online, and we cannot use any information about that particular streamer except for how long they have been online. Find the stream length distribution on Twitch. This will not be a Gaussian distribution. Cut the part that\'s shorter than %online, then normalize the remaining distribution, and you should get a distribution for the remaining time of the stream you joined.\n\n\n\nThanks, the reason I ask is because every time I check how long a streamer has been online, I get the (unjustified) feeling that they are about to quit their stream any minute if they\'ve been online longer than 6 hours.\n\nIf I had a crystal ball, I would ask it what is the current stream time in which I have at least a 50% chance of them streaming for at least 60 more minutes. For whatever dumb reason, I cannot ""enjoy"" watching streams that are close to what I feel like is their stopping point. I like to watch the stream until I\'m bored rather than watch the stream until the streamer is ready to quit for the day. ', '> Look into the exponential distribution, or gamma for more flexibility. A lot of people use this to model time until events so you could use this for time until logging off.\n\nPerfect, this is what I likely needed, thank you. ', ""I see no particular reason to think so. \n\nUnder some simple assumptions it might be a reasonable approximation but I strongly doubt those assumptions would actually be the case.\n\nMy first thought was that a negative binomial might be a more-or-less reasonable approximation but I wouldn't consider asserting that even that more general model (it encompasses the Poisson as an edge case) was actually the case."", ""> The distribution would be closer to poisson, no?\n\n\nThis is what I thought also, but I can't reasonably argue matters of statistics without a relevant degree or expertise."", ""> I would ask it what is the current stream time in which I have at least a 50% chance of them streaming for at least 60 more minutes.\n\nWe would need data about the actual distribution, but it's possible that there is no such time if the distribution drops fast enough."", ""> We would need data about the actual distribution, but it's possible that there is no such time if the distribution drops fast enough.\n\nAs long as the mean is more than a couple of hours (which it definitely is), then any small number should satisfy the condition (such as 0.05 hours).\n\nThis would yield a higher than 50% chance of them streaming for at least 60 more minutes if they just barely started their stream."", 'https://nobody.live/ is a website that shows streams with no viewers. These should be more representative of truly random streams than twitch suggestions (which are heavily biased towards popular streamers, who presumably make longer streams).\n\nStream online times sample from there: 16 min, 35 min, 33 min, 14 min, 18 min, 1h 26 min, 50 min, 35 min, 17 min, 8 min.\n\nThese suggest much shorter streaming times for unknown streamers.', ""> https://nobody.live/ is a website that shows streams with no viewers. These should be more representative of truly random streams than twitch suggestions (which are heavily biased towards popular streamers, who presumably make longer streams).\n> \n> \n\nRight, that's why I specified popular streamers in the title and in the body. (preferably those who do it for a living)\n\nIf you've ever watched an unpopular streamer (with under 5 viewers), they are extremely dull & boring. \n\nHowever, if I see someone playing a good old NES classic like Shadowgate or Metroid, then I'll watch them for hours, lol.""]"
[R] Trying to pick the best method for interpreting data of three groups with one value per group," For a project with 3 groups and each group has one single value over a period of time, just finding it difficult to see what the best method would be.",12ox0yo,DangerousSwing,1681695457.0,1,0.67,"['How about finding the means?', ""I don't have a sample size big enough to use the means of the groups. It's the number of injections of a vaccine given over a period of time and there's 3 groups, in  three different weight classes""]"
"[Q] How to use effect size (e.g., Cohen's d) to conduct indirect treatment comparison and subsequent cost-effectiveness analysis?","Hey all,

I'm trying to conduct an indirect treatment comparison (ITC) so that I can then perform a cost-effectiveness analysis (CEA) between two treatments. I'd need to conduct an ITC because there are no head-to-head randomized controlled trials (RCTs) between my treatments of interest. There are meta-analyses that I believe are suitable for the ITC, but they are reported as effect sizes, particularly standardized mean differences (SMD), Cohen's d. My questions are:

How can I use effect size, specifically SMD or Cohen's d from 2 meta-analyses to conduct an ITC between 2 treatments?

How do I get from the ITC to CEA using effect size (SMD, Cohen's d)?

The meta-analyses use functional measures that can be converted to EQ-5D (via a mapping equation that's published), which I understand is necessary for QALY calculations and then for CEA.

Really appreciate any help with this!",12orm16,pantaloonsss,1681684306.0,18,1.0,"[""You can use smd's weighted by one over the standard error...e.g.\n\n    sd_k, k=1,2...\n     Standardized mean difference in study k\n\n     se_k, k=1, 2, ...\n     Standard error of sd_k\n\n      theta = [ sum_{k=1}{K}  sd_k/se_k ]/ [ sum_{k=1}{K}  1/se_k ]"", ""Hey, I really appreciate your reply. \n\nThere is a meta-analysis already available for one of the treatments vs. placebo, and this study has standardized mean difference already available. It's only one value though. So Do I just divide the SMD by SE and that will give me the EQ-5D value? Sorry for asking such a basic question.. I'm still quite new to this. \n\nWould you mind providing an example or maybe a webpage that can explain this reasoning a little more in-depth? Thank you for your help!!"", ""I don't know what you mean by EQ-5D value. Please explain. And while you're at it, why not flesh out the whole thing a bit more.""]"
"[Q] Using Propensity Score Matching to Reduce ""Class Imbalance"" Biases?"," Suppose I have a dataset where 100 patients have the disease (e.g. information such as height, smoking, weight, age, disease status) and 10000 patients do not have the disease (i.e. class imbalance).

I am interested in using Logistic Regression to try and understand what patient characteristics appear to influence the odds of having the disease or not. As such, there are significantly more patients without the disease compared to those who do not.

I fear that fitting a Logistic Regression on the entire dataset might partly invalidate the results as patients without the disease will have more influence in the model estimates. To potentially mitigate this problem, I am thinking of using Propensity Score Matching to select 100 patients who do not have the disease - in a way such that we only select patients without the disease so that they have an ""approximate analog"" in the disease set. As a result, I will have a dataset with only 200 patients and the the ratio of disease to non-disease will be balanced.

I had the following question: By using this Propensity Score Matching approach, I will end up discarding lots of information corresponding to the non-diseased patients and a result might be forfeiting large amounts of valuable information that might be beneficial to the model. However, by including this information, I fear that I risk ""flooding"" the model with too much information corresponding to the ""non-diseased patients"" and suppressing information belonging to the diseased patients.

In general - can Propensity Score Matching be used to mitigate problems/biases associated with class imbalance when fitting regression models to such types of problems?",12onzs8,SQL_beginner,1681677316.0,3,0.71,"['I wouldn\'t bother. I think the problems of class imbalance are often exaggerated for LR. Trying to ""correct"" class imbalance can do more harm than good (e.g. [van den Goorbergh 2022](https://academic.oup.com/jamia/article/29/9/1525/6605096)). They don\'t cover a Propensity Score Matching approach, but I imagine it is fraught with possible sources of bias for your coefficients and throws out perfectly good information about the distribution of predictors in the control group.\n\nThe key things to do are:\n\n1) Let the sample size of the smaller class guide the complexity of the model. I.e. with 100 patients don\'t have 30+ predictors, even if you have 10,000 controls.\n\n2) If you care about sensitivity / specificity (i.e. you\'re not just going to use AUROC) then choose the classification threshold carefully. Either use some objective method of selecting a classification threshold that balances sens + spec (e.g. geometric mean), or just threshold at the observed prevalence (which is often not far off the former).', ""Short answer, no.\n\nA bit more detail:\n\n1). Propensity Score Matching is not typically used to balance your outcome class. It is used when you don't have a randomized experiment (or you have a poorly randomized experiment) to remove biases between your treated/non-treated groups.\n\n2). If the disease is truly rare and you do not have a selection bias then logistic regression will be robust due to it being a probabilistic prediction. \n\n3). If you are adamant in balancing your data then you could do a 2-step regression where in step one you fit the model to a data set that has the minority class oversampled resulting in robust parameter estimates for the covariates but a biased intercept. Then in step 2, you would fit the model to the original data to re-estimate the intercept coefficient."", 'https://gking.harvard.edu/sites/scholar.harvard.edu/files/gking/files/0s.pdf', 'What are you trying to estimate?', 'Matching is an estimation strategy not an identification strategy. Matching is a way to estimate treatment effects, regardless of whether the treatment assignment is known or not known. It is, in a sense, orthogonal to whether the student is an experiment or not. In the same way OLS is too.', ""Don't use PSM (watch Gary King). You may want to use other matching methods.\n\nOr consider direct adjustment of potential confounders or adjustment using propensity scores."", 'thank you so much for your reply! in the article you linked, they discuss Logistic Regression for classification tasks - I am interested in using Logistic Regression for inference tasks. Does the same advice from the paper apply? thanks!', 'thank you for your reply! I will check this out!', 'thank you for your reply! I am trying to estimate the effect of different predictor variables on the outcome.', ""Yes, that's a good point. As far as I'm aware the issues around running logistic regression with class imbalance entirely refer to either: \n\n 1) Prediction modelling / classification issues.\n\n 2) Sample size / precision issues (i.e. if you hold sample size constant and vary the imbalance you confound [small sample bias](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-9-56) with imbalance)\n\nI've not aware of imbalance being a problem for inference on logistic regression models beyond those two points."", 'Ok then you definitely shouldnt use propensity score matching.']"
[Q] Undergraduate courses necessary to be competitive for a phd in statistics,"I posted [this](https://www.reddit.com/r/statistics/comments/11vok8u/q_what_courses_are_a_must_in_order_to_be_a/) a while ago asking about courses necessary to be competitive for a masters program in statistics. Out of curiosity, what would be the necessary courses that one would need to be competitve for a statistics phd program?",12onedq,redreaper71_,1681676190.0,4,0.75,"['The big one would be real analysis and I imagine most will require that as a prerequisite. Topology is also relevant for the same reasons. I tend to believe anything after topology, probability theory, math stats and real analysis is kinda buyers choice, so long as its upper level math. Typically people take a semester of abstract algebra, and sometimes things like numerical analysis and partial differential equations. \n\nA lot of people also have masters in stats or math and that gives them a little edge of just having completed more serious upper level math/stats thats hard to complete as an undergrad.', 'The big three would be analysis, linear algebra, and probability theory.\n\nBy far and away, taking as much analysis as you can should be your first priority. If you can, also try and take a measure theory class (but this is not strictly necessary so dont stress about it too much). Linear algebra should be your next biggest priority, try to take a difficult proof based linear algebra type of class (so not one of those linear algebra classes where youre just learning how to multiply matrices)\n\nIf youre going really deep into stats then topology can be helpful but is certainly not necessary. \n\nIt helps to have taken other math classes like abstract algebra just so you have a better picture on what tools exist in math and for your development in constructing proofs, but in terms of subject matter itself I highly highly doubt youre going to see very much abstract algebra/number theory type of math in a stat PhD program.']"
[Q] Statistical test selection,"I have data for 18 months prior to an intervention, and for 6 months after an intervention. 

I have one data point for each month and am looking for a test which can distinguish whether the intervention had a statistically significant effect on the the resulting data.

The data points are all rates (e.g., 0.45 specific infections per 10,000 patients).

Sorry for the noob question. Just looking for some thoughts/direction here, thanks!",12olujk,caughtdeadlol,1681673277.0,4,1.0,"['You could do a Pearson independent samples t-test. Dataset 1 would be pre-intervention data points and dataset 2 post-intervention data points. \n\nYour null hypothesis would be that mu1-mu2 = 0 or rather that post intervention and pre intervention data are same and the intervention doesnt make a difference. Your alternative hypothesis would be that mu1-mu2  0 or rather that your pre-intervention data is statistically different than your post-intervention data. \n\nIf you run the test and your p value is under 0.05, then you can claim that there was a statistically significant change in data pre infection vs post infection. BTW someone should correct me if Im wrong']"
[Q] Appropriate Statistical Test to Compare Means Between Two Small and Imbalanced Samples,"I am currently working on a project where I need to test the statistical impact a binary variable has on a continuous variable, with the end goal of creating a linear regression model. However, each group has a small sample size and are different in size (n = 9 and n = 23). Due to this, is a two-sample t-test still the most appropriate test? I have confirmed that there are no outliers, the data in each group is approximately normal, and the variances are approximately equal. 

The t-test suggests that there is no significant difference between the two groups. Assuming this is the appropriate test, is there anything I can/should do to confirm that this is not a type-II error?",12og37n,hopquinhos,1681662540.0,9,0.85,"[""It sounds like *t*\\-test is fine for this application.  There's no way to get a better of idea of if the means in the sampled population differ except to get a larger sample.  You may want to report Cohen's *d*, hopefully with a confidence interval, to better describe the differences in the two means.\n\nBTW, there are lots of other tests to compare two samples, if the mean isn't what is actually of interest."", 'Agree that a t-test is appropriate here.\n\nAs for a type II error concern, due to the low sample sizes, you probably have low power and a high probability of a type II error. You could use a power analysis tool to find either the minimum effect size detectable with a certain desired power given your sample sizes, or the power for detecting a certain relevant effect size, given your sample sizes.', ""> Due to this, is a two-sample t-test still the most appropriate test?\n\nCan you talk about the response variable a bit please. What sort of thing does it measure / what values can it take?\n\n> Assuming this is the appropriate test, is there anything I can/should do to confirm that this is not a type-II error?\n\nIf such a thing were possible it would already be incorporated into all hypothesis testing so that we would avoid type II errors. If it was possible, *you'd know about it* because literally everyone would do it. The way to reduce type II error is to increase power, and that comes down to a good sampling design (or experimental design for an experiment), a suitable model for the conditional response and decent sample size (well, there's also the possibility of using a high type I error rate, but that's usually an unpopular choice for increasing power; there are a few situations where that makes sense though).\n\nYou will not have great power against small effects unless the variation around group means is very small. \n\nType II error is clearly a distinct possibility\n\nA t-test will likely be fine, and probably close to as good as you could do. \n\nIf you were worried about the normality assumption you could have done a permutation test but it wouldn't have been likely to change your conclusion. Similarly a generalized linear model would be possible, but it sounds like the model you have is already reasonable, so again, it would have been unlikely to change  anything much. Either way, you have already done your test so these possibilities are moot (any attempt to try something else to get significance on this issue would be p-hacking).\n\n[For next time ... I'd suggest doing a sample size calculation before you think about collecting data so you know what sample size is necessary to attain a reasonable level of power; that is the usual way people try to reduce type II error, but you don't control population effect size, so it's always going to remain a possibility. Do not attempt to compute post hoc power - i.e. using the information from your sample in a power calculation, like estimated effect size.]"", 'Its not a good idea to conduct additional tests just because the one you did first was not significant. If there is at least a hint of an effect you could try a replication study.', ""[Welch's t test](https://en.m.wikipedia.org/wiki/Welch%27s_t-test) perhaps"", 'Kolmogorov Smirnoff Test', 'Doesnt that test wether a distribution is stochastically higher than another in at least one point?']"
"[Q] MPE and MAPE Inf, why ?","Hi everyone,

I guess I shouldn't be getting Inf for MPE and MAPE, am I doing something wrong ? 

[https://imgur.com/PhxMqza](https://imgur.com/PhxMqza)

This is the entire code, nothing special going on there

[https://paste.ofcode.org/33JaM3had6KnkCc89GJ3ZCn](https://paste.ofcode.org/33JaM3had6KnkCc89GJ3ZCn)

Thank you",12o51cn,murdafeelin,1681643390.0,1,0.6,"['Because there are 0 values in return\\_PLTR\\_test, MPE and MAPE are returning infinite?\n\nMPE = (1/n) \\* SUM\\[(actual - forecast) / actual\\]  \nand  \nMAPE = (1/n) \\* SUM\\[|actual - forecast| / actual\\]\n\nso when actual = 0 it just errors out and says infinity', 'Consider using WMAPE. MAPE does not work when you have an observed values equal to 0', 'As side note --- since I was looking at the data to answer the question --- \n\nYou might plot the ""predicted"" values and the ""actual"" values.\n\n`plot(as.numeric(preds) ~ as.numeric(return_PLTR_test$PLTR.Adjusted))`', 'Thank you for your answer, so, its ok that it says infinity ? There are 0 values in return_PLTR_test, I cant change that.', ""Hi, thank you for your help. I tried plotting predicted/actual values.\n\nI get this plot but I'm also not sure how to interpret it. \n\n[https://imgur.com/D0XfAOU](https://imgur.com/D0XfAOU)\n\nThank you again"", 'If your ""actual"" values can be 0, then probably MAPE and MPE just aren\'t useful measures.  Imagine if instead of 0, the values were 0.000001.  Then you would have a quite large MAPE value.  But what would that tell you ?', 'I was just showing why it says infinity. As for if its okay or not, that depends on what youre trying to show or prove', 'As a caveat, I essentially never work with the kind of model you are working with, so it\'s possible I\'m not understanding your values... If the model perfectly predicted the observed values, you would see points laying along a diagonal 1:1 line.  Here, most of your predictions are -0.22, and don\'t really vary systematically with your observed values.  You can google *predicted vs. observed* . If you were to calculate a pseudo r-squared for your predicted and observed values, it would be quite low.  (Because, essentially, the model typically predicts just -0.22, which is close to the mean of the observed values.\n\n>`library(rcompanion)`  \n>  \n>   `### note, this will change the default call for accuracy() to rcompanion::accuracy()`  \n>  \n>`efronRSquared(predicted=as.numeric(preds), actual=as.numeric(return_PLTR_test$PLTR.Adjusted))`   \n>  \n>`### EfronRSquared`  \n>  \n>`### 2.75e-05`  \n>  \n>  \n>  \n>`efronRSquared(predicted=as.numeric(preds), actual=as.numeric(return_PLTR_test$PLTR.Adjusted), statistic=""MAE"")`  \n>  \n>   `### Same as your value for MAE`']"
[Q] Calculate residency using Bayesian inference,"Hey all!

Hoping this is the correct place to post my question. I'm researching survival between two different sub-groups of Black-tailed Godwits. These groups breed on meadows in Frisia (the Netherlands) and migrate south after the breeding season. These birds are colour-coded and actively monitored (more or less) in all important locations during their life.

There are two destinations they can go, they either fly to sub-Saharan west-Africa or they stay in the Iberian Peninsula. These long lived birds choose one of these locations for life: they either go to Iberia or to Africa, but don't change that destination between years.

Since these birds are ringed with colour-codes, once a bird is seen in Africa we know they belong to a certain group. For Iberia, it is more difficult, because a lot of African birds use Iberia as a stopover/resting place. So if you see a bird in Iberia, you don't know if it will stay there or migrate further to Africa.

We know from GPS and solar-geolocator data, that for a certain week in Oktober, if a bird is seen in that week in Iberia, it will *probably* stay there for the rest of the winter. But, of course, this is still a mixture of both type of birds. Meaning: a bird seen in that specific week is probably an Iberian bird, but not a 100%.

What I would like to do, is **to ascertain what the chance is that a bird seen in that week is an ""Iberian"" bird**, with credible intervals. The data used for this will be from solar-geolocators and GPS trackers.

The difference in survival between African and Iberian wintering birds will be modeled with CJS ([Cormack-Jolly-Seber](https://www.montana.edu/rotella/documents/502/CJS.pdf)) models using Bayesian inference (using the program JAGS).

But what statistical test would I need to use to answer ""what the change is that a bird seen in that week is an ""Iberian"" bird, with credible intervals"". I'm a bit at a loss here.

This research is for my final Masters project and the idea is to publish it, that is why we need to be fairly certain we can correctly identify the wintering type and with what confidence, otherwise, my CJS model loses power.

Thanks!",12o2yo4,Moonl1ghter,1681638149.0,21,0.97,"[""Rather than split this into 2 models I strongly recommend building one model that estimates everything you're interested in. The probability of an Iberian bird being full time Iberian rather than a stopover will probably affect the estimates of differences in survival which is why you should take account of the uncertainty in that quantity, meaning you should have one model that estimates both things simultaneously"", ""> We know from GPS and solar-geolocator data, that for a certain week in Oktober, if a bird is seen in that week in Iberia, it will stay there for the rest of the winter. But, of course, this is still a mixture of both type of birds.\n\nI've read this section multiple times, and it seems like something is worded incorrectly bc to me it says that 100% of birds observed that week would be Iberian, but your entire project is trying to find out the split between birds types that week in Iberia, which suggests it can't be 100%"", 'Your research question is perfect for Bayesian inference!\n\nWith regularizing priors, the GPS data, and colour coding, you can estimate the probability of a bird seen in Iberia being an ""Iberian"" bird.\n\nA Bernoulli or binomial model (depending on data aggregation) erotic be able to directly model the probability and associated uncertainty.', ""> with confidence intervals\n\nIf you're fitting a Bayesian model, so *you will not get confidence intervals*, since that's frequentist inference. If you really want a confidence interval you should use analysis that gives you that.\n\nYou can compute Bayesian intervals (credible intervals) but those are NOT confidence intervals and do not have the meaning nor the  properties of confidence intervals."", ""Thanks you! Did not think about that. Do you think that is possible?\n\nDeciding if a bird is Iberian or African is done on the hand of GPS trackers and solar locators. We know then always were a bird is and can thus decide that it never travels to Africa but always takes Iberia as it's farthest point southward.\n\nThe survival estimates are based on capture recapture data (rings in this case) which take into account the probability of not seeing a bird, but the bird still being alive.\n\nAre these two compatible?"", 'Thanks for the reply!\n\nYou are right, that was worded wrongly. We assume that a bird seen than is an Iberian one, but we know that is not true but it is still a mixture (but with mostly Iberians).\n\nMy project is not about specifying that week, but the difference in survival. For that, we need to have a good way to assign birds to the Iberian group and know how certain we are that a bird is Iberian if seen in that week I hope that makes sense.\n\nOtherwise, a peer reviewer might just say: yeah there is no difference in survival, but that is because your Iberian group is still a mixture. If we can say that we are 95% sure that a bird we classified Iberian is Iberian, that would give more power.\n\nThanks for the help!', ""Thank you!\n\nI gave it some thought in the meantime, and indeed a logistic regression might be the best course of action.\n\nCould even (and probably should) include random or fixed effects, not sure on that front.Gonna suggest and discuss it with the team, but at least, now I don't come empty handed to the meeting.\n\nEdit: I started doing Bayesian inference 2 moths ago, only having had statistic course dealing with the simple frequentists models.\n\nKinda amazed how versatile and useful this type of statistics is and wondering why it is not part of any course.  \n\n\nModeling survival for both groups has been a blast in JAGS and being able to state how sure we are that the true value for survival of a group lies between two values is really nice."", 'I love Bayesian inference, but I have yet to find the Bernoulli model quite that exciting.', 'Yes! Sorry! Only working with Bayesian for about two months, so things like that are ingrained,.but I know the difference. Thanks!', ""Absolutely. I would say start by writing down the algebra/pseudocode/model specification you're planning to use. Somewhere in the survival model you'll be utilizing the probability that a bird is stopover vs resident, and the band data. And somewhere in the stopover model you'll be using the GPS data... So you just need to include all the relevant data into one big model, and then meld it together.\n\nJAGS is ok, but I recommend you take a long look at Stan and read the Stan reference manual which is almost like a book on practical Bayesian modeling."", 'Yes and it is rather simple. Instead of two models in one dimension, you have one model with two. However, they share information when you do that. \n\nJust as a terminology note, you are looking for credible intervals not confidence intervals. They are conceptually different.', ""I code my models in Stan, which is a fantastic probabilistic language. Your question would be pretty straightforward to implement and I'd be happy to assist.\n\nI'm teaching an introductory Bayesian worship soon and can let you know the date, if you're interested"", ""If there something more exciting for probability of membership in one of two groups that you recommend?\n\nHere's the CJS model in Stan\nhttps://mc-stan.org/docs/2_18/stan-users-guide/mark-recapture-models.html"", 'Thanks! That makes a lot of sense. Gonna talk it over with my prof.   \n\n\nI could switch to Stan, but JAGS is what the whole research group is using (spread out over 2 universities) so it is easiest I keep using that.', 'Yeah thanks! Quite new to Bayesian inference, so sometimes have trouble using the correct terminology :).', 'Awesome! I would love to follow that!\n\nI code my models (for now) in JAGS. Read Kery & Schaub 2011 but am far from an expert. Still a lot of things about Bayesian statistics that are a bit vague for me.\n\nMy CJS model (with fixed and random effects) is done, so gonna move over to the other problem (determining if a bird is Iberian or African) and would love feedback! Thank you so much for the offer!', ""It's fine as long as JAGS can fit the model. Stan will typically run 10-100x as fast (in terms of effective sample size per second)"", 'I might want to dive into Stan (if anything, just for the fun learning experience) are there good resources available to start if coming from JAGS/BUGS?', ""https://mc-stan.org/users/documentation/\n\nStart there. The documentation is very good.\n\nAlso consider Turing.jl in Julia which is quite fast and has access to the entire Julia language. It's what I use at the moment when I am doing Bayesian stuff, but Julia is a whole additional leap, one that absolutely is amazing but not as quick to jump into as Stan for a more limited purpose""]"
[Q] Can someone ELI5 “contemporaneous correlation of errors” and how do I test for this in panel data?,I want to be sure that my model has contemporaneous correlation of errors before I perform PCSE regression. The Breusch Pagan Cook Weisberg test already confirmed heteroskedasticity. T > N.,12nenoc,EbiraJazz,1681585034.0,15,0.95,[]
[R] Seeking Current/Former Grad Students for Survey on Motivations (For final dissertation chapter!),"Hey,

I am a doctoral student in public policy at Georgia Tech looking for participants for a survey on the motivations of current and former graduate students. This is for my last dissertation chapter, so participation would be greatly appreciated!

The **survey takes approximately 10 to 15 minutes** (average of 12 minutes). Before the survey, there are a few questions ensuring eligibility followed by the consent form (IRB Protocol H23126).

The survey begins with some scales measuring things related to motivation and needs satisfaction in graduate school and some associated outcome scales. After that, the survey asks for some information on your degree program and personal background. The survey finishes by asking a few questions related to social media use and the option to opt-in for a follow-up interview.

All information collected is password protected and information used for interview opting-in is additionally protected by an encrypted folder.

It goes without saying that you must be over 18 to participate, must be a current or former graduate student, and must not be located in China. You can find the survey here:

[https://gatech.co1.qualtrics.com/jfe/form/SV_et7R7JA6pN4R1v8](https://gatech.co1.qualtrics.com/jfe/form/SV_et7R7JA6pN4R1v8)

Thanks!",12nef2s,Anidel93,1681584572.0,15,0.9,['/r/SampleSize/']
[Q] Why is my algebreic error propagation not match my covariance matrix calculation?,"Hello,

I've been attempting to understand error propagation by the old plug/chug method, and my math is wrong somewhere, but I don't quite know where.

Let's say you have a function

    x=2000
    x_error=4000
    y=2e-3
    y_error=3e-3
    f(x,y)=(x*y)/(y+1)
    #therefore
    f(x,y)=3.99

Now to calculate the error of f(x,y)

There are a number of ways to do this, the most straight forward (in terms of easy to understand) is the algebreic method:

    errorx*errory=x*y*sqrt((errorx/x)**2+(errory/y)**2)
    errorx/errory=x/y*sqrt((errorx/x)**2+(errory/y)**2)

So doing the math for that (assuming I've done it correctly), gives you

    f(x,y)=3.99
    error f(x,y)=10.74

Which surface level makes sense to me. The values themselves have high errors, so the function should have a high error. This, while tedious, I understand and can easily follow.

However, a more extended/linear algebra form of this is the covariance matrix

    [df/dx df/dy][covariance matrix][df\dx df\dy]^T

where

    df/dx=y/(y+1)
    df/dy=x/(y+1)^2
    cov(x,y)=sum(x-xavg)(y-yavg)
    #but since its a single value x=xavg so I assume cov(x,y)=0
    error f(x,y)=sqrt((df/dx)^2*error_x^2+(df/dy)^2*error_y^2)
    #plug and chug
    error f(x,y)=9.97

While close, this is not exactly the same as the other method.

    algebreic error f(x,y)=10.74
    covariance matrix error f(x,y)=9.97

Why don't these 2 values match? Did I make some assumptions wrong or is the math wrong somewhere?",12nbrwi,DrBobHope,1681579392.0,5,0.78,"[""Despite your statement that the algebraic method is simpler, I've never been exposed to it before, but the second method I use many times daily in my job....  This means I can't comment on why there are differences, but I do know that the second method is derived from a Taylor series expansion, but only retains the first order terms, so it's an approximation.  If you look into variations that retain higher order terms the answer may get closer to the algebraic method.  You can also look into using an unscented transform to approach the problem."", ""So are you saying if feasible, the algebreic method is always more suitable/accurate? From my understanding the Taylor series method should be quite accurate, even if it's just an approximation (especially for something like this)""]"
[Q] Randomized Test v. Nonrandomized Test,"I'm a bit confused about what a randomized test is. It seems that most of the tests that are taught in an elementary mathematical statistics course are non-randomized. Are these classified as non-randomized because once the data are observed, it is no longer random whether the sample is in the rejection region or not?

What is an example of a randomized test?

&#x200B;

Edit: Sorry, this is a bit ambiguous. To clarify, the definition I was given was:

A non-randomized test is a test where the probability of rejecting the null is either 1 or 0. Otherwise it is a randomized test.

Additional Edit: To add some context, I am in an ""elementary"" Mathematical Statistics course in a PhD program. I put ""elementary"" in quotes because I'm finding it very difficult.

&#x200B;

Final Edit:

The way we talk about statistical tests is strange.

So a non-randomized test is ""random"" in the respect that before we have observed a sample, we do not know with certainty what the statistical decision will be with a given critical region. However, once the sample is observed, the decision is dictated by the critical region with probability 1.

The ""non-randomed"" refers to after we have observed the sample. If we do not know with certainty whether the observed sample point is in the rejection region, then this must be because some other randomness was introduced after having observed the sample.

The statistical tests we use in most(all?) applications do not introduce additional randomness after the sample has been observed.",12n7x8z,zebrapaad,1681573110.0,7,0.82,"['Randomized tests like you are referring to pop up in discrete data settings. When data are discrete the test statistic will generally also be discrete and you often cannot find a test with an exact significance level corresponding to any particular alpha.\n\nIn this case you can supplement your data with a single draw from a continuous uniform distribution, and this provides the flexibility to satisfy an exact alpha level. It makes the test random though as you will get different p-values with the same dataset (as the supplemental uniform data point changes every time).', 'The definition is correct but not especially enlightening if you\'re not familiar with the idea.\n\nA randomized test doesn\'t just compare a test statistic with a fixed rejection region; that would be non-randomized; every time you had the same statistic the decision would be the same (you either reject for sure or you don\'t reject for sure).\n\nWith a randomized test there\'s (at least for some values of the test statistic) a chance of rejecting that\'s not 0 or 1.\n\nWhy would you do this? When you have a discrete test statistic; it allows you to attain an exact chosen alpha rather than being stuck with a few fixed significance levels. \n\nEven if you\'re not inclined to randomize a test in practice (and typically you wouldn\'t, though there can be situations where you might just consider it), they\'re extremely useful in the theory of hypothesis testing, and they do also have some practical uses (such as when comparing power of two different discrete tests). \n\nFor example in the latter situation,  the book *Discovering Statistics using SPSS*  by Andy Field (I forget the edition right now but I can try to find it again), he says that the Kolmogorov-Smirnov test should be used instead of the Mann-Whitney for small sample sizes:\n\n>  Kolmogorov-Smirnov Z: In Chapter 5 we met a KolmogorovSmirnov test that tested whether a sample was from a normally distributed population. This is a different test! In fact, it tests whether two groups have been drawn from the same population (regardless of what that population may be). In effect, this means it does much the same as the MannWhitney test! However, this test tends to have better power than the MannWhitney test when sample sizes are less than about 25 per group, and so is worth selecting if thats the case.\n\nNo evidence is offered in the book for this claim, so how should we check it? \n\nIf we just directly compare power functions at some set of sample sizes, we\'ll give an advantage to one test or the other (whichever has the higher actual significance level); if we\'re very lucky we might be able to find a few values of n1 and n2 near some reasonable significance level where the attainable significance levels are very close to each other, and where the advantage is small enough to ignore (because the difference in power is substantial by comparison). \n\nClearly we don\'t want to rely on that working out the way way want. It\'s better if we can make both tests have the same exact significance level to compare power at any sample sizes we like, and the way to do that is to randomize between the significance levels either side of alpha (say for alpha = 5% given that\'s almost certainly the significance level the test would be done at by the people reading the book, and by the author).\n\n[... The claim in that book is quite false, by the way, but the claim provides a useful context for wanting to use randomized tests.]\n\nSo for example if at n1=4 and n2=5, our Mann-Whitney has available significance levels of 3.1746% and 6.3492% near 5%. In that case we\'d want to reject all the test statistics that a non-randomized test would reject at the 3.1746% level and *some fraction* of the cases at the 6.3492% level in order to make the overall rejection rate under H0 equal to 5%.\n\nIf we reject 0% of the additional cases we\'d get alpha = 0.031746\n\nIf we reject 100% of them we\'d get alpha = 0.063492\n\nIf we reject a fraction r of them, we get alpha = 0.031746 + r(0.063492-0.031746), so if we want that to be 0.05 we can solve 0.05 = 0.031746 + r(0.063492-0.031746) to find r:\n\nr = (0.05 - 0.031746)/(0.063492-0.031746) = 0.5750\n\nor about 57.5%. That is, when the test statistic is  in the rejection region when alpha = 0.063492 but *not* in the rejection region when alpha = 0.031746, you generate a random Bernoulli with p=0.575 and if it\'s a ""1"", you reject. Outside that ""on the boundary"" case, you act as you would for the non-randomized test. This new random rejection rule gives alpha = 0.05. We could then do the same kind of calculation for the two-sample Kolmogorov-Smirnov test and actually compare power functions for that n1, n2 combination by giving them the same significance level. The same trick could be used for a whole collection of other sample sizes in order to investigate the  general claim that the Kolmogorov-Smirnov has more power when n is less than 25 (and see that it\'s not true at all; given that (a) it\'s easy to check and (b) pretty clear it would be false a priori, how that came to be in the book is a mystery).\n\n[As it happens, when I addressed a question on this, rather than explain randomized tests, I instead chose a particular case where the two tests had a very similar exact alpha and it turned out that the effect was quite strong, enough to act as a clear counterexample to the claim (the MW was considerably more powerful than the KS on situation where you\'d reasonably use MW for n>25). Nevertheless, a full accounting of the advantage of Mann-Whitney over two sample Kolmogorov-Smirnov would really need use of many combinations of small sample sizes and hence randomized tests]', 'Basic statistical tests are just tests. They arent inherently random. They usually work best when applied to data that has been selected at random. That is, randomization typically refers to the data selection process, while the statistical test is a mathematical formula/algorithm designed to be applied to some kind of data. Theyre separate concepts. \n\nThere are more advanced methods that rely on taking random samples of the observed data to compute plausible intervals for things like mean values. But that is about using randomization in the algorithm to compute certain statistics. Theres still a separate concept about whether the observed data is a random sample of some sort.', 'Do you mean [randomized controlled trial](https://en.wikipedia.org/wiki/Randomized_controlled_trial) (RCT) or randomization test (for which there is no good web reference), which is one way (but far from the only way) to analyze an RCT.  The latter is a permutation test for which the assumptions are justified by the randomization in the RCT (the permutations mimic the randomization).\n\nOr if you mean a randomized test like they teach in PhD level math stats and no one uses, see [Geyer and Meeden (2005)](https://doi.org/10.1214/088342305000000340).', ""There are a few things you could be talking about -- perhaps you're referring to randomization tests like permutation tests?"", 'Awesome, thank you so much for this example. This is really helpful.', 'no \n\nThe definition in my notes is that a test where probability of rejecting the null is either 1 or 0 is a non-randomized test, and otherwise it is a randomized test.\n\n&#x200B;\n\nEdit: I think its the second one lol. I am in a phd program', ""sorry about the ambiguity, I'll update the post with the definition I was given"", 'They mean https://en.wikipedia.org/wiki/Randomized_test - i.e. a hypothesis test with a randomized decision rule - not a randomization test.\n\nEven though they refer to different things, a randomization test is one case where a randomized decision rule might be used (e.g. it would be useful in computing a power function, in order to compare powers of tests with different attainable alpha)', ""Well.  Randomized tests are an important part of Neyman-Pearson theory.  Without them UMP and UMPU tests for discrete distributions do not exist.  But AFAIK no one uses them in applications.  It's just theory.  The cited paper does try to make them suitable for applications (by not actually doing the randomization, just describing it, so decisions, confidence intervals, and P-values become *abstractly* random (rather than *realized* random))."", ""The lesson to learn from randomized tests is not that they are useful for applications (see other comment) but rather that nonrandomized tests for discrete distributions are not similar to those for continuous distributions.  A lot of intuitions you get from t-test don't carry over to discrete distributions (or actively mislead).  See also https://www.stat.umn.edu/geyer/5421/notes/fuzzy.html.""]"
[Q] newbie: what is the proper way of calculating odds ratio?,"2 people who are blue are tall. 3 people who are who are blue are short.


8 people who are red are tall. 2 people who are red are short.


How much more likelier are you to be tall if you are red? 


Is it: 8/(8+2) / 2/(2+3) = .8 / .4 = 2X more likely?

Or is it: 8/2 / 2/3 = 4 / .66 = 6X more likely?",12n1r21,moon___fish,1681563871.0,4,0.75,"['If the question is specifically odds ratio given red vs. blue, then the correct calculation is the second one;  6x higher odds ratio.\n\nThe first calculation is called the Relative Risk or Risk Ratio.\n\nIf you need more direct formulas/math respond.', 'Edit: **TL;DR: the second calculation is the correct way to calculate the odds ratio. But be careful about the wording/interpretation of what the odds ratio means.**\n\n>\tHow much more likelier are you to be tall if you are red?\n\nBased on the wording of this question and the examples, it sounds to me like youre more interested in relative risk than odds ratio, but please correct me if Im misunderstanding.\n\nFirst the basics, which you already have:\n\n-\tProbability of being tall given you are red: 8/(8+2) = 0.8\n-\tProbability of being tall given you are blue: 2/(2+3) = 0.4\n-\tOdds of being tall given you are red: 0.8 / (1-0.8) = 4\n-\tOdds of being tall given you are blue: 0.4 / (1-0.4) = 0.67\n\nThere are two distinct quantities being touched on that we can use to compare these probabilities:\n\n1.\tRelative risk: how many times likelier are you to be tall if you are red vs. if you are blue? In this case, 0.8/0.4 = 2 times likelier, so relative risk is 2. This one is more intuitive imo, its simply the ratio of the conditional probabilities.\n2.\tOdds ratio: this one is more complicated. Similar calculation to relative risk, but as the name suggests, we use the odds instead. In this case, 4 / 0.67 = 6.\n\nThese are different quantities with different meanings and interpretations. Anecdotally, I find that people are often more interested in relative risk, but odds ratios are used a lot in practice in part because the coefficients of logistic regression models encode changes in odds ratio.', 'If there is a 50% chance of someone being tall, well the chance would be 50% or 0.5. For odds it would be 1/1. Equal chance of both of them. You want the ratio between two odds.']"
[Q] Help with setting up variables for word-based analysis,"Hello! It's been a long time since I've looked at a statistics program but I'm trying to get back into it to help me optimize an algorithm.  


Here's my use case:   


I need to analyze words and phrases based on my ability to complete an action using them. I'd like to then be able to determine things such as the likelihood of being able to complete an action based on the letters within a word, the number of vowels, and the length of the word/phrase.  


For a concrete example, let's say I have:  


addfueltothefire - not possible to complete action  
aerosolcontainer - possible to complete action  


Of course, I have a bunch of these to input. My thinking tells me to make each letter of the alphabet a variable and use it to add the number of that letter that appears, then a variable that is the word/phrase length, a variable that is the number of vowels, and then a variable that is whether I was able to complete the action with that word or phrase. And then run ANOVA tests? I think?

My end goal is to be able to determine the likelihood of being able to complete my action based on any given word/phrase.

Am I going about this the right way or am I way off course? Any guidance would be much appreciated!",12mmg9m,kjivxx,1681525034.0,10,1.0,"['Weka will let you run a multinomial naive bayes on text where your actions are the class its trying to predict based on the input words.  It will then give you the probability that given some input text that it belongs to that class.  Ive used it to scan text from a drug prescription transaction to predict if the transaction will be rejected.', 'It would help know more about what youre trying to do.  Are you trying to do something like a boggle solver where given some letters you see what words it makes?  Or perhaps trying to find the edit distance between two words?', 'Fair enough! I make anagramatical puzzles where two parts of a puzzle are anagrams of each other. One part results in a word or phrase. When i build these puzzles, I start with the word or phrase to see if I can make the second part (a crossword-style grid). I have an algorithm that brute forces solutions by trying every possible combination, but I\'d love the ability to be able to tell the likelihood of being able to make a puzzle from any given word or phrase, instead of sitting and waiting for the algorithm to go through every possible combination.  \n\nMy end goal is feeding it a word or phrase, and it returns an estimated chance of a solution being possible based on the letters the word or phrase has. For example, say the phase has four ""j""s, then I already know a solution is highly unlikely, whereas a phrase with around half vowels with some ""s""s, ""t""s, and ""n""s has a good chance of finding a solution.', 'Say you have the phrase bad credit and want to quickly check to see if you can turn that into debit card. Normally, this would be done in factorial time complexity or O(n!) which would would be 9! different permutations or 362,880\n\nHowever, there is a simple way to check the if the phrases are anagrams that would only  be O(n) in complexity, where n is the length of the string\n\nTake the ascii value of each character in each string and add it up and see if it equals:\n\nThe sum of ASCII values of the letters in the phrase ""debit card"" is:\n\n100 (for \'d\') + 101 (for \'e\') + 98 (for \'b\') + 105 (for \'i\') + 116 (for \'t\') + 32 (for \' \') + 99 (for \'c\') + 97 (for \'a\') + 114 (for \'r\') + 100 (for \'d\') = 963\n\nThe sum of ASCII values of the letters in the phrase ""bad credit"" is:\n\n98 (for \'b\') + 97 (for \'a\') + 100 (for \'d\') + 32 (for \' \') + 99 (for \'c\') + 114 (for \'r\') + 101 (for \'e\') + 100 (for \'d\') + 105 (for \'i\') + 116 (for \'t\') = 963\n\nIf the phrases are equal in total ascii value it\'s highly probable they are anagrams.  \n\n\nThere could be exceptions to this or phrases where the ascii values sum up but are not anagrams but I think overall this approach could pan out.', ""That's an awesome method for determining anagrams, love that, and might come handy in other avenues of the puzzle! In my case, I force them to be anagrams (the algorithm must use the letters of the word or phrase only), and so the issue isn't whether they are anagrams, but whether I can put the letters of the word/phrase in a crossword grid such that all letter combinations both vertical and horizontal are legitimate words. Apologies, I didn't know the final puzzle itself would be so relevant!\n\nHere is an example of the puzzle if that helps things: [puzzle](https://i.imgur.com/Tdg0Cll.png)"", 'Another way to think about this approach is if two phrases are anagrams, they will have the same sum of ASCII values. So if you had a 9-character phrase and wanted to check 100 different 9-character phrases looking for anagrams, you are only doing 100 ASCII sum operations or 900 Character lookups plus 100 sums / comparisons = 1000 total operations versus 362,880 \\* 100 = 3,628,800 brute force combinations.  That ASCII approach is going to run in seconds.  Once an ASCII match is made then creating a vector with entries marking the frequency of each letter and seeing if the sum of frequencies matches up per letter would then be a more definitive check.']"
[Q] trying to figure out the statistical test to use for my experiment,"hi! i am an undergraduate researcher who studies how early life stress and the estrous cycle impact the incubation of cocaine craving in female rats. i have three independent variables: day (1+30), condition (ELS+control), and estrous phase (estrus+nonestrous). i was originally going to run a three way anova but the day variable is repeated measures and estrous phase depends on the day. im not entirely sure if im articulating this correctly but basically estrous phase is an independent variable that depends on a different independent variable? i run all my statistics on prism if that matters",12mlq9o,neurobioqueen,1681523376.0,1,1.0,"['If day is a predictor of both estrous phase and cocaine craving (or whatever your main outcome is), then including both day and estrous cycle as predictors in a model would probably yield spurious results because of the presence of a collider variable.\n\nI would suggest fitting 2 mixed models. One with day and condition as the predictors, and one with estrous phase and condition as the predictors. You can then compare the results.\n\nIn biology terms, run 2 two-way repeated measure anovas.', 'Would it be feasible to replace *day* with *cycle\\_count*, tracking the aggregate number of estrous cycles that have occurred since the experiment began?\n\nThat will result in *estrous phase* and *~~day~~* *cycle\\_count* being independent of each other, but still allow you to track time, albeit with lower precision. It might be problematic if the experiment itself affects the cycle length.', 'Agreed with this. You need to eliminate confounding variables in the form of your day and estrous cycle being measurements of the same thing. The interaction between the two variables will be very high in your ANOVA.', 'thank you so much!', 'i will definitely consider this, thank you!', 'thank you for your feedback!']"
What Statistical Analysis Test for Salmon Emigration Over Time? [Q],"Dear All,

I am new to statistics, and have to complete a project over Pink Salmon Fry populations over time. 

I am unsure as to how I should proceed. 

The data is very numerous, spanning around 1100 pieces, over 10 years. This is not an even time distribution, and the data was taken from between march and may (roughly, since dates changed each year slightly).

Pink salmon emigrate from their home stream immediately upon hatching, and then return in 2 years, thus causing two distinct populations to form, this is split into two different population groups, 1 and 2. 

however, I am trying to assess the trend in populations over time, to see if the average peak dates, tail ends of spawning (season time), and population totals have changed over the past 10 years. 

This is the point where I am stuck. I have all the data, laid it into tables for each year, and made basic scatter plots of each year. 

I have done the same for the total amounts of fry per year, for each population.

I was wondering what Statistics tests I would use for this type of data? 

in my class, the instructor stated that it would be good to use the Shapiro-Wilk test to find normalicy, then use the breusch-pagan test for homoscedasticity. 

however, my data cannot be applied to the shapiro-wilk test, nor its larger sample size counterpart (the royson), as Fry count is discreet.

what is your recommended analysis, or breakup of my data? 

is my breakdown of data into different groups good (into the 2 population groups)? should I use a different approach, such as doing overall group changes? i've broken it down into both overall trend in population as well as the graphs of each year's counts over time. the latter is a bell-shaped graph, but I am unsure on how I would proceed with it.

I may do a graph on date changes for the peaks, but since the peaks can be outliers and off of the ""true peak"" that one would see in a normal distribution, I am unsure on how I would do it.

If needed, I can post images of the graphs I have laid out. This is my first ever statistical analysis project, so I'm struggling quite a bit.",12mhru3,Artist_mugi,1681514834.0,5,0.86,"['You have a time series, which could probably be analyzed a number of ways depending on what your specific question is. \n\nYou need to ask a question of the data first, then the statistical approach to answer that question is decided afterwards.\n\nSo, what is the question?', 'the question would be\n\n""how has this population changed over the time period?""\n\nthis incorporates several smaller questions, such as \n\n""has the average amount changed,"" \n\n""has the range of time changed,"" and\n\n""has the ranges in which the maximum occurs changed?""\n\n&#x200B;\n\nfor the maximum, I was wondering how to do that specifically. since it\'s got some peaks outside of the normal-ish range (a scatterplot of count vs time has it being a very close shape to a bell). in counting those peaks, it\'d be very hard to truly estimate when the maximum is likely to occur, since it\'s only for a single day out of the season. I was wondering if this could be solved by instead trying to find the range in which the maximum occurs? such as using the top 10% and their dates instead?', 'Model each year as a logistic function (with time as an X and % of total annual observations in a given year as the Y), where you accumulate observations over time in each year. Then the inflection point of that curve would most likely be the best estimator of peak activity. You can also then play with elements like synchronicity, etc.']"
[D] How to concisely state Central Limit theorem?,"Every time I think about it, it's always a mouthful. Here's my current best take at it:

> If we have a process that produces independent and identically distributed values, and if we repeatedly sample n values, say 50, and take the average of those samples, then those averages will form a normal distribution.

> In practice what that means is that even if we don't know the underlying distribution, we can not only find the mean, but also develop a 95% confidence interval around that mean.

Adding the ""in practice"" part has helped me to remember it, but I wonder if there are more concise or otherwise better ways of stating it?",12mg9yy,actinium226,1681511862.0,67,0.93,"['Many samples, mean approaches normal.\n\nI think that is the most concise way to say it that is mostly correct. Because why use many word when few word do trick.', ""Even if it's not normal, the average is normal :) \n- statquest"", '""The distribution of a random variable that is a sum of n independent shocks will tend toward Gaussian as n goes to infinity.""', 'I pull out my Galton Board ([https://galtonboard.com/](https://galtonboard.com/)) and explain what is happening. Makes it a little more real for the students.', 'The sum of measurements of a process approaches the normal distribution if the measurements dont influence each other\n\nThis implicitly handles the assumptions of the CLT too. Here measurements are the random variables. Measuring the same process implies the underlying distribution will be the same (assuming its a stationary process  doesnt change in time). The fact that the measurements dont influence each other implies independence of the RVs. \n\nNote that the average is just a sum divided by the number of data points, so its also normally distributed too with different parameters than the sum', 'Consider the shape of the sampling distribution of the sample mean of n iid observations. Under very mild conditions, as n gets big this will completely concentrate at the population mean (it will just look like a big spike there!). That is the law of large numbers. Now, if you modify the x-axis values to ""zoom in"" on the population mean (so that your x-axis is centered on mu and has range on the order of n\\^{-1/2}) then that big spiked distribution will look more and more like a gaussian distribution as n gets large. This latter fact is called the central limit theorem (and is true under quite mild assumptions).\n\nI think the ""zooming in"" part is something that people very very often miss. The CLT is sort of the ""second order term"" in an expansion where the LLN is the ""first order"" term (or maybe the CLT gives the first order term, and really the LLN gives the zero-th order term)', 'What you stated there is not any of the CLTs, nor is it true.\n\nIf you care to actually state a CLT you must choose a specific one, give the conditions under which it applies (i.e. what was assumed in the proof) and then state what was proved.\n\nIt looks like the classical CLT comes closest to what you\'re after.\n\nBut be warned.\n\nIt says nothing whatever about any finite sample size. If you want to make a claim about n=50 or any other finite sample size, you\'ll either need another theorem or some other evidence for your claim, either sufficient to count as a proof ... or you\'d need to add enough disclaimers to differentiate what you did show from what you didn\'t. If you want to make a fairly vague  point about what usually tends to happen to the sampling distribution of  sample means for finite sample sizes, ... that\'s not the Central Limit Theorem.\n\nWhat you state there in the first paragraph is demonstrably false. (The second paragraph is explanation of an implication of it - were it true - not a statement of a theorem.)\n\n\n\n----\n\nEdit (was on phone before, had to wait until I could get to a decent keyboard for this bit): \n\nLet\'s attempt to translate an actual CLT into words before we worry about trying to make it any more concise. We\'ll take the classical CLT in mean form. \n\nAn actual statement of that CLT is more or less like this:\n\n> Let Y, Y, ..., be an infinite sequence of  independent, identically distributed random variables, *with mean  and  (finite) variance ^(2)*. Let Z = (-)/(/n), with cdf F. Then in the limit as n, F (the cdf of a standard normal).\n\n(by comparison, see https://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT and its reference, Billingsley... I stated it somewhat differently, because I wanted something that (a) translated into words easier and (b) was easier to relate to what people try to talk about when they mention the CLT, but otherwise it should be equivalent)\n\nAttempt to translate the above into brief words without doing too much violence to it: \n\n> As the sample size increases to infinity, the distribution of a standardized sample mean of independent, identically distributed random variables *with finite variance* converges to a standard normal distribution.\n\nThat\'s pretty concise and not so far from precise. \n\n[There are other CLTs that relax the identically distributed assumption or the independence assumption.]\n\nAttempt to say something with practical implications that\'s not actually false (but isn\'t the CLT):\n\n> As sample sizes become sufficiently\\* large, the distribution of sample means (sampled under conditions that produce independent, identically distributed values) should be close to normal, provided the population variance is finite.\n\nThis is essentially true, but not much use on its own because we can\'t pin down ""sufficiently"" nor ""close"" from the CLT. \n\nWhat does the actual theorem from earlier tell us about sample means at n=50? Or n=30? or n=127? Of itself, nothing. Literally nothing. \n\nHere\'s an example *where the above stated CLT definitely holds*\n\nhttps://i.stack.imgur.com/741J0.png\n\nThis is a histogram of simulations from the **log** of the sum of a sample of  50,000 observations (the histogram of the log of the means would look exactly the same, only the numbers on the x-axis would change). If this histogram had looked normal, it would indicate that the collection means had approximately a *lognormal* distribution, which would itself be distinctly right skew given the standard deviation on the log-scale. But even its log is too right skew for that. So means of 50,000 values can still show strong - even extreme - right skew (much more right skew than a corresponding lognormal), even when the CLT holds perfectly. Even sample sizes in the low millions are not sufficient for this example. This is not some isolated case; a plethora of other examples are perfectly possible.\n\nWhat would it take to say something about what ""sufficiently"", ""close to normal"" mean and relate it to sample size? Another theorem. \n\nThe Berry-Esseen inequality does say something along these lines, but it relies on knowing something about the third absolute moment of the parent distribution to say something concrete about *how close* we get. In a practical sense, unless you restrict the distribution *shape* in some particular sense, you aren\'t going to be able to guarantee a useful practical outcome (you can\'t make a useful general statement without doing that). \n\nWhen will you know with a high degree of confidence conditions on the shape are met when you don\'t know the population distribution? Sometimes, perhaps, but not all that often.\n\nMuch later edit: even were we in a situation were we do have an explicit bound on |F  |, at finite n, that doesn\'t grant us every property of a normal distribution. (a) What we do get: If we\'re looking up z-tables to get a tail probability, it tells us how large the absolute error might be, which is potentially useful in those situations (though in many cases a relative error would be more like what we want practically, so we can say something like \'with at least two figure accuracy\'). (b) An example of what we don\'t get: consider that for any finite bound on the absolute difference in cdf, |F-G|< there\'s a distribution F (fairly easily constructed) which has infinite variance but is within  of G. So if you\'re relying on F *behaving like* G in a broad sense beyond the cdf-distance, such finite-sample bounds may not generally be much use.', 'The exact statement, if you look it up, is pretty concise itself and it can\'t get more concise without losing information. If you want a simplistic ""intuitive"" statement, I\'d say something like:\n\n""Sums of IID observations have the normal distribution""', 'The Sample average of iid samples, scaled by its variance, with mean zero, converges in distribution to the standard normal distribution.', 'Just about the most concise I can come up without saying something somewhat ambiguous or outright incorrect: \n\nThe sample mean of random variables with shared finite first and second moments is asymptotically normal.\n\nEdit:\nEven still I left out information on what normal distribution and what the convergence rate is in trying to be concise.', 'The means of random samples of a population will converge towards a normal distribution, regardless of the distributions of the samples themselves or the distribution of the population.', 'Put all the dudes in a bucket. Grab 10, find the groups average height. Put them back in the bucket. \n\n[if the audience is un-statsy, I add this bit]\n\nNow that first grab could have been 10 NBA players which isnt going to look like average height dudes. So we have to do it again. Next time it could be 10 jockeys. So we do it again. \n[end]\n\nWe do that process enough times and then look at our averages as a new bucket, the average in the heights bucket will be the same as the average of the guys in dude-bucket. \n\nSo we dont have to measure every dude. But we can measure a bunch of groups of dudes and get the same answer.', 'Repeated trials fall within a bell curve', 'Your take on it is really good, what are you looking for an ELI5 version?', 'The averages of averages is normal', '[deleted]', 'If your observations are iid then the mean converges to normal.', 'sample # go up, approaches normal model', 'The distribution of the means of *n* i.i.d. random samples tends to approximate normality as *n* increases.', 'There is no cutoff where it ""works"".  The goodness of normal approximation depends on four things\n\n * finite variance, otherwise it does not work for any sample size,\n\n * the sample size n, the larger the better,\n\n * the population distribution, the more skewed the larger n has to be, the more heavy tailed the larger n has to be,\n\n * the question being asked: approximation error is absolute not relative, so when software prints P = 1.7 * 10^-11 this really means something like P < 0.001 so all such spurious accuracy is highly misleading to naive users.\n\nSo it\'s complicated.  The only simple statement that is valid is that assuming finite population variance the sample mean of an independent and identically distributed sequence of random variables is approximately normally distributed for sufficiently large sample size (and you rarely know how large unless you have done simulation to check).\n\nAlso there are CLT for not identically distributed (Lindeberg\'s condition) and not independent (CLT for stationary stochastic processes, Markov chains, and martingales).\n\ntl;dr it\'s complicated', 'What is your audience? \n\nI would probably explain the CLT to my undergrad business students taking their only statistics class differently than I would explain it if asked by a thesis committee member.', 'Averages tend to be normal.', 'As you see from this thread, there are many ways you can go about providing a concise definition for the CLT. The important part though is being clear as to WHY the CLT is important. As [Berkeley\'s Data 8 textbook](https://inferentialthinking.com/chapters/14/Why_the_Mean_Matters.html) states: ""If a property of random samples is true regardless of the population, it becomes a **powerful tool for inference because we rarely know much about the data in the entire population**."" Your second bullet starts getting at this.', 'A good ELI5 doesn\'t need to be right, but it does need to avoid being misleading. Which is tough, because as this thread shows the CLT is very easy to misunderstand (not to mention the existence of multiple different CLTs which are similar enough that most people don\'t even know there\'s a difference).\n\nMy best attempt at explaining the CLT would be something like this:\n\n>Suppose we want to measure the average height of people in the population, but we\'re not able to measure everyone\'s height. Maybe we can pick 50 people at random, and measure their height, and find their average, and use that as our guess as to what the average of the population is.  \n>  \n>It\'s pretty unlikely that the average height of our sample is going to be exactly equal to the average height of the population. And in fact it might not even be close - we might pick the 50 shortest people, or the 50 tallest people, and then our guess is going to be way off. But there are a few things we can say:  \n>  \n>1. If we look at every sample we *could* have taken, the average heights will have a distribution that is centred on the average height of the population, meaning that our average *guess*, across all of those possible samples, is exactly the value we\'re trying to measure.  \n>  \n>2. The way those guesses are spread is probably going to look a bit like a bell curve - only a few samples are going to give us really bad guesses, while a lot of the possible samples will give us results that are pretty close to the right answer. And in fact, the bigger our sample is, the more the spread of the sample averages is going to look like a bell curve.  \n>  \n>Because of this, we say that if our sample is big enough, then the distribution of the sample average is approximately a normal distribution, and we can use our knowledge about the normal distribution to create estimates of what we think the population average actually is.\n\nThis glosses over a lot of details (like the relationship between the sample size and population size, and the specific properties of the population and sampling distribution, and what ""big enough"" even means), but I think it gets across most of the important parts.', 'Why is it _mostly_ correct?', ""I like where this is going. I'll take a shot at making it more accurate.\n\nEven if it's far from normal, the average is closer to normal. Especially if you keep averaging.\n\nNot as concise unfortunately :/"", 'n independent shocks? Did autocorrect get you?', 'Aka the Quincunx I taught with that for many years.  Love the one at the Boston Museum of Science!', ""Ah, that's a good point about the average affecting the distribution only in a constant way. Thanks for that!"", 'This is very concise', ""this feels unnecessarily pedantic. There are plenty of quantitative versions of the CLT which give finite sample results, and don't require scaling; ie. slightly more formal versions of\n\nP(\\\\bar{X} > x) = P(Z > x) + o(n\\^{-1/2})\n\nwhere \\\\bar{X} is the sample mean of n iid obs with finite variance and Z is a gaussian RV with mean and variance that matches \\\\bar{X}, and x is any fixed real number; and for distributions with finite absolute 3rd moment we can bound that error term explicitly (eg. an unscaled version of berry-esseen).\n\nThat said, I do agree that most of what people wrote is not correct in important ways. In addition, it is amazing to me the number of people who don't understand how the LLN and CLT engage/compare (and why the most common statements have that \\\\sqrt{n} pre-factor... or alternatively, why my statement above needs to have an error of o(n\\^{-1/2}) rather than O(n\\^{-1/2}) in order to be meaningful outside of just restating a quantitative LLN in a very clunky way).\n\nps. I was not one of the downvotes. That said I do think your comment was poorly received because it isn't particularly helpful for someone trying to build intuition. Obviously the original poster could have looked up a formal definition of the classical CLT, and that is clearly not what they were looking for. (edit --- it looks like you made your response more cordial and useful since I posted)"", 'Okay, if you happen to have a few minutes to look at this (no worries if not), I wonder if you can comment on the way I explain the CLT to my students (intro stats students in psychology; none of them are interested in any math whatsoever). I am definitely relying on the ""classical CLT,"" apparently, though I can\'t imagine any situation where these students would even think about another one...\n\n>If we were to draw many samples from a population (actually, an infinite number of samples), each sample with the same N, and calculate a mean from each sample, then certain things will be true of the resulting distribution of means:\n\n> -- It will be more normal than the parent distribution\n\n> -- As N gets larger, it will be even *more* more normal\n\n> -- The variability of the sampling distribution will be smaller than that of the parent distribution\n\n> -- Specifically, the SD of the sampling distribution--the SE of the mean--will be the parent distribution\'s standard deviation divided by the square root of N\n\nAnd that\'s pretty much it. I realize I\'m missing the ""finite variance"" piece from your explanation, so that is already noted. Is there anything else you\'d change or that I\'m not being accurate about? I\'m definitely aiming more toward applied data analysis than deep theoretical understanding, but I don\'t want to be teaching inaccurate things.', ""Not true. Also not actually the CLT. \n\nCheck the law of large  numbers which tells you what means converge to and under what conditions\n\nedit: To clarify; it's not true, because (i) the sample mean converges to a constant, under fairly broad conditions; if you want to talk about convergence to a normal, you need to standardize the mean\\*, and (ii) you can't say 'regardless of the distribution' unless you exclude the cases where it doesn't work.\n\n---\n\n\\* or at the least, subtract the mean and multiply by n"", 'This is incorrect.', ""> Your take on it is really good, \n\nIt's a pity it's not the CLT and that its also also untrue. Outside of those two tiny problems, sure."", 'The distribution* of averages is normal', 'Not what the clt says', 'I think you meant the sampling distribution of the mean approaches a normal distribution as n increases.', 'Not so', ""This is a good point, and I think one of my stumbling blocks with it is that, OK, that's nice to know that the sum of iid variables approaches a normal distribution, but how does that help? The only thing I've been able to think of is a 95% CI around the mean (or other alpha level). Are there other useful things we can get out of CLT?"", ""I don't think it does because if you just take the average across all samples, not the averages of the guess but just the average of all samples, that will approach the mean since the average is an unbiased estimator. No need for CLT. What the CLT does provide, though, is a way to quantify the uncertainty of that estimate, i.e. give it some bounds."", 'the underlying distribution needs finite first and second moments', 'Yea statistics... ', "">Quincunx \n\nI'm not familiar with a Quincunx. How does it relate to the central limit theorem?"", 'Hay-O!', 'Hi, thanks for your reply; sorry I didn\'t see it earlier. That\'s the sort of response where we might be able to get to saying something useful. Yeah, the initial post was not as helpful as I wanted to make it but it\'s hard to do much from my phone and all I could see was roughly ten comments, none of which came sufficiently close to discussing the CLT, which is what the post claimed to be about. We should at least be able to use the right words if we want to discuss the ideas.\n\nIf any of the below sounds combative, I apologize, it\'s not my intent; I\'m trying to figure out what this gains us.\n\n>  ie. slightly more formal versions of\n\n> P(\\bar{X} > x) = P(Z > x) + o(n^({-1/2}))\n\nI\'ll certainly grant you the o(n^(-1/2)) convergence under suitable conditions, and I\'ll grant that that says something that the classical CLT I mentioned did not. (Given your disclaimer about \'more formal versions\', I\'m trying to stick to the substance of your point.)\n\nHowever, I\'m not sure that when we really examine this closely, we\'ve really improved anything beyond what I say above.\n\nClearly this (or a suitable equivalent) doesn\'t yield something like the ""n=50"" in the original post. Nor does it say something concrete  about *any specific sample size*. Isn\'t my point about ""tells you nothing about any specific sample size"" then correct?\n\nConsider my example with the histogram in the edit above -- it\'s a case where we certainly *are* in a situation where that  o(n^(-1/2))  applies. What actual use was it there? .\n\nDoes that o(n^(-1/2)) really grant you anything beyond what I said?\n\nUnless we know some more about the distribution, we just don\'t have any decent bound on how bad that will be\n\nSimilarly with my other statements/claims -- what do you get with stronger versions of the CLT that actually makes a substantive difference to them?\n\nClearly there\'s a problem with the original post. \n\nIf you try to write something in words that\'s not demonstrably false by counterexample, what can you write that\'s markedly distinct from what I wrote, without invoking something like say Berry-Esseen (so we can bound the n^(-1/2) bit, though we\'re no longer talking about CLT then), and even then, what actual use is it for the sort of situation the OP was hoping to use it for?\n\n> that is clearly not what they were looking for\n\nThat\'s kind of the point. I\'d say that two principles need to be applied when bringing up this material:\n\n1. If we\'re not talking about CLT, don\'t call whatever you are talking about ""the CLT"". For example, talk about ""finite sample behaviour of sample means"". \n\n  [This is not OP\'s fault, but the fault of the *huge* number of ""no-mathematics"" textbooks that simply misrepresent things. But it does worry me that *so many* of the responses chimed in without any of them pointing out any of the problems. *Something* needed to be clarified.]\n\n2. If we are going to talk about ""finite sample behaviour of sample means"", don\'t claim things you don\'t have evidence for.\n\nWhen textbooks, websites and videos fail on both counts (which so very many do), I claim they\'re misleading the students that are stuck with using them, in ways that often lead to poor choices down the line.', 'I think this explanation reveals some misunderstanding on your part. And that\'s a good thing, because the reality is even simpler:\n\n> If we were to draw many samples from a population (actually, an infinite number of samples), each sample with the same N\n\nThis is precisely **not** how the CLT works. The CLT says that the sampling distribution of the mean *for a single sample* approaches normality as the sample N approaches infinity.\n\nIt is a very common misconception to think that distributions of sample statistics describe ""samples of samples"", which isn\'t actually necessary. A single statistic measured from one sample has its own distribution, and we can speak in terms of that distribution to make inference about the statistic *even with only one value of the statistic*. In simple terms, we identify what underlying distributions would reasonably produce values consistent with that statistic in 95% of cases (this is literally the exact definition of a 95% confidence interval).\n\nYour explanation was wrong because the CLT depends *entirely* on the N of the sample, and has absolutely nothing to do with the number of samples. To that end, any discussion about ""number of samples"" is extremely misleading to build students\' intuition about properties of sampling distributions. Almost always, there is exactly one sample, and that\'s perfectly fine. The distribution is a theoretical (usually unobservable) concept, but we can use even just a single draw from it to infer its properties.', ""Okay, cool, I'll come back on this when I'm near a keyboard. (edit: now posted separately. Sorry, it's long)"", 'That\'s not terrible.\n\nLet\'s focus on the independent identically distributed case (classical CLT) throughout.\n\nUp front, one big issue is that the thing you want students to understand and use is an approximate finite sample result (loosely, that sample means of large samples will tend to have a distribution that\'s approximately normal and the larger the sample, the better the approximation), but the CLT is not a finite sample result, and there\'s a gap there; you\'re trying to teach facts that are related to the CLT instead of the CLT. A second big issue is there\'s no good basis for a general rule of when the normal approximation is close enough for your purposes (purpose and degree of tolerance for approximation will vary with application, and how close you get will depend on parent distribution and sample size).\n\nI\'d suggest just not calling things that aren\'t the actual theorem ""the CLT"". Some of them might be things implied by the CLT, and some of them might be things that aren\'t really implied by the CLT, but might nevertheless be true (perhaps implied by other facts). Accommodating this would require little change, just some rephrasing here and there.\n\nThere may be some point at which you choose simplicity over accuracy in your teaching. I won\'t say that choice is necessarily bad, but in many cases saying something very close to right is not really more complicated than the more inaccurate thing people tend to say -- it just requires figuring out how to explain the more accurate thing fairly simply.\n\nFor now I\'ll focus on conveying a more accurate sense to you, and we can perhaps come back in more detail on ways you might approach avoiding some of the errors when trying to teach it; if you understand better, you\'ll have a much better chance of avoiding problems. \n\nMy explanation here will involve a tiny bit of notation in order to make more precise what exactly I\'m talking about but I\'ll try to avoid more than that and stick to words as much as possible (for all that it\'s less precise and involves replacing a few lines of mathematics with paragraphs of text). \n\nIf you get confused at any point, rest assured I have very likely been confused by it some point as well, and so have many others. There\'s hardly an error I have not made, hardly a mistaken idea I have not held. A degree of confusion along the way is extremely common when coming to grips with a lot of statistics. It may take more than one kind of explanation or exploration to get some ideas clear. This is really typical.\n\nIf I say anything unclear (very easily done), please ask.\n\n> If we were to draw many samples from a population (actually, an infinite number of samples), each sample with the same N, and calculate a mean from each sample, then certain things will be true of the resulting distribution of means:\n\nThe first few words here suggests a confusion, one so common in non-mathematical texts that it has become almost universal. I wouldn\'t mind this one so much, but it leads to other confusions down the line, common to a large fraction of students (some of whom then become researchers, or who give lectures or who write textbooks or web pages):\n\nYou don\'t need ""many samples""; the sample mean for a *single* sample has its own \'population distribution\' (the sampling distribution of the mean). \n\n[However, it is important to have a sense of the difference between the mean you\'ll get on a random sample you\'re going to take (loosely, that sample\'s mean is a random variable) and the observed value of that mean once you have the sample - the *realization* of that random variable.]\n\nAs n increases you get a sequence of finite sample sizes (n=1, 2, ...). At any specific n, you have that the sample mean (the random quantity you\'re about to get a realization of) has some distribution.\n\nThe only reason you would need to worry about ""many samples"" is if you want to *illustrate* what that sampling distribution looks like without directly computing it exactly -- by simulation. Don\'t confuse such an illustration of what the distribution roughly looks like (using many samples to take advantage of the convergence of sample distribution to population distribution ^([1]))\n\nLet\'s think about a simple example to show what I mean: if I toss a fair die, each of the outcomes from 1 to 6 has probability 1/6; note for later the distinction between the distribution over the possible values and the *single number* that actually rolling the die gives you (the random variable has a distribution that you\'re draewing from, the observed outcome is just a number). If I toss two such fair dice (Y1, Y2) and sum them (S = Y1+Y2), that has a distribution I can calculate: its a discrete triangular distribution, P(S = 2) = 1/36, P(S=3) = 2/36, ..., P(S=7) = 6/36, P(S=8) = 5/36, ..., P(S=12) = 1/36. \n\nSimilarly I can compute the distribution of the average on the two dice -  = S/2 = (Y1+Y2)/2. It\'s still discrete triangular, but the possible values for  are half the values of the sum, S. That is P( = 1) = 1/36, P( = 1.5) = 2/36, ..., P(=3.5) = 6/36, ... P(=6) = 1/36. \n\nThis is the population distribution of , and so the sampling distribution of the mean of Y1 and Y2, our sample of size 2. We are discussing the distribution of the mean of exactly *one* sample here. \n\n[If we observe it, of course, we just get a number (i.e. if we actually roll two dice and add them up and divide by 2, we just get a single value, a single draw from the distribution of , which we\'ll denote ), but we\'re focused on the population distribution that  was drawn from.]\n\nHopefully the fact that ""many samples"" is not in any sense involved there. We\'re just talking about the mean of one sample, and *we don\'t mean that we observed that sample*. We\'re looking at , a random variable and discussing its distribution, not at a single observed value .]\n\n(I am playing a bit fast and loose with the mathematical definition of a *random variable* here but the technical definition will lead to more confusion unless we actually do some serious mathematics -- that I don\'t want to bring in now.)\n\nBut now let\'s imagine we lacked the tools to do that probability calculation. We could still simulate rolling a pair of fair dice and computing their average, and obtain many such sample means. The more means we draw, the closer our sample distribution will tend to get to the population distribution of means we\'re sampling from (again, [1]). We could then draw a picture to see approximately what that distribution looked like. *That* is why you\'d look at ""many samples""; it\'s about drawing a picture, not really to do with what the term *sampling distribution* means.\n \n\n> It will be more normal than the parent distribution\n\nThis will often be the case and (given some conditions) this claim might actually be true in a specific sense, but the problem is the CLT doesn\'t give you this. If you want to say this you will either need a proof for the claim (there may well be a theorem for this) or you\'ll need to water it down slightly. \n\nI\'d just say ""will tend to be more normal ...""  -- that should be okay. \n\n> As N gets larger, it will be even more more normal\n\nWell, yeah, this is true in the direct sense that a good bound on the biggest distance between the cdf of the mean and a normal distribution function with the same mean and variance does get smaller as n grows. (I\'d stick \'tends to\' in there again but this is not so necessary if we take specific suitable senses of ""more normal"")\n\nThat this is true follows more or less from (for example) the Berry-Esseen inequality. This is probably not so important to bring up in a class, but it does illustrate that this fact is not necessarily a direct consequence of knowing the limiting distribution of the sample mean; if you have a CLT proof that *does* imply it at a sequence of finite sample sizes, then its fine to say it follows from the CLT, but otherwise it\'s more like a fact that\'s related to the CLT. \n\n> The variability of the sampling distribution will be smaller than that of the parent distribution\n\nThis is true (when the variance of the parent distribution is finite, given the i.i.d already assumed),  but this is NOT part of the CLT. This is a more fundamental fact that the CLT relies on. It follows directly from properties of the variance.\n\nhttps://en.wikipedia.org/wiki/Variance#Properties\n\n> Specifically, the SD of the sampling distribution--the SE of the mean--will be the parent distribution\'s standard deviation divided by the square root of N\n\nSure, but this follows from the variance properties I linked above -- this fact is again not something that\'s part of the CLT, it\'s really a more basic fact that the CLT makes use of. It would be a long-known and readily established fact even if the CLT had never been proved.\n\n---\n\nMost of what you\'re trying to convey there is more-or-less right, but just not what the CLT actually tells you.\n\nOne way to avoid saying ""the CLT"" a hundred times when you\'re really talking about approximation in finite samples is to explicitly say that you\'re talking about approximating the sampling distribution of sample means by normal distributions. You\'ll likely want to mention the CLT in there somewhere (since anything else they read will definitely do that) but mostly talk about the finite sample facts rather than the limiting case.\n\n\n\n---\n\n[1]: the fact that an empirical (i.e. sample) distribution function approaches the population distribution function more and more closely as n grows is true -- it\'s another theorem I won\'t bother you with right now, but it\'s what you\'re relying on when you look at a sample to try to infer something about population shape. If you look at a *histogram* rather than the sample cdf you\'re relying on another (slower) sort of convergence, but again, it does work (though the conditions are slightly less general). The CLT (and the Berry-Esseen inequality) is about the convergence of the CDF, rather than of a histogram.', 'What do mean bro. The CLT states that the convolutions of any random variable for n>>25 converges to a normal distribution.\n\nA convolution is is a random variable itself.\nFor example a convolution of 2 bernoulli is binomial a convolution of n Bernoulli converges asymptotically to a normal. \n\nThe is a convolution or random variable and its a new random variable by itself which converges asymptotically to normal distribution.\n\nAnd although the original CLT holds true for IID\nThere are other proofs such as Lyapunovs proof which proofs the CLT form non IID.\n\nBottom line is that CLT states that the random variables occurs by the convolution of a large number of random variables will always be a variable that follows normal distribution.', 'Yee', ""Yes, the fact that the sample mean is an unbiased estimate of the population mean is completely independent of the CLT, but it's also an important result that feeds into the final conclusion."", 'So convolution of IID from uniform for n going to infinity does not converge to normal?', 'Nice man', 'Sorry for the delayed response.  Its the same thing as the Galton Board, just another name for it.', 'I think Berry-Esseen easily falls under ""the clt"" --- I generally hear it referred to as ""the berry esseen clt"" (we could alternatively talk about the lindeberg CLT or stein\'s CLT that all give quantitative bounds); I agree that you always need information that you won\'t generally have in practice to know exactly how useful that bound is, but the CLT generally gives very good approximations if we care about the 0.05 tail, without an oppressive number of samples (as compared to something like semi-parametric inference, where the next-higher-order term comes from an empirical process). It is also used as justification for like 90%+ of statistical practice, so it seems odd to indicate that it tells us nothing about finite sample behaviour (it might not ""guarantee"" anything, but it indicates a whole lot! If one is waiting for guarantees though, statistics might not be the best field) \n\nTo take that a bit further, justifications that I have seen for the bootstrap are essentially all asymptotic as well. I imagine one could find a formal bound in terms of the second order term of the edge-worth series, as with Berry-Esseen for the CLT, but  that would still be based on higher order moments, and it is used in practice in ""finite sample"" situations without a complete justification based on only quantities which we can observe.\n\nAlso, as you note, my statement: \n\nP(\\\\bar{X} > x) = P(Z > x) + o(n\\^{-1/2}) \n\ngives no additional information over yours; it should be exactly equivalent to\n\nP(\\\\sqrt{n}(\\\\bar{X} - \\\\mu)/\\\\sigma > x) = P(Z > x) + o(1) \n\nwhere Z is a standard gaussian --- my rate looks faster because I haven\'t scaled \\\\bar{X}. I wrote it because I thought you were taking issue with the fact that people were talking about ""convergence of the sampling distribution of the sample mean to a gaussian"", as it actually converges to a degenerate distribution (and you need the centering/scaling to fix that), and I was just noting that one can make the other intuition precise without that somewhat odd-looking \\\\sqrt{n}-scaling.', '> The CLT states that the convolutions of any random variable for n>>25 converges to a normal distribution.\n\nNone of the versions of the CLT says that. \n\nThere\'s at least four things wrong in what you wrote there. \n\n> convolutions of any random variable \n\n1. You convolve densities / pmfs, not random variables; when you add independent random variables the density of the sum is the convolution of the densities. \n\n2. If you do convolve the densities (or pmfs)  - and hence, sum random variables - then in general you don\'t get convergence in distribution, because (among other things) the mean of the distribution of the sum will usually head off to . You have to define a process that does converge in distribution, which means you need to avoid it heading off to infinity but instead you need some form of location and scale to be constant or go toward constants, and in general summing doesn\'t work (nor does plain averaging). \n\n    If you don\'t talk about convergence, but about approximation at finite sample sizes, you can speak more loosely about convolutions of enough terms leading to approximate normality, but it\'s no longer CLT you\'re discussing.\n\n3. ""for n>>25"" ... no CLT mentions a finite sample size. \n\n4. ""for any random variable"" ... ""converges to a normal distribution""\n\n    This is not so; there are plenty of distributions for which the CLT doesn\'t hold. You need to specify some conditions.\n\n---\n\n> A convolution is is a random variable itself.\n\nI expect you didn\'t quite say what you meant there.', 'Can you show CLT for iid rvs with Cauchy distribution?', 'All moments of the uniform distribution are finite.', 'Oh, okay then we were slightly at cross purposes talking about different things; I was assuming the formalism you skipped in the statement was different from what you intended. I did complain elsewhere in comments about people talking about convergence of sample means without proper standardizing  (where I did raise the law of large numbers) but that wasn\'t really the main thrust of the first comment we\'re chatting under here, where I focused on what the CLT gives us and how it relates to finite sample conclusions. \n\nThere is a O(1/n) effect though, in that |F  | does decrease like that (which is what I thought you meant), given an additional mild condition. The problem is bounding it without invoking additional information beyond that assumed for the usual CLT that I think is intended (i.i.d, finite variance).\n\nI have to say, I\'ve never seen Berry-Esseen called ""CLT"" -- and I\'d strongly argue that it shouldn\'t be, because it\'s a finite-sample result, not a limiting (n) result, so there\'s no ""L"" there. It\'s a finite sample bound on the approximation error in the cdf, not a limit theorem.\n\nOf course, you could use it to talk about what happens lim_{n} by considering that limit, but that\'s neither its main value nor helping us get closer to what the OP wanted.\n\nIt doesn\'t sound like we have all that much of substance that we are disagreeing over. It\'s mainly some small details and minor differences in terminology.', 'Also uniform is not defined on a continuous unconstrained domain like the real numbers', ""Note thst fir tge actual CLT this does not matter, since the support of the distribution of standardized sample means  (i.e. 'range' of possible values) spreads out as n increases. In the limit it covers any given value on the real line""]"
[Q] Do I use FGLS or PCSE? I don’t want to go the RE/FE robust standard error route.,I am working on a project where I am evaluating 8 companies over 10 years. There appears to be no serial autocorrelation as prob > F = 0.92 on the Wooldridge test for autocorrelation. Breusch-Pagan / Cook-Weinberg test for heteroskedasticity shows prob > chi2 = 0.0000. Which of the above is most appropriate?,12mfzyy,EbiraJazz,1681511307.0,6,1.0,"['Why dont you want to use FEs', 'The output did not display Wald > chi2 results. The space was blank.', 'Do you have within unit variation in your DV?', 'If I understand correctly, are you referring to the unit root test?']"
[Q] What is the Power Function?,"Is the power function just the probability of rejecting the null hypothesis for a value of the true parameter?

I'm not sure why I struggle so much with this one. I think I get confused marrying this idea with the type I and type II errors. I want to decompose the power function into

*P( Reject Ho | Ho )* ***1****(Ho) + P( Reject Ho | Ha )* ***1****(Ha)* 

where ***1****(x) = 1* if condition *x* is true and 0 otherwise.",12me86z,zebrapaad,1681507900.0,3,1.0,"['There\'s nothing to ""decompose"". For a given value of the parameter, either Ho is false or it\'s true.\n\nStrictly, power is only when its false, though typically the type I error rate is included in the function to close the hole.\n\nSo power is  P( Reject Ho | Ha ). Focus on that. If needed, worry about the Ho case once you\'ve sorted that section of the curve (for an equality null that will generally be all but one point).', "">Is the power function just the probability of rejecting the null hypothesis for a value of the true parameter?\n\nYes.  Your decomposition corresponds to the whole lower row of this table\nhttps://en.m.wikipedia.org/wiki/Type_I_and_type_II_errors#Table_of_error_types\nwhen power actually corresponds only to the lower right cell.\n\nTo get the your decomposition to resolve to a single unconditional probability of getting a significant result from your test (instead of a branching path, as it does), you'd have to apply a prior probability distribution to the whole range of possible parameter values."", ""Prior to taking a mathematical statistics class I always considered the power to be the probability of correctly rejecting Ho. However, the more general power function is usually what I am required to find these days, which includes the type I error rate. I think that's the part that gives me a headache. The fact that the power is equal to the type I error rate when theta = theta\\_0 (the null value for a simple Ho)."", '> The fact that the power is equal to the type I error rate when theta = theta_0 (the null value for a simple Ho).\n\nIt\'s not strictly speaking ""the power""; power is still what it always was. You can however, define your \'power function\' to include it, but what you really have then is the rejection-rate function across the whole parameter space, but it gets called the power function (and in most cases, that\'s almost entirely accurate, since you\'re just filling in the missing point).\n\nIt\'s basically just a common (if slightly misleading) terminology. Don\'t sweat over that detail so much, there\'s lots of slightly odd terminology in stats.']"
[S] Beyond 20/20 Data Browser Alternatives,"Hello, this is a rudimentary question about data browsing software, and based on a Google/Reddit search, this sub seemed the best place to ask this question.

In Canada, we use a data browsing software called Beyond 20/20 quite regularly, as this was the default program that Statistics Canada provided data for when looking for compiled data beyond CSV Excel files.

Its functionality is mirrored the most by Excel pivot tables. It looks similar, and provides similar functions, except that Beyond 20/20 is far more intuitive to use, and the data usually pre-built by Stats Can.

I was wondering if anybody might be familiar with software that can most closely mimic this functionality, something that does the same things that an Excel pivot table would do, being able to swap different dimensions out or sort data. I've been tasked to find such software, as Beyond 20/20 may not be an option for the future for our team possibly.

I've considered SAS EG, Stata, EViews, Power BI/Tableau, and IBM Cognos Powerplay so far, with Powerplay being the closest, but we need a software that's easier to build for than Powerplay. If anybody has any suggestions, will greatly appreciate it, thanks so much.

Some links for further info on Beyond 20/20,

[Professional Browser | Crime Insight by Beyond 20/20 (beyond2020.com)](https://www.beyond2020.com/professional-browser/)

[Beyond 20/20 Professional Browser (statcan.gc.ca)](https://www.statcan.gc.ca/en/public/beyond20-20)",12maqqq,ironcub14,1681501432.0,1,1.0,[]
[Question] Wordle Tie Breaker,I play Wordle with my two sisters daily with the winner getting the correct word with the fewest guesses.  Often there is a tie.  I've suggested that the tie breaker goes to the person who got less correct (less clues) in their first guess.  As their subsequent guessing is more difficult than those who got a correct letter.  My one sister suggests that it should go to the person who guessed the final correct answer with the fewest clues at that stage as that is the harder and more difficult route. We all play the hard mode.  Who is correct?  Which version is statistically more difficult?,12m9duj,kvkid75,1681499410.0,2,1.0,"['I\'m inclined more towards the former since it seems like it\'d make sense to reward clever guessing past the first word to try and maximize information gained. Neither one is gonna be perfect since not all clues are equal (a yellow Q might be worth ""more"" than a green E for example).', 'Why not use the Wordlebot, which assigns you a skill score and a luck score?']"
How do departments view candidates who got in via waitlist? [Q],"This years admissions cycle was a complete wash for me when it came to phd programs in statistics. I applied to 13 PhD programs, and three MS programs. I had received 12/13 rejections, and got acceptances to three MS programs.

The two Ms programs (Columbia, Michigan) offered me no financial aid. The MS program at Miami (oh) provided me full funding for an MS in statistics. I accepted the offer two weeks ago, and visited the campus three weeks ago. For a long time they were my only good acceptance and I just decided I would go there because thats all I had been offered. I had mentally prepared to go to Miami, and even signed a lease for a place in Oxford Ohio. 

A few days ago I was told by an admissions committee member from a phd program I applied to that i had gotten off the waitlist and was offered admission into the program. At this point I dont even know what to do now, cause I have already mentally committed to this MS program for the last 3-4 weeks now when I thought admissions were pretty much over, yet now I get an acceptance so late.

However, I wanted to ask you all if its worth going to a school which I was originally waitlisted for. In my mind I feel it means that I wasnt really the departments first choice, and they are kinda just settling for me. Ie. The fact that Im a backup candidate feels weird, and they might already have low expectations of me coming into the program since they know I was off the waitlist. 

Am I over thinking this? What do you guys thinks? Is going to a phd program where I was waitlisted originally a bad idea?",12m63mn,AdFew4357,1681495158.0,1,0.6,"['assuming the acceptance is funded, I would strongly consider the PhD program. If you go the MS route, you will be re-applying in two years all over again (and moving, etc.). Not the end of the world but life is simpler that way. \n\nOTOH Miami is a solid school overall and Oxford is a nice little town.', ""Sometimes the best advice is the hardest to follow, but: try as much as possible to take the emotion out of your decision. Focus on your career goals and how best to achieve them. Specifically:\n\n- Don't decide based on mental commitments that you've made over the past 3-4 weeks. I can tell that you're experiencing whiplash and feeling a bit jerked around, but those 3-4 weeks are nothing on comparison to the multi-year commitment of graduate school.\n\n- Don't let the fact that you were waitlisted bother you. Most PhD programs get many, many more qualified applicants than they can accept, and they have a very limited amount of information on which to base a decision. If you are in a personal relationship with somebody who feels like they settled for you then it could affect how well they value and treat you in the future, but in graduate school nobody will know or care how you got into the program.\n\nThe fact that you applied for so many PhD programs suggests that you have judged getting a PhD to be the best way to pursue your career goals. If that is still true, then accept the offer. If your goals have changed, or if you have a concrete reason why this specific program won't help you achieve them, then consider the MS program."", ""It's unlikely that anybody at the department knows that you were on the wait list, and anybody that knows it now is likely to forget soon. It's important to you but other people are not really that focused on it.\n\nMy advice is to ignore the wait list issue and make the choice according to any other factors."", 'I got in off the waitlist at my department. It literally does not matter.', ""Once you're in I doubt most people will spend effort worrying about how exactly you got there. The ones that know are unlikely to care. \n\nThat doesn't constitute advice about what to choose -- that's a matter for you -- but it does seem like you're overthinking it."", '> Is going to a phd program where I was waitlisted originally a bad idea?\n\nIt depends on your familiarity with the department & faculty research emphasis. There will be a rotation option for you if you are undecided about your potential supervisor. If funding is not a concern, that Ph.D. option should still be on the table.\n\nWorst-case scenario: you could bail yourself out with a master if everything went south.', 'Part of the reason Im hesitant is cause my interests with regards to what I want to research are literally all over the place. I have phases of time when I get interested in a specific subtopic or research area and then nothing sticks. Around the time I applied I felt like was interested in biomedical imaging and thus the PhD program I got into has lots of people doing neural data analysis (neuroscience) related stuff. But now that was 4-5 months ago and fast forward to today Im interested in time series and quant finance related topics and have been reading about options and derivative pricing theory. \n\nPart of the reason Im wondering if I should go to the MS first at Miami is so I can get a more narrow vision for what it is I want to research. I mean if this quantitative finance interest persists by the time I graduate with MS I would have probably applied to econometrics/OR focused phd programs. This has also been on my mind.', 'Thanks for this. I 100% agree that I should definitely take the emotion out of it. I said this to another commenter above so I will paste it here again:\n\n\nPart of the reason Im hesitant is cause my interests with regards to what I want to research are literally all over the place. I have phases of time when I get interested in a specific subtopic or research area and then nothing sticks. Around the time I applied I felt like was interested in biomedical imaging and thus the PhD program I got into has lots of people doing neural data analysis (neuroscience) related stuff. But now that was 4-5 months ago and fast forward to today Im interested in time series and quant finance related topics and have been reading about options and derivative pricing theory.\n\nPart of the reason Im wondering if I should go to the MS first at Miami is so I can get a more narrow vision for what it is I want to research. I mean if this quantitative finance interest persists by the time I graduate with MS I would have probably applied to econometrics/OR focused phd programs. This has also been on my mind.\n\nWhat do you think about this? I know its 2 years wasted I guess but is it worth it to sort out my interests?', 'I was the same  I went PhD route because you might end up spending an extra year doing MS separately. \n\nI dont think any program reasonably expects PhD students to come in with a fully-fleshed idea of their interests  what if they change?\n\nNCSU does an en route MA and MS-holders begin with Year 2 coursework  4 + 2 years already = 6 years, as compared to the usual 5 out of Bachelors.', ""I'd argue that very few incoming PhD students have a realistic idea what they end up doing research in. A LOT of it depends on which faculty is taking students when you start doing research and what topics they are interested in. If you really want to do topic X and everyone in the department only works in Y, then you aren't writing a dissertation in X (at least if you want to graduate). I started my program interested in Econometrics and ended up doing.... computational systems biology. As it turned out, the math was mostly the same (an underlying continuous process typically observed at discrete timepoints), but Systems Biology was where the funding was in the early 2010s.\n\nIn your first two years, your focus should be on forming relationships with faculty members and learning the fundamental research tools necessary. If you do those things well, a dissertation-able (I'm trademarking that adjective) topic will ordinarily present itself over time."", 'It\'s natural to put a lot of emphasis on what you\'ll be working on in graduate school, but this is actually backwards. They key to making this sort of decision well is to figure out what sort of job you want and then work your way backwards.\n\nIf the job you want is ""tenured university professor"", then you need to get a PhD. Unless you are certain that the department that offered you a spot in their PhD program is completely wrong for your interests, that is going to be the fastest way to achieve your goal. You\'ll have a year or two to take classes and figure out your interests, and if during that time you realize the program isn\'t right you will be in a good position to transfer to another department or perhaps even another university.\n\nIf the job you want is ""hedge fund manager specializing in options pricing"" then you need industry experience. You might find eventually that you need a graduate degree in order to get the right kind of experience, but odds are the fastest way to reach your goal is to get an internship at an investment bank.\n\nAnd so on. I\'ve seen a lot of people flame out of grad school because they realize half way through that they don\'t actually want the jobs that their degree would unlock, and that sort of mistake is often avoidable by putting some thought up front into what you want to do, and finding the fastest path to that goal.', ""You can get that vision during the program, it's not expected (and would be quite unreasonable to expect) PhD students come in fully prepared for research, particularly from a bachelors. If you want to go the PhD route eventually, do the PhD program now. PhD acceptance doesn't hang on trees.\n\nAs for being waitlisted, nobody will care whatsoever. Everyone understands admissions is a bit of a shitshow and barring few exceptions is little better than intuitive guess work."", 'Im also worried about my math background/comfortablity with pure mathematics. I have done very mediocre in my undergraduate analysis classes (B range) and Im worried I will be behind with not having measure theory before hand like some people have.', 'This is a great point. If you were to ask me if I want to be a tenured professor, Id say that thought has never popped into my head. The job I want is to be a quantitative researcher working on statistical/mathematical modeling of financial assets in banks/funds. Thus Ive realized that maybe the internship/industry experience is necessary for me over a phd.', 'But my issue is just my overall motivation for doing a phd doesnt make sense anymore. I want to be in quantitative finance. Everyone on that subreddit says you dont need a phd for quant, and for the phds who are quants, they did it with the sole goal of doing research, not becoming a quant. So its like Im wondering why my motivation to do a phd is worth it if I want to be a quant in the industry.', ""You may need to work a little harder once you get there, sure.\n\nI had Bs as an undergrad in both real analysis and metric spaces. Somehow I still finished grad school.\n\nMedical schools don't expect you to be a doctor when you show up. They are there to teach you how to be a doctor. Same goes with PhDs."", 'I see. So your advice in my case is, go with the phd, I can figure out the things I need to in those first two years, and leave if I want, rather than go the pure MS first and then reapply?', 'My personal .02 is that life will probably be simpler that way from a purely academic standpoint. Completing your PhD in a department that you\'ve started in substantially de-risks the process because you\'ve spent your first two years getting to know the faculty, understanding the qualifying/candidacy process, and gathering research ideas and who potentially could advise you on what. You will get to know students a few years ahead of you that are completing the process and learn from them how they got to that point. In a department with only an MS and then transferring, you do not have those opportunities.\n\nThere are other reasons that you may choose to go to Miami over that program (lifestyle, closer to family or a significant other, high likelihood you will settle in Ohio/Indiana/Kentucky in the future, whatever). Ultimately there are many possible reasons.\n\nI faced a similar choice between U Chicago/Univ Washington (that applied to as an MS student) and other PhD programs I got admitted to. Sure U Chicago was prestigious but when I toured campus all the students literally looked like they hated their lives. Seattle looked nice but I didn\'t know anyone there and it was a bit daunting moving somewhere completely new and not knowing anybody there. So I took a PhD acceptance at a ""lowly"" top 50 or 75 school where I had friends in that city. \n\nLife was much simpler just diving straight in with the knowledge that I could leave after two years if it wasn\'t working. And 10 years into my career nobody gives a shit where my PhD came from (with the caveat that I\'m not an academic). The only thing that ever comes up in interviews is (i) the fact that I have a PhD, (ii) vaguely what areas my methodological strengths are in, and (iii) my industry experience.', 'I see. Is mastering out of a program as painless as your making it?', 'Well see heres the thing. The issue Im having is the department that accepted me, I had originally at that point of my life when applying thought I was interested in biostats, and applied to the dept cause it had lots of faculty in neural data analysis and biomedical imaging. Now I realize I like financial applications more and this department cant support those interests of mine.', ""Well like, how do you know you'll be interested in Finance continuously for the next 5 years of a doctorate. I didn't know what my research interests were when I got into my PhD (none of my friends did either), and even when I picked the area and advisor, what I've been working on has changed like 3 times as I've gotten a better understanding of the literature.\n\nReading your comments, I think you should do the doctorate. If you get your master's a lot of PhD programs will make you redo all of their core classes for necessary credits. So you run the risk of this doctorate taking 7 instead of 5 years which you should not do under any circumstances and at best, it really limits yourself to departments that let you transfer classes.\n\nNobody cares if you were waitlisted, no one cares if you are uncertain about what you are interested in, what you have to do is make the best decision you can given the information you have access to. It seems like you are dead set on doing a PhD, so go do the thing that get's you a PhD, not an MS."", 'The thing is I *thought* I was deadset like 4-5 months ago. But now Im realizing I dont know for sure. Because ultimately I want to end up in industry (finance specifically, like quantitative finance). I know I want the MS for sure because the minimum is MS to be qualified for those jobs I want.']"
[question] I can’t remember the name of a property,"Hi all, so there a property in statistics where if you do for example the following hypothesis testing:

Does the name of person affect their height (which is obliviously nonsense)

using 3 names only eg John, jack and James and fail to reject the null hypothesis then you can generalise for all the names. 

Whats the name of that property?",12m5arq,Emergency-Agreeable,1681494245.0,0,0.5,"[""I have no idea what you're getting at here, and even your example seems unclear -- I bet Jill, Joan, and Jeanne are indeed shorter on average than John, Jack, and James (to say nothing of Juan, Jamal, or Ji-hu).  Are you asking if there's something special about a sample size of 3 in particular?  It's generally not going to afford you a statistically-reliable conclusion about anything.  You'd at least want multiple examples of each of Johns, Jacks, etc to say *statistically* if there's a difference between guys with those names."", '> Whats the name of that property?\n\nSounds like ""poor reasoning"" is a fair description.\n\nYou have two big flaws there --\n\nThe first is you didn\'t provide a basis to generalize (you\'d at least need to be relying on some form of random sampling of the population of names to make arguments like that, but it\'s not going to be much of an argument unless you sample a large fraction of ""names""; you had neither random sampling of names nor did you look at many names; maybe you just missed the names where the effect was obvious)\n\nThe second flaw is absence of evidence of an effect  doesn\'t establish absence of the effect even in the names you *did* sample. It just means your sample size was too small to find it.', 'I am a bit confused about the question but, \nAre you talking about that in frequentist statistics \n\na) one refers to the underlying sampling distribution, and such the inference that one can make is only available in terms of a long run frequency? \n\nb) The p-value only provides evidence on rejecting the Null hypothesis and not any statements (evidence) w.r.t the alternative hypothesis? \n\nc) correlation is not causation?', 'I agree with you its unclear. John, Jack and James in that example are not individual points but categories/factors. \n\nThe Qestion is does then name of men affect their height?\n\nAnd the way to answer that is to sample the height \nFrom 100 Johns 100 jacks and 100 James \n\nIf you compare the means using one way anova you will find that the categorical variable namedoesnt affect their height.\n\n\nHowever in practice to be true to your results you would need to test all the other names out there as well however its obvious that you wont get different results. And that ability to generalise has a name, what is it?\n\nEdit: 100 is arbitrary n,k and j would do as well', 'Hi mate, so when I was in uni we were given a data set with two features one was the blood pressure of a person and the other was the name of the person. The name category had 3 factors in it randomly selected, the sample size was big. All them were males and citizens of the same country. We were asked to check if the blood pressure is affected by the name. As you can imagine the null hypothesis was not rejected. Once that happened the professor said that we can generalise our conclusion to all the male citizens of that country. And the conclusion is that name of the person doesnt have an effect on their blood person. \n\nThe test we did was anova and all the assumption held true.\n\nNow this intuitively makes sense to me. There is no way the name would have an effect in the blood pressure, so generalising that by having only tested 3 factors seemed alright. Does that make any sense to you?', 'Hi mate, so when I was in uni we were given a data set with two features one was the blood pressure of a person and the other was the name of the person. The name category had 3 factors in it randomly selected, the sample size was big. All them were males and citizens of the same country. We were asked to check if the blood pressure is affected by the name. As you can imagine the null hypothesis was not rejected. Once that happened the professor said that we can generalise our conclusion to all the male citizens of that country. And the conclusion is that name of the person doesnt have an effect on their blood person. \n\nThe test we did was anova and all the assumption held true.\n\nNow this intuitively makes sense to me. There is no way the name would have an effect in the blood pressure, so generalising that by having only tested 3 factors seemed alright. Does that make any sense to you?', 'Are you thinking of the term *representative*? \nIf those names are representative of names in general then you could treat them as a random sample of names, say via random effects modelling, but then the hypothesis you are testing no longer makes sense.', ""Are you asking about *induction*?  You've seen 3 white swans and conclude that no swans are black?  It should be obvious that your conclusion about the lack of difference in average height between Jacks and James's may *not* correctly generalize to Jills and Ji-hus, so there shouldn't be a principle saying it will."", 'I\'ll grant you the random selection (though I might have multiple quibbles about how it would probably be done).\n\n>  Once that happened the professor said that we can generalise our conclusion to all the male citizens of that country\n\nThis is still something that I am taking issue with. Three is so small that if there was a strong effect in say 10% of the population of names (and not much to speak of within the remainder), this would be very likely to miss them all. Even if it were larger (say 100 names), you wouldn\'t use ordinary ANOVA, you\'d want a random effects model.\n\n> There is no way the name would have an effect in the blood pressure, so generalising\n\nThis form of reasoning sounds like a mix of *petitio principii* (begging the question/assuming the conclusion)  -- ""given that it can\'t be true, it\'s reasonable to only check 3 names"" -- and argument from incredulity -- ""I can\'t see how it could be true, so I\'ll proceed as if it\'s false"".  \n\nhttps://en.wikipedia.org/wiki/Begging_the_question\n\nhttps://en.wikipedia.org/wiki/Argument_from_incredulity\n\nAnd lastly, failure to reject a null is still not a demonstration that the effect is absent, even among the three names you looked at. An equivalence test would be a better approach here I think (at least showing the effect is small among the names considered, rather than absent).\n\nIt may have been an interesting motivating example (and so useful from a teaching perspective) but it\'s very weak sauce as far as establishing a fact.\n\nI doubt there\'s any connection myself (I agree with you on that), but if you want to demonstrate something you doubt isn\'t true, you don\'t start by leaning into that bias; rather you should do the opposite. Imagine it *is* true, but hard to find (that you\'re not just lucky enough to pick out some names where the effect is clear). How do you make reasonably sure to find it if it\'s there somewhere?\n\nMy prior thoughts have been demonstrated wrong enough times that for most things I don\'t tend to assume they\'re true, or only hold them pretty weakly.', 'Thanks for you response. Its exactly what I was looking for. The reason I asked for that is it never sat well with me and I was wondering if there is a property justifying that. Looking back I presume the prof made a light hearted comment or I dont remember something correctly or he was just wrong.\n\nI have made another post displaying the problem as a hypothesis testing. Where I asked people if they had this hypothesis:\n\nDoes the name of males affect their blood pressure\n\nHow they would approach it. This time instead of indicating the solution I left it open ended to see peoples thoughts but no one gave an answer about how they would solve it.  I guess your take on the solution is using 10% of the population of names a decent sample size of blood pressure for each name and a random effect model?\n\nEdit: if you have time have a look on my other post.\nThe main comment from the people is omitted variable bias although I dont indicate any approach which I find odd.', '>  I guess your take on the solution is using 10% of the population of names a decent sample size of blood pressure for each name and a random effect model?\n\n10% is  really just an example. If we started with some vaguely plausible thoughts on ""imagine I\'m in *this* situation, how much I bear to miss, and with what chance"". It was a very very rough ballpark, and that  in turn for a made up instance.\n\nIn short, you\'d need to do some proper thinking to figure out whether you really needed 8% or 18% or perhaps 95% for your specific circumstances (what you can afford to miss, which depends on the real problem). Or indeed whether you needed a percentage of the total rather than some other specific function of sample size. I just can\'t come up with a plausible situation where ""three"" will give us sufficient grounds to generalize at all.']"
Sample Group Accuracy [Q],"I have this nagging question and it seems like this group is an appropriate place to ask it. In politics, it always seems like people have statistics to point to to criticize or justify decisions, ""85% of Americans support blah-blah-blah"". Yet, of course, it's certain that not all relevant Americans participated in the poll, nor probably did most of Americans.

So, when pollsters conduct a poll, is there a certain sample group size in proportion to the population where they can be reasonably confident of an accurate poll? If so, how does the theory behind that work?",12lvwnx,ancalagon777,1681475995.0,3,0.81,"['Great question!', 'The (quite interesting, perhaps counterintuitive) reality is that statistical accuracy depends entirely on the raw sample size, **not** on the proportion relative to the population size.\n\nThere are a few ways to consider this:\n\n- Some populations are practically infinite but we can understand them fairly simply with relatively few observations. For example, consider the set of all coin flips that will ever occur. We can never sample a meaningful percentage of this population, but it doesn\'t take long to see that the percentage is roughly 50/50. Other distributions may be slightly more ""complex"" in nature, but the same logic still applies.\n\n- Think about the population distribution as a law of governing percentages. ""50% of observations tend to be in this region;"" ""1% of observations follow this abnormal behavior;"" etc. Now, when we take even a small sample, what\'s the probability that ALL of them follow some abnormal pattern? E.g., if I sample 50 people, what\'s the probability that all 50 come from the ""abnormal 1%"" part of the distribution? Basic probability tells us that would be (0.01)^(50), which is a function of the 50 units sampled, NOT the size of the original population. Hopefully this line of thinking can illuminate why sample size is the important quantity for uncertainty, not population size.\n\n- Large population size fortunately has no effect on inflating our uncertainty, but we can actually use *small* population size to *narrow* our uncertainty. For example, if the full population is only 100 units and I sample 99 of them, I can be *extremely* confident in my estimates because there is only 1 single unit of uncertainty left. There is a correction factor that describes the reduction of variance called the [Finite Population Correction](https://en.m.wikipedia.org/wiki/Standard_error#Finite_population_correction_(FPC\\)), but notice that it approaches 1 if n << N (i.e. multiplying by 1, which has no penalty on the estimator). If the population size were much larger, then I would follow the logic of the second bullet point above, which says that variance is directly proportion to n (of the sample).', 'So in that case, a political poll must strive to sample as many people as possible? I mean, if it is motivated toward painting an accurate picture, one might say ""the more the merrier.""\n\nAnd on the other hand, a poll which is seeking to confirm bias can be highly selective, and thus persuade those who are un-critical of its methods. \n\nMy hope was that there was some kind of statistical ""plateau,"" but I guess given the diversity of human thought that\'s some kind of magical thinking. Regardless, this was a great, informative answer. Thank you!', '> So in that case, a political poll must strive to sample as many people as possible?\n\nThis is as close to ""universal law"" as anything in statistics :) yes, larger sample sizes are always inherently more reliable (though we eventually reach a point of diminishing returns). \n\n> on the other hand, a poll which is seeking to confirm bias can be highly selective, and thus persuade those who are un-critical of its methods.\n\nSmall sample sizes are still unbiased estimates of reality. They simply carry higher uncertainty (variability). So a small sample size on its own doesn\'t help to confirm bias.\n\nHowever, if ""highly selective"" means that they are sampling *non-randomly* from the population, that can quickly and easily bias the results. I would guess that non-random sampling procedure is probably the #1 source of bias in most polling.\n\n> given the diversity of human thought that\'s some kind of magical thinking\n\nAs individuals, humans are incredibly and unpredictably diverse. In larger groups... not as much so :) luckily that helps employ statisticians!', 'The plateau happens when the sampling fraction is significantly close to 1, so for most practical purposes we tend to ignore it.\n\nIt\'s very important to note, though, that all of this is predicated on three assumptions:\n\n1. Every person in the relevant population had a non-zero probability of being selected in the sample; and\n2. We can accurately calculate that probability for anyone selected in the sample.\n3. There is a direct relation between the data we collected from the sample and the thing we\'re reporting on.\n\nAs long as all of these are true (or true enough), then the theory gives us clear ways to estimate not just the thing we\'re measuring, but also some measure of error so we know how wrong we\'re likely to be (so a survey could report that 25% of people approve of policy X with a 3% margin of error, meaning they think the true value is probably somewhere between 22% and 28%).\n\nThere\'s also plenty of theory out there to improve the quality of estimates both by (a) designing the sample to get a better spread of the population (e.g. it\'s usually better to select 100 people in each state rather than picking 5000 people randomly across the country and accidentally selecting the entire population of Lower Upper Trumpville), and (b) adjusting the estimator to correct for meaningful differences between your sample and the population (so if you *do* get an over-representation of people from Trumpville, you re-scale the estimate so that they only wind up representing other Trumpville residents and not people from New Bidensberg).\n\nIf you do all of this, then typically you wind up with an estimate that is either unbiased (meaning that across every possible sample you *could* have taken, the average estimate is exactly equal to the true value), or at least has sufficiently little bias that you can have confidence in it.\n\nThe places where bias can creep into an estimate include:\n\n1. A sample selection process that fails to include certain parts of the population - for example, a phone poll that only dials landline numbers will miss a lot of younger people.\n2. Non-response - i.e. people selected in the sample who do not answer the survey - particularly where there\'s a demographic difference between people who do respond and those who don\'t. For example, people who are very busy might not want to sit on the phone for a 30-minute survey; or people from various minority groups might be suspicious about responding to questions that could be used to target them.\n3. Conceptual differences between the data collected and the thing being reported on. This could be because of poor survey form design (like [butterfly ballot](https://www.smithsonianmag.com/arts-culture/redesigning-the-vote-111423836/) forms), poorly worded or [leading questions](https://www.youtube.com/watch?v=ahgjEjJkZks), or representing the statistics differently (if 20 people say ""I like it"", 10 people say ""I hate it"", and 30 people say ""I don\'t care"", you can say that twice as many people like it as hate it or you can say that only a third of people like it and neither is technically wrong).\n\nSo in reputable survey organisations, a lot of work goes into selecting a good sample and *also* collecting and reporting the right data, and if they\'re really doing their job right then also being honest about the accuracy and limitations of their statistics. Which doesn\'t stop politicians, the media, etc. from using the data to support whatever narrative they\'re interested in, because that\'s human nature.', 'This is a great answer. You speak about it as though you have experience conducting polls, is that the case?\n\nEither way, I very much appreciate this thorough response!', ""You got me :) I don't conduct the polls myself, but my background is in the nuts and bolts of this stuff (I don't work with any of the form design stuff, but I've hung around those people long enough to pick up the basics).""]"
"[D] Discussion: R, Python, or Excel best way to go?","I'm analyzing the funding partner mix of startups in Europe by taking a dataset with hundreds of startups that were successfully acquired or had an IPO. [Here](https://docs.google.com/spreadsheets/d/1AsFgEXnzId7frEmqSpuAiQV-QbTSI2eJ4DcxCicj1w8/edit#gid=2100307022) you can find a sample dataset that is exactly the same as the real one but with dummy data.

I need to research several questions with this data and **have three weeks** to do so. The problem is I am not experienced enough to know which tool is best for me. I have no experience with R or Python, and very little with Excel.

*Main things I'll be researching:*

1. Investor composition of startups at each stage of their life cycle. I will define the stage by time past after the startup was founded. Ex. Early stage (0-2y after founding date), Mid-stage (3-5y), Late stage (6y+). **I basically want to see if I can find any trends between the funding partners a startup has and its success.**
2. Same question but comparing startups that were acquired vs. startups that went public.

There are also other questions I'll be answering but they can be easily answered with very simple excel formulas. I appreciate any suggestions of further analyses to make, alternative software options, or best practices (data validation, tests, etc.) for this kind of analysis.

**With the time I have available, and questions I need to research, which tool would you recommend? Do you think someone like me could pick up R or Python to perform the analyses that I need, and would it make sense to do so?**",12lvjei,yunnospllrait,1681475185.0,22,0.83,"[""You'll be able to achieve what you want with all three tools. Assuming you've never worked with a programming language before, I'd say Excel. That way you won't get bogged down by installation, setup, learning the syntax etc.\n \nR/Python is of course better in the long term and/or if you want to something advanced, but what you're trying to do is pretty straightforward and you have a deadline, so Excel will get you what you want in the least amount of time with the last amount of friction."", 'Honestly, if you have three weeks, go for Excel **unless** the dataset is really big. Excel will start to malfunction with very large datasets.', 'Since you have time constraint, why not just use a proprietary software? Ive used JMP in the past before I knew any programming languages. It was very user-friendly. I think they offer a free trial period.\nFor the long run, definitely learn R for statistics work.', 'Just want say if you decide to R there is some point and click https://r4stats.com/articles/software-reviews/r-gui-comparison/', 'Three weeks means excel.\n\nIf you had 3 months, maybe R would be viable.', ""What I would recommend is SPSS.  That will give you the most, quickest and easiest.  If you later are doing this on a deeper and longer basis (meaning several months on up), I'd recommend switching to R.\n\nIf you are connected to a university, see if you can get a student license for SPSS or use it in a student lab.\n\nIf not, check for a free trial copy.  This won't last for long, so clear your schedule to the extent humanly possible. \n\nSearch YouTube for SPSS Tutorials.  That should get you several playlists.  Review those first, then work on your data.\n\n&#x200B;\n\nIf you have no access to SPSS (or SAS, or JMP), then look into JASP ([https://jasp-stats.org/](https://jasp-stats.org/)).  I've only just touched that.  One thing I believe is that JASP (as well as JMP) will allow/block off tests and analyses depending on the nature of each column.  This means that, for example, if you have groups A, ..., Z, the software will treat those as non-numbers, which can only be used as inputs for variables which are categories.  This is only a major problem if the categories are missassigned upon import, so look up 'variable types' early on."", 'Sample Dataset [Here](https://docs.google.com/spreadsheets/d/1AsFgEXnzId7frEmqSpuAiQV-QbTSI2eJ4DcxCicj1w8/edit#gid=2100307022)', 'If you want basic descriptive statistics quickly then Excel is helpful as all the different kinds of descriptors can be done. Python and/or R are useful and more time efficient if you know programming and the libraries or packages to use for more advanced mathematical concepts, data modeling, quicker visualizations, etc. But Excel isnt that bad and can still get the job done without the code.', ""Three weeks is easily enough to do this in R or Python, unless you have never before analysed any data, and don't know anything about the subject matter either, and are effectively learning it all from scratch. Or if despite the three week deadline, you only really have a few hours.\n\nPick R or Python and follow a beginner's guide on how to read in, transform and analyse your data, only use your data instead of the example data of the guide."", ""Excel is a terrible data tool, except for very small data sets. Avoid Excel (and other spreadsheet apps) for data projects whenever possible. The most common thing most data scientists do with spreadsheet apps is to *Save as CSV* so that we can easily import the data into R (or Python).\n\nThree weeks is *plenty* of time to learn R well enough to do some statistical analysis and hypothesis testing. That, of course, assumes that you're willing to put in the work during those three weeks. There are a ton of free & paid resources on line, textual and video, and your analysis sounds pretty straightforward, maybe an hour or two of work for someone already familiar with R (or Python).\n\nIf you can find an R tutor, you should be able to streamline your R learning. They can guide you through the programming aspects of setting up similar problems."", 'Everything will work, but knowing some R/Python is a great skill to acquire. Personally i believe that R is way more intuitive to learn, but obviously Python is more versatile if you want to use it for non Data Science projects...', 'r and ask chatGPT every step.', 'R or Python not Excel', 'If you at least know programming in one other coding language, use Python. The Pandas library running in a Jupyter Notebook is the go-to approach for processing data like this.\n\nIf you arent comfortable programming in any language, use Excel. Its designed to be easy enough for non-programmers to pick up on the job. The other two are absolutely NOT designed with that in mind.', ""I couldn't imagine trying to learn a new programming language in 3 weeks"", ""That makes sense. Thank you very much for your input! I was thinking the same at first, but then got worried talking with some colleagues that recommended python or R. I'm glad someone else can get behind excel for my specific case.\n\nDo you think I could run into trouble trying to run statistical tests in excel? From what I know you can do basic tests in excel. I don't think I'll need any test that is too complex, but I would still love to get a second opinion on that."", 'How large do you consider too large for excel? The real dataset is 6000 rows. Same column and info type as in the sample dataset.', ""Hadn't even considered that. I didn't know something like that existed. How does it work?"", ""Yeah, I was going to recommend Jamovi.  It's gui-based, easy to import data from, say, a .csv file, and it conducts common analyses.  The output tables and plots are attractive.  And it has a fair number of options for the analyses (like effect size statistics and confidence intervals)."", ""That's superuseful. I'll read through the article and see which software may fit what I need best. Is there any specific one you would personally recommend? \n\nAlso, is this similar other softwares like JMP which u/hillybillyAcademic mentioned? \n\nAnd thank you for bringing this up!"", ""Three weeks is plenty of time to learn enough R to do the OP's project (and much more). Three *days* would be easily doable for someone with experience programming in a different language."", ""This is great input. Thank you! I'll definitely look into SPSS as well. \n\nTo anyone with experience with the software, do you know if it is similar to BlueSky Statistics? I've seen that BlueSky is based on SPSS, and it is open source (free). So maybe its the better option for this case?"", ""Yup, I'm thinking from all the input that I'll be using either Excel or an alternative software with a no-code friendly GUI."", ""I see. Thank you for the input. I think right now I'm leaning towards experimenting with point and click software that uses R. If that fails, I'll probably have to use excel given the time constraint.\n\nI say this bc I don't have experience with R or Python and must also write a very long report in those three weeks. So time is of essence in my case.\n\nI definitely agree with you though, and will start learning R / Python for the long term. I'll probably start with Python given how multipurpose it is."", ""Agreed. I'll def start with Python for the long term"", ""Lol I thought about it for a brief moment. I imagined I would still need to get familiar with the tool and have a previous understanding of how it works the same way you would use chatgpt in any other programming language. However, from what I've heard R is pretty straightforward so who knows.\n\nWould love to hear everyone's opinion on this."", ""\\+1\n\nIt sucks that you're getting downvotes, because your advice is spot on."", ""I'll probably end up using point and click R software or Excel given my time constraint. But I still appreciate other input. I think from what everyone has told me, I'll definitely get into Python for the long term :)"", 'Downvoters could never figger out Python tryna predict 10000 SKUS for 10000 locations with excel', ""Honestly, I can't imagine taking three weeks to learn a programming language, unless it's the very first one learned."", 'I know both R and Python, so I don\'t use Excel ;-) But in seriousness, something like Pearson\'s or Student\'s test should be pretty straightforward to calculate ""by hand"", even if there isn\'t a dedicated function for it in Excel.', '6000 is small enough for excel', 'Generally, it depends on the performance level of your machine, but 6000 rows with the number of columns you have in your example should be fine for most PCs.', 'You will be fine', 'Excel can handle 100x more rows', ""I think excel is about 115,000 limit. You'll want to keep it simple with this project and use Excel."", ""JMP is just one of many. They're basically point-and-click statistical software. You import your data, typically in csv format, and choose a particular test you want to run. I've used it for model selection, regressions, correlations, that kind of thing. That's just a small fraction of their capability. I recommend downloading a trial version and check it out. I think you can be up and running with your data after a few minutes of getting used to the interface."", 'I never used them, or JMP.', 'I would download BlueSky first, try it out, and see how it works.  If it seems good, then go with it.\n\nThree weeks is a short time.', ""Don't. If you know what you're doing, then ChatGPT can be a good co-pilot, but to suggest that someone can use without some knowledge is like relying entirely on your car to drive itself."", ""If you read OP's post and comments, the advice clearly is not spot on."", 'Some people its from my cold dead hands with Excel', ""Sure, but that's not OP's use case?"", ""That's good to hear. Thank you so much for the guidance. I'm guessing excel is going to be it"", 'Perfect, thank you!', ""As a side note, I DO NOT recommend Excel with large datasets. I have a horrible memory of it auto-correcting some numbers into a date, and it was only discovered later because some results looked funky. Otherwise we would've never caught it."", ""That might actually work as well. I'll definitely check it out and play around with it. Thank you!"", 'Makes sense. Thanks for the input!', ""That's clearly not the attitude people in this thread are demonstrating when they are recommending the use of Excel."", 'Excel comes packaged with an add in called the analysis toolpak which you can use to run a few tests as well as fit a regression model, get descriptive statistics etc', 'Excel does like 1 million rows. Your pc might struggle before that if it is ancient.', ""If you are going to use Excel, you might also want to consider the [Real Statistics](https://real-statistics.com) add-in for Excel, which extends Excel's capabilities."", 'Especially if you have any cells with functions that update regularly (e.g. vlookup).', 'I think I should be fine then, my PC is relatively good. Thanks!', 'Will def take a look at it. Thanks!']"
[Q] P = .000 But how could such a thing happen... repeatedly?," 

I hope this message finds you well. I ran into the darndest thing, trying to do a bivariate Pearson Correlation through SPSS.

I am conducting various regression analyses on Teleworking,  Income, Education, Race during the 2020-2021 transition. I gathered my data from the US Census Bureau's Household Pulse Survey. After some  brief curation (removing variables that I did not intend to study;  limiting N to \~1,000,000 by removing respondents who were likely  retired), I fed the .csv into SPSS and it delivered a few tables where I  saw statistically significant relationships between Teleworking and the  other 3 variables (not reportable, though, correlation is too weak).  But frequently throughout my Tables, I mostly got .000 for my p value  and the occasional .001.

In the  back of my head, I'm thinking, ""Surely, this is impossible."" So, what do  you think? What did I screw up? Thank you for your time.",12ln7jz,Cmdrindie,1681454695.0,2,0.57,"['With such a high sample size, estimated error (or any value that depends on n) will most likely be very low, giving significant results even for weak correlation or associations.', ""It means that p<0.001\n\nIt's just how SPSS displays very small p-values. Only the first 3 decimal places are printed, so 0.00083, is truncated to read 0.000"", 'The p values are not actually zero, they are just rounded. You can report them as p < 001.', 'I think you can double click the value in the output and it will show you all of the decimal places', ""1. Sample size. It's always sample size.\n\n    >  N ~= 1,000,000\n\n    Yeah, that'll do it\n\n     A sample size of a million means that in the usual test of H0: rho=0, the standard error of the correlation under the null is 0.001, so a correlation of even such a tiny value as say 0.005 would produce a z-score of 5, which would have p-value of roughly 0.00000057\n\n2. However, a test makes no sense when you have a *census* of the population (or at least nearly so). The usual calculation is wrong for this case."", '1) Maybe some of the variables are obviously correlated.\n\n2) With big samples, tests become very powerful, and may be able to detect even quite weak relationships with high confidence.\n\n3) Maybe your observations are not independent, so failing to correct for that distorts the p-value.', 'Do you really have 1 million observations?\nIf you do then almost everything will be highly significant, since test statistic values will have element of sample size in them\nYou need to re think what you are sampling to find a useful aggregate', 'I kind of hate that SPSS reports p values at 0 because its impossible but theyre just very low and it rounds off.\n\nRegarding why theyre so low, check out Lindseys Paradox and this post from Daniel Lakens. At such large sample sizes, even under an effect practically equivalent to the null, the distribution of p values will be pushed towards 0\n\nhttps://lakens.github.io/statistical_inferences/pvalue.html', 'P < 0.001, very normal to see that in a paper. If you use R or something you can round out to a further decimal place. The p-value is just that small, like 0.0003 or something', 'As other folks have mentioned, the significance isnt actually 0.000, but SPSS rounds to only three decimals. The p-value is also not meaningful with sample sizes that large because increased sample sizes are always going to cause the p-value to lower. It would be helpful with a sample this significant to include effect sizes as well, because those can demonstrate significance much better.', ""I'll owksjdjemr p 4oli4ii44u4iejdjejej"", ""how do you correct for collinearity? i'd think that you can just drop one if x and y are two variables that are perfectly correlated with one another."", '>You need to re think what you are sampling to find a useful aggregate\n\nWhat does this mean?', 'Yeah I was wondering similar\n\nEither aggregate, or employ a different analytic strategy. If you do use p values, probably need to correct your alpha as a function of sample size or something', '1) Who said anything about perfect correlation?\n\n2) Why are you trying to reduce colinearity?', 'the basic assumption that you are making is that you have a random sample from the population and so the calculations are based on individuals\n\nHowever, the sample has a structure, eg, income, age, etc, probably coded and therefore categorical.  Think of this as a high dimension cross tab and each one is a ""box"" with a large number of observations.  additional observations do not add any additional information to the boxes.  For example, think of the distribution of telephone call during the day.  The millions add precision to the measurement for each timeslot on a particular day; however it is useless for a prediction of the next day as the sampling unit should be the day and not individual calls.\n\nSo, the aim is to find the relationship between the boxes.  This is a very reduced number from the 1 million that you are starting with. \n\nThis is how I would start and as I got to know my data better, I would start to look for an analysis technique that fitted my perception of how the data was shaping', 'I see - basically, dependence between the errors.']"
[Q] Is there a way to test whether maximum likelihood estimation in Kalman Filter reached a global maximum?,"I am estimating the parameters of a state-space model with the use of Kalman Filter. The problem I ran into is that the final parameter estimates are very sensitive to the starting parameter guesses. If I understand it correctly this is a feature of estimating parameters via Kalman Filter, but I was curious whether there is a way to ensure that I am getting correct parameter estimates. Since the model has quite a few parameters and dimensions it is computationally not feasible to do grid searching within reasonable intervals and with reasonable steps.

I would also appreciate any advice regarding this, as I am quite new to state-space stuff.

Also I am doing this in Python with statsmodels.tsa.statespace if that helps.",12laaq1,horux123,1681426766.0,1,1.0,"["">Since the model has quite a few parameters and dimensions it is computationally not feasible to do grid searching within reasonable intervals and with reasonable steps.\n\nIt's so costly that you can't repeat the estimation many times? That's bad for anything but a theoretical investigation of whether your estimation procedure works. Maybe you can stare at the objective function a long time and determine that it is non-convex after all. In which case your optimisation procedure might simply reach a local but not global optimum\n\nAlternatively, you could estimate a simpler model which is not as costly and try with many different stating values. Also, simulate data and estimate your model on it, see if it can recover the parameters. Once this works, make it more complicated again."", 'I am not super familiar with the matter but the answers on those threads might help: \n\nhttps://stats.stackexchange.com/questions/409496/kalman-filter-parameter-estimation\n\nhttps://stats.stackexchange.com/questions/215229/maximum-likelihood-estimation-and-the-kalman-filter', 'It may be that you don\'t have enough data to overcome your prior (conceiving the starting state and state covariance as priors in a Bayesian context). It\'s possible to set things up so that your prior in a KF is less ""informative""; if you have enough data to pin down the whole covariance matrix you will, if you don\'t have sufficient information you may have arbitrarily large variance (and possibly covariance) terms left after you have gone through the data. There\'s not much that can be done about that; if you don\'t have information in the data to estimate all your states, you don\'t. \n\nThere\'s some papers by Robert Kohn and Craig Ansley that address this sort of thing.\n\nHere\'s one relevant paper:\n\nhttps://www.jstor.org/stable/2241356', 'The original paper that proposed the model included parameter estimates for a certain data set, which I was able to recover fairly well (since I knew what starting parameters to choose). The problem is not that I am not sure whether the procedure works per se, but that on a new data set I have no way of ensuring that I am reaching a global optimum. I can rerun the estimation from a variety of starting parameters but that feels unsatisfactory, as I am afraid that I might miss the actual correct parameters by chance. That is why I wanted to know whether I can ensure that global optimum was reached at least in one of my tries.', 'Thank you, these are indeed helpful.']"
[Q] Analyzing data of a poll and frequency vs. rank,"Hello,

I hope this is the right Subreddit. Im going to do a poll with a group of friends. Were fans of a singer. Were trying to decide which are our favorite 10 or 20 songs, so I think Im going to ask the members to rank their 10 favorite songs. Keep in mind that this artist has a huge pool of almost 700 songs and I expect about 30 people voting, give or take. Normally you would award a song ranked #1 10 points, #2 9 points and so on, until a song ranked #10 gets 1 point.

Now, since we are not a huge group, if only one person lists a minor song in the poll and its their favorite (#1) it will get 10 points. On the other hand, if 4 people list one of his most popular songs as their #9 or #10 that well-liked song will get overall only 5 or 6 points, which I find unfair since they still like it enough among 700 songs. In other words, with this system it would take 10 #10 votes for 1 #1 vote.

I thought about reducing the scale, so that #1 gets 10 points, #2 gets 9.5 down to 5.5 points for #10. With this scale it will only take a couple of votes to overtake a #1 vote and a song mentioned several times will get rewarded over a song ranked #1 by a single voter.

Im not sure about the scale though. Is there a formula that I can apply so that its balanced in the correct way (maybe based on the number of votes that came in)?

Sorry if I didnt explain my problem well. I know nothing about polls and statistics, but this is bugging me. The answers probably very simple. ",12l93ad,Stememole,1681425009.0,1,1.0,[]
[Q] Recommendation for screening/optimization design book,"I know in theory there is supposed to be a screening design followed by a response surface design. In theory, the first design is supposed to screen out variables, and the second is supposed to provide greater resolution on the optimal settings. I just don't know where  to find a good vetted source on the subject. I am trying to establish brand new process parameters, so i would also like to take a wide range and narrow it down to a much smaller range. Does anyone have recommendations on a good book tha walks through methods to address this?",12l5rwf,flapjaxrfun,1681421323.0,17,0.87,"['Look into Design of Experiments as a topic. This is a huge field with many different designs, e.g. Plackett-Burman and fractional factorial designs, can be used for screening variables before constructing response surface (e.g via Box-Benhken and Central Composite Designs). \n\nMontgomerys Design and Analysis of Experiments is a good intro text. Alternatively, the OG textbook on DoE is Statistics for Experimenters by Box and Hunter.', 'You can check out Art Owens lecture notes on experimental design. Might be a good starter and also has further references.\n\nhttps://artowen.su.domains/courses/363/', 'Box and Hunter is a good text and I like it more than Montgomery.  The NIST Handbook of Statistical Methods is online and free, and is pretty good, too.  \n\nUnfortunately, many of the DOE texts are sort of outdated now.  They contain a lot of material about ""cell means"" calculations which in my opinion are kind of pointless when you have the ability to fit models with software.  And they won\'t contain anything about newer designs like definitive screening designs.  You may want to read a review article.', 'Box Hunter and Hunter, for the screening, Box and Draper for RSM. Both of them great overview and good at explaining. Hinkelman Kempthorn feels a bit more technical, less explaining. Most of it is old, you might find them second hand for a small price.', 'Why is this tagged as NSFW and Spoiler XD', '> the OG textbook on DoE is Statistics for Experimenters by Box and Hunter.\n\nand Hunter, as there were two Hunters', '+1 for NIST handbook: \nhttps://www.itl.nist.gov/div898/handbook/\nCheckout the ""improve"" section.', ""Just keeping things exciting! Also, it feels nsfw until I get a better idea of what I'm doing."", 'You are right, thanks for the correction!', 'Hahaha. Love it. Good luck!']"
[Q] How do I analyse 2 risk models in spss without ROC?,"
I want to analyse which one of the 2 risk models is better for analyzing an binary outcome. However, the second risk model is just the first risk model with 1 variable added.

If I do an ROC with paired sample, it wont be significant because its almost the same model (just 1 variable is added). Does anyone know how I can analyse The efficacy of my new risk model compared to the old one without ROC?

TLDR: read title",12l5i0j,Big_Category7915,1681421021.0,3,1.0,[]
[E] Undergrad UCB vs UCLA,"
So I got admitted for both undergrad for stats (Ucla changed the major name to stats and data science) and was wondering which one to commit to. I plan on minoring in data science at Berkeley or Data Science Engineering at UCLA. I've seen on this sub that undergrad really doesn't matter but I wanna work in data science in the future and was wondering if the opportunities for research/internship is similar at both universities. If anyone has been to one or the other can you provide more info on your experiences? Thanks

Also how much does gpa matter to get into grad programs cuz I can prolly get a better gpa at la than Berkeley with its supposedly brutal curves.",12l2usx,MrManCheeks,1681418118.0,28,0.89,"['Oh hey, i graduated from UCB in stats and intending on attending UCLA for grad school stats. \n\nPM me and id be happy to share my experience', 'Go berkeley', 'Not sure if this is the best approach but here we go. Look into what classes each degree offers. Then look at the description of each class. If you can, try to get previous syllabus for said courses. Compare all this data points to one another and make your choice. (Most of the time this info is available to the public)\n\nJust keep in mind that no matter how good or bad a program is. It all depends on how you take advantage out of it. \n\nSome unsolicited advice. Always keep learning and be curios about everything.', 'Berkeley stats is absolutely top tier, I can attest to that. DM me if you have questions', 'One thing worth considering is that Berkeley is closer to Silicon Valley. Theres opportunities in DS and tech in LA too but imo if you wanna go into tech it helps to be in the bay. Im sure UCLA has good networking too, but I think proximity to tech HQs and startup stuff probably helps + Berkeleys reputation for CS/EECS could help too.\n\nI will say cal was brutal for me, and you dont get a lot of help. Stats was a lot smaller when I was there, I imagine its harder now (more competition)on the other hand maybe theres more help and resources now though. I wouldnt know. Ill say, I became ok with Cs in my time therewhich is why Im not a statistician by title lol.\n\nIm sure you could manage to get into some masters program, really depends on how you go about it. If you go direct undergrad to grad school I think GPA is more important. \n\nLast thing Ill say is that if you can visit and you absolutely hate the environment in either area, that should be important. It can be lonely in these schools if you cant make friends, and undergrad is a great time to build friendships if you are able to live on campus. UCLA is in more of a car area, Berkeley has more walking. Think about stuff like that tooI generally agree that undergrad program isnt the most important thing, esp if youre picking between two good ones.', 'UCB has produced such luminaries as Aubrey Plaza, Ellie Kemper, Donald Glover, the Game Gr- oh wait youre not talking improv groups.', ""Berkeley certainly has a better stats program, but UCLA has a much better overall undergraduate experience (for me, it depends on the person). The grades / connections you make that influence your post grad plans are almost entirely dependent on how happy/motivated/non-depressive you are throughout undergrad, rather than the prestige of the program itself. So I would pick the place where you are most likely to fit in and enjoy yourself, even if your goal is purely to best position yourself post grad. I'm a student at UCLA, so feel free to ama."", 'Berkeley, no fucking doubt. Stats program is insane there. The only thing I noticed is weird is theres no course with stuff for\nODEs and PDEs.', 'https://www.reddit.com/r/berkeley/comments/tsjmvy/just_a_reminder_for_all_incoming_freshman', 'Berkley, hands down.', 'The rankings in undergrad dont matter that much especially since both are good schools. I would go more by which school you would fit in better. I went to UCLA for undergrad myself and loved it.', ""Berkley's stats program is a beast.  Top 5, at least top 10 for sure."", 'Are both seen as just about equally as prestigious? Im gonna visit both but I think I already click with ucla much more than Berkeley. (Plus I dont wanna deal with all the issues Berkeleys got rn haha). I have some connects in LA that could help me in internships/research vs nothing in Berkeley.', "">Just keep in mind that no matter how good or bad a program is. It all depends on how you take advantage out of it.\n\nI disagree a bit with this. But I think Berkeley and UCLA are close enough in rankings that this isn't a material difference. If you're talking about a top 50 vs a top 350 institution, then I'd say that there are differences in the outcomes. Mostly I think it would be a lot harder to land at a top 5-10 PhD program if you're at a top 350 compared to a top 50. \n\nI'm just comparing the top students graduating from two institutions and the top 50 top grads always land at a top 10 or better school. There are a couple examples for the top 350, but it's an exception. Of course this isn't a rigorous analysis."", 'Commenting to add to this. Berkeley stats is a top 5 program without a doubt, and most would argue its a top 2 program (only behind Stanford). UCLA stats is probably a top 20 program; its still a great program but its no Berkeley.', 'Ill be visiting each campus and deciding based on that too. Since you talked about not being a statistician, whats ur job/title now? (Up to you if you wanna answer that). \n\nI think competition will actually decrease (at least within majors) now that stats was declared a high demand major and its much harder for people to switch into. A ton of people are saying the new high demand major policy will help with Berkeleys extreme competition too. \n\nhttps://admissions.berkeley.edu/wp-content/uploads/First-Years-New-High-Demand-Majors-Policy.pdf', 'Haha that was my first thought as well.', 'They cover differential equations alongside linear algebra in the same lower division math course', 'Moreso thar a degree from cal has a bigger rep than ucla in all grad admissions. Both are good and education will be roughly the same.', ""Berkeley has the edge for prestige --- but don't go there just because it is prestigious. Go where you'll fit in better."", ""Internships will come along from simply going to Berkeley and you can always go intern in LA in the summer. Berkeley has more prestige nationally - substantially so. That being said it's more expensive to live out there than UCLA and the student life is quite different."", ""Yes. You'll get a good, rigorous stats education at either place. Both are equally prestigious, *although* if you are absolutely dead set on something like quantitative finance or HFT then the Berkeley name would give you a slight edge. Maaaybe. But if you want to do data science work after undergrad, it won't matter. \n\nGo to the one that fits you better. Berkeley and UCLA have very different vibes, and folks tend to fit in better at one than the other. Anyway, PM me if you have questions, OPI did my undergrad at UCLA and am now faculty at Berkeley so I have deep experience with both places."", ""They're not seen equally, Berkeley is better"", ""You won't go wrong at either place.  So I would suggest looking less at the program ranking and more at what else the universities offer you.\n\nOne might be better than the other, but the gap is a lot smaller than you might think.  When it comes to most state schools we're talking about more marginal differences than massive ones at the undergraduate level.  Especially when talking about UCLA vs UC Berk.\n\nPick which one fits you better and the place that racks up less debt.  If they both cost the same just take your pick.  But if say one has way better financial support for you than the other, I would suggest leaning towards that one.  The difference between the schools is not worth an extra 15-30k in debt."", 'It took me some time but I worked in analytics and data science, considered trying to do data engineering but now somehow Im in product management.', 'Hmm I did want to explore the business side of stats a bit and Berkeleys cluster courses are good for that Im not sure if UCLA has anything like that', ""Yeah, depending on what you mean by the business side I don't think the UCLA stats major has anything like that. The cluster courses are pretty unique. You could take the UCLA business/finance courses as electives although they probably won't count towards the major. UCLA also has the 140/141 series which let you work on stats consulting projects with a professor and are a great way to get some real-world experience though.""]"
[Q] is this spurious regression ?,"Hello,

I am working in credit risk and for some reason I need to translate a default rate in exposure to a default rate in number of obligors.

One option discussed so far is to regress the default rate in number by the total exposure and by the exposure at default at a given time.

It does make some sense as the default rate in exposure is indeed the exposure at default divided by the total exposure.

However, due to newly originated loans, the total exposure is not stationnary and just gaining ~5% every year => it looks clear to me the exposure has a linear relationship with time. This makes me worry on whether my regression is spurious.

On the other hand, it is how the default rate is defined so it makes sense from a business pov.

What do you think about it?

Hopefully we have other options.",12l20qc,Sydno,1681416499.0,1,0.67,[]
[Q] How do I test these data?,"Hi everyone!

I am having difficulty doing a small analysis of a test that I have carried out. The test consisted of sound stimulus tests. I conducted a total of four tests for two groups (n=30 and n=24).

In each of these tests, participants had to respond to exactly 8 sound stimuli (8x4=32 stimuli in total).  I segmented the tests in pairs, meaning that the 8 stimuli from tests 1 and 2 were located in specific positions, while the stimuli from tests 3 and 4 were in different positions compared to the previous pair.  I obtained reaction times for the observation of the stimuli, therefore, I would now like to compare which sounds the participants were able to detect more quickly.  That is, compare the 8 stimuli for the first pair (1-2) and see which one was detected more quickly, and then check the other 8 stimuli in the second pair (3-4).  Should I conduct a two-way ANOVA in this case?"" 

&#x200B;

 Thank you very much for coming in and your help.",12l1fut,GD-Zero,1681415510.0,3,1.0,"[""It's not clear from your description what you're comparing. Are you comparing reaction times for all four stimuli? Or just comparing 1 to 2 and 3 to 4?\n\nIf the first, I think it would be a one-way ANOVA with appropriate post hoc tests to index where any significant differences in mean reaction time lay. If the second, it would be two separate t-tests.\n\nEdit: just remembered ANOVA requires your data to be independent. If my take on your experiment is accurate, you've got a within subjects design (i.e. not independent)."", ""the second! comparing the 8 stimulli for 1-2 and for 3-4. So... i should make two separate t-tests, isn't it?"", 'My apologies, if this is your design then it\'s two one-way ANOVAs. I missed that your stimulus has 8 levels, the ANOVA will compare the means of all 8 levels in test 1 with the means of all 8 levels in test 2 (and so on for test 3 vs test 4).\n\nHowever, as I mentioned, ANOVA requires that the data be independent. Since you are comparing data from the same subjects, this assumption of independence is violated. I\'m not sure then which test is most appropriate, you might try googling ""ANOVA for nonindependent data"".', ""thank you so much for your help! maybe it's the anova repeated measures test?"", ""Are you looking to compare individual stimuli against one another (1/32)? Or groups of stimuli against one another (1/4)? Or are you aiming to compare individual stimuli across four conditions (1/8/4), sort of like a 4-factor ANOVA (not that such a thing exists, I don't think: possibly a MANOVA)? Maybe if you described your aims/hypotheses a bit it would be easier to understand your design.\n\nEdit: I'm talking rubbish in that last part: if it was to compare individual stimuli across four conditions it would, as you already suspected, be a 2-way ANOVA (IV1: stimulus, IV2: stimulus group). I'm guessing this is what your design is, then. In which case, you have a mix of between subjects and within subjects design and I'm not sure which version of ANOVA would best handle that: but you're certainly on the right track!\n\nEdit 2: I've just realised you're repeating the entire study for 2 groups of participants and am confused again. Unless you're not looking at individual stimuli at all, but rather at stimulus group (IV1) and participant group (IV2). In which case, it's still 2-way ANOVA, and yes I'd go for repeat measures."", ""Thank you very much for taking the time to respond, I appreciate the effort you have put in!\n\nThe study was conducted to see if a type of sound was more effective in being detected by users than another sound. To do this, we carried out 4 tests with 2 groups in different orders. The first group would do the order 1-2-3-4, and the second group would do the order 4-3-2-1. To illustrate this better, I'll take the example of 1-2.1 and 2 present objects in 8 positions (which are the same in both 1 and 2) but with different sounds. Perhaps it's a bit complex and I'm not explaining it well.\n\n&#x200B;\n\nI think it would be appropriate to conduct a repeated measures ANOVA (8 for 1-2 and another 8 for 3-4, independently and considering the two groups of people), rather than a two-way ANOVA, since the measures are actually dependent (coming from the same subject)""]"
[Q] Trying to figure out how to test these data - correlation for stratified/range of data?,"Apologies if some of my terminology is incorrect, I'm certainly not a stats expert.

I'm a cardiologist doing some research trying to analyse some data I've collected for some patients to gain a little more insight into mechanisms of disease. I have two groups of patients where I've done some experiments on the electrical conduction within the heart. Within each group, I've got a measurement of the degree of electricity abnormality (fractionation) during different conditions.

Groups A and B

Conditions A, B and C

For each condition, I've measured abnormal electricity (% of signals that were fractionated) and because of the way we've measured, we cannot get specific values, only stratified into a range. The range is 0-24%, 25-49%, 50-74%, or 75-100%. I want to show that the degree of fractionation during condition A correlates or doesn't correlate to degree of fractionation during condition B and/or C.

So for example

Group A

Patient1 - condition A=0-24, B=0-24, C=25-49

Patient2 - A=24-49, B=50-74, C=25-49

etc

Group B

Patient1 - A=75-100, B=75-100, C=75-100

Patient 2 - A=50-74, B=75-100, C=50-74

etc

First I want to do this correlation for all patients regardless of which group they are in, so I can show that the measurements correlate based on conditions. Then I want to compare the measurements for each condition between the groups.

I can't quite figure out which tests to use in order to do this because I don't have specific integers, only a range of numbers/stratified for each data point. Could someone help?

I'm using GraphPad Prism as my stats software. I'm guess very few/noone here would use that, so I just need the name of the test/an explanation of how to test for it, and I'll figure it out within the software.

Many thanks in advance for your help!",12kuouw,Pow_Surfer,1681403216.0,5,0.86,"[""So for non-parametric ordinal data like yours, \n\nTo assess the correlation between conditions A, B, and C for all patients (regardless of the group), you should use the Spearman's rank correlation coefficient.\n\nTo compare the measurements for each condition between the two groups (A and B), you can use the Mann-Whitney test. This compares the distribution of the ordinal data between two independent groups by testing the null hypothesis that the two groups have the same distribution.\n\nIm 99% certain as Ive been studying for grad stats finals the last few weeks. Im not sure how to find those tests in the software youre using but hopefully this is helpful."", ""Thanks for your reply! I'm having a few issues with that.\n\nIn order to do a correlation coefficient, I need to plot the data in columns only, so two or three columns of Group A,B,C. But in terms of the values I am allowed to input, it will only allow integers, not a range of values within each cell.\n\nThe problem with trying to do a MW test on this data is that it tells me MW tests are only for data in columns ie. there is only one variable which is group A or B. When I add in a second variable (the degree of fractionation as 4 rows, 0-24%, 25-49%, 50-74%, or 75-100%), and then plot the number of patients in each group that fall within these four categories, it has to be a contingency table in this software, and then it's telling me to do a chi square/fisher test, because that is what you do for contingency table data."", ""Apologies for the confusion. You're right; the format of your data requires a different approach. Since you're dealing with ranges instead of specific values, using contingency tables and chi-square or Fisher's exact test would be more appropriate.\n\nIf I understand the first analysis correctly, it is designed to examine the relationship between the fractionation measurements during different conditions, regardless of the group the patients are in. Basically, It will help you understand whether the degree of fractionation during one condition is related to the degree of fractionation during other conditions.\n\nFor correlation between conditions A, B, and C for all patients, you can create a contingency table between each pair of conditions(6 in total). Then, use the chi-square test (or Fisher's exact test as appropriate depending on sample size and the expected counts in the contingency table. \n\nFor comparing the degree of fractionation between Group A and Group B for each condition, you can also use contingency tables with the chi-square or Fisher's exact test. In this case, create a contingency table for each condition (A, B, and C) with the two groups (A and B) as rows and the four fractionation categories as columns.""]"
[D] Books/References on Linear Mixed Models,"I am looking for a book/reference on linear mixed models which specifically describes how the model parameters are estimated. I have found several books, but these details are often skipped over. Can someone please reccomend something?

Thanks!",12ksvml,SQL_beginner,1681399676.0,10,0.92,"['I have three.  ""Multilevel Modeling Using R"" by Finch.  ""Mixed-Effects Models in S and S-Plus"" by Piniero.  ""Data Analysis Using Regression and Multilevel/Hierarchical Models"" by Gelman. Honestly, I think I learned through a combination of Finch, Gelman, examples online for lme4 R package, and just thinking about it for a while in the back of my head.  It was a difficult subject for me to learn and no single source really did the trick for me.  \n\nI wish you good luck.  It\'s actually an amazing subject and now I use it all the time in my work.  The combination of maximum likelihood + multiple levels of coefficients is ingenious. It controls overfitting for categorical variables with many levels, allows for predictions for new levels of categorical variables, describes hierarchical relationships that exist in many real problems, and more.', 'Thank you for your replies everyone!', 'While it is software specific, this book does a great job going through the equations and exactly how parameters are calculated. \n\nThis is just the free sample. The whole book is also a doorstop. \n\nhttps://support.sas.com/content/dam/SAS/support/en/books/sas-for-mixed-models-an-introduction/68787_excerpt.pdf', ""I don't think you'll find details about the estimation in books, as this is mostly unnecessary for daily use. Most R packages come from a scientific publication, so you can check this. Iirc there are good publications explaining lme4 for instance."", ""Multilevel Analysis: Techniques and Applications by Joop Hox might be interesting. I think you can look at a preview on Google Books? Chapter 13 is Advanced Issues in Estimation and Testing, so that might give you an idea of what you're getting.\n\nIt's also a supremely readable book. The intro chapters are targeted at applied researchers then the whole thing builds itself up very nicely."", 'Consider : \n\nRegression: Models, Methods and Applications  by Fahrmeir\nIt helped me a lot during my undergrad studies\n\nIt provides quite an in-depth overview of the different models ( from LM to GAMMLS)\n\nEdit:  Mixed models can be found in Chapter 7', 'Currently re-reading Applied Regression Analysis & Generalized linear models by John Fox. Not specific on mixed models but I feel like you need the complete picture (I started with linear algebra even in Calculus books) to actually get it.', 'This indeed is the useful stuff that makes scientists understand their data, research and the world that they are suppose to represent. But this for sure is also the stuff that makes politicians go ""I don\'t trust these fancy models that I don\'t understand"" lol :)', 'I have to disagree here. The better one understands how and what is being calculated, the better application and interpretation of the statistical technique. There are plenty of reasons to look at actual estimates.', ""I agree it's useful, but books are usually introductory and don't cover this, at least those I've seen.""]"
[Q] Nondeterministic Tournament,"How to calculate the probability of A positioning better than B if that was a nondeterministic event?

Suppose we have a race event with 8 possible placements. Do I have to calculate every possible placement? Like: What are the odds of A being 1st, and if first, what are the odds of B being in a better placement. (In that case, 0%, but if A finishes 2nd, the odds are around 14% for B to be better placed.) Or there's a formula to calculate this?",12krm2u,iTheMistery,1681397154.0,1,0.67,"['We can\'t do anything without a better probabilistic description of the race event.\n\nIn the most basic scenario the performance of each contestant, presumably the (random) amount of time it takes to finish the race, would be independent and identically distributed and there are no ties.\n\nI\'m not sure what you mean by ""placements."" Are you saying that there could be more contestants than placements? For example 15 contestants but only the top 8 get a ""placement?"" I\'m going to consider all positions in the race, not just placements. It makes things easier. You can introduce placements again later.\n\nThe *probability* of a single contestant being in a particular position will be 1/(total number of positions). For example, the probability that A is in first is 1/8.\n\nFor your second question, think about the possible remaining positions that B could be in given that A is in position ""x"". If A is in position 3, then there are 8-1 = 7 remaining positions for B. In order for B to place better than A, then they must be in position 1 or 2. The probability that they are in position 1 is 1/(total remaining positions) = 1/7. The probability that they are in position 2 is also 1/7. The probability that they are in 1 OR 2 is the sum of those two probabilities, 2/7.\n\nYou can use the same strategy given that A is in any position.\n\nThis isn\'t a very realistic model, though. Contestants probably interact during the event which violates the assumption of independence and their performances are probably not all equally distributed. To capture these kind of details one would need to describe a ""richer"" probabilistic model and the calculations would be more involved.\n\nOne final note:\n> if A finishes 2nd, the odds are around 14% for B to be better placed.\n\n\nHere you\'ve not described the odds, but the *probability* that B is better placed than A, given that A finished 2nd. The odds would be 1:6, i.e. 1 success to 6 failures. The probability is 1/7 = (number of successes)/(total possible outcomes) = 0.143 or about 14% as you\'ve concluded.\n\nhttps://en.wikipedia.org/wiki/Odds#', ""In general (if each player has different chances of winning), yes, you'll have to consider every possible permutation of players to get a complete answer to someone's expected ranking. There will be shortcuts if you have several evenly matched players, or several players you don't care about. Similarly if A and B are in the same subpart of a bracket, you'll have fewer matches to enumerate than if you have to find their chances of making it all the way to a final.\n\nThe 'independent chip model' is widely used to predict outcomes in poker tournaments, and equitably divide prize money if the tournament ends early. In that model, A will beat B with probability (A's chip stack)/(A's chip stack + B's chip stack.), but if you care about expected winnings, you have to find a probability of landing in each of the top places. \nBoth very early and very late in the game, there are simplifying shortcuts. Right when you first hit the prize money, it's hard."", 'Are all possible permutations equally likely? If so, then A will finish before B in exactly 1/2 of the races because of symmetry.', 'Sorry, I didnt give all the details. The number of placements in that case is equal to the number of participants. \n\nBut I can imagine that the outcomes are equally distributed, by imagining so, the odds are 50% for A to be better positioned than B, and vice versa. (?)', 'If the chances of winning are equal for each player, the probability in this case will be 50%, since we have an even number of placements? And if so, how do I calculate that?', 'Yeah. If there were 9 possible placements, that would be the same?', 'Yes.\n\nBut again, ""odds"" are not the same as ""probability"". The *probability* for A to be better than B is 50%. The *odds* that A is better than B is *1 to 1* or 1:1, i.e. equal odds.', '> If the chances of winning are equal for each player, the probability in this case will be 50%, since we have an even number of placements?\n\nIt\'s not because there are an even number of placements. In fact, it\'s not about the number of placements or other participants at all. All that matters that the two participants you care about have independent and identically distributed (IID) race times.\n\nIf you have two independent and identically distributed continuous random variables (race times), say A and B, then the probability that A < B is 1/2, P(A < B) = 1/2.\n\nThe IID assumption implies that \n\nP(A < B) = P(A > B).\n\nAn axiom of probability is, loosely speaking, that an event must occur. In the race, either contestant B is faster than contestant A or vice versa. There are no ties. One of those outcomes must happen. This means \n\n1 = P(A < B) + P(A > B).\n\nWe can add the probabilities because the events are mutually exclusive. A beating B can not happen simultaneously with B beating A. \n\nIf you substitute P(A < B) in for P(A > B) in the second equation, then you have\n\n1 = 2P(A < B)\n\nwhich means\n\nP(A < B) = 1/2.\n\nThen we must have P(A > B) = 1/2 as well.\n\nhttps://math.stackexchange.com/questions/3387964/proof-pxy-1-2-when-x-and-y-i-i-d-continuous-symmetry\n\nNote: Asking ""what is the probability that A beats B?"" is a different question than ""Given A finished in position ""x"", what is the probability that B finished in a lower position?"". The first question concerns the *joint* distribution of the racers A and B. The second question concerns the *conditional* distribution of B given a realization (known information) of A.', 'Yes you can generalize this to any number of placements.', 'Oh, nice, I had this misunderstanding for quite a long time. Thank you!', 'Thank you. Great explanation!']"
[Q] contingency table; two approaches to measure dichotomous behavior. Which statistical parameter would fit?," Hey,

we measured a behavior with two approaches. Both is a dichotomous variable (Behavior is shown, yes or no) . Example below.

Which statistical parameter wold be the best? I though about interrater-reliability but we did not have raters. Any ideas?

Thanks!

&#x200B;

|||Measure 1||
|:-|:-|:-|:-|
|||yes|no|
|Measure 2|yes|550|20|
||no|10|300|
|||||",12kqrth,skippydi34,1681395473.0,1,0.67,"['It depends on what question you are trying to answer.\n\nIs one *Measure* the ""correct"" one, and you want to see how often the other measure is also correct ?  (I\'m thinking of a confusion matrix here with the associated statistics like sensitivity and precision).\n\nOr do you want to know if there is a significant shift in in *Measure 2* from ""no"" to ""yes"" relative to ""yes"" to ""no"" in *Measure 1* ?  (I\'m thinking here of McNemar\'s test).\n\nOr something else ?', 'The term you\'re looking for is ""interrater agreement"" or ""interrater reliability"". A classical measure is Cohen\'s kappa. Personally, I\'d recommend[newer measures](https://link.springer.com/article/10.1186/1471-2288-13-61) such as Gwet\'s AC1.', 'What are you trying to find out?', 'Neither is the ""correct"" one. It\'s a behavior that has never been measured and it\'s hard to tell which is the absolute correct one.\n\nComparable, easy example: Measure 1 asks people in a self-report which food they buy regularly and by these information you could conclude if this person is a vegetarian or not. Measure 2 directly asked people in a self-report if they are vegetarian or not. However, our behavior of interest is a little bit more complicated and vague.\n\nMeasure 1 was before Measure 2.', 'Thanks. I wrote it as a possibility in my post. However, I feel like there aren\'t ""two raters"" which is why I thought it is not appropriate', 'Are you trying to determine something specific with the statistics ? Or just looking to add some ""statistics"" to report to look more rigorous.  (No judgement intended).\n\nPerhaps the table you\'ve created is the most informative, perhaps also presenting the information as proportions.  (Although what proportions make sense in this context is difficult.  Maybe the measures differ on 30 / 880 responses, or about 3%.\n\nYou could also apply McNemar\'s test, telling you that  one measure tending to report ""yes"" when the other reports ""no"" isn\'t significant.  That is, the 20 and 10 in your table are relatively balanced, if that makes sense.', 'If you want to assess agreement between the two measures, it does apply, in my opinion.', ""We took the self-classification as a kind of second way of measuring. The data above is not real, the discrepancy is a little bit bigger unfortunately. This could mean that either our first way of measuring wasn't good or that people have trouble to self-identify themselves in regard of the behavior. As I said, the behavior that we measured is not that easy because there is high risk that people who do not show the behavior think they do. Nevertheless, my analysis  above is exploratory and also labelled as such and everything was preregistered. So just adding statistics is not the main motivation. \n\nMc Nemar does not sound good actually, I will look into it. Thanks"", ""One question relating McNemar's: It's not a repeated measurement in my case because it's a different measurement. I don't know if this still fits?""]"
[Q] Access to Statista?,"Hello all,

Does anyone have a paid account or university access for Statista? There are a couple of statistics I would like to see but I need a paid account for that. However, I'm not wealthy enough to pay such a sum of money per month and being stuck for a year.

Could anyone help me? 

Hope to hear soon. :)",12kp1bn,cheaphookah,1681392134.0,0,0.36,"['I checked my uni but they dont subscribe to Statista either. \n\nSucks you cant just pay for one month', ""I'm currently in China. Don't trust the 2023 numbers. They're about to increase by 1 in a little bit."", 'https://data.gov/\nhttps://data.census.gov/', 'For those who need a bit of help, I have a guy who sells genuine Statista subscription accounts for $9.90 a month. I have been using it for months without problems. total life savor. If anyone is interested, his telegram is adobe\\_king', 'Thank you so much for checking !! That was very sweet of you', 'Hi! Thanks for the links, but the data I am looking for is about sex crime rates in South Korea, Japan, and India. I am not looking for statistics regarding the U.S.']"
Help for an Autistic Person [Q],"I am currently a transient student who is 18TF and I have 39 total credits (transfer/in-school) from my local university. However, they have not been as accommodating and I have been depressed. I want to pursue a B.S. and then a PhD in Statistics, but due to my anxiety I had to take a mental health break. Ive been getting discouraging messages lately saying that the degree might be too stressful for me and that I should major in something else. Are there any statistics programs at other universities in the United States willing to accommodate someone with severe anxiety? 

*Note: I apologize if my post is wordy. I just want to know if people have been doing through anything similar. If this is the wrong subreddit, please let me know where I should cross post this.",12kf384,WhiteRicePatty69,1681368314.0,0,0.48,"[""You have my heartfelt sympathy. You are not alone. I don't know that there is any university better than others. All I can say is that, and this may sound trite but I don't mean it that way, but you be You. \n\nOthers opinions have nothing to do with who you are, and for damn sure should not discourage you from pursuing your interests. \n\nAnxiety is NOT abnormal in this. Don't beat yourself up. It's hard, it's supposed to be hard, getting through it isn't nearly as much about a piece of paper, but that you are capable of doing something hard. That's worth more than whatever topic you choose to study. \n\nHope that helps somehow, and I wish you the best. If you want to do stats, go for it. If you want to do something else, go for it. It is YOUR choice to direct your life. If you want to get into hard math and learn it, go for it. You've got to want it and be willing to work for it. It's the willingness to work for it that is the value. \n\nGetting what you want in life is easy. Knowing what you want is hard. Best of luck to you ."", ""go where ever is cheapest for you to study. instead of finding a university that will accommodate anxiety, go see a mental health professional and get your anxiety under control. think about happens after you finish school, an employer won't be as accommodating, they'll expect you to not miss work and to make deadlines"", 'I went to a very Christian university, and bear with me, they offer an array of classes and programs for people with anxiety. The first week of school, the new students are assigned a mentor that will reach out to them and basically be a go-to for any questions regarding registration, and all questions in general. Its basically a bunch of extroverts that adopt the new students and make sure they have what they need for the whole semester. Then, there are college life classes that basically prepare you with the skills you need to get through college. The class focuses on maintaining communication with professors and forming study groups with peers, etc. I had pretty intense social anxiety and this helped me a lot. Also, they allow you people with depression and anxiety to drop their classes without penalty to look after their mental health (no refund after a bit of the way through the semester, but i had to do a medical deferment due to depression once and it was a relief to know the classes werent counted toward the GPA). Lastly, everyone was assigned a family group that they were to meet with once a week, which guaranteed I had friends even when I wasnt confident in my ability to make them on my own. \n\nI know theres a stigma around christian universities but some do pander to people with mental illnesses very well. If youd like some personal recommendations feel free to DM and we can talk a little more in depth on specific schools I could recommend', 'My biggest advice is take your time. Youre young and academia can be a total shitshow of a time. I came in with 45 credits, finished my BS in physics in 3.5 years, masters in teaching in two more and just finished my PhD in education policy(heavily quant focused which is why Im on the stats sub). I rushed my bachelors and failed to form strong connections with the others in my program and ultimately a social network would have been helpful if I went through with more physics. One professor said, Are you sure you want to study physics in my physics two class: that same professor gave me the opportunity to start education policy research. Network network network.\n\nDoing a PhD while researching/TAing is hard work mentally and psychologically taxing even at a supportive institution. Remember, institutions arent your friend and theyre not going to bend too much for your anxiety or depression, they will do just enough to make sure they cant get sued. Your money and time and effort are equivalent to the asshole cograduate student who says awful things to or about you. \n\nTldr: focus on the getting undergraduate course work done and acing grad school prereqs, getting research experiences under your belt during the year and summers and making a strong social network of peers who also enjoy stats!', 'Grad school is tough and will be stressful for anyone thats one of the reasons so few people can get a phd', ""Different schools will offer different accommodations for neurodiverse folks. Some schools offer (small) private exam rooms and/or extra time on exams, among other accommodations. Some schools offer next to nothing.\n\nThis is something you should research before deciding on any particular school, but that's something that well meaning HS teachers/counselors probably forget to mention sometimes.\n\nFor what it's worth, neurodiverse and neurotypical students alike often struggle through their first 2-3 applied stats courses. It can all be overwhelming until it *clicks*. It's actually much simpler than it first appears, but that's of little comfort in the beginning."", 'Ive been in your shoes with severe anxiety hindering my studies. My entire first semester had to be repaired the semester after, I literally had gpa below 1.0. However, Ive repaired it over the semester or two and am relatively back on track. Id recommend George Mason University, its a very diverse and inclusive school. Im in their statistics program and have enjoyed it so far, they have a decent amount of accommodations and you can call their disability services or check their website to see all the things they do. Just keep focused on the end goal and reasons youre going into your studies and I can definitely assure you it all gets better and is going to be alright! :)', ""As an autistic perdon, you will probably get more stress from social environment than from the topic you are studying. \n\nYou will have to develop strategies for coping with a new environment full of noisy unknown people. It will probably be hard, but doable. \n\nMy advice. Don't change your major unless you find that you don't like the topic.\n\nYou may want to crosspost to r/aspergers.\n\nPS. Why the NSFW tag?"", 'Im not in your shoes but:\n1. Ive had at least one teacher suggest and push for some random program completely unrelated to what I wanted to pursue, for arbitrary reasons orthogonal to my ability to succeed in what I wanted to pursue\n2. Ive had one professor of X tell me verbatim I know you find Y more interesting but X is also important so you should make more of an effort (I had good grades in Y, I was more interested in Y, I had zero interest in X outside of its relationship with Y, but had better grades because the professor of X was just better)\n3. Ive overheard a couple of professors say things like forget about them even changing major, they should let go of science altogether and pick up painting instead about a student\n\nI guess what Im trying to say is professors themselves know their program but dont necessarily know other programs, and most of them dont know *you*. Some also have a bias towards making their job easy (e.g. lets tell that student to go somewhere else instead of accomodating their needs, or the classic lets tell anybody who doesnt have innate interest in maths theyll just never be good at maths)\n\nAt the very least, take advice from people you know care about their students and will take the time to understand you, your shortcomings, your stengths, your ambitions. There are plenty.\n\n\nNow Im a firm believer that you can achieve whatever degree you set yourself to achieve, as long as you know the prerequisite (and even then ) so Im tempted to say: go for it. But, you do you.\n\nAlso no offense to anybody here but I fail to see whats so special about statistics that it would somehow be too much for you, but e.g. mech eng would be fine.', ""First year (youre 18 so im assuming first year) should be a relative breeze and the amount of stress and anxiety it gives you is going to be nothing compared to what grad school is going to be like. Remember graduating from these programs isn't just a signal you've learned the material but also a signal you can do difficult work, meet deadlines, and do what it takes to push through the difficult challenges of school to get done what needs to get done. I recommend looking into what you can do to cope with the workload"", 'Please pm me. Im also trans so idk if a Christian university would want me as a student.', 'Look, I want a PhD and I can afford an expensive school. I dont care about employers right now. I just want to learn.', 'This might be ok for someone who is cishet but I think OP is saying that theyre trans, so going to a Christian university (especially in America) would be about the worst move possible in this situation', 'I only added the tag bc it was required and the other ones were not close to my post at all.', 'Pm sent', 'Then focus on your mental health? Get that in order, because that seems to be whats hindering your learning. If you can afford an expensive school, you should have no problem affording to see a mental health professional as well', 'Ahh I see. It may not be something to consider then. \n\nMy younger brother is low functioning autistic and a Christian school has been super accommodating to him, he has someone who takes notes for him in classes and someone who checks in on him often to remind him of homework and to set a plan with what he needs to do for that night. If you dont feel comfortable in a Christian school then it might not be the best fit, but in my brothers case, we were able to contact a bishop in the area and got him a room with other autistic kids, and they sit at home and happily play video games next to each other (usually not with each other) many nights\n\nIf OP really is trans a Christian or a conservative leaning school certainly may not be the best choice']"
[Question] What analysis to use for insect behavior study,"Hello all! I'm looking for a little help on how I should go about analyzing data for my master's thesis.

In  my experiment I am recording the calls of an insect over 24 hours. My  aim is to work out what time of day this species is most likely to call.  For each recording so far I have worked out how much of each time is  spent calling each half hour (i.e. dividing 24 hours into 48 half hour  blocks). I have noticed a pretty strong bimodal distribution as to peak  calling times. There is little variation in where these peaks are  between different recordings, but there is variation the total time  spent calling.

I will probably end  up discussing this with a stats professor later, but it'd be handy to  know where to start looking so I have something to bring him.

I can provide more info on my data if needed <3",12k8jw0,Dolce99,1681352577.0,1,0.67,"['Can these recordings be done over many days and then check for a correlation between these calls and environmental factors like temperature or humidity? Check Dolbears law for an example of predicting temperature with cricket chirps.  Can you do any spectral modeling of the insect calls and provide some analysis on the composition of the sound waves that make up the call?  Maybe this varies with some information known about the time of day, environment, or specific goal of the insect..']"
[Q] high school senior deciding major,What are the potential internship and job outlooks that can be expected by a statistics major. I have been admitted into uiuc for statistics and I want to know what possible jobs are available as a statistics major. What are internship opportunities that can be expected? I have heard that statistics majors are more valued when they have a masters degree rather than an bachelors.,12k5v2r,ConcentrateNo829,1681346538.0,0,0.5,"[""Plenty of ____ analyst type jobs are hiring stats BA or BS degrees. Data analyst, business analyst, finance analyst, etc.\n\nI would recommend a minor in computer science alongside it. Won't have trouble getting a job"", ""As they say, statisticians can play in everyone's backyard. Stats majors in undergrad pair well with other majors. (Plus econ makes econometrics, plus psych makes psychometrics, plus CS gives you an edge in business analytics or machine learning.) You can pair it in anything and it'll enhance it, especially in STEM. (I wish I took more stats for my undergrad publications.)\n\nIf you want to get a Masters, there are also research positions at lots of institutions. I went to grad school for statistics and currently publish in global health. \n\nSo you can really use it anywhere: academic research, businesses, whatever. Just figure out what problems you want to solve."", 'Second this  Im headed for grad school but minored in CS and doubled in Math']"
"[Q] How do I find the correlation between two categorical variables from two different samples from the same population? (urgent question, any help is appreciated!)","Hi! To start, I'm a senior in high school and this assignment is quite overdue, so please read and respond as soon as possible . I am doing a research paper for school (the class is AP Research), and I have (foolishly) endeavored to find out if there is a correlation between the psychological and economic impact of super typhoons on residents of the Philippines. I intend to do so by using data collected after 2013 Typhoon Haiyan. I have very little knowledge of statistics (I took AP Stat last year, but it turns out that wasn't enough), so I'm really lost and overwhelmed. I had planned on finding the correlation by taking data from a study/survey about psychological impact and data from another study/survey about financial impact and correlating the two data sets, but I've realized two issues:

1. the data would be categorical, so the typical linear regression (Pearson's r correlation coefficient) doesn't seem possible, and
2. the data would be from two different samples, so I can't do the chi-squared test for independence (can't do chi-squared test for homogeneity either because there are two variables).

My vague understanding of stat tells me there's probably a way to do this using chi-squared tests, but I'm not able to connect the dots. So if there's any way to do that, please let me know! If there's any other test for correlation that works for the above type of data (two categorical variables from two different samples from the same population), please tell me about that too! If not, and/or if there's something fundamentally wrong with the way I'm trying to do this, please enlighten me lol.

I've tried to be clear and concise, but please ask questions if something is confusing. Thank you!",12k4z0e,urbanwaves10,1681344614.0,6,0.63,"[""You'd probably try to group your participants along some lines that vary in typhoon impact, region might be one."", 'You\'re on the right track about having two separate issues here, which I\'d describe as \n\n1.) you need the right analysis method for the data type (categorical/ordinal/numeric) and research question, and\n\n2.) you need the right data source(s) for any analysis to actually be able to address your question \n\nFor the first issue, \n\nIf you had the data you\'re looking for (which would probably be ordinal variables, with each variable measured on the same sampled units), you *could* do something with a chi-square test of independence, but it likely wouldn\'t aim at your particular question -- it would be ""is there *any* pattern of association between these (implicitly unordered categorical) variables?"".  There\'s no concept in that test of different categorical values being bigger or smaller than others, just different.  Your real question is like ""do bigger values of A correspond to bigger values of B?\' (these only collapse to the same type of question if both variables have exactly two categories/levels, where one can be considered \'big\' and the other \'small\').\n\nYou might be able do something with linear regression/ordinary Pearson correlation, but the result would only be meaningful to the extent that the variables are each measured on intelligible numeric scales, which it sounds like they wouldn\'t be.\n\nWhat you\'d probably be aiming at is some type of rank correlation, like https://en.m.wikipedia.org/wiki/Spearman\'s_rank_correlation_coefficient  -- this specifically addresses \'does higher A correspond with higher B\'?, even if A and B are just ordered categories, rather than regular numbers.\n\nFor the second issue, you\'ve noticed your confusion here -- if you want to associate variable A with variable B at the scale of individuals, you\'ll have to have data about A and B on the *same* individuals -- measurements of A for one set of individuals and of B for another independent set of individuals isn\'t going to be any use for conclusions at the individual scale.  If you can\'t get such data at the individual level, you might be able to piece something together by aggregating different data sources to the same shared scale, (e.g. by geographic region), like u/good_research says, and treating those aggregates as your data points.  However, in that case, (a) your sample size at that level of aggregation might be too small for useful inference, and (b) you\'d be opening the door to the https://en.m.wikipedia.org/wiki/Ecological_fallacy.  Still, it might be the best you can do here.', ""You can transform categorical variables into dummy variables and use them in a regression. \n\nIt helps to model an equation before even running a regression.\n\nFor example\n\nlog(income) = c + beta1 \\* typhoon+ other variables of interest  + e\n\nlog is the natural log function,    c is a constant,  typhoon a dummy variable that would = 1 if, after the typhoon or 0 for before, and other variables would be added to create a well-specified model.  The beta1 variable is the regression coefficient assigned to typhoon which would represent the effect that typhoon has on the percentage change of income since in the equation we are taking the log of income.  For example, if the beta1 came out to be -.162 then that would represent that income was 16.2 percent lower after the typhoon. The stats package you use should provide a p-value for typhoon and you can use that p value to determine if typhoon had a statistically significant effect on income.  The  e is the error that we don't observe. It contains all the things not modeled in the equation that affect income, for example, ability or education.\n\nWhat types of variables do you have?  Did you study regression / ordinary least squares (OLS)  in your AP stats course?"", ""Couldn't you treat the categorical data as count data and compare the proportion of those suffering poor mental health outcomes in Population A to the proportion of those suffering poor economic outcomes in Population B using z-test? You'd need to acknowledge they were from different samples, but if both are truly random (you'd need to check the studies for any difference in sampling method and discuss these) they should both represent the population.\n\nDisclaimer: I'm a social scientist not a statistician so there may well be glaring issues with what I've suggested! Hopefully others in the sub will chime in if so."", 'If the data is collected from a survey then it is possibly ordinal(at least some part of it), check out polychoric correlation.\nThis can calculate correlation between ordinal variables.', 'Hi, thank you so much for your response, it\'s very clear! I\'ll look further into rank correlation, it seems feasible if I discuss ecological fallacy in my limitations. You mentioned ""aggregating different data sources to the same shared scale"" -- does this mean that I would, for example, give the value of 1 to ""worse mental health"" and ""significant decrease in income,"" a value of 2 to ""no change in mental health"" and ""no change in income,"" etc. so that they have a shared ranking scale? Or am I misunderstanding?', '**[Ecological fallacy](https://en.m.wikipedia.org/wiki/Ecological_fallacy)** \n \n >An ecological fallacy (also ecological inference fallacy or population fallacy) is a formal fallacy in the interpretation of statistical data that occurs when inferences about the nature of individuals are deduced from inferences about the group to which those individuals belong. ""Ecological fallacy"" is a term that is sometimes used to describe the fallacy of division, which is not a statistical fallacy. The four common statistical ecological fallacies are: confusion between ecological correlations and individual correlations, confusion between group average and total average, Simpson\'s paradox, and confusion between higher average and higher likelihood.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', ""Thank you for your response! I did study linear regression briefly in that course, but we used a ti-84 calculator to make the equations (we did learn how to calculate the error, though). The variables will likely both be ordinal categorical variables -- I've been doing some googling and it seems that Spearman's rank correlation might work with this kind of variable. Would you know if this is a viable option if the data is from two separate samples?"", ""I had posted a longer reply, but deleted it when I realized I'd elaborated it upon an error, where I mistakenly thought there was a way that something related to your proposal could work in some circumstances. Upon reflection, I realized that in fact there's no way to make anything like that work.  There's just no way for the 'marginal' information you're suggesting using there to inform you about the 'conditional' relationship -- not even to put bounds on the strength of the association.  \n\nIf you know you have 100 students, 60 are male, and 90 graduate, can you infer anything about the association between sex and graduation, even in this sample, without more information?  No, you really can't. All of the 10 students who didn't graduate could have been male, all of them could have been female, or anything in between.  The sample odds ratio (boys/girls) for graduation by sex could be 0, or infinity, or various values in between.  If, say, 59 boys and 31 girls graduated, the odds ratio (boys/girls) would be (59/1)/(31/9) = about 17, where if 51 boys and 39 girls graduated it would be (51/9)/(39/1) = about 1/7, and both of these, and many other values, would be consistent with the available data, while reflecting opposite or nonexistent associations between the variables."", ""I could have been more clear on this point, by 'shared scale' here I meant 'shared unit of aggregation', not 'shared scale of measurement'.  I don't know what sort of aggregation units would be relevant in this context, but by analogy to US geographic districts, if you had averages or proportions or whatever by 'state' for one variable from one study, you'd also need aggregates by 'state' for the other variable from the other study to make the aggregate level comparison. If you had averages by 'county' (a lower level of aggregation) available from both studies, that might be an improvement."", ""Less-statistical aside, I'd be skeptical of many data sources about mental health impacts of this tsunami, for reasons described in section III, 'PTSD in Sri Lanka' of this book review https://astralcodexten.substack.com/p/book-review-crazy-like-us by a psychiatrist.  It describes how after the 2004 tsunami, western aid workers went to southeast Asia and applied what were, arguably, culturally-nonsensical categories to describing and treating the mental health of the impacted residents, with minimal awareness of how their schema didn't fit the context."", 'Check out the Mann-Whitney U test', ""Thanks for taking the time to reply (I read your earlier post as well). I've actually realised that what I'm suggesting doesn't in any way answer OP's question. How about the following:\n\nTake the proportion of good MH prior to and after treatment, and the proportion of good economic health prior to and after treatment, test the proportions of after against the proportions of before for each variable (using z tests). Would there be a way to test for a correlation between the two movements in value from before to after?\n\nMy thought is that, as another commenter suggested, OP would need several such points across time in order to construct a linear comparison."", ""That's closer to something that could work -- I suggested in another comment looking at different points across space.  If the measurements are dichotomous for both variables, OP might want something like the correlation between the differences in proportion before vs after of one variable vs the differences in proportion before vs after of the other, where that correlation is computed across different regions in space which each have their own differences in proportions.\n\n(sorry about the long post before, if I'd just noticed that I was defining odds wrong I could have saved a lot of words.  I caught my error as I considered how odds ratios should be symmetric across choices of which is the 'positive' outcome, and mine weren't.)""]"
[Q] Eager vs Lazy Learners in Statistical Machine Learning,"Hi! 

Apologies if this is not the community to ask, but in what cases should I use eager learners (like random forest) and lazy learners (like KNN) ? Does it have to do with dataset size? 

Thanks!",12k1d6x,fishytata,1681337493.0,1,1.0,"['As far as Im aware there are no *statistical* considerations for picking between eager and lazy learners. Practically speaking theres going to be differences in actual time taken during prediction and training, which means there may be considerations relevant to applications of the two methods in practice.']"
[Q] Does a related measures Anova test apply here?,"Ive never used this test for anything before so Im uncertain, but it seems like the best fit.

I have a data sample of different feeding behaviours expressed by ringtailed lemurs when their food is presented in three different forms. The data only focuses on one group and there are 7 different behaviours expressed. The dataset shows the latency of behaviours and instances of behaviours.

Thanks so much in advance",12jxgeh,tjking62,1681330284.0,2,1.0,[]
[Q] Binary Logistic Regression vs. Survival Analysis,"I am modeling the probability of my food products to fail shelf-life testing (giving a ""yes"" failed or a ""no"" failed at the point of testing).  There are a number of covariates I explore, including the food recipe, the temperature of storage, and the lighting conditions.  I have many empirical data points, a number of which don't actually fail because I didn't test upper limits (""right censoring"" in survival analysis).

Historically I've explored this through binary logistic regression, and simply predicted the probability that I would get a ""yes"" as a function of covariates that include time in storage, temperature, etc.  Thus, I can estimate for a given set of predictors what my probability of failure is.  This is useful because I can set a cut-off for my estimated probability of failure (say >= 25%), and thus recommend avoiding any combinations of shelf-life conditions that exceed that prediction.

However, I've recently been reading about survival analysis and while the intent seems much more applicable to my application, I'm not quite clear how different they are in practice.  I understand that there are different approaches, including classically Kaplan and Meier estimation and Cox estimation.  I also understand that the survival function gives me the probability that the sample would survive at time t or greater, and that the function will change as a matter of covariates.  However, that seems pretty similar to what I've already done above, with the exception that you get survival not just at the specific linear combination of time t + other covariates, but also > time t **with the** other covariates.    


Am I thinking about this right and if so does this suggest that survival analysis is really what I should be doing with my type of data?",12jr7ba,Excellent_Baby_3385,1681316254.0,15,0.95,"['Logistic regression seems like the more appropriate choice here because it sounds like all of your test samples have been tested for failure (you know if they did or did not). So in that regard, there is no uncertainty in the outcome. \n\nSurvival analysis is useful when you either observe the event of interest (failure) or right censoring occurred (you dont know the outcome either way). You could turn the question a bit, and use survival analysis to estimate the time to failure (or some other feature of the failure time distribution). In this case, products that havent failed are censored at the time of testing because you know the product hasnt failed at that time, but you also cant retest the product later).\n\nEdit to add: since these are food products, there may be better or more appropriate aspects of failure (such as measured bacterial growth) that will give more reliable or useful indications of food spoilage.', ""Are you measuring failure y/n at the exact same time for every item? \n\nSay you are examining egg salad sandwiches. Do examine them all for failure at 10 days? Or do you examine some at different time points, e.g. 7, 10, 14, and 30 days?\n\nIf the former then logistic regression will work. However if you follow items for different lengths of time survival analysis is more appropriate. But survival analysis also assumes that you capture the timing of the failure. So if a sandwich fails at day 3 and you don't examine it for failure until day 7 then you're not getting accurate time-to-event data. \nIf you collect data on the exact time of failure then survival analyses will give you much richer information on products. Because you can say well at 7 days we see a x% failure rate, but at 14 days the failure rate is y%. Also with survival analyses you can look for effect*time interactions. This would help you tease apart which of your covariates is effecting failure as a function of time. For your example you might find that lighting doesn't make much of a difference on failure early on (<3 days) but then after that the effect of lighting increases."", ""Thanks for the thoughts!\n\n&#x200B;\n\nRegarding censoring, I test several time points (e.g. 10 days, 14 days, 21 days) as a matter of meeting a target, but I know that if I tested further time points, they would fail.  In other words, there's an increase in hazard associated with further time points.\n\nI've seen with my regressions that when I only test time points that my product can tolerate, then extrapolating (e.g. to 25 days) will inappropriately predict that my product will be fine.  I'm not sure if this is an aspect of right censoring that survival analysis is more suited to; i.e. the acknowledgement that the study could not continue.\n\n&#x200B;\n\nFor the metric of survival, there are indeed quite a few factors that go into a binary consideration of failure.  They include microbial spoilage, product appearance, and the sensory attributes.  If any fail, we would consider that to fall within the fail bucket.  If none fail, we consider it not failing.   In practice I don't use microbial spoilage, simply because it's costly and time consuming to collect microbial data on all of my samples, as well as it being an uncertain predictor of whether or not the product is acceptable sensory wise (there are situations where microbial counts are elevated but sensory is fine, which would result in false positives for predicted failure if the microbial counts are themselves still tolerable from a food safety, regulatory standpoint but used as a proxy for sensory failure)."", 'I am running a couple studies where I specifically look for visual signs of failure on packages aged out to 45 days.\n\nEg I have package IDs 1 through 45 in several different aging conditions, and every couple days I will inspect them and mark them as 1//0.  I believe different conditions will cause different failure rates for a given time, but I expect that by the end of the study, all packages will have failed (45 days).\n\nYour comment about looking at a covariants effect on survival over time is something I hadnt thought about, but I can definitely see it being apparent with plotting', 'The notion of all products failing as you test them further down the line / further in time is in perfect agreement with the assumptions of standard survival analysis (in the end, all subjects will experience the event of interest). Since you have a rather rough time scale or at least it appears that you test your samples routinely every x-amount of days, you should take a look into discrete time survival analysis.', ""So perhaps to help you think about survival analysis, you might look at discrete time survival analysis.( Eg probability of not spoiling for another 7 days)\n\nYour base model ( no covariates) has survival curve of form (1-p)^n \n\n\nWhere p is probability of spoiling in next seven days and n is the number of seven day periods.\n\n[ Does that look correct to you?]\n\nYou then just use logistic regression to estimate spoilage  in the next 7 days using your covariates.\nThis requires you to regularly test your product... Ie if you only test at 21 days how do you know whether it hadn't already spoiled at 7 days""]"
[C] What is a normal W2 rate after 5 years biostats experience?,"I was making 145k+12% bonus (162k total) at my last full time employee role working remote. Now going into the contract world, is $90/hr reasonable? I figure the loss of PTO is worth 10k, and health benefits another 10k, so $90/hr (180k - 20k) = ~ 162k.",12jq69o,SnowceanDiving,1681314218.0,0,0.5,"['Typically the calculation is made this way:\n\n2.5 x annual total compensation / 2000.\n\nSo for you, it would be:\n\n2.5 x 180k / 2k = $225/hr', 'Im pretty sure for contract work, you charge a lot more than 10% more. Dont forget that independent contractors pay A LOT more in taxes that are no longer paid by your employer. Id do at least 30 percent more minimum, so 200k a year, but Id recommend more. You may want to seek out professionals in contractor negotiation if you dont know what youre doing.', 'Are you coming from Pharma or CRO what specifically in biostats where you doing. Clinical trial analysis or data management?', '225/hr as in annually $468,000? Is this a joke? I dont think the loss of benefits and PTO amount to a ~300k difference. Or did you mean 225k annually? (still seems high lol(', 'Unfortunately, diagnostics. The most rare, easy, and useless field. Hugely regret getting into it.', 'Not a joke. Just google it.', 'Understood. Still the R skills can transfer. Data management is moving to r from SAS. They have now submitted a full FDA eDTC entirely done in r that was accepted. Some of the largest Pharma companies in the world have announced they are all in on r and open source and moving from SAS. Five years in good time to transition.', 'like, I get taxes take more. And health insurance is maybe worth 20k. But even after PTO Im not sure where this 300k difference comes from. Regardless, I doubt anyone would ever pay me this for just 5 years experience (especially in R not SAS and no FDA experience)', ""You did (substantially) undervalue your insurance and PTO, not to mention your employer's share of taxes and retirement.\n\nBut the *big* thing is that you are unlikely to have anything close to 2000 billable hours. You get to buy your own computer and your own desk and your own high-speed internet, and spend a bunch of your own time setting that computer up and installing software on it, and spend your own time reading papers and learning new techniques and doing practice projects, and only get paid for *actually producing something.* \n\nIf someone actually offers you 6+ months of fulltime work without benefits, sure, it is fine to only ask for something like 1 times what you'd ask for as a salaried worker. But if you are trying to assemble a bunch of small consulting jobs into a year's worth of money, 2 times really is standard. You will be very very busy if you actually get 1000 billable hours in a year."", 'Got it. I just need to avoid contract work then. Absolutely no way Id be offered $225 on W2! Thanks very much for this info']"
[Q] do I use probit or logistic regression?,"

I am dealing with corporate disclosures with binary dependent variables. 1 if company discloses whistleblower policy and 0 if it doesnt. Independent variables are number of directors on the board, percentage of women to men on the board and percentage of independent directors on the board. This is panel data from 42 firms over 15 years. Do I use probit or logistic regression?",12jm1tc,EbiraJazz,1681305593.0,12,1.0,"[""Use Logit. \nProbit and logit produce basically identical marginal effects and logit is easier to interpret.  Either way, it doesn't matter too much, just make sure you get the marginal effect and don't use the coefficients directly."", 'Of the two, logistic is usually a good default. The probit flattens out fasterthat is, the normal CDF approaches its asymptotes more rapidly than the logistic functionso it may be a better fit in situations where it makes sense to expect probabilities very close to 0 and 1.', ""Logistic assumes that the effects are additive on the logit scale. Probity assumes they're additive on the probity scale. If all predictors are discrete and you saturate the model, then reality fits either assumption. To the extent your model isn't saturated in either case means your curve fitting.\n\nThat said, the interpretation of coefficient estimates...in a logistic model they are the logged odds ratio e.g for a binary predictor, \n\n     log( (p(Y=1|X=1)/p(Y=0|X=1))  \n        (p(Y=1|X=0)/p(Y=0|X=))\n\n\nIf X is continous then replace X=0 and X=1 above with X=x and X=x+1.\n\nFor probit the interpretation is that there is really an unobserved continous outcome which we observe the thresholded version of and the effects of covariates is to shift the threshold around. It's used alot in psychometrics.\n\nI would go with logistic\n\nFinally, good point raised that the observations are most likely not statistically independent. You'll need to use something like a mixed generalized linear model (logistic with mixed modeling capabilities) to add a group effect if you can decide how to break your observations into independent groups."", 'Do logit, but also do probit and show that table in the appendix to prove you got similar results.', 'I found out that this is actually often an artifact of the field youre in. As everyone else has said, it practically makes no difference. But, one is usually preferred over the other depending on your field. Its a neat little thing!', 'Whats the distribution of the dependent variable? How are you handling temporal dependence?', ""None of what you've described is dispositive for choosing between them."", 'Only use probit if you have ordinal variables with > 2 levels.', 'This is a hot take but Id just use OLS (Im an economist lol)', 'Can you expand on getting the marginal effect and not using the effect directly. Do you, say, integrate the effect over the variable?', ""\n> For probit the interpretation is that there is really an unobserved continous outcome which we observe the thresholded version of and the effects of covariates is to shift the threshold around. It's used alot in psychometrics.\n\nThis is definitely the more common motivation for probit, but the latent variable interpretation is also available for logistic models, in which case its logistic distributed. Further, the probit model can be motivated simply in terms of a binary response but with probit link, no direct reference to latent variables required."", 'I second this. The boards are likely not independent for each decision. You might want to consider adding a random effects covariate to account for the non-independence of the board members and a covariate that accounts for time. A priori I wouldnt necessarily expect time to have a linear effect either. It could be that there are changes in the environment (eg. legal decisions, financial factors) that affect the decision making in a non-linear way. Not sure how exactly to propose handling the time factor maybe also a random effect.', 'OLS is a poor choice given the binary dependent variable.  I know some people do that, but frankly it doesnt make sense.', ""[Even tho he doesn't LPM](https://www.reddit.com/r/economicsmemes/comments/s3ni9v/how_true_is_this_do_economists_only_use_linear/)"", 'Perfectly fine to do if the 0 and 1s are balanced', 'Why?  Are you just a fan of linear probability model', ""In a regular linear model, you have y=bx, when x increase by 1, y increase by b.  That's the marginal effect.  dy/dx.  \n\nWhen you do probit/logit, you don't have a nice linear effect, so when x increase by 1, prob of y doesn't increase by b.  With logit, you'd get something similar, but the coefficients are not the same as  percentage point change, which is what you usually want.  The signs are the same but magnitude of change isn't"", 'Marginal effect is a super confusing term because it commonly has two almost exactly opposite meanings. \nFor people who come from a more economics adjacent background its common to use the term to refer to derivatives of the conditional mean function. The root of the name comes from economists focus on a marginal change where you barely perturb covariates.\n\nFor people with a more epidemiology adjacent background it is common to use the term to refer to effects that are averaged over some distribution. Marginal here is referring to marginal distributions as opposed to conditional.', 'Many of them do it because of the concern that with logistic regression omitted variables that are uncorrelated with the variables in the model nevertheless lead to bias in the estimates. In linear regression they just inflate error variance\n\nThis is not necessarily the greatest argument - theres problems using regression as well - but it is a reason often used', 'I agree. I think economists do it because its more interpretable', 'It actually does make sense a lot of the time.', ""Isn't this one of the main selling point of the logistic regression, that each coefficient will give a easy-to-interpret multiplicative effect on the odds?"", 'So is the procedure of getting tue marginal effect the same as that for getting a marginal distribution, i.e integration?', ""Logistic regression has fine interpretability... although maybe because economists work with percentage data a lot, it's a little confusing as you'd be interpreting percentage changes of percentages?"", ""Curious, how does modeling something as continuous, when it's actually binary, make sense?"", ""You might be thinking of just logging one or both sides.\n\nLogit is ln(p/(1-p)), or log of odds ratio.\n\nGenerally, you want to know how much more/less likely an event will occur if you increase your dependent variable by 1.   This number is in percentage points, not relative odds.   \n\nEx: if number of directors on the board increase by 1, how much more likely will the company discloses whistleblower policy.  So if the average marginal effect is 0.05, then increasing number of directors by 1 will increase the probability the company discloses by 5 percentage points on average.  Of course, you can also get the marginal effect at a specific point.\n\n\nEdit: I'm talking from economics perspective.  Maybe other fields prefers using odds ratio.  Although The scenario in this thread seems to fall under economics"", 'Thats not the reason', 'Because it doesnt matter. Look at the logit curve. Look at a straight line. Theyre the same when there isnt heavy class imbalance. Using a normal linear model allows us to do all sorts of additional adjustments (try doing fixed effects in a logit).', 'Linear regression models require that the conditional mean is representable as a linear function of covariates. This doesnt specify any hard constraints on what the response should look like, and can be valid for discrete variables. There is risk that extrapolation of the linear mean does not respect the bounds of the dependent variable. However, with a sufficiently flexible function the linear model can still accurately capture the true conditional mean. A flexible basis that reduces extrapolating behavior, such as penalized spline bases, can also help to make linear probability models better behaved. \n\nAs for why to bother with LPM? For some settings such as panel data the LPM can be much easier to work with theoretically than logistic models.', ""Sure, I get what you're saying w.r.t. marginal effect.It just gets a bit messy since the increase is not linear, i.e., 2x number of directors won't give 2x the marginal effect.With logit, the odds p/(1-p) = exp(beta0)\\*exp(x\\*beta1)\\*... and so on, which means that there is some reasonably quick interpretation of coefficient-effect on the odds.\n\nI'm a bit skewed by medial research though, where odds-ratios are the bread and butter of the analyses."", 'What? A logit curve can be approximated with a straight line if *none* of the data points are near 0 or 1, but in most applications *all* of the response data points are 0 or 1. It might be defensible if your model\'s range of applicability is limited to, say, 25-75% probabilities *and* your data is dense and complete enough that the average y-values in each ""bin"" are constrained far from 0 or 1, but your model loses all meaning anywhere close to the ends. It\'s a pretty limited range of applications you could get away with this, it\'s a lot harder to think of a case where it works than one where it doesn\'t.', 'This assumes that the logit curve is right. We dont know that. Read: https://blogs.worldbank.org/impactevaluations/whether-to-probit-or-to-probe-it-in-defense-of-the-linear-probability-model\n\n\nhttps://www.mostlyharmlesseconometrics.com/2012/07/probit-better-than-lpm/', 'No model is ""right"" but a logistic model is pretty much absolutely less wrong for modeling probabilistic outcomes compared to a linear one. That doesn\'t mean there aren\'t *specific cases* where you can reasonably approximate with a linear model but you have to give a very good reason to justify it in each individual case.', 'Maybe true if you ignore all the other benefits of OLS, and in this case, we have.panel data!!!']"
[Q]Testing a difference for statistical significance,"Hey guys, I hope this is the right sub. Basically, I have a Dataset with thousands of observations. Each observation includes a name, a number and is either labeled A, B, C, D or E.
What I have done so far that I calculated the mean for every subgroup, I.e. the mean for all observations labeled A is 0.372, for all labeled B it's 0.264 and so on.
I was wondering if it's possible to test whether the differences among all 5 groups are statistically significant and if so, how is it done?",12jlwcy,lalelu20022001,1681305247.0,5,1.0,"['Sounds like you might be looking for an ANOVA.', 'Another way to test for the significance of the differences among the means of the five groups is to use a set of pairwise t-tests to compare each group to every other group. This approach is known as multiple comparisons or post hoc testing.\n\nTo perform pairwise t-tests, you would calculate the t-value and associated p-value for each pair of groups. The null hypothesis for each pairwise comparison is that there is no significant difference between the means of the two groups. The alternative hypothesis is that there is a significant difference between the means of the two groups.\n\nThere are several methods for conducting pairwise t-tests, including Tukey\'s HSD (honestly significant difference) test, the Bonferroni correction, and the Scheff method. These methods adjust the p-values to account for the fact that multiple comparisons are being made, which reduces the risk of making a type I error (rejecting the null hypothesis when it is actually true).\n\nIn general, the ANOVA approach is preferred over pairwise t-tests because it provides a more comprehensive and efficient way to test for differences among multiple groups. However, if the ANOVA results indicate a significant difference among the means of the groups, post hoc testing can be used to determine which specific groups are significantly different from each other.\n\nMany statistical software packages, including R and Python, have functions for conducting pairwise t-tests with various correction methods. For example, in R, you can use the `pairwise.t.test()` function to perform pairwise t-tests with the Tukey HSD, Bonferroni, or other corrections. Here is an example code in R:\n\n```\n# create a data frame with your data\ndata <- data.frame(\n  name = c(""observation 1"", ""observation 2"", ""observation 3"", ...),\n  number = c(0.372, 0.264, ...),\n  label = c(""A"", ""B"", ...)\n)\n\n# perform ANOVA\nmodel <- aov(number ~ label, data = data)\n\n# perform pairwise t-tests with Tukey HSD correction\nTukey <- TukeyHSD(model)\nTukey\n``` \n\nThe `pairwise.t.test()` function takes the form `pairwise.t.test(data$number, data$label, p.adjust.method = ""method"")`, where `data$number` is the response variable, `data$label` is the grouping variable, and `p.adjust.method` is the correction method. The `TukeyHSD()` function provides the Tukey HSD correction for pairwise comparisons.', ""The ANOVA that other people suggest only works if you **explicitly don't care which of the five groups are different**, and just want to know whether **any of them differ**\n\nThat's almost never the case, but if it is what you want to know, fair enough\n\nOtherwise you need the tests which are usually called post-hoc tests because often done after an ANOVA. But they don't depend on the ANOVA and you can just do them."", 'One way ANOVA.  The number is your dependent variable and A thru E is group or independent variable.  Excel can do this, as well as R, Minitab, etc.', 'Yes, it is possible to test whether the differences among all 5 groups are statistically significant. One way to do this is to use analysis of variance (ANOVA). ANOVA is a statistical technique that can be used to compare the means of three or more groups.\n\nTo perform an ANOVA, you first need to test for the equality of variances among the groups. You can use the Levene\'s test or Bartlett\'s test for this purpose. If the variances are found to be equal, you can proceed with the ANOVA. If not, you can use a modified version of the ANOVA that takes into account unequal variances.\n\nThe null hypothesis in ANOVA is that there is no significant difference among the means of the groups. The alternative hypothesis is that at least one of the means is different from the others. If the p-value associated with the test statistic is less than the significance level (usually set at 0.05), you can reject the null hypothesis and conclude that there is a significant difference among the means of the groups.\n\nThere are many software packages that can perform ANOVA, such as R, Python, and SPSS. In R, for example, you can use the `aov()` function to perform ANOVA. Here is an example code in R:\n\n```\n# create a data frame with your data\ndata <- data.frame(\n  name = c(""observation 1"", ""observation 2"", ""observation 3"", ...),\n  number = c(0.372, 0.264, ...),\n  label = c(""A"", ""B"", ...)\n)\n\n# perform ANOVA\nmodel <- aov(number ~ label, data = data)\nsummary(model)\n```\n\nThe `aov()` function takes the form `response ~ factor`, where `response` is the variable you are interested in (in this case, `number`) and `factor` is the grouping variable (in this case, `label`). The `summary()` function provides the ANOVA table, which includes the F-test statistic, the p-value, and the degrees of freedom associated with the test.', 'Good point about not needing to do the ANOVA first. I think it is unfortunate that the convention is now to call tests such as the Tukey hsd post-hoc since they should be specified a priori but there is no going back.', ""I would also add there's no need to check for the assumptions of normality and homoscedasticity. I would suggest to conduct a Kruskal-Wallis test anyway, followed by a Wilcoxon (nonparametric) post-hoc."", 'Thank you for your replies, they have been very helpful :)']"
[Q] How to convert effect size (derived from meta-analysis) to another unit for downstream analysis (cost-effectiveness analysis)?,"Hey everyone,

I'm trying to conduct an Indirect Treatment Comparison (ITC) between two treatments for pain. I would then like to conduct a Cost-Effectiveness Analysis (CEA) between the two treatments based on the findings of the ITC. There is a recent meta-analysis of randomized controlled trials (RTC) for one of the treatments that I think is suitable for the ITC analysis. However, they report their findings in effect size instead of EQ-5D values (which from what I understand is the necessary measure required to then conduct CEA).

My question is:

From my understanding, I can perform the ITC between the two treatments using effect size and standard error values only. I do not need to convert effect size to EQ-5D values, for example, in order to conduct the ITC. Please correct me if this is wrong!

Then, I would somehow need to derive EQ-5D values from the effect size derived from the ITC. How do I go about this? Do I simply multiply an EQ-5D value (derived from the literature) for one treatment by the effect size to then obtain an EQ-5D value for the other treatment? I don't think this is the correct way but I have no idea.

Thanks for any help you can provide with this--really appreciate it!!",12jjjze,pantaloonsss,1681300056.0,4,0.84,"['>Do I simply multiply an EQ-5D value (derived from the literature) for one treatment by the effect size to then obtain an EQ-5D value for the other treatment?\n\nAbsolutely not! \n\nYou would need to determine if the outcome from your meta-analysis is predictive of quality of life, and if so, the appropriate functional form. Only then can you think about applying a treatment effect.', 'There is a meta-analysis for different injectable treatments for knee pain. They used a disease-specific outcome measure that is distinct from EQ-5D. However, there is a conversion factor between the disease-specific measure and EQ-5D that can link the two. The problem is that the meta-analysis reports their findings in effect size . Is there a way for me to convert the effect size to the disease specific outcome?\n\nThanks a lot for your help!']"
[Q] Calculating mortality rate from survival rate," I'm working on a meta-analysis, and one of the studies has the survival rate given. Can I calculate mortality from the survival rate? For eg if survival is 35/50, then can I do 50-35=15, so 15/50 is the proportion of people who died?",12jcn71,Choice-Walk-9571,1681280934.0,6,0.87,[]
[Q]Difference between rejecting the null hypothesis and accepting the null hypothesis,"I have been thinking about how we do not accept a null hypothesis if we reject it, and I am not sure if i do not understand it well enough, what  I think is that we do not accept the null hypothesis because when we fail to reject the null hypothesis we are only saying that the alternative hypothesis is incorrect but that does not make it impossible to another alternative hypothesis to appear and this one be correct. Please let me know if this is correct

&#x200B;

In case that the last paragraph is correct then I do not know why we say that we do not accept the null hypothesis if this is based in how we think things are, would it not be more appropiate to say that the null hypothesis is correct when we compare it to the the alternative that we just reject, because we do not know which alternative hypothesis might make us reject the null

Thank you",12ja2p4,Unhappy_Passion9866,1681274156.0,31,0.95,"['The wording I have consistently learned (and taught) has been\n\n\\> we do not have evidence to reject the null hypothesis.\n\nThat, regardless of what the alternative hypothesis is, we are speaking in terms of rejecting the null hypothesis (or not). \n\nThe reason for this is that for common statistical tests like t-tests we are proposing a distribution with known parameters as the null hypothesis so that we can test how likely it is that the observations we have made came from that distribution. The alternative hypothesis is often simply ""the distribution has different parameterization"". In the case of a t-test that is to do with the mean of the distribution, but just simplifying here,', 'So the reason we cannot say that the null hypothesis is correct/incorrect is because the basis of hypothesis testing is making an assumption. When we state a null hypothesis, we are assuming that it is true (or equal). Therefore, the results of our hypothesis test is saying: based on our testing, we do not have enough evidence to reject the null hypothesis OR we have enough evidence to fail to reject the null hypothesis. \n\nI think im rambling, but what im trying to say is our main goal is to see if we have enough proof to reject the null hypothesis, so either we had enough proof and yay we rejected or we didnt have enough proof and we failed to reject it. \n\nHope this clears it up a bit', ""Imagine if I have a coin. I want to test if it's a fair coin. If I don't flip it, there is insufficient evidence to reject the null hypothesis of fairness, but it doesn't make it a fair coin."", 'Suppose the null hypothesis is that a given coin is fair. We flip the coin 100 times and it lands heads up 98 times. Can we reject the null hypothesis? Yes. If the null hypothesis were true, such an outcome would be extremely unlikely.\n\nBut suppose instead the coin lands heads up 53 times. Certainly we cant reject the null hypothesis. But can we *accept* the null hypothesis as true? Can we conclude that the coin is fair? Well no, we dont know that the probability of landing heads up is exactly .5, of course. I mean, maybe P(heads) is .51 or .53, who knows.\n\nAll we can do is retain, or fail to reject, the null hypothesis.', ""To me, the main reason you (generally) cannot accept the null hypothesis is that the likelihood that you reject the null hypothesis when there is no effect present (type I error rate/alpha, almost always set at 0.05) and the likelihood that you will fail to reject the null hypothesis when there *is* an effect present (type II error rate/beta, set variably, but often at 0.2) are not the same.\n\nThat is, consider that you have set your effect size of interest and you have enough statistical power to detect an effect at least 80% of the time. If you fail to detect an effect, there is still a 20% chance that an effect was there, you just did not observe it. This is still well above the generally accepted error rate of 5% for type I errors. \n\nOverall, the point is that asking whether a difference exists between two groups is a different question to whether no difference exists between two groups, so you can't really use the one test to answer both (unless you are a Bayesian ;D)"", "">when we fail to reject the null hypothesis we are only saying that the alternative hypothesis is incorrect\n\nNo, we don't. It sounds weird, but hypothesis tests provide only indirect information about the hypothesis. What they provide you with is Prob(sample|H0), not Prob(H0|sample). More importantly, they say nothing about Prob(sample|Ha), let alone Prob(Ha|sample)."", 'Long story short, you never accept the null hypothesis because there will never be enough evidence for it. That is within the realm of classical statistics. In sequential analysis and Bayesian statistics, however, you can accept H0.', ""Think about the p-value as an example. \n\nSay you have a p-value of .04. Then, we are saying if the null hypothesis were true, there is a 4% chance we would've observed this data or more extreme data. If we set alpha=.05, then we would accept this as sufficient evidence that the null hypothesis is unlikely, and we reject the null hypothesis.\n\nIf the p-value is 6%, this means there is a 6% chance we would've observed this data or more extreme data given the null hypothesis was true. Again, if alpha=.05, we say this isn't sufficient evidence to reject the null hypothesis. But on the other hand, 6% is still a pretty small chance, so we certainly do not want to accept the null hypothesis."", 'Accept/reject language is kind of a hang over from fixed level testing.\n\nIf you are basing your conclusion on a p-value, then you can talk about the strength of evidence against the null hypothesis.', 'The way I was taught was to compare it to guilty vs. not guilty verdicts in court cases.', 'Its sometimes helpful to approach this in terms of confidence intervals. Every value outside the interval can be rejected but the data are not inconsistent with values in the interval. Its not just 0 that cant be rejected, but (for a continuous measure at least) an infinite number of values somewhat close to 0. There is no basis to conclude that any specific one of these infinite values is the true value of the parameter of interest.', ""Hello I think total acceptance of a proposition/conjecture is hard to be comfortable with when you are aware of the limits of sampling procedures. I assume accepting the null hypothesis will be okay if there ever was 'definitive' evidence for it. But I think accepting the alternative hypothesis is something we do implicitly too whether we admit it or not when we find evidence to reject the null."", 'Accepting or rejecting hypotheses is a matter of what is true. Truth is a very difficult thing to get at, and it requires a lot of careful arguing and evidence that is NOT limited to just performing statistical tests. In general, no traditional (frequentist) hypothesis tests can ever tell you about the truth or falsity of some hypothesis. In fact, theres a lot of discussion around statistics that comments on how most null hypotheses (e.g., that the mean of two groups is equal) are known to be false with a high degree of certainty. For example, two random populations almost certainly dont have the same mean height. If we could measure the heights of every member of both populations and compare them, then any discrepancy between the two numbers would prove the null hypothesis false with a higher degree of certainty than any random sampling/test procedure. \n\nSo whats the point of using p-values to test null hypotheses? P values can tell you how consistent your observations are with a specific null hypothesis, and this can potentially be an interesting piece of evidence based on the broader purpose of your statistical inquiry. We may already believe that two variables should be related to each other based on theoretical reasons and want to test the relationship with data. Seeing that our observations are very inconsistent with the null hypothesis of no effect increases our confidence in our alternative hypothesis. Keep in mind in this example we already knew the null hypothesis was likely false, but still tested our data against that null hypothesis to further increase our confidence in our thinking. On the other hand, seeing that our data could be consistent with a null hypothesis does not mean the null hypothesis was true all along. We already had reason to believe it wasnt true, and now we have to ask ourselves some critical questions about our theories, the data we collected, the testing procedures we used, etc to decide if the results we got tell us something unexpected that should force us to reconsider what we think we know. As a part of our discussion, we may decide that we still strongly believe the null hypothesis is false because there are other considerations we didnt think of (e.g., the relationship only exists when certain conditions are met), but its not the p-value that ultimately decides what we consider to be true. Hence, we dont accept the null hypothesis, we just note that our data from that one experiment could be consistent with it.', 'In the Null Hypothesis Significance Testing framework, you do not say ""accept the null"" because you can fail to reject the null for two reasons:\n\n1. The null really is true\n2. You have not collected enough evidence (data)  \n\n\nYou never know which one is the case, so it does not make sense to claim the null is true.', 'in null hypothesis significance testing you\'re not making probability statements about any hypothesis. You\'re controlling the type I error rate by assuming the null is true. So if your significance level is 0.05, when you say you reject the null if the p-value is under 0.05, what you mean is that in the long run you will only be mistaken 5% of the time. ""rejecting the null"" doesn\'t mean it\'s false, or likely false, or that the alternative hypothesis is true or likely true.\n\nThe alternative hypothesis only has a role if you also want to control for type II error rate. Imagine that for a particular alternative hupothesis you construct a test with 80% power. This means that if you reject the alternative every time the p-value is above the significance level, then you will be making a type II error 20% of the time, i.e. you will incorrectly reject the alternative when it\'s actually true.\n\nAgain, all this ""rejecting"" and ""accepting"" still doesn\'t mean that the null nor the alternative are true, false, or whatever. It\'s just a set of rules that if followed will guarantee that in the long run the type I error rate and the type II error rates remain below a set of fixed parameters (alpha and beta).\n\nIf null hypothesis significance testing sounds like bullshit to you, it\'s because in a certain practical sense it can be.', 'Sounds like homework with fluff for disguise.', 'Thank you!!', 'Thank you!!(all was reallly clear btw)', 'This type of questions were your homework? sad...', 'Damn... sucks you never made it past 101.']"
[Q] Differences between the likelihood ratio test and the power function,"I have been reading about the likelihood ratio test and how this allows us to compare two different parameters to decide wheter we reject the null hypothesis or we do not reject it. But I also know that the power function allows us to know the probability to reject the null hypothesis. So I do not understand well enough why would we need the LRT if we have the power function and viceversa, what is the diffence in the purpose of both of these methods",12j9ym5,Unhappy_Passion9866,1681273867.0,1,1.0,"[""The power function is not a method. It's an intrinsic property of *any* given hypothesis test. \n\nOn the other hand, the LRT for a given set of hypotheses is just one, amongst many, possible way of choosing a test. It has a power function, just as do any of the others. Other potential choices are score tests, Wald tests, sequential tests, or even ad hoc tests imagined up by a tester. They all have their own particular way of defining a rejection region, which inherently determines the power function."", 'The power function is a *property* of a test. It is NOT a ""*method*"". Once you fully specify your rejection rule and the assumptions (along with relevant assumptions you want to compute power under, and a sample size), you should be able to compute your power function. \n\nA likelihood ratio test is one way of obtaining a test -- more specifically, it defines a way to obtain a *test statistic* (and consequently, to obtain a rejection rule), based on the ratio of two likelihoods. LRTs have some attractive properties. The power function of an LRT follows from the rejection rule of the test (and the assumptions you\'re applying under H1) -- the power function doesn\'t tell you when to reject. \n\nRather than just read *about* these things, you will understand better if you learn enough to perform some basic computations -- (a) given a test statistic, and some assumptions to apply under H0, to be able work out how to obtain a rejection region that  yields a desired significance level alpha\\*, and (b) to compute the power at any given effect size (given some set of assumptions under H1), and hence obtain a power function. This doesn\'t have to be for an LRT specifically -- the concepts of test statistic, rejection rule and power are quite general. Once you\'ve actually gone through that process a few times, the concepts should be crystal clear.\n\nIndeed, the power computation can be done using a computer -- at any point under the alternative, you can simulate from the situation the assumptions you make (under H1) specify, and see what the proportion of rejections is (along with its standard error which should be quite small if you\'re doing enough simulated samples at that effect size). If you do that across many effect sizes, you will see the power function as a function of the effect. \n\nHere\'s a comparison of power curves for three (paired-differences/one sample) tests:\n\nhttps://i.stack.imgur.com/UEAaD.png\n\n... there being compared under the assumption that the distribution of the pair-differences had a t distribution with 3 degrees of freedom.\n\nThere are several different power curves (power functions) here:\n\nhttps://stats.stackexchange.com/questions/367125/how-to-graph-wilcoxon-test-power-r/367128#367128\n\nconsdered in each case for some specific set of assumptions (sufficient to simulate) under which the power of some tests were being compared.\n\n\nYou might also be interested to look at the power function as a function of n given some effect size, but it\'s usually not as interesting to look at because of the way standard errors of statistics nearly always scale in proportion to 1/n (which I refer to as the ""root-n"" effect), at least for sample sizes that aren\'t too small. Once you have one point (or in some situations, perhaps two), you can usually work out the power at other values of n immediately from that. By contrast the power function considered as a function of effect size can be a at least slightly more ""interesting"" to look at (though the broad properties may still be pretty obvious in most cases, even if not quite as numerically tied down).\n\n---\n\n\\* i.e. so the test doesn\'t exceed a given type I error rate when H0 is true -- though you\'ll want it to be as high as possible while respecting that constraint', 'Ah ok this really make sense, but then can i say that the power function, rejection region, etc are a consequence of the test, in this case LRT. \n\nLRT(or any other method)->creates rejection region, power function, etc', 'Yes. A hypothesis test consists of a set of hypotheses and a decision rule. This decision rule determines all of the properties of the test, with the most relevant property being the power function. How you choose is somewhat arbitrary, in the sense that there are an infinite number of choices, but the typical route is to set a significance level and find a decision rule which gives the best power at certain alternatives. The LRT is nice in that it provides a general framework in choosing this decision region in a wide variety of testing situations. It generally has no guarantee to be the ""best"" choice, but in situations where it isn\'t the best it\'s typicality an intractable problem to determine what that best decision rule would be (if it even exists).']"
[Q] How to think about power if my interest lies in in not rejecting H0,"I don't have experience in designing AB tests such as a price elasticity one, where **I want to see whether an increase in the price of my digital product causes sales numbers to drop**. In this scenario, a good result for the team would be to conclude that *""data does not suggest that price B generates less sales than price A, go ahead and raise its price"".*

I intend to use **conversion % as a binomial success metric** (sales / number of trials started). Considering this is a product that doesn't sell a large amount of units per day (let's say 4-5 sales on average, from something like 15 trials started).

Here, if I accidentaly underpower my test, I risk actually hurting company revenue, just because my data were too small. 

So,

**tldr: Are there specific good practices that must be taken into account if the ideal outcome of an AB test revolves around not finding significant differences between two groups? How many days are too many when stretching test duration to achieve a given minimum sample?**",12ixijr,pdr07,1681246853.0,7,1.0,"['Youre hitting on an important issue with NHST which is that it can provide an absence of evidence but not evidence of absence. Absence of evidence could be due to issues such as being underpowered. To draw inferences about evidence of absence, you would likely want to implement something such as an equivalence test to test whether your confidence interval is practically equivalent to a region around zero, which suggests its approximately equivalent to 0', ""Sounds  like what you really need is a nonferiority test (or perhaps an equivalence test,  but the description  sounds more like noninferiority). \n\nThen you'd care about power, because low power wouldn't establish what you require."", ""This sounds like an economic question, I also don't see how this is applicable to using binomial distribution. The change in conversion % per change in price is what you want to know, is it not?"", 'Compute a confidence interval. Then the uncertainty due to a low sample size will be evident.', ""after the write up, I realize calling this a price-elasticity study is not 100% accurate, for the reason you're stating there, I apologize.\n\nIn the end, we just want to see if a small bump in product price is expected to hurt conversion or not, not so much in the conversion change by any monetary unit.""]"
[Q] Help with (I think) binomial probability calculation,"I've tried looking this up online but can't quite find what I'm looking for. Would appreciate any assistance.

I have a deck of 20 cards, each is unique. I am drawing 5 cards from the deck. There are 3 cards I'd consider successes in the deck. No replacement. 

How do I calculate the odds of specifically drawing all 3 on one draw of 5 cards? Of drawing. 2 of the 3? 1 of the 3? None?

Appreciate the help. All the examples I found didn't involve both unique cards and multiple cards drawn without replacement. Cheers.",12io0dl,MoxVachina1,1681228056.0,1,1.0,"[""> How do I calculate the odds of specifically drawing all 3 on one draw of 5 cards?\n\nI presume you want *probability* rather than odds (they're not the same thing). \n\n\nYou can use the hypergeometric distribution\n\nhttps://en.wikipedia.org/wiki/Hypergeometric_distribution\n\nWhich in this case is easy enough to do by hand (well, maybe with a calculator) after a bit of cancellation.\n\nOr you can just use a computer ...\n\nIn R:\n\n    dhyper(0:3,3,17,5)\n    [1] 0.39912281 0.46052632 0.13157895 0.00877193\n\nIt can also be done in Excel\n\n     =HYPGEOM.DIST(0, 3, 5, 20, 0)\n\ngives\n\n     0.399122807017544\n\nand\n\n     =HYPGEOM.DIST(1, 3, 5, 20, 0)\n\ngives \n\n    0.460526315789474\n\netc"", 'The odds are the number of ways you can get a success divided by the number of ways you can draw 5 cards.\n\nFor the denominator (number of ways you can draw 5 cards), you do (20 choose 5) = 20!/(5! * (20-5)!). \n\nFor drawing all 3 success cards, there are (3 choose 3) * (17 choose 2).\n\nSo you get (3 choose 3) * (17 choose 2) / (20 choose 5).\n\nFor 2 of the 3, its (3 choose 2) * (17 choose 3) / (20 choose 5).\n\nFor 1 of the 3, its (3 choose 1 )* (17 choose 4) / (20 choose 5).\n\nFor 0 of the 3  its (3 choose 0) * (17 choose 5) / (20 choose 5).\n\nA little trick is that for these kinds of problems, all of the numbers before choose in the numerator should add up to the number before choose in the denominator.  Same with the numbers after choose.  So for the 2 of 3 case, 3+17= 20, and 2+3=5.', ""This is very helpful, especially the Excel formula, since that's where I'm running the calculations (which I should have specified up front, first time asking stat questions on reddit).\n\nFollow-up: I'm making a probabilty tree (I think that's the right term?) for a sequence of several different draws. If I'm calculating the probability of drawing one specific card *without the other two cards that are considered 'successes' generally* from the deck (still when drawing 5 cards, no replacement), I originally thought the formula would be\n\nHYPGEOM.DIST(1, 1, 5, 20, 0)\n\nbut that yields 0.25. So then I tried \n\nHYPGEOM.DIST(1, 3, 5, 20, 0) / 3\n\nand that seems to (along with the rest of the formulas) average out to 1, meaning it checks out. Is that the right way to structure the formula? Thanks again!"", 'So the choose function is nice if you think about it in words.  Sometimes on a calculator its written nCr.  Its the function thats n! / (r! * (n-r)!).\n\nIn your head, you can think of this as a function that counts the number of different ways you can select r things from n total things.  So 20 choose 5 is short for the count of different groups of 5 I can choose from 20.  Thinking of it in words makes it easier.\n\nIts also may be a bit unclear why this works. \n\nImagine that when you choose 5 cards from 20, you do it like this:  First, shuffle the whole deck and lay it out in order front of you.  Second, separate the deck into the first 5 cards and the last 15 cards. \n\nThe number of ways you can shuffle 20 things is 20!.  So thats the starting point for step 1.\n\nHowever, a lot of these shuffles lead to the same hand of 5, just in different orders.  A hand thats ABCDE has the same cards as one that came out in the order CABDE, ACDEC, etc  in fact, if we were to count the number of ways to rearrange those 5 cards, it would be 5!.  Theyre all redundant and the same thing.  For the remaining 15 cards, they could come out in 15! orders.  \n\nSo, there are 5! * 15! different ways the two piles could get scrambled up and it would result in the same set of 5 cards being in our hand.\n\nWe want the total ways you can choose a unique set of 5 cards from 20.  When we shuffle the deck, there are 20! different orders. But only 1 out of 5!*15! is a unique set.  Or, rather, each unique set of 5 has 5! * 15! other shuffles that result in the same set of 5.  So we divide by that to get the number of unique sets of 5 cards we can draw from 20.\n\n20! / (5! * (20-5)!) = (number of ways to shuffle 20 cards) / (number of duplicates per unique outcome) = (number of ways to shuffle 20 cards) / ((number of ways to shuffle 5 cards )*(number of ways to shuffle 15 cards))', ""Sorry I didn't follow what you were trying to calculate but neither of those look right"", 'This is a helpful way of conceptualizing it, I appreciate you writing all this out for me to digest. Thanks!']"
"[Q] For an intention-to-treat analysis, what to do if new-users cannot be identified to prevent selection bias?","In observational (real-world evidence) studies about drug effectiveness, it's encouraged to restrict the study population to new-users. This can be e.g. done by including a washout period in which the drug has not been used (like non-use for 180 days before study inclusion). This is done to prevent selection bias (or prevalent user bias) that can bias the effect estimate. 

I am doing a RWE study, in which we want to do an intention-to-treat analysis because I am interested in the effects of *initiating* a drug vs. control. I however don't have pre-study inclusion data about drug use (or other covariates), so I cannot identify new-users.

If I include prevalent & incident users, I'm really biasing my results, so I was looking if there is anything I can do to mitigate this. I have follow-up data at 3-month intervals, and restricting the analysis to new-users at 3-months is also not possible since I don't have information on confounders beyond baseline. 

Does anyone know if there are any methods or new proposed techniques that can somehow deal with this at the analysis stage?",12il53q,Shoddy-Barber-7885,1681222431.0,19,0.93,"['I don\'t have any advice for you, I just wanted to object in the strongest possible terms to the misappropriation of the term ""intention to treat"" (ITT). It\'s not your fault, you didn\'t start it, but the concept is from RCTs and using it for so-called RWE is all part of the effort to pretend proper evidence is not really needed even when it\'s perfectly possible to do a pragmatic (""real world"") RCT.\n\nITT means that everyone randomised is analysed in the group to which they were randomised regardless of compliance or crossover. It is done to preserve the integrity of randomisation (because randomisation is done to ensure that the groups only differ by chance). ITT doesn\'t have any meaning in an observational study and you should find a different term because it is misleading.', ""I don't think it's possible to define new users without information on their medication history. \n\nYou could make an assumption that if the prescription date is close to the diagnosis date, they should be new users if and only if the drug is a known treatment of that disease for which they're diagnosed *and* they have no history of other diseases that are also treated by that drug.\n\nThis idea assumes treatment initiation is your time zero, and the grace period between diagnosis and prescription shouldn't be too long (clinician input needed)."", "">This can be e.g. done by including a washout period in which the drug has not been used (like non-use for 180 days before study inclusion). This is done to prevent selection bias (or prevalent user bias) that can bias the effect estimate.\n\nHuh? How does this prevent selection bias? There can be all sorts of reasons people end up being new users of a drug that correlate with whatever outcomes you look at, and are not the effect caused by taking the drug,\n\nIf you don't have tons of covariates, so you can adjust for some of those reasons, it seems very unlikely you get to estimate the effect of the drug."", 'I was confused about the term intent to treat population. Like you, I was thinking of the usual definition for RCT. \n\nI think youre too hard on real-world evidence (RWE). I agree it shouldnt replace an RCT, its but its useful when RCTs arent feasible or when used as a supplement to RCTs.', 'indeed, its embedded in the causal inference literature. ITT is used as a term to describe the causal contrast of interest that comes from the hypothetical target trial when we do trial emulation. There is ofcourse no such thing as ITT when there is no randomization, but when we emulate a trial with big data, the closest thing coming to ITT is iniating a drug & having no confounding by indication. \nIn observational data you cannot fully prevent confounding (ie unmeasured confounding), so you cannot 100% copy/emulate ITT. But its used to make the whole trial-thinking explicit.', 'Very clever take, but Im afraid that I cannot make that assumption, since the population is multi-morbid and the drug is for blood pressure. So the treatment has multiple indications.', ""RWE isn't just another term for observational evidence, it's part of a renewed assault on evidence, which is part of the endless war waged on behalf of Pharma shareholders. That's why they came up with a new term for it. And why they're misppropriating RCT terminology."", ""It's a technical term with a very specific meaning. It should not be co-opted.""]"
[Q] How to visualize correlation matrix over time?,The only technique I can think of is a heatmap as a gif. Other methods like a line graph wont work because I have too many variables and it would get crowded quick.,12hqtxs,bernful,1681150066.0,45,0.99,"['Heatmap as a gif sounds like an interesting idea to me.', 'Just spitballin, but depending on how many timepoints you have and how big the visualization can be, you could potentially make a matrix of graphs (arranged like a correlation matrix) where instead of just having a correlation value in cell of the matrix, you could put a line graph of the correlation values over time.', 'What do you want to convey with the visualization?  \n\nIf you have a simple message, then you can devise a simple visualization.  \n\nThe inclination to show everything often comes from not knowing what you want to say.', 'Maybe some 3D grid with one axis showing time, and each cross section being a timepoint?', ""So there's this paper that does this but they have  discrete timepoints\nhttps://www.nature.com/articles/s41586-021-04210-x"", 'For a non-fancy idea, perhaps you could just make a series plot (spaghetti plot) of the correlations over time? Might call your attention to some things.', 'What do you think you might see varying across time? If you have expected patterns like linear trends, you could fit trends ""per pixel"" over time then visualize the fitted.coefficients (e.g. slopes for a line fit) as a raster image.', 'How big is the correlation matrix?  Can you pick out a few values and show a plot of those values over time?', ""Here's an idea. Apply latent trajectory modelling to identify trajectory clusters for correlations and so reduce dimensionality. Then for the figure: in panel A) show either a baseline, or perhaps time-average, correlation matrix. In panel B) show a matrix which labels each correlation by its trajectory cluster. In panel C) show line plots of representative trajectories for each cluster in panel B). Use a common set of discrete colours between panels B and C so the link between the clusters and the correlations is clear."", 'I know you are in the exploratory phase but i not really sure of the use case. And with 100 features it might make sense to start dimension reduction.', 'What about performing matrix seriation on the correlation matrix averaged across time and then using the resulting index order to order the rows and columns of a correlation heatmap animation that changes over time?', 'At least make a dashboard with sliders or something so that you can control the speed and go through each plot manually. Or better yet just show them all at the same time on huge plots side by side so that you can compare them directly.', 'I have like a 100 variables I think it would look too cluttered like that', 'If the trends were roughly linear, you could just calculate the a slope for each cell and use that was the heat value', 'Im not sure yet its more so a visualization for myself to see if any discernible patterns arise', 'I like this idea', 'another thing (depending on your research question):\n\nIf you just want make a global comparison over time, just dimensionality reduce your entire dataset into a single dimension, then plot against time.\n\nIf you want to see which features change over time, run a linear model for each feature over time, then extract something like the slope and then only display a slope with a significant pval', 'Thats the thing, Im not really sure what Im trying to see', 'max 208, but I could trim the fat down to ~100', 'Ill look into this thx', ""That's a good idea. I think pywidgets in a Jupyter notebook with matplotlib could be used to do this, for instance. You could mock it up using those tools at least."", ""If you aren't willing to just do a bunch of line plots (understandable -- 100 variables means 4950 correlations each being tracked across time), I think it's best if you look into something like PCA for multivariate time series - there's lots of approaches out there (dynamic factor analysis, etc.), mainly in the area of econometrics and financial time series, AFAIK. The idea would be to plot changes in the underlying (lower dimensional) components.\n\nIt does complicate the interpretation somewhat, but with that many variables, it may be the only solution.""]"
[Q] How do I set up my data to perform Wilcoxon Signed Rank test?,"Sorry in advance if this is confusing, but I'll try my best to explain the layout of my data and what I am looking for.

Firstly, my data is set up like this:

I have about 100 participants and 6 speakers who produced 3 variables (let's label them a, b, c) which each have 3 variations within them. Participants each heard one variable from each speaker (for example participant 1 would hear Speaker 1 variable a, Speaker 2 variable b, Speaker 3 variable c, Speaker 4 variable a... etc.).

I've already checked the data and I have a non-normal distribution within the data. 

I now want to test the statistical significance of the variables from one another using the Wilcoxon Signed Rank test, and I have been advised by my supervisor, I should be using the difference (subtracted from the baseline - in this case, variable a) to calculate. So basically he wants a - b, and a - c. 

Now where my problem lies... if my participants only heard one variation from each speaker and they were balanced (participants hear 2 a conditions from different speakers, 2 b, and 2 c) how do I set my data frame up to calculate these differences? In a past experiment, my participants heard all variations, so I could easily calculate the difference and run the test since each participant heard the baseline condition, however in this structure of experiment that is not the case. 

&#x200B;

Any advise would be extremely helpful!!",12hn3ha,razzerpears,1681142461.0,2,1.0,"['If each row/observation has a separate column for each condition, that should be the appropriate dataframe for a Wilcoxon signed rank']"
"[Q] Newbie to Statistics, I have 109 sample stdev is that normal?","For a context, I am trying to do data visualization on count of product by category, there are 6 category and grandtotal of 1500 prosucts. The calculated mean was 214 and stdev.s was 109.
As i'm trying to do visualize this on a bell curve, is this stat normal? Or am i doing something wrong here",12hfztv,Admirable-Length178,1681126242.0,0,0.36,"['Are you calculating the mean and standard deviation per category ?  That may make more sense than calculating the grand mean and standard deviation.', ""So you have 1500 products, each of which is part of 1 of 6 categories?\n\nIn that case, the sample mean of the count of products per category is constrained at 1500/6 = 250. Something went wrong if you get 214, or the explanation you gave doesn't paint the full picture.\n\nThe count is likely not going to fit a bell curve, if anything counts are more suited to a Poisson-type model. However in this case there are some dependencies that might mess that up too. There is for example a degree of freedom lost in the last category. If you know the counts of the other categories and the total number of products, the last count is not a random variable anymore."", 'Before you even worry about bell curves and normality and things, I recommend you always just look at your raw data - in this case, a simple table of counts by category. For visualization, a simple [dotplot](https://core.ac.uk/download/pdf/153214994.pdf) works well for this kind of data (categorical on one dimension, and quantitative on another). Or alternatively a simple bar chart of counts or proportions.\n\nNow, to answer your other question of whether a 109 standard deviation is normal, I personally wouldn\'t put much trust in an estimate based on 6 numbers. (The magnitude doesn\'t look odd, or anything, though, fwiw -- if there was a huge positive skew you might have a SD that is ""inflated"" relative to your mean. For example, if you had a 200 SD with a mean of 200, that would be a red flag -- such a distribution would put 16% of the distribution below 0, which can\'t happen in a distribution of counts.)\n\nIt\'s also not a metric I would expect to perform well for this type of question in general (although it may not be bad for this particular set of numbers).\n\nFor ""counts of things"", something like a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) is usually a better starting point (it will approximate a normal for large counts, so may not be better in this particular case, but will tend to be more appropriate in situations where counts are not large).\n\nAnother approach frequently used for this kind of data -- if you are looking for information about the uncertainty in the numbers of products within categories) -- is a [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) model. In that approach, you would be estimating not the mean and standard deviation, but the proportion of the total (1500 in this case) allocated to each categories, and the uncertainty in those proportions. (And then you might use some other approach, like a Poisson, to model the total counts across different time periods, etc.)\n\nBut perhaps more importantly, I simply wouldn\'t (necessarily) assume counts of products across different categories to have a normal distribution in general. In fact, it\'s not unusual to have some kind of non-normal distribution when it comes to product counts. (See, e.g., things like [Pareto](https://en.wikipedia.org/wiki/Poisson_distribution) analyses, or various [""product quantity"" analyses](https://en.wikipedia.org/wiki/ABC_analysis) more generally -- note that I know nothing about conducting the latter, only that they exist!)\n\nEdit: saw your update that you want to test whether product sales are balanced across categories. For that, as someone else said, a simple chi-square test of counts will work.', ""Thanks everyone for your input: for better context I'm trying to examine whether or not the products sales are balanced across the 6 categories.   \nIf the bell curve does not work here, how do I examine the balance?  \n\n\nTIA"", 'Seems fine to me, that would just mean that on average a sample differs from the mean by a 109 products', 'Very roughly, you would expect these results if the following is true for your data: There are about 105-326 products for any given category for around 70% of the product categories. That would give you a mean of 214 and a standard deviation of 109. If that seems to hold true for your data, then the numbers you gave may be fine. If you look at your data and what I described doesnt seem to fit, you may want to dig a bit deeper and see why or try the calculations again.', 'Hi no im trying to calculate mean and stdev of numb of products by category.', 'Each product may have count, which would be an observation.', 'Hi, thank you for your input, if it makes things easier to understand, my purpose with this is to examine whether or not the products sales are balanced across the 6 categories. If the bell curve does not fit here, how do I examine the balance?', 'How do you visualize this on a curve bell though, do i have to calculate the distance ?', 'Then I think what you are doing makes sense.  ... However, as some of the other comments intimate, it would be a good idea to plot a histogram for each category.  If the distribution of values is very skewed the mean and standard deviation may be less helpful.  The median might make sense.  And the interquartile range (25th percentile and 75th percentile) may be helpful.  Half of the observations fall between the 25th percentile and 75th percentile.  In any case, looking at the distribution of data, and several summary statistics --- minimum, 25th percentile, median, 75th percentile, maximum, mean, standard deviation, number of observations --- will give you a lot of valuable information.', 'This sounds like a chi square test, which you can use to see if the categories were sold an equal number of times. (or if they are significantly different from a prior distribution of the amount of products in each category, to see whether they are over/undersold compared to the base frequency of a category).', ""https://www.automateexcel.com/charts/bell-curve-plot-template/ , this seems to be a decent tutorial, however a sample size of six is not large enough for a bell curve (at least 30 values is suggested), also btw stdev isn't exactly the average distance from the mean (I just instinctively think that way about it), more specifically it's a measure of how spread out your data is from the mean, you might wanna look up the formula for the exact definition (the formula is also different for a sample and for a population)"", ""If you're not doing it in excel you can also just look up the formula for a normal distribution and calculate it. However you'll also need to make shure that you're in fact dealing with a normal distribution, there are many tests for verifying that. I'd write more but honestly I don't consider myself to be an expert in statistics, I deal more in the programming side of things"", ""Even without knowing anything about the data calues, you shouldn't assume counts across different products would be anything like normally distributed - you'd expect the counts to be very skewed, perhaps even something like a zeta\n\nKnowing the mean and sd, it looks like the counts are pretty skew. However, it's not clear to me what value there is in putting a distribution across different products; their individual counts will be related across time"", 'thank you!']"
[question] 1-way ANOVA testing with incomplete data sets," 

Hello statisticians!!

I am analysing data from a health and QOL questionnaire for 20 patients who have received surgical treatment at different times pre-procedure, post-procedure and at 3 month intervals for 2 years.

I am looking to assess whether there is a statistically significant difference between the different time intervals. Not all patients completed each questionnaire so responses at each time interval vary from 16 to 20.

I initially performed a GLM ANOVA test on SPSS however due to incomplete data sets this only analysed 16 data sets.

Is there a way to perform ANOVA test to include the patients who did not complete every questionnaire or should I be looking at a different model of statistical analysis.",12h9ygv,purezion,1681109228.0,3,1.0,"['Do you have dependent data? Like are patients measured multiple times, or are the observations independent/does a different patient belong to each timepoint? \nIts quite unclear to me what you are doing.\n\nIn case you do have dependent data (one-way ANOVA cant be done), but GLMM (mixed models) are an alternative if you have missing data. ANOVA can only do complete case analysis, unless you impute the missings. But GLMM is quite annoying in SPSS. \n\nIf its not dependent data, there is no alternative apart from imputation. But this depends on why values are missing.']"
[Q] Test for likelihood from a spec distr?,"If I have a normal distribution (known mean and SD), and a known sample, I have an idea how to estimate the probability the data point came from the distribution (look at either the quantile or do some kind of t-test?).

But what if I have three samples? This might be a tool issue -- I don't know of an R, JMP or excel function that takes N-values and checks likelihood they came from a defined normal distribution. 

For example, let's say I drew an 11, 14, and a 20 -- and I want a p-value for null hypothesis they came from a bin with mean = 12 and SD = 3?",12h4d72,FlyingApple31,1681094103.0,1,1.0,"[""> Test for likelihood from a spec distr?\n\nBeware using the term 'likelihood' as a synonym for probability (based on the switch to 'probability' in the next sentence, I'm assuming that was your intent). Likelihood has a very specific meaning in statistics, distinct from probability though connected to it. \n\n>  I have an idea how to estimate the probability the data point came from the distribution\n\nThe *probability* that any specific value came from *any* given continuous distribution (within its support) is 0.\n\nIf you're looking instead for a p-value, you need to specify a test statistic and a way to define your rejection regions (equivalently, a way to define the meaning of 'at least as extreme', e.g. 'larger absolute value of the statistic').\n\n> But what if I have three samples? This might be a tool issue -- I don't know of an R, JMP or excel function that takes N-values and checks likelihood they came from a defined normal distribution. \n\nIf you actually *do* want to use the likelihood as a statistic, then just literally use the likelihood (or a simple function of it), or base a statistic off the likelihood or log-likelihood.\n\n> For example, let's say I drew an 11, 14, and a 20 -- and I want a p-value for null hypothesis they came from a bin with mean = 12 and SD = 3?\n\nWait ... bin? as in binomial? i.e. with parameters n=48, p=0.25? \n\nYour opening sentence specified that you have a *normal* distribution with known mean and s.d. and now you're asking about a binomial? Was the normal just meant to be a different example? Why abandon it rather than pursue it? And why the shift from a continuous to a discrete example? That changes the considerations slightly.\n\nIf you really want to use the likelihood as a test statistic (it's not necessarily the best idea, depending on what sort of alternatives you are interested in), then we can pursue that but I want to be sure we're really talking about the same thing now. \n\nPresumably also you intend that the observations be treated as independent. You should clarify.\n\nPlease avoid edit to avoid any change between normal and binomial, though. Just stick with one or the other."", 'Hi! Sorry for confusion. I think I meant ""bin"" as in ""urn"". Let\'s say we have an urn containing marbles made from a process that generates them with mean = 12g, SD = 3g (or whatever I said before). So yes, it\'s a continuous variable, marbles are independent, and process outputs are normally distributed.\n\nI find 3 marbles in a baggie on a shelf. I want to know if the null hypothesis is non-significant that they came from that urn. One of them is pretty different (20g), but the other two aren\'t. Let\'s just say I know for sure that all 3 were all made together (but are independent samples of whatever urn they came from).', '> I think I meant ""bin"" as in ""urn"".  Let\'s say we have an urn containing marbles\n\nOkay, that\'s even worse than just the change to a discrete distribution, because now we\'re adding the issue of whether you\'re sampling with or without replacement.\n\n> So yes, it\'s a continuous variable\n\nIt can\'t be. There\'s a finite number of possible outcomes, \nthe number of marbles in the urn.\n\n> marbles are independent, \n\nWell, no. Back to the sampling without replacement issue. If you take a marble out, you can\'t sample that marble again, because it\'s no longer in the urn. If you don\'t mean that and you want to replace drawn values you\'d need to say so.\n\n>  want to know if the null hypothesis is non-significant that they came from that urn.\n\nOh, okay, you\'re definitely not sampling with replacement then, because all three are out of the urn at the same time (not possible if you\'re sampling independently) and they\'re *certainly not in the urn* at the end, so you\'re not replacing them.\n\n\n[Even if you made the number of marbles infinite (avoiding the distinction between replacement and not), it still couldn\'t be continuous, because it\'s still *countable*.]\n\n... this is more of a confusion of ideas than the thing I thought I was reading.\n\nUrn, die and coin models don\'t work for representing ways to obtain samples from continuous distributions.', ""Ok, forget the urn.  I am interested in the process. If the marbles came from the same machine. And the machine's process is characterized to produce marbles with the given mean and SD.  There is a bucket of other marbles that confirm that characterization, but let's not worry about them.  There is no sampling w or w/o replacement. \n\nAnd the measure at hand is weight. Just assume test method variance isn't an issue and we know the weight exactly."", 'Ok, I think I thought of a solution.  To get the 95%CI around the mean and SD for the 3 ""sample marbles"" to see if it contains the ""machine\'s"" known mu and SD.  If it includes them, then p>0.05 that the marbles came from that process.\n\nIf there is another way though, to maybe get a single p-value instead of just saying it\'s within two CIs, that would be fantastic.', '>  I am interested in the process. \n\nOkay, sure, that works. \n\nAs mentioned previously, what statistic you use depends on what sorts of alternatives you\'d like power to detect. There are various omnibus (in effect ""any alternative"", or at least nearly so) goodness of fit tests, but each will have more power or less power against different collections of alternatives -- you literally can\'t do the best at everything, there\'s no uniformly most powerful test.\n\nI don\'t follow the reasoning in your mention of a CI. What quantity is this a CI for, and if it\'s a CI for the mean, how would it pick up the possibility that the SD was too low but the mean was correct? How would you pick up correct mean and sd but wrong distribution-shape?\n\nWith the likelihood itself, you can construct a statistic from that, but as I mentioned it\'s not necessarily very satisfying. In this case it boils down to being equivalent to computing the sum of squares of z-scores (hence, a chi-squared test with n d.f., rejecting low-likelihood cases means rejecting large chi-squared values); the problem (yet again) is that low-sigma alternatives will not get picked up, but it will have good power for the mean being wrong or for the sd being larger than hypothesized. You could reject in both chi-squared tails of course, but that\'s rejecting for the likelihood being *high*(!)\n\n---\n\nA reasonable omnibus choice for the fully specified mu and sigma case is probably the ordinary\\* **Anderson-Darling** test. The Kolmogorov-Smirnov is another possibility, though there are many others. All have their advantages and disadvantages. \n\nOn the other hand, the most commonly used statistic for the unspecified mu and sigma situation is probably the Shapiro-Wilk test. It\'s a fine test, does pretty well in general and it\'s easy to find implemented in plenty of packages. It\'s a bit hard to do if you\'re trying to self-implement. \n\n---\n\n\\*  Beware: not to be confused with  the variant for the general normality case.', 'So I had started reading about the Anderson-Darling and KS test, but thought they were also unspecified. I might look into them more now.\n\nThe 95-CI strategy I mentioned just involves looking at the 95-CI range for the distribution estimated from the 3 data points (95CI for parameters mean and SD).  That range contains the likely ""true mean"" and ""true SD"" of the distribution that produced those 3 marbles with 95% confidence.  So if those ranges do not include the known true mean and true SD for the process I\'m looking at, that suggests alpha is probably < 0.05 that those marbles came from that process -- which is probably good enough for what I\'m doing.', '> So I had started reading about the Anderson-Darling and KS test, but thought they were also unspecified. I might look into them more now.\n\nThere are ""unspecified"" versions of each (Lilliefors for KS and a couple of different ones for the AD) but the original tests are fully specified.\n\n> I mentioned just involves looking at the 95-CI range for the distribution estimated from the 3 data points (95CI for parameters mean and SD).\n\nHere\'s where you lose me. A CI for the *distribution function* is not at all the same thing as a (presumably) joint confidence region for  and .\n\nThe later part of what you write there sounds like you want a joint CR for   and  *assuming* normality, which would be different again from good ways to test whether the observations are consistent with the data being from a normal distribution with that mean and sd.\n\nBut please don\'t go back to discussing marbles, because then it *can\'t* be normal, as already described. Instead stick to some normal *data generating process*.']"
[Q] How can I conduct simple Indirect Treatment Comparison?,"Hi everyone,

I'd like to conduct a simple Indirect Treatment Comparison (ITC) between two treatments. There is no head-to-head data available in the literature. However, there are a few (5 or so) randomized controlled trials (RCT) comparing one of these treatments to placebo, and there's a single RCT comparing the other treatment to placebo. The patient populations and study designs between these RCTs is similar enough for comparison.

I know having 2 RCTs is not a lot of data, but from what I understand, it should be enough to conduct this ITC analysis. (Although, I have been told from different people that it's enough and that it's not enough. If it's not enough data, I'd just like to ask for an explanation as to why?)

Because my analysis is relatively simple, I believe I can use the following:

*The effect of intervention B relative to intervention A can be estimated indirectly as follows, using the direct estimators for the effects of intervention C relative to intervention A (effectAC) and intervention C relative to intervention B (effectBC):*

*effectAB = effectAC  effectBC*

*The variance of the indirect estimator effectAB is the sum of the variances of the direct estimators:*

*varianceAB = varianceAC + varianceBC*

*The corresponding two-tailed 95% confidence interval can thus be calculated as follows:*

*\[effectAB - Z0.975 x sqrt(varianceAB); effectAB + Z0.975 x sqrt(varianceAB)\]*

*Z0.975 here refers to the 97.5% quantile of standard normal distribution, which gives a rounded value of 1.96.*

Source: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4678383/#BX1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4678383/#BX1)

I'd just like to ask if the above equation correct and applicable for my analysis.

My understanding is that if the ITC is more complex, then network meta-analysis using either a frequentist or a Bayesian approach.

I appreciate any help you can provide with this!!",12h3651,pantaloonsss,1681091169.0,20,0.95,"['What you have described above as a ""simple ITC"" is commonly known as ""[Bucher\'s method](https://pubmed.ncbi.nlm.nih.gov/9250266/)"" in comparative effectiveness research.  (I know, I know, it\'s a bit crazy that applying a basic property from Stats 101 warrants this type of credit). The formulas you have summarized above are correct in general. For one comparison, you have 5 trials comparing treatment A vs placebo (C) and the other comparison of treatment B vs placebo (C) has one trial available. Typically in each trial the difference of each active treatment vs placebo would be studied (e.g., hazard ratio on OS for treatment A vs C). The indirect comparison for treatment A vs B is simply the difference of differences: i.e., mean(A vs B)=mean(A vs C) - mean(B vs C) and variance var(A vs B)=var(A vs C)+var(B vs C).\n\nYou state above you have 5+1=6 RCTs but then later go on to say 2 RCTs. I think you mean you have 2 general comparisons (A vs C and B vs C) and 6 RCTs (5 RCTs for A vs C and 1 RCT for B vs C). You will need to think about how to pool the 5 RCTs for A vs C if you plan to conduct Bucher\'s ITC. One approach could be to pool the results for A vs C using standard meta analysis approaches and then conduct a Bucher ITC. Alternatively, a network meta-analysis could be conducted. Depending on how the analyses are conducted, the two approaches may yield very similar results.\n\nNote, this is all assuming you have conducted a proper systematic literature review to identify all relevant trials (and publications) and a feasibility assessment to confirm a Bucher ITC / NMA are feasible and appropriate.']"
[Q] Logistic Regression : Classification vs Regression?," I have noticed that Logistic Regression ([https://en.wikipedia.org/wiki/Logistic\_regression](https://en.wikipedia.org/wiki/Logistic_regression)) is a model that used significantly for both Regression problems and Classification problems.

When used for Regression, the main purpose of Logistic Regression appears to be to estimate the effect of a predictor variable on the response variable. For example, here are some examples in which Logistic Regression is used for Regression problems:

&#x200B;

* **Modelling of binary logistic regression for obesity among secondary students in a rural area of Kedah** : [https://aip.scitation.org/doi/pdf/10.1063/1.4887702](https://aip.scitation.org/doi/pdf/10.1063/1.4887702)
* **A logit model for the estimation of the educational level influence on unemployment in Romania** : [https://mpra.ub.uni-muenchen.de/81719/1/MPRA\_paper\_81719.pdf](https://mpra.ub.uni-muenchen.de/81719/1/MPRA_paper_81719.pdf)
* **A logistic regression investigation of the relationship between the Learning Assistant model and failure rates in introductory STEM courses** : [https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0152-1](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0152-1)

When used for Classification, the main purpose of Logistic Regression appears to be to estimate the probability of the response variable assuming a certain value given an observed set of predictor variables. For example, here are some examples in which Logistic Regression is used for Classification problems:

* **Using logistic regression to develop a diagnostic model for COVID19: A singlecenter study** : [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277749/pdf/JEHP-11-153.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277749/pdf/JEHP-11-153.pdf)
* **Logistic regression technique for prediction of cardiovascular disease** : [https://www.sciencedirect.com/science/article/pii/S2666285X22000449](https://www.sciencedirect.com/science/article/pii/S2666285X22000449)
* **A Study of Logistic Regression for Fatigue Classification Based on Data of Tongue and Pulse :** [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8917949/pdf/ECAM2022-2454678.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8917949/pdf/ECAM2022-2454678.pdf)

**Based on surveying such articles, I noticed the following patterns:**

* When Logistic Regression is being used for Regression problems, the performance of the Regression Model seems to be primarily measured using metrics that correspond to the overall ""Goodness of Fit"" and ""Likelihood"" of the model (e.g. in the Regression Articles, the Confusion Matrix is rarely reported in such cases)
* When Logistic Regression is being used for Classification problems, the performance of the Regression Model seems to be primarily using metrics that correspond to the ability of the model to accurately classify individual subjects such as ""AUC/ROC"", ""Confusion Matrix"" and ""F-Score"".

The interesting thing being that regardless of whether you working on a Regression problem or a Classification problem - if you do decide to use Logistic Regression, in both cases you can calculate Classification metrics such as the Confusion Matrix. Based on these observations, I have the following question:

**My Question:** Suppose if I am using Logistic Regression in a regression problem (e.g. estimating the effect of predictors such as age on employment vs unemployment) and the model seems to be performing well (e.g. statistically significant model coefficients, statistically significant overall model fit, etc.). Even though I technically still able to calculate Classification metrics such as the Confusion Matrix, F-Score and AUC/ROC - am I still obliged to measure the ability of this Regression model to successfully classify individual observations based on metrics such as ROC/AUC? Or am I not obliged to this since I not working on a Classification problem?

I feel that it might be possible to encounter a situation/dataset in which the goal was to build a Logistic Regression model for a Regression problem - and the resulting model might have good performance metrics used in regression problems, but might have poor ROC/AUC values. In such a case, is this a good Logistic Regression model as it performs well for the regression problem as intended - or is it a questionable model as it is unable to perform classification at a satisfactory level?

Thanks!",12grros,SQL_beginner,1681065817.0,10,0.86,"['It is the same technique: it does one thing, estimates the probability of some binary response as a function of a set of predictors.\n\nSometimes you are more interested in the responses, and sometimes you are more interested in the slopes (describing the strength of relationship with each of the predictors.) But you can\'t have one without the other.\n\nNotice that it *is not a classifier.* Read that again. Logistic regression never outputs classifications. It always outputs probabilities. (It can be the probability of belonging to a class, if that\'s the response variable you modeled.) \n\nPeople sometimes *use* those probabilities to classify things. Doing so throws away information, just like calling everyone who makes over $100,000 a year ""rich"" and everyone who makes less ""poor"" throws away information. \n\nIt\'s ok to do that, if you really only care about exceeding a threshold, and not about how close to the threshold you were. It is important to remember that the model does not seek to maximize number of correct classifications (it can\'t- that classification rule was a post hoc thing you stacked on top of the regression model.)\n\nIf you evaluate a logistic regression model with a classification metric, you are evaluating only a part of the model\'s performance. I will stop short of saying ""that\'s the wrong way to evaluate a logistic regression model"" - but at least be wary of using that as the *only* way you evaluate it.', ""There's one additional detail you haven't mentioned.  If you want classifications from logistic regression, you have to choose the probability threshold.  You might assume it should be 0.5, but it depends on your appetite for sensitivity/specificity.  After fitting the model, you will have to make this additional choice.  There is an R function ROCR::performance specifically for this step."", '> \\- am I still obliged to measure the ability of this Regression model to successfully classify individual observations based on metrics such as ROC/AUC?\n\nHow would such an obligation arise? \n\n(Even in a classification setting, who is creating *obligation*?)', 'Lots of things vary with the terms. If I had to guess, ""classification"" mostly occurs in machine learning context, where we want to make predictions, whereas ""regression"" is mostly used in the context of inferential statistics. I would also assume that a lot of logistic-regression-as-classification cases actually use penalized glm, not maximum likelihood (iirc that\'s actually the default in sklearn). Otherwise, it should be the same model, and you\'re always free to test the metrics you want, possibly on unseen data.', 'A statistically significant coefficient or model fit doesnt really tell you whether the model fits the data well either. Its like with linear regression, you could have something really nonlinear like y=x^3 and if you fit a linear function to the data, the coefficient/model will still be significant, but the fit is not good. Same applies to logistic.', 'I think what you obsverve is one special case of the predict vs explain question which is a general discussion point in many apllications. What you describe as classification would correspond to prediction as the main goal of analysis while regression corresponds to explaining. As far as I know there is no definite answer if one has to also look at the prdictive performance if the focus is on explaining and it is more of a philosophical question. From what I see in my work it is not very common, but it might be useful and is sometimes considered good practice to look at metrics for both goals. The paper ""To explain or to predict?"" by Shmueli (DOI  10.1214/10-STS330) might be helpful for your question as the differences between both goals and good practices are discussed there.']"
[Q] I'm a poker pro with a straightforward sample question for how to calculate the significance of a value derived from a small sample,"I'm hoping if I can learn the formula once, then I can use it to establish a mental framework for future estimations. I'll provide the sample question first, then go into more detail for those who are curious.

**The question:** Group A makes an action 10% of the time. Group B 20%. Group C 30%.

A player can be in only one group.

Player 1 has made the action 4 of 5 times. Player 2 has made it 6 of 30 times.

What are the odds that each player resides in each group?

**The backstory:** In poker, decisions should be based partly on the tendencies of your opponents. In live games this is done through observation and memory. Online, pros typically use permitted software that records all data & displays it in a customizable HUD. There are hundreds of possible values to display because there are 4 rounds of betting, seat positions, preceding actions, etc.

My primary HUD currently has 58 values plus the number of hands I've played with each opponent. When you hover your cursor over each value, it displays the sample size (i.e. (13/171)) (this denominator is of course different than the number of hands you've played with them because many actions are only possible in a minority of hands, e.g. no data for the 4th round of betting if everyone folds in the 1st round). Once I've played thousands of hands with someone, most of the values are sufficiently fleshed out to be mostly useful. However, for less common actions or for opponents I've only played dozens or hundreds of hands with, the values are less useful & even likely to be misleading.

I've made very effective use of HUD data, but I often have to make a quick assessment of how much weight to assign a value in swaying my action one way or the other. Sometimes you're completely on the fence & can use anything to sway your decision (the way someone put the chips in live, or a minuscule sample size online); whereas other times you're inclined to take an action 99% of the time, but an extreme data point (or combination of data points) over a significant sample will allow you to make that non-standard decision. Point is, it's not optimal to think, ""Eh, I'll wait till I have a robust sample on this opponent to apply any of the data."" It's a sliding scale from the first hand you play together.

Let's say the example above is for a player raising on the flop (2nd round of betting), & in a specific situation I would fold to a player in group A, call if group B, and re-raise if group C. If the data for that opponent is 63/198 I would comfortably assume they were in group C & re-raise. However I only have a vaguely intuitive sense for smaller samples. Sometimes it may be (4/4) so I'll think, ""ya, good enough chance they're a maniac"" & re-raise, but sometimes the night goes on & the value is (6/42), & I realize they were probably just on a hot streak when they first sat down.

I *might* be able to calculate the grouping odds when the value is 100%, but even then I could be wrong: 1% chance a (4/4) player is in group A, 16% group B, 83% group C. (take each group's tendency to the 4th power, & divide that value by the sum of the 3 values). But I don't know where to begin in calculating my example at the top.

Thank you for any help & further insight into the statistics at play here!",12gnknx,existenceawareness,1681056584.0,38,0.96,"['This is straightforward bayes. But you need a prior distribution, what percentage of people are in each a,b and c groups. If 99.999 percent of people are type A, even if you see it done 4 out of 5 times, the person is still likely a type A person.\n\nThe way to do the update quickly is to look at the bayes factor https://en.m.wikipedia.org/wiki/Bayes_factor\n\nOr slightly less dry: https://youtu.be/lG4VkPoG3ko\n\nIn the poker setting, getting the base rates right will depend on your player pool. If you are a pro and making an investment into this, then buying hand histories for the site and stake you play can give you an idea of this. The advantage of gto style play is that it is theoretically indifferent to player pool.', ""Take a look at the beta distribution. In reality, there aren't really discrete categories like you're using. Instead, there's some likelihood that an individual's decision will be A or B. Beta allows you to put confidence around a point estimate of the individual's likelihood choice."", ""That is the Bayes Theorem:\n\nX=Player Raising in the flop    \nY=Player is in an aggressive table.    \nP(X|Y)=P(Y|X) \\* P(X) / P(Y)\n\nIn your example you are looking for the probability of raising in the flop given the player is in a table of high raises (this is your mental model and models the idea of having 3 groups, high, mid and low raising type of tables).\n\nP(Y|X) is tricky, it's sometimes called the likelihood, it is the probability of the player being in an aggressive table given he rises in the flop.    \nTo have this, you need some knowledge of how the player behaves in different tables. Let's assume you know this player was in an aggressive table 40% of the times he raised in the flop in the past.    \nTherefore,    \nP(Y|X)=40%    \nP(X)=32% the one coming from 63/198. Sometimes called the prior.    \nP(Y)= assume 20% of tables are this aggressive. (depending on your mental model).\n\nThen the probability of raising in the flop given the player is in a table of high raises=64%.\n\nWith this value you may now contrast it against your current observations in the table, let's say you observe him raising in the flop 80% of the time in your aggressive table, you know this is higher than your 64% expectation and you might think he has a very strong hand (or he might think he is in a more aggressive table than you think.)\n\nThere are a lot of assumptions made so be mindful to incorporate all information you can to come up with a conclusion."", 'You are looking for something similar to a Bayes\' factor here, which conditional on the history of the opponent\'s preflop raise tendencies, would give the relative likelihood of a player belonging to your aforementioned archetypes (TAG, nit, fish, maniac, Laggy, etc.)\n\nSo in your example, you would need to calculate P(raise 4/5 hands | overall preflop raise tendency = 30%), P(raise 4/5 hands | overall preflop raise tendency = 20%), etc. and then to compare the ratios of those things. However, you would need to further weight those by the relative frequency of players in each category.\n\nSo Bayes Factor of Category C vs Category B =\n\n(A priori frequency of players in Category C \\* (P raise 4/5 hands | overall raise tendency = 30%) / (A priori frequency of players in Category B) \\* ( P Raise 4/5 hands | overall raise tendency=20%)\n\nIn practice, I would guess your brain already does something like this which is to say ""this player sort of looks like a maniac, but might just be someone LAGgy with a few good hands. I\'ll keep an eye on them for a few more rounds.""\n\nAnother thing I haven\'t seen mentioned in this thread is that you don\'t quite care about just the relative frequency of each player category, but rather the assigned range of hands if the player is in each category AND the change in expected value in each of those scenarios.\n\nFor example: player overbet shoves 3x pot on the flop and you hold top pair. You don\'t really care if the player is a maniac vs just a LAGgy good player. You care about your expected value in each of those scenarios against the ranges those players would respectively hold.', ""The prior distribution thing is a fundamental component I hadn't even considered! It makes so much sense though.\n\nI think in the HUD program I can generate a histogram for a given stat. I'll just filter the player pool down to a range of stakes & players with >x hands.\n\nAll of this is just to generate samples to get an idea of how to make better mental approximations on-the-fly. I won't be referencing anything directly to make a specific decision, so it doesn't need to be super accurate, but using groupings & prior distributions from those histograms will give me realistic study material.\n\nMy own database is approaching a million hands overall & should be sufficient. Buying & selling hand histories may be frowned upon, if not mentioned somewhere on a user agreement or restricted programs page, because the HUD won't even let you collect data by just observing a table you aren't being dealt into. Though I've heard of pros sharing data to develop strategies for major heads-up rivalrys. Anyway that won't be necessary here."", ""Yeah, for mental calculations, there is a simple rule for obtaining a beta posterior from a beta prior.    OP could do it dirty on the fly, since it's easy to visualize variations of beta distributions (after training with feedback).  \n\nAlso, probably less useful for mental calculations, but there is a multi dimensional variation of the beta distribution called the Dirchlet distribution.... If OP really wanted to get the finest resolution possible, then program some Dirchlet inference into the HUD."", 'The beta distribution wiki looks like quantum physics to me, but I should be able to make more sense of it after some more time with all the great launching points you guys are giving me.\n\nYou\'re right that it\'s not discrete categories, I just simplified it down as much as possible to give me a basis to begin creating study material. Think of it like, ""if the player was confined to one of these three groups, what are the odds that it trends toward each of these groups after thousands more trials based on this small sample"". But yeah, I didn\'t realize a prior distribution was a necessary component.\n\nAlso, you raise an interesting point that it\'s not either/or. When you run a hand through a \'solver\' the game theory optimal output often involves a blend of actions even for the same combination of cards in your hand, partly so the opponent can\'t narrow down your holdings (I say partly because even decisions in the final round of betting will sometimes be x% call & y% fold for a given combo). & even your average Joe will do one thing or another based on any number of factors or just whim, but none of that is really relevant for my purposes here, because ultimately as the sample size grows the value will only trend toward one number that represents their overall tendency.', ""I think you're so much more experienced with statistics than me that you just assumed I included all the necessary variables then naturally reverse engineered that into a different question than what I was asking. Which is actually impressive, like how an AI might do it, lol.\n\nI just realized I could have asked ChatGPT this... which is kind of sad because I seriously appreciate that people are taking time to help me with this. Well, more content for this sub & some discussion branches that an AI chat wouldn't result in!\n\nAnyway, I think it threw you off that I didn't include prior distributions, really any arbitrary numbers would do. If you read it again you'd probably understand, but table dynamics aren't a part of this request (though often relevant in other ways), & the 63/198 was not a part of my sample question. I just wanted to know how to calculate the odds that each player is in each group, but I should be able to figure it out now!"", 'yeah, you arent looking for small exploitable edges from GTO like those pros are. So Id just plug in some rough guesses for priors. \n\nIm not sure if youve seen it, but the bot that beat a bunch of pros in 2018 or something (Pluribus) did more or less what you are thinking of. It had a base strategy and then had a more aggressive, more passive variation that just nudged it a bit one way or the other. It turned out that adjustment made a big difference in the opinion of pros. ReBel from Facebook ai does an even more extreme version by explicitly modeling every single possible holding (chance you have AA, chance you have AK etc)\n\nSo you really dont even care about the probability, you just need the direction to adjust your play at the margin. But if you want probabilities that formula is your friend.', 'Woah! We may have just come up with the most revolutionary change to a type of software that has only been incrementally improving for 20 years. \n\nI\'m still making sense off all this, but do you mean if the sample for a player is (2/2), it would not show the useless & misleading ""100"", but rather a lower value based on the distribution of the player pool & sample size? Like, ""this player is slightly likely to do this more than others, but only slightly, & it sure as hell ain\'t 100, so here\'s an approximation.""\n\nI would certainly find that valuable, but I\'m not a programmer, so the time spent learning how to incorporate that into my HUD without breaking something would probably never be worth it. But now when I see this in my HM3 update notes in the year 2035 I can say, ""Yes! Finally!""', ""Take a look at this link, it's a simple we beta calculator: https://homepage.divms.uiowa.edu/~mbognar/applets/beta.html\n\nAlpha is the number of successes, beta is the number of failures. In your case, alpha can be the number of times the player does action A, beta can be the number of times he doesn't. The beta curve illustrates the probability of the probability that he will choose action A. The peak of the curve is the point estimate, and the width of the curve will tighten up as your sample size increases."", 'Im a stats/poker guy with decent coding chops. Id be interested in exploring this problem a little deeper. Send me a pm', '>I\'m still making sense off all this, but do you mean if the sample for a player is (2/2), it would not show the useless & misleading ""100"", but rather a lower value based on the distribution of the player pool & sample size? Like, ""this player is slightly likely to do this more than others, but only slightly, & it sure as hell ain\'t 100, so here\'s an approximation.""\n\nYes, pretty much!   There\'s many ways to do this... Especially with neural networks and machine learning algorithms.  It doesn\'t need to be done with the exact approach I had in mind.']"
[Q] How is Pascal random variable sum of k independent geometric random variable?,"My self study question asks me to find the expected value of Pascal(x, k) random variable distribution.

The  answer sheet says: ""because X\_k is essentially the sum of k independent geometric RV: X\_k = sum(Y\_1...Y\_k), where Y\_i is a geometric RV with  E\[Y\_i\] = 1/p. Then E\[X\_k\] = k \* E\[Y\_i\] = k/p.""

I  understand how we find expected value after converting Pascal to  geometric but I can't see how we convert it. I tried to search online  but the two results I found were an inductive proof and a Moment  Generating Function, both of which are out of topic for this lecture. I  would appreciate any help.",12gngd1,steQuill,1681056312.0,0,0.5,"['You will get more hits for ""negative binomial distribution"" than for ""Pascal distribution."" But the proofs will be about the same. If you are really good with infinite sums you can directly evaluate the sum of x * f(x) for x from 0 to infinity, but I think most everyone finds induction or MGFs easier.\n\n""How we convert it"" (from pascal to geometric) is simply, if the Pascal distribution is the time it takes to observe k successes, we can write that as the time until the first success + the time between first and second successes + ... + the time between the k-1th and kth successes. Since expectation is linear, you can always find the expectation of a multi-step process by adding the individual steps.', 'A Pascal distribution models the number of trials until the k^th success, where each trial has success probably p. You may also find more information by searching for the ""negative binomial distribution"".\n\nThink of it this way: the number of trials until the first success is one geometric RV. Then from the first success to the second success is another geometric RV. So on and so on, until the total number of trials until the k^th success is the sum of the trials until each individual success, which is the sum of k geometric RV\'s.', 'I get it now, thank you.']"
[Q]can a non-random sample produce an unbiased estimate of population proportion.,,12gn44w,Viper_4D,1681055552.0,2,0.67,"[""Clearly, a biased sample -- one where not every member of the population has an equal chance to appear in the sample -- *can* still turn out to have the right value on average.\n\n*Guaranteeing* that it will, across all the possible population distributions you might have, though, and all the various kinds of non-random sampling you might be dealing with -- that's going to be *hard*. \n\nEssentially impossible, unless you restrict your non-random sampling to very specific kinds of designs built around making the expected values of particular quantities of interest work out correctly (which is not what most people are doing when they engage in non-random sampling).\n\nIn most cases, *random sampling* is much easier to engage in than nonrandom sampling that works. Some forms, though do work and some of those are commonly used (like stratified sampling, which is basically random sampling of identified subsets of the population; it still relies on random sampling, but attempts to make sure that the strata each get sampled at some fixed fraction of the total sample size). \n\nOutside deliberately-designed nonrandom sampling, you're pretty much relying on luck. For sampling that's done in some haphazard/happenstance way (like convenience sampling, say), this is really not going to work out, in a similar sense that all the nitrogen (N2) and oxygen (O2) molecules in a small balloon *could* by chance briefly sort themselves into nitrogen at the top and oxygen at the bottom -- but they don't."", 'Yes but not necessarily. Suppose any subset of your population is representative of your population. Then choosing any subset nonrandomly would suffice.', ""If by non-random, you mean that not everyone in the population has an equal chance of being in the sample, you can still get an unbiased estimate if the probability that each person is selected is unrelated to the thing you are trying to estimate. For example, suppose people born on Sunday are more likely to be in the sample than people born on other days. If you are estimating average height, your estimate would be unbiased, because height is unrelated to one's birth day of the week."", 'You need a representative sample to get an unbiased estimate. Random samples are the best way to get representative samples. However, you can try to obtain representativeness using non random (or imperfectly random) samples. The key is to understand what a representative sample should look like, and try to adjust the sample to get that representation. This is often required when its impossible to get a random sample. Its rarely perfect, but it often gets close enough. \n\nPolling usually works this way. Its impossible to get a random sample of the population using phone calls or door to door visits. Theres too much bias in who picks up the phone or opens their door. So they start with random phone calls, then weight the people who actually answer based on demographic information. They get close enough to accurate estimates that people continue to use them.', '**Bias**, in statistics, is the difference between the expected value of the estimator and the population parameter it is trying to estimate. And the **expected value** of a random variable is the average value it takes, where you calculate the average by summing all possible values it can take, weighted by the probability it takes that value.\n\nSo, for example, if I have a random variable that takes the value 1 with probability 10%, the value 2 with probability 40%, and the value 3 with probability 50%, then its expected value is 1x.1+2x.4+3x.5 = 2.4.\n\nWhen we talk about the expected value of a sample estimator, we are looking at the possible values it can take across every different possible sample, weighted by the probability that we draw that particular sample. If, by ""non-random"", you mean the sample is completely deterministic, then there is a single value the estimator can take so its expected value is whatever that value is - so the only way for the estimator to be unbiased is for that value to be exactly equal to the population value. That\'s basically impossible except under very specific circumstances (for example, if whatever you\'re trying to measure is perfectly determinable from the information you get from the sample).\n\nIf by non-random you just mean ""not a simple random sample"", for example if you select people from the population with unequal probabilities, then we would actually still call that a random sample and there are plenty of ways to construct unbiased estimators as long as (a) every person in the population has a non-zero probability of being selected in the sample, and (b) you are able to calculate the probability of someone being in the sample either beforehand or during the selection process.', 'Thank you for confirming this.', '> You need a representative sample to get an unbiased estimate. \n\nUsually but not automatically correct. By way of counterexample  ... imagine you have some symmetric distribution (say on the values 1,2,3,4,5,  like a Likert item), and your sampling was nonrepresentative in the following way: values above (/below) the mean got more likely to be in your sample by their distance from the mean (your approach led to better chance of seeing the more polarized opinions), but without regard for direction. \n\nThen the sample mean (and median etc) would be unbiased for the population counterpart.\n\n(edit: lost the ""below"" part in editing while composing the above on my phone -- I put it back)', 'Hah. Fair. The information in that sample is pretty useless for anything other than the estimate itself, but the estimate is indeed unbiased.']"
[Q] Have some questions about the feasibility of a Research Project I am doing on social media Echo Chambers. Any help would be greatly appreciated!!!,"So I'm currently doing a descriptive research project titled ""Which demographics are in social media Echo Chambers within the US""

I will be using the ANES 2020 Social Media Study available here: https://electionstudies.org/data-center/2020-social-media-study/

I am planning on creating an index of variables which are associated with a typical user in an echo chamber that align with a social media account in an echo chamber, and then using answers from this survey such as ""Do you get your news from x platform"", ""how many of your Facebook friends are Dem/Republican"" amongst others to assign each respondent with an ""echo chamber score"", before regressing these against binary demographic variables to see which are associated with a higher echo chamber score. I just have a couple of questions:

- The variables in this datadet are mostly associated with Facebook. Would it be better to limit the scope of the project to Facebook?

- Are there any better datasets out there that anyone knows of that would be better for me to use? (I am not allowed to scrape twitter/Facebook due to ethics concerns according to my uni unfortunately).

- Is it okay for me to arbitrarily assign weighting to each of the variables in my index that make up the ""echo chamber score""?

- Does anyone know of any existing literature about which demographics are in echo chambers? I've tried looking but I can't find anything.

- The ANES dataset has post-election and pre-election (2020 presidential vote) data. Is there any way I can use this? Should I remove data that is from one half? I really am not sure what the best way to approach this would be.",12gjvi7,Drewplo,1681047908.0,3,1.0,"[""From a quick look, it looks like the data is only about those that use Facebook: 'The ANES 2020 Social Media Study will allow direct linkage of survey responses with data from participants Facebook accounts', so that's your scope unless you have some other data.\n\nYou can have arbitrary weighting in creating the indexes, but you need to defend it and it has to make sense.  If someone say, why didn't you make the weighting like so and so, you need to have an answer for it.  Your inferences will be limited to the index that you create.  Really, all indexes have some arbitrary components to it. \n\nPre and post election would be a useful stat to see if the election have any effect on your echo chamber effect.  At the minimum, you can make some nice summary tables and graphs from it to see if there are any obvious differences;  compare your index and important exgenous variables pre and post election.\n\nFor your regressions, you can use difference-in difference or maybe a simple F test to see if the election have any impact."", ""Thank you so much for your response!! There are variables in the dataset containing participant responses to questions on twitter but yeah I may just have to limit the scope of the project.\n\nThank you as well for your point regarding the arbitrary nature of the index. I'm glad to hear that it's okay to be relatively arbitrary; I intend to justify the weighting using the existing literature if and where I can.\n\nAlso will look into using a difference-in-difference test to see if the election had any impact, although I would imagine that I will go with F testing seeing as I'm more familiar with that.\n\nThanks again for taking the time to respond to me, really helpful and just what I needed to hear. Have a lovely rest of your day!""]"
[Q] How do I state the hypothesis rejection after insignificant results?,"So, the hypotheses  suggest that x and y  intervention causes people to donate more. (as two different hypotheses)  There were created one control and two treatment groups for that.  Based on my data analysis, the treatment groups showed no significant effect on the donation.  Does that mean the hypotheses were rejected or there is not enough evidence to support them. How should I mention that in my thesis?",12gdlqb,heypeanutperson,1681029660.0,3,0.71,"[""I think you are confusing the expectations that you as the researcher have about the effect of the two treatments, and the concept oh hypothesis in the hypothesis testing framework.\n\nWhile testing the effect of a treatment with Control and Treatment groups, we want to estimate the Average Treatment Effect (ATE), that under certain assumptions can be obtained by subtracting the average score of the treatment and the average score of the control group.\n\nWhat you want to test is whether if the ATE is significantly different from zero. As your results suggest that the treatments didn't have an impact on donation, then you do not have enough evidence to reject the null hypothesis."", ""You can either reject or do not reject the null hypothesis. If you reject the null hypothesis, this means you are 'accepting' the alternative hypothesis, which means there's an effect. If you do not reject the null hypothesis, this does not mean you are 'accepting' the null hypothesis. It simply means that there is no evidence. Absence of evidence is not the same as evidence of absence."", 'One way to say it is there is not enough evidence to draw a confident conclusion about  the directions of the differences or whether or not the differences are 0. Confidence intervals on the differences could be helpful to see what differences are not inconsistent with your data.']"
[Q] Data presentation tools?,Anyone know of any neat data presentation tools? I tried looking around for private desktop versions of Gapminder or Google Data Explorer platforms but came up empty.,12gclr9,EnaGrimm,1681026418.0,15,0.87,"['Different tools for different jobs. Most of the time, R is perfect for producing graphics. I tend to find the simplest graphics are the most effective. I tend to scorn bells and whistles, things like 3-D barcharts and pie charts will get heckled by other statisticians. You can also incorporate the use of tools like [Highnote.io](https://Highnote.io) to make your presentation more appealing.', 'What do you need to accomplish?', ""Depends what you're trying to do. Jupyter notebooks lend themselves well to presentations about data."", ""Ggplot2 and Rmarkdown (Quarto now, I suppose)\n\nIf you need to present a table, there's a lot of options to export them. I'm not above using the Snipping Tool.""]"
[Q] How do I improve my approach towards this feature selection and model building for large multiclass dataset?,"I know this is technically assignment question.

The training data contains 9000 observations and 900 features. I have to build the model to predict the testing data, which contains roughly 5,000 observations and same number of features as the training data.

I am wondering that since there are so many features, we use PCA for feature selection? I tried random forest, lasso regression, but they are so slow. I am not hitting the good accuracy using the features given by PCA.

Should I use random samples for random forest or lasso regression?

For modeling, I notice that SVM overperforms compared to naive bayes, neural network, and linear discrimnant analysis. Should I just use SVM over the ensemble method, because of multiclass data? I am getting roughly 88 percent correct on hold one leave one out of training data, but 25 percent correct on testing data.

For the binary data, I was able to get good accuracy(not perfect) by lasso for feature selection and doing ensemble method of logistic regression, neural network, svm, qda, and lda.",12g9ewh,edsmart123,1681016487.0,14,0.95,"['To improve your approach, try:\n\nAlternative dimensionality reduction techniques like t-SNE, UMAP, or LLE.\n\nEfficient tree-based algorithms (XGBoost, LightGBM) for feature selection.\n\nFeature engineering using domain knowledge.\n\nSmaller random samples for computationally expensive methods.\n\nEnsemble methods (bagging, boosting, stacking) with multiple models.\n\nRegularization and cross-validation to address overfitting.\n\nHyperparameter tuning with grid search, random search, or Bayesian optimization.', ""Don't forget for any feature selection methods you use, that you must do some internal validation (eg nested cross-validation) rather than using the whole dataset, otherwise you'll massively over-fit. This is a really common mistake in the literature and is discussed well [here](https://stats.stackexchange.com/questions/64825/should-feature-selection-be-performed-only-on-training-data-or-all-data).\n\nIn addition to the methods in others' comments, I've had good experiences with [Boruta](https://www.jstatsoft.org/article/download/v036i11/417)."", 'Thank you will do hyperparameter.\n\nQuick question: after i tested my model on leave one hold one out on training data to see the accuracy.\n\nWhen it is time to predict the testing data, do i use the whole training data or cross validation of training data.', ""Thank you, splitting the training data and using boruta on each of the splits turn out better.\n\nQuick question: after i tested my model on leave one hold one out on training data to see the accuracy.\n\nWhen it is time to predict the testing data, do i use the whole training data or cross validation of training data.\n\nI will try hypertuning, but it doesn't seem to give out a lot of improvement""]"
"[R] Moving estimators (agnostic) vs ARMA-ARCH-like philosophy (arbitrary dependence) on example of evolution of all parameters of Student's t-distribution: mu, sigma, nu","Plots: mu, sigma, nu evolution for Dow Jones Industrial Average time series: https://i.redd.it/e5c6kw62mlsa1.png , article: https://arxiv.org/pdf/2304.03069

For nonstationary time series there are dominating ARMA-ARCH-like models in hundreds of variations, assuming some arbitrary dependence type.

I wanted to discuss alternative more agnostic approach - shift local (EMA) estimators instead, e.g. allowing to estimate also evolution of nu for [Student's t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) here, also  leading to better log-likelihood evaluation.

Which philosophy is more appropriate in which cases? Is there a literature for such moving estimators?

Also, this looking novel nu behavior is very interesting, e.g. heavy tails rho~|x|{-nu-1} nearly always but ~1970s when it goes to Gaussian (why???), the tails become much heavier just after WWII ... is it discussed in literature?",12g8inu,jarekduda,1681013824.0,6,0.8,"[""Just made a table comparing probabilities of extreme events with Student's t for DJIA series below ... something comparable for S&P500 can be found in https://www.sixfigureinvesting.com/2016/03/modeling-stock-market-returns-with-laplace-distribution-instead-of-normal/\n\nhttps://i.postimg.cc/v890LsJK/obraz.png""]"
[Q] t-test for ANOVA when testing treatment against positive & negative controls,"So in my experiment I have several treatment groups along with a positive control and a negative control.

I'm just wondering what kind of test / tests would be optimal to assess these data. When I run a one-sided ANOVA, the individual p-value produced between my negative control and one of my treatment groups (A) does not line up with what I get with a one-sided t-test between those same groups (neg control and A).

Should I maybe do two separate ANOVAs with our treatments vs our + control and our treatments vs our - control?",12g6vux,kewlaid27,1681009326.0,12,0.94,"['I assume you mean ""one-way"".\n\nYou should drop your negative control from the analysis. You expect there to be no effect, but lack of statistical significance cannot mean there was no effect, so there\'s no point in including it.\n\nTest your treatments against the other control with ANOVA and then use Dunnett\'s post-hoc test to compare all treatments to control.\n\nIf your samples consist of single measurements for each condition (i.e. tbey are the average response of cells in a dish or the like) you should use random-block ANOVA with one factor being treatment and the other being experimental replicate. It can have much higher power than one-way ANOVA, and never has very much lower power.']"
[Q] Requesting Step-by-step Tutorial to Conduct Indirect Treatment Comparison or Network Meta-Analysis,"Hi everyone, I'd like to conduct a comparative analysis of 2 different treatments for a medical condition. There's no available head-to-head randomized controlled trial (RTC) between the 2 different treatments, so I'd like to conduct an indirect treatment comparison (ITC)/network meta-analysis. I tried looking for step-by-step tutorials to conduct this analysis on YouTube and also various webpages but was unable to find one openly available. I'd prefer to use WinBUGS or OpenBUGS, but I'd be open to trying on other open-source statistics software too, such as R.

Thanks for any help with this!",12g1nuj,pantaloonsss,1680995661.0,0,0.4,[]
[Q] How does error propagate across multiple different functions,"Hello, 

Since it's been difficult to find resources on error propagation, I'm having difficulty understanding more complicated scenarios. In many online examples I see:

    x,y --> f(x,y)
    error of f(x,y)=[df/dx df/dy][covariance matrix][df/dx df/dy]^T

Now variances and covariances can be calculated using x,y and df/dx and df/dy can be calculated using finite approximation (assume direct derivation of function not possible). But how about more complicated example?

    x,y --> f(x,y) --> g(f(x,y)) --> h(g(f(x,y)))

So now you have 3 transformations of the original data. How would one go about calculating the error of  h(g(f(x,y)))? Is it just

    h(g(f(x,y)))
    error of h(g(f(x,y))) = [dh/dx dh/dy][covariance matrix][dh/dx dh/dy]^T

Is dh/dx and dh/dy just simple finite approximation? How bout the variances and covariances? How are these calculated?",12fyziv,DrBobHope,1680989305.0,2,1.0,"['The error seems to be the quadratic term in a tailor series expansion from the looks of it. Do the same with g first ie write the formula for error with dgdx etc and then apply the chain rule of partial derivatives. If the function is not linear you might not have a closed form. In that case you can try to use some Lipschitz continuity approximations maybe. This way you can bound derogated of h by derivatives of f I think.', 'Think about your errors as falling on a hilly surface and then ""shifting"" around at random.  You view the result directly from above.    You can imagine the shifts of a given magnitude are more likely in some directions (i.e. those with a shallower slope can move farther with less ""cost"").     This helps me visualize the way error propagates through layers of a model.  \n\nTo move away from the analogy and into analysis, the dimensions of your surface are the functions in your composition.    The gradient has the same dimensions.  The product of the gradient represents an application of the chain rule... So the gradient of your log-likelihood taken with respect to the functional composition is now related to the components of your gradient by either summation or product.      Once you\'ve done this exercise, you should see how the differentials in your post lead to the error propagation matrix.    If not, try again using a normal likelihood function / sum-of-squares for cost.   It all comes down to the derivative of that sum-of-squares taken with respect to the model parameters.']"
"[Question] How to handle multiple overlapping, incomplete time series for regression (forward curve data)","Hello, I am looking for some methodological help.

I have FFR futures contracts probabilities as independent variables (well, they're not really independent, but i'm not inferring). I suspect that the FFR futures forward curve has a very similar relationship to treasurys (especially bills). I want to regress the current FFR and FFR forward curve on a dependent variable, like s&p500.

I think it would also be good to include other factors, like prime age epop, dxy, liquidity measures, M2, etc. But leaving that alone for now:

How the hell do i run this regression with overlapping and incomplete time series??

Here's a handy little image to explain what's going on:  
[https://imgur.com/GvFB0CQ](https://imgur.com/GvFB0CQ)

And the FFR forward curve, as a treat (i like it at least):

[https://imgur.com/EaeN2Jw](https://imgur.com/EaeN2Jw)

So how do i regress these overlapping, incomplete time series on my target? Thank you so much for reading and pondering my orb with me.

Edit: I had thought of cohorting it as this is my usual lazy approach, but i don't think that works here since contracts are fixed to specific dates (so i can't generate a rolling x-day feature). I suppose i could just generate a ""days from hike"" value for each contract for each observation though - i will try that approach while i await feedback!",12fx59e,throwawayrandomvowel,1680984977.0,9,0.85,"['Hey I am stats/trading enthusiast only and be talking bs, but thought a little on ur problem. This is what I would think of:\n- days to maturity for each given date, instead of thinking in overlapping contracts. Also, In my view , investors would approach similarly the 85th or 92nd day maturity, but would look way more differently the 180 or 360 days contracts, so you could join similar maturities together in some groups by some metric (mean?).\n- what is that is changing s&p500, really? Changes in the curve? so your focus variable should be it. Then you would have variables of changes in rate for each grouped days to maturity for (say) monthly data, and regress it against changes in s&p500\n- btw, I would double check but I think futures of interest rates are just current rates adjusted by the cost of money until maturity, so it may have an almost 100% correl with changes in current rates.\n\nI didnt understanding ur approach of cohosting/rolling-x, if you could share an example I would appreciate!', 'Hey, I recently had a professor using the slope as simply the difference between the 10y with the 3 month bonds. Maybe that is enough. He also correlated stock returns with inflation and consumption as measured by the BEA.\nDid your analysis yield any results?', ""Thank you - yes I did do dtm. That's cohorting. I just won't have a continuous time set but now I no longer need it, so I no longer need to deal with time series which is nice.\n\nAnd also agreed - the shape of the curve is what matters. \n\nAnd to your final point about causality / correlation etc - yes it's very interesting. If markets are perfectly efficient the ffr curve should look like the front end of the treasurys curve, but i have not checked and since the function of the fed is step on the scale (especially since ~91) so i would not be surprised if interest rate expectations (viewed through ffr futures) and inflation expectations (treasurys)."", ""my dude we are way past any 2-point slope, let alone the 10-2 or 10-3. This is like adjusted volatility skew. I'm looking at the entire shape of the curve - but it turns out most of the information is in the skew within each contract rather than any differential itself"", 'Ok, hope u find something! About the ffr vs current fr curve, I think its not a question of efficiency, more of arbitrage. I will study it a little in time to properly discern and post here, but that was my rationale in the answer.', 'Why?', ""Rates and inflation are no connected by markets, nor have they been for a while. That's why it's so interesting""]"
"[C] We have a psychometrician position (full time and internship) open. Someone who is into stats would be a good fit for it. 100% work from home, but you must be based in the US. Full time, salaried, and we're 100% employee owned.","I'm a dev turned manager here and handle all our IT, but I've hired most of my teams from posting on reddit and so I am just trying to help a co-worker out with hiring. As I mentioned, we're 100% work from home (we don't have a physical location) and have been since 2005.

I can probably get you a salary range if you're interested. 

Feel free to ask in here or IM me with questions.

https://www.alpinetesting.com/careers/psychometrician-assessment-operations/

https://www.alpinetesting.com/careers/psychometric-summer-internship-2023/",12fx01s,andrewsmd87,1680984641.0,7,0.82,"['Wow. So I have a B.S. in engineering with a minor in mathematics. Self studied and currently trying to break into the Psychometrics industry. Not currently enrolled in a PhD program but wondering if you have roles for non PhDers. Side note. Im considering everything from an Eds in school psychology to an MD in psychiatry for my future path.', 'We do have an internship open and that might be a good way to get your foot in the door. I think if you did well with that and were working on your PHD we could probably make something happen. Make sure to apply!']"
[R]Which is the most effective treatment?,"Statisticians of Reddit! Here's a challenge for you. I have a dataset with responses from physicians about their preferred treatment for headache in migraine. I have grouped the data under various headings such as drugs therapy, surgical therapy, behavioral therapy, calculated the means and standard deviations for each group. But how i go about analyzing the most effective treatment? Please help!",12fuxik,capedlover,1680979962.0,1,0.67,"['Im not a statistican but how do you want to find out whats effective if all you have is data about whats used the most ?', 'So the dataset basically has various pharmacological and non-pharmacological treatments with their frequency, as answered by physicians through a cross-sectional worldwide survey. Its around 400 entries. \nI dont mind the scut work; I just wanna know if there any easier way to go about it?', ""You can't find what treatment is the most effective from a survey where physicians selected their preferred treatment. You can only figure out which treatments your population of interest is most likely to prefer.\n\nAnd I would probably fit a poisson GLM."", ""Depends on what your response/outcome is. You can fit a multivariable model, if it's count, use poisson/negative binomial, if it's normally distributed, use linear, if it's binary yes/no, use logistic. \n\nAlso consider the potential confounders to adjust for in the model. Since it is a worldwide survey, is it possible for some treatments to not be available in some countries and ultimately, not a preferred treatment? What about the level of severity or pain scores of patients at point of seeking treatment?""]"
How do I fix this ? Spss [Q],"Hi, Im using SPSS for my dissertation trying to test for collinearity but when I try to run it I get a pop up saying  there are no valid cases for models with dependent variable (dv name) statistic cannot be computed) Im using dummy variables as instructed by my lecturer. Please help what have I done wrong?",12fryfc,No-Band937,1680973733.0,0,0.25,"['Check if the variables are coded as numeric or some kind of string or character. Models often treat strong data as missing.', 'Worked out this is only happening when a specific two of my variables are put into it, but both of these are coded numerically']"
[Q] Question about choosing null and alternative hypotheses,"I teach a probability and statistics course in a university but I'm teaching outside my field so I'm definitely not an expert. I have a question about choosing the null and alternative hypotheses and haven't been able to resolve it via googling. I teach in an engineering department so examples about drug testing aren't as relevant.

Question: does the choice of Ho and Ha depend on which ""side"" of the claim you're on, ie if you want to prove or disprove it?

Let's say a lightbulb manufacturer claims their bulbs last on average at least 800 hours. If I work for the manufacturer, I want to conclusively demonstrate via my hypothesis test that my claim is true, so it seems that I would want Ho : mu <= 800 and Ha : mu > 800 so that I could reject Ho with a certain level of significance and be confident in my claim.

However if I'm a consumer and I don't believe the manufacturer's claim, it seems that I want Ho and Ha to be the reverse, so I could conclusively determine that their claim is false and that the true lifespan is less than 800 hours, so that I'd have evidence that they're being dishonest.

Can anyone confirm if the above logic is correct, that sometimes the choice of whether the stated claim is Ho or Ha depends on if you want to prove or disprove the claim?

Thanks in advance!

Edit: here's an example from the textbook, for an idea of the types of problems I'd like to be able to write:

>A manufacturer of a certain brand of rice cereal claims that the average saturated fat content does not exceed 1.5 grams per serving. State the null and alternative hypotheses to be used in testing this claim and determine where the critical region is located.  
>  
>Solution: The manufacturers claim should be rejected only if  is greater than 1.5 milligrams and should not be rejected if  is less than or equal to 1.5 milligrams. We test  
>  
>H0:  = 1.5,  
>  
>H1:  > 1.5.  
>  
>Nonrejection of H0 does not rule out values less than 1.5 milligrams. Since we have a one-tailed test, the greater than symbol indicates that the critical region lies entirely in the right tail of the distribution of our test statistic Xbar.

To me, this problem seems to be written from the perspective of a test engineer at the FDA who wants to try and prove the company's claim wrong. If I worked for this manufacturer, wouldn't I want to switch H0 and H1, so that I can reject the claim that mu>1.5?",12fr958,kinezumi89,1680972220.0,35,0.96,"['The null is ALWAYS the opposite of what you want to prove. It is related to modus tollens. If A then B and Not B therefore not A.', 'Yes, if you have a one-tailed hypothesis there is a distinction between a right tail hypothesis and a left tail hypothesis.', 'The simplest way to think of it that applies to all scenarios is this: the null and alternative hypotheses, together, represent thee entire event space; said differently, they are mutually exclusive and exhaustive.', '> Let\'s say a lightbulb manufacturer claims their bulbs last on average at least 800 hours. If I work for the manufacturer, I want to conclusively demonstrate via my hypothesis test that my claim is true, so it seems that I would want Ho : mu <= 800 and Ha : mu > 800\n\nThe problem is that you\'re covering the claim within the null, so you can\'t ""prove"" anything about the claim with this setup.\n\nEven if you designated H0: mu <= 799, you would have prohibitively low power to test the manufacturer\'s claim that mu = 800.\n\nThe most sensible approach is probably just to collect data and create a confidence interval. Roughly speaking, the confidence interval represents ""the set of hypothesis values which are reasonably consistent with this data at X% confidence"", so you could simply see whether the upper bound of the interval is below 800.', 'I actually heard a well known PhD statistician say all one-tailed tests are bogus (in casual conversation, not in a paper).  But I have a somewhat less stringent argument.  In order for a one-tailed test to be valid, it must be *blatently obvious* that if the result had come out in the other tail you would have *thrown all of your data in the garbage* and *never mentioned it again*.  If it is not that obvious that your one tailed test is testing the only tail that makes any sense, then it must have been pre-registered.  Otherwise, just double the p-value.', '[deleted]', 'In my experience, the overwhelming majority of hypothesis tests that are conducted in practice are not directional, but rather relate to the extremity observation relative to the null. In simple tests of the central tendency, this usually means running a two-tailed test. In regression, this means that any parameter deviating from the null *in either direction* will be judged significant if enough evidence suggests as much.\n\nPart of the reason for this, in practice, is that it erases several forms of HARKing that can be used to inflate the rate of significant results (and, in so doing, increase the rate of false positives). In your light bulb example, measurement error more or less guarantees that your sample won\'t have a mean of exactly 800, so it should be obvious that, with a large sample, you can put your foot in the scale quite a lot by choosing which one-tailed test to run after looking at the data. That\'s obviously cheating *from the point of view of the analyst* but that cheating isn\'t obvious to an outside observer unless the analytic strategy has been pre-registered.  The act of trusting that the analyst has acted entirely in good faith *is an important statistical assumption* that should never be taken for granted. By contrast, if H_: mu = 800 and H_a : mu  800 (the standard assumption of a 2-tailed test), deviation in either direction will be detected. The elimination of an unnecessary decision reduces the degrees of freedom that the analyst has to influence the outcome.\n\nThis is not to say that one-tailed tests have no uses. But it is also a mistake to try to encode the entire logical process into a single hypothesis test. If you run a two tailed test, and conclude that the light bulbs significantly *outperform* their manufacturer-guaranteed minimum, you need only use your human brain to conclude that this is a qualitatively different result than having the bulbs significantly *underperform* the guarantee. ""Significant = bad, non-significant = good"" is a terrible target to aim for (as is the reverse; those judgments are not what *p*-values are for). No statistical test is a substitute for human reason, and no policy should be set based only on a naked *p*-value that lacks an accompanying human interpretation.\n\nAnd for what it\'s worth, seeing that one-tailed tests have been run *at all* is often a canary in the coal mine that the authors have taken sides and are not evaluating the data impartially. It\'s not proof of dishonesty but it\'s a good reason to proceed *much* more cautiously in interpreting the rest of the results.', 'You will have exactly the same numbers but in one case p and in the other 1-p. In one case you can reject the null in the other case no. The goal of the game is always to try to reject the hypothesis because you gain information. Anyway it is the same', 'H0 is chosen to give you an easy test statistic. The you have to worry about type I and type II errors, sometimes called producers Risk, ie, light bulbs are too good, also called quality giveaway, and consumers risk, light bulbs do not meet specifications. These considerations tell you which side you want your rejection region', 'Makes sense, thank you! Strange that of all the websites I visited, none explained it so clearly. Seems rather straightforward!', 'The attempt of a ""probabilistic modus tollens"" is indeed the motivation behind null hypothesis significance testing. But as statisticians and logicians have been pointing out for decades, it is also [fundamentally misguided](https://doi.org/10.1177/0959354395051004).', 'But for the same hypothesis (lightbulb lifespan is > 800), could you set it up as either a left-tailed or right-tailed test, depending on the conclusion you are hoping to prove (or disprove)?', 'But you can set up a one-tailed test with the claim either as Ho or as Ha and still take up the entire event space, as long as Ho and Ha are always complements. How do you choose between a left-tailed and right-tailed test?', ""My question isn't specifically about how to formulate a hypothesis test about lightbulbs, it's about how to choose Ho vs Ha in general for one-tailed tests in general - the lightbulbs were simply an example. Similarly, what if a coffee company claims that their coffee has an average of no less than 100mg of caffeine per cup? Ho : mu >=100, so I can conclusively prove the company wrong (if I'm a consumer)? Or Ho : mu <= 100, so I can conclusively prove the company right (if I work for the company)?\n\nHypothesis testing and setting up confidence intervals are essentially the same thing - a two-tailed hypothesis test at a significance level of alpha is equivalent to constructing a 100\\*(1-alpha)% confidence interval and checking if the test statistic for the sample falls within the interval or not.\n\nBut my question is: for a one-tailed test/interval, how do you choose between left-tailed vs right-tailed? Do I choose the critical/rejection region to be what the company claims, or the complement of what the company claims? It seems that if I want to prove the claim right I would choose one side, and if I want to disprove it I would choose the other side. (The reason I included the equals sign with Ho is because I've read that that's the standard, no matter the test type the equals sign goes with Ho)"", ""It's interesting how a process whose use seems so widespread is still so hotly debated!"", 'My example has nothing to do with treatment.', ""[This Wikipedia page](https://en.wikipedia.org/wiki/Foundations_of_statistics) explains this in detail, if you'd like to delve into it. You can also see that the form of modern hypothesis testing is a result of numerous heated debates and countless nights of hair-rending contemplation."", 'I am well aware of the criticisms of them and on a qualified basis, agree with those criticisms. Like confidence intervals, they dont do what most people, including professionals think they do. \n\nHowever, I am a pragmatist. If I need to test if the population mean is exactly five, or some other sharp null hypothesis, I really lack a good alternative choice. Likewise, if I need a general consensus, I wont find a universally loved prior distribution. \n\nYou should try Fishers tea tasting experiment and see if you can tell the difference. A p-value works just fine.  As a note, I prefer my milk in afterwards rather than before. I like the taste the burnt proteins create in the tea.', ""Yes. Easy way to think of it is that the alternate hypothesis is what we usually think of as *the* hypothesis. So if you're trying to prove the bulbs last more than 800 time units, the null is that they last less. If you're trying to prove that the bulbs fail faster than 800 time units, the null is that they last more."", 'The direction you are interested is set as the alternative hypothesis. The direction you want depends on your scientific interest. For example, maybe you only care whether some parameter is larger than some hypothetical value (Ha: theta > delta) because you only care about larger values or because smaller values are not possible.', 'I don\'t think my answer was specific to light bulbs. More to the point, I explained the fundamental error with exactly this question:\n\n> Similarly, what if a coffee company claims that their coffee has an average of no less than 100mg of caffeine per cup? Ho : mu >=100, so I can conclusively prove the company wrong (if I\'m a consumer)? Or Ho : mu <= 100, so I can conclusively prove the company right (if I work for the company)?\n\n*Neither* of these directional hypotheses would help you if the company is remotely close to being correct, if the true mu is anywhere near 100. The reason is that you have approximately zero power to reject the null in such case, which is how you would *prove* anything about the hypothesis.\n\n> Hypothesis testing and setting up confidence intervals are essentially the same thing - a two-tailed hypothesis test at a significance level of alpha is equivalent to constructing a 100*(1-alpha)% confidence interval and checking if the test statistic for the sample falls within the interval or not.\n\nYes, they are mathematically dual. More specifically: the confidence interval represents the set of null hypothesis values which would not be rejected by the associated dual hypothesis test. However, identifying a hypothesized value within the confidence interval is not conclusive evidence that the value is correct, in the same way that failing to reject the null does not mean the null hypothesis is correct. It is simply evidence of consistency, which is a different standard from rejection.\n\n> But my question is: for a one-tailed test/interval, how do you choose between left-tailed vs right-tailed?\n\nSimply put, you are asking questions about equivalency, which neither set of directional hypotheses are relevant here. This is because **you are including the hypothesis you wish to prove within the null, which means it can only be falsified, not proven**.\n\nTo the contrary, you want to follow the standards of [equivalence testing](https://en.m.wikipedia.org/wiki/Equivalence_test), which is what I described in my original comment.\n\n> The reason I included the equals sign with Ho is because I\'ve read that that\'s the standard, no matter the test type the equals sign goes with Ho\n\nIt\'s not just because ""it\'s a standard"", there is good reason. To form a null distribution of the test statistic, you must specify a single parameter for the underlying distribution. In a interval hypothesis, this is taken as the most ""extreme"" value of the null hypothesis, to guarantee nominal coverage. If you don\'t include the equals, there is no singular max, so the value could not otherwise be specified.', 'I wouldn\'t say ""hotly debated"" since the issues are straightforward.  If a study has not been pre-registered, all claims by the authors that they are not cheating on the statistics are doubtful.  We know that lots of authors cheat on statistics a lot (data snooping, HARKing, reproducibility crisis).  Choosing which tail to test after the data are known is a prime example of HARKing.  We know many authors do that, so why should we trust any particular author not to?  Only in a case where that would make no sense at all.', 'Great, thanks for the explanation!', ""I've realized what I should have done is share an example from the textbook, so you can see what types of problems I'm trying to write:\n\n>A manufacturer of a certain brand of rice cereal claims that the average saturated fat content does not exceed 1.5 grams per serving. State the null and alternative hypotheses to be used in testing this claim and determine where the critical region is located.  \n>  \n>Solution: The manufacturers claim should be rejected only if  is greater than 1.5 milligrams and should not be rejected if  is less than or equal to 1.5 milligrams. We test   \n>  \n>H0:  = 1.5,\r  \n>  \n>H1:  > 1.5.\r  \n>  \n>Nonrejection of H0 does not rule out values less than 1.5 milligrams. Since we\r have a one-tailed test, the greater than symbol indicates that the critical region\r lies entirely in the right tail of the distribution of our test statistic X.\n\nThe fact that the book only states null hypotheses as equivalences (never >= or <=) always confuses students, so I use the approach where the null hypothesis is the complement of the alternative (in this case, H0: mu <= 1.5; this is also how I was taught).\n\nThe logic the book uses for one-tailed tests (and one-sided confidence bounds) is that sometimes we're only interested in/concerned with one direction, such as ensuring the concentration of a contaminant is below some value. With the example of the lightbulbs, a consumer shouldn't care if the lifespan is longer than 800, only if it is less than the claimed value. For saturated fat, the claim is that it does not exceed a certain value; if it is significantly lower, consumers would not mind.\n\nSo, this is the basis of the examples I provided previously. My goal is to come up with more examples of a similar nature, but want to make sure I'm setting up the hypotheses correctly. I understand in practice (IE by actual working statisticians/data analysts/etc) it may not be done this way, but this is an undergraduate-level course and I only have a few days to cover hypothesis testing. Most students will only need a cursory treatment (any more in-depth knowledge will be provided in future courses).\n\nBy the way, I appreciate your thoughtful responses!"", 'You\'ve actually switched some things around without realizing it. Compare the original example to this new example:\n\n> If I work for the manufacturer, I want to conclusively demonstrate via my hypothesis test that my claim is true\n\n> Solution: The manufacturers claim should be rejected only if  is greater than 1.5 milligrams\n\nIn case one, we want evidence to prove the manufacturer\'s claim. In case two, we want evidence to disprove the manufacturer\'s claim. Perhaps counterintuitively, these two things are not the complement of one another: lack of evidence against one is not necessarily evidence of the other. These are two qualitatively different tasks, which is where I think you and I were diverging. It sounds like you are targeting the second example (disproving a claim) but you were speaking in terms of the first (proving a claim).\n\n> The fact that the book only states null hypotheses as equivalences (never >= or <=) always confuses students, so I use the approach where the null hypothesis is the complement of the alternative (in this case, H0: mu <= 1.5; this is also how I was taught).\n\nThis will probably help students to set up the problem correctly, but conceptually, I\'m not sure it\'s a good way to *understand* the approach. If the goal is to describe hypothesis testing most accurately, then I may go something like this:\n\nNull distributions always assume a single value of the parameter. Now, choose any 5% region of that null distribution as the ""rejection region"". These values represent the cases where we will reject that null distribution. Note that this yields two possibilities:\n\n1) That null distribution is accurate, but we will ""accidentally"" fall into the rejection 5% of the time (by definition). These are ""false positive"" detections (aka *type 1 errors*).\n\n2) That null distribution is actually not true, so we would correctly reject the null hypothesis in these cases. The rate of detection depends on the actual value of the parameter (which is different from the null value in this case), which is the definition of *statistical power*.\n\n**We are free to choose the 5% rejection region however we want**; we achieve the same type 1 error rate regardless of what region we choose (by definition of construction). Hence, if we only care about alternative hypothesis values in one direction, we are free to build our entire 5% rejection region in that direction. This gives us *maximum statistical power* in that direction, at the expense of power in the other direction (which may be worthless to us).\n\nIn this formulation, defining ""H0: mu = M"" vs. ""H0: mu <= M"" really has no distinction and is inconsequential. We only add those other values into the null because we are incapable of rejecting them, but conceptually, I don\'t think the distinction provides any value whatsoever. **The meaningful part is the definition of the rejection region**, not any difference in the formulation of the null distribution.\n\nGranted, it all depends on the aim of the coursework. If the goal is to make better statisticians, this approach has good merit IMO. If the goal is to prepare them for a standardized exam (and I\'m not knocking that; I understand the realities of teaching, especially when teaching statistics to non-statisticians) then your approach is probably more suitable for just getting across the basics (although I\'d caution against leading the students to think that this is the entirety of how statistics are actually properly performed; that\'s how we got the current mess of people with highly over-inflated estimates of their own statistical abilities... but I digress).', 'Hmmm...I definitely understand that the most important part is the definition of the rejection region. I guess the confusing part for students is the rest of the event space, outside the rejection region, that isn\'t taken into account by a null hypothesis of equivalence.\n\nLet\'s say we\'re testing a drug which should slow the progression of a disease. Ho would be ""no change to progression of disease"", Ha would be ""progression is slowed"", but what if the drug actually makes the disease progress more rapidly? One would conclude ""insufficient evidence to reject Ho"" but the evidence does not support the claim that the drug has no effect on disease progression. \n\nIs this simply something that happens with one-tailed tests, and the researcher must be careful to consider this outcome? How would you explain this phenomenon? If I can understand and explain it correctly, it\'d be for the best as then I could follow the presentation in the textbook and not have to refer to another.', '> One would conclude ""insufficient evidence to reject Ho"" but the evidence does not support the claim that the drug has no effect on disease progression. \n\nFailure to reject H0 is not evidence that H0 is true. This is also related to the points I made above about equivalence testing. The null hypothesis can only be falsified, not proven.\n\nIt\'s tough, because as people we often think in terms of ""A or B is true"". Therefore, we think that ""not B"" means that A is true (i.e. we mistakenly think ""don\'t reject null -> null is true""). However, ""insufficient evidence"" is a perfectly valid outcome in statistics, in which case we don\'t conclude A or B.']"
[Q] Interpreting log10 transformed variable coefficients,"Hello,

I am trying to interpret some independent variable coefficients which were transformed using log base 10 and I just want to check I am using the right formula. Please note only the independent variables have been log transformed and I am using multiple linear regression.

To understand what impact a 50% increase in my independent variable will have on the dependent variable I am using the formula:

coefficient beta \* log10(1.5)

Is this correct?

Many thanks",12fqtfu,tothemoon360,1680971259.0,1,1.0,[]
[Question] Not correlated but significant. How do I interpret it?,I found that X is not correlated with Y (r = 0.19) but is statistically significant (p = 0.02). What does it mean and how should I interpret the result?,12fkeq1,autumninmarch,1680956676.0,2,0.57,"['The test you are probably doing is whether the correlation is significantly different from 0. 0.19 is different from 0, with a large enough sample size.', 'They are correlated.', ""Sounds like homework...\n\nShort answer: whenever you have a small effect that's significant, the cause is essentially always the same... *sample size*.\n\n> Not correlated but significant.\n\nBut ... there's no basis on which to claim it's *uncorrelated*. The correlation is small, not absent.\n\n> What does it mean \n\nIt means that the standard error (under H0) was a bit smaller than half the estimated effect size, and it was that small due to the sample size not being small (looks like you had somewhere in the ballpark of n=144)."", 'They are correlated its just a low correlation.', 'Do you have a background on the variables? .19 could be small or moderate depending on the field.', 'Everything is correlated. Some things are correlated 0.', ""There is a correlation. Whether it is noteworthy isn't a purely scientific question, but must involve some assessment of the expected value of acting on such information."", 'Everything but 0 is statistically significant with a large enough sample size. \n\nFolks often forget the difference between statistical significance and practical importance though.', '>\tSounds like homework\n\nYep, many AP Stats classes are covering linreg t-tests this week.', 'And it may also have a lot of practical importance depending on the effect. If it is measuring something like a medication and the number of years lived post diagnosis, then it can be quite an important effect.', 'Yes, the following R code looks at the t-statistic and p-value for a correlation of 0.19 with sample sizes ranging from 3 to 1000.\n\n    r <- .19\n    n <- 3:1000 \n    t <- r*sqrt(n-2)/sqrt(1-r2) \n    p.value <- pt(t, df=n-2, lower.tail = FALSE)\n    plot(n, t) \n    plot(n, p.value)\n    abline(h=.05, col=""red"")\n    \n\n&#x200B;\n\n&#x200B;\n\nPlots at\n\n[https://imgur.com/jpitgQt](https://imgur.com/jpitgQt)\n\nand\n\n[https://imgur.com/HUhYxTC](https://imgur.com/HUhYxTC)', 'Doing gods work.']"
Welch's confidence interval the same as Students. [Q],On jamovi it gives me a confidence interval for students but not Welch's. Was wondering if the answer is the same for both.,12fjqno,rockdjcool,1680955082.0,7,0.82,"['> Was wondering if the answer is the same for both.\n\nNot usually, no. For example:\n\n    > t.test(x,y,conf.int=TRUE)\n    \n            Welch Two Sample t-test\n    \n    data:  x and y\n    t = -3.8088, df = 22.952, p-value = 0.0009059\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -22.473614  -6.652683\n    sample estimates:\n    mean of x mean of y \n     19.83943  34.40257 \n    \n    > t.test(x,y,conf.int=TRUE,var.equal=TRUE)\n    \n            Two Sample t-test\n    \n    data:  x and y\n    t = -3.5334, df = 23, p-value = 0.001777\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -23.089296  -6.037002\n    sample estimates:\n    mean of x mean of y \n     19.83943  34.40257', ""I assume you did that in R, for this assignment I have to use jamovi. When I click on both Welch's and T test I only get results for CI for T as seen [here](https://imgur.com/a/rBY0jL3)\n\nWhat is your opinion on this?"", ""Your image is showing the confidence interval for the effect size. If you want the confidence interval for the mean difference, you click *Mean difference > Confidence* interval.  It shows for both Welch's and Student's."", ""The correct calculations don't depend on the package.\n\nYou're not doing what you asked about, but note that homework is not allowed""]"
[E] Box and whisker vs bar graph,"
Ill try to keep this simple and can expound if necessary.

Background: 
Im a veterinary surgery resident relatively new to research and statistics.


I was told that if data is normally distributed you should not graphically depict that data as a box and whisker and instead as a bar graph and vice versa. However, my recent paper (and others Ive seen) was accepted for publication in a top veterinary surgery journal with box and whisker plots for our normally distributed data. 

Is this recommendation about appropriate figure choice accurate or is it more nuanced than Im making it out to be?",12f4cpf,mtbdadalorian,1680911716.0,13,0.89,"['Box plots are better in my opinion if you want to compare distributions across groups. However, if youre showing just one box plot, it would be more informative to have a histogram.', ""Graph choice doesn't hinge on normality. It just doesn't.\n\nBox plots, dot plots, violin plots, and combinations of those should be the default choice when Y is continuous and X categorical.\n\nI'd only ever tolerate bar graphs if Y is counts. Bar graphs suck in general."", ""> should not graphically depict that data as a box and whisker and instead as a bar graph and vice versa. \n\nThis sounds like nonsense\\* to me. \n\nI presume they meant a dynamite plot showing mean and sd or mean and se of mean rather than a true bar plot\n\nWhat was the reason they gave for doing so?\n\n(In addition I bet your data weren't actually from a population that was normally distributed -- and that we could assert that for sure before seeing any actual data, just based on the definitions of the variables -- not that it's important for any of this.)\n\nIf youre using a t-test a display of mean sd and se of mean isn't silly but I wouldn't use it *instead* of a boxplot. If the n's were middling (*say 9ish to 90ish per group*)  I'd try to use a display that showed all the data, along with means, sds,  se of means, medians and quartiles all on one plot. Not hard to show\n\n---\n\n\\* For a more nuanced reaction; I think I probably get *why* they might have said this but if I do correctly guess at their reasons, those will be based on a series of misunderstandings (misunderstandings common in non-stats textbooks on stats) combined with some less-than-ideal traditions in your area."", 'I prefer the box and whisker because it is simply more informative- it shows potential outliers plus the IQR can easily converted into a sigma hat assuming normality that is not influenced by extreme values', 'I really prefer violin-plots for plotting and comparing distributions, not really a feasible viz before computers so not included in a lot of old-school curriculum.', 'Use the median for the line in the middle of the box plot and plot every single data point in the box plot with jitter so they dont overlap.', 'I love it that makes so much sense to me and I agree. It was a paired t test looking at cadaveric fracture models repaired with different fixations and the box and whisker made much more sense to me to more clearly see the distribution of results rather than just showing the mean with a variance.\n\nThanks for your response.', 'Excellent yeah that was my feel too and yes they were referring to dynamite plots. I mean, the figures were accepted for publication again in the top Veterinary surgery journal (not meaning to suggest that reviewers are infallible) and reviewers took issue with other things so I figured it was likely a non-issue but I have really enjoyed getting feedback for this hive mind as I dive more into clinical research.\n\nThank you', 'No problem. And yeah your intuition was correct. If you have some time on your hands, I strongly recommend ""Basic statistics in cell biology"" by David L Vaux, 2014. It\'s a very short read and it\'ll arm you with rock solid arguments for both modelling and plotting when discussing with collaborators or replying to reviewers.']"
[Q] Two-Sample T test Valid?,"I'm comparing data sets between age in months as the independent variable vs presence of symptoms as the dependent variable (coded as 0 or 1). Because the independent variable is quantitative and the dependent variable is categorical, I thought a two-sample t-test would be best. However, I feel like this test is often used when the independent variable is categorical and the dependent variable is quantitative in most cases. Is there a better test to use than the two-sample t-test in this situation? 

&#x200B;

I can provide more information as needed. Thanks in advance :)",12f3y6w,CompetitiveHat2510,1680910865.0,2,0.75,"['Logistic regression', ""> thought a two-sample t-test would be best. \n\nDV and IV are the wrong way around for that. You condition on the wrong variable. It should still work if what you're interested in is the p value but the amount of discussion required to justify it to readers of your research may be off putting\n\nWhat's wrong with a binomial GLM, such as logistic regression?"", 'Sounds like you might looking for a chi squared test.', ""Nothing wrong with binomial GLM, just that someone else did some analysis on the data before I touched it, and they did a multiple linear regression. It was kind of a mess. It sounds like the general consensus is chi-square or logistic regression, and I'm leaning towards logistic regression. I genuinely just didn't think of logistic regression because I was fixated on the fact that the previous analysis was done w/ linear regression, and it didn't seem like the best representation of the data."", 'Thing is, there are 36 different age groups so i wasnt 100% certain about chi square, which is why I thought of the t test. Would a t test not be valid in this case, or would the chi square test just be more fitting?', ""T test wouldn't be appropriate with 36 age groups.\n\nYou do want logistic regression, AKA binomial GLM.""]"
[Q] significant regression weight but not overall model - what to do?,"Hello,

&#x200B;

I would like to calculate a **simple linear regression**. I have fulfilled all the prerequisites for this. In RStudio I get the following result: 

&#x200B;

MODEL FIT:

F(1,142) = 3.804, **p = 0.053**

R = 0.026

Adj. R = 0.019

&#x200B;

\---------------------------------------------------

Est.    S.E. t val. p

\----------------- -------- ------- -------- -------

(Intercept) -0.000 0.083 -0.000 1.000

meanGPSK 0.162 0.083 1.950 **0.053**

\---------------------------------------------------

&#x200B;

I have previously made a directed correlation hypothesis, so to my knowledge **I may divide the p-value of meanGPSK by 2**. So I would have a significant predictor, but the model as a whole would just not be significant. What exactly am I doing now? 

&#x200B;

I am actually primarily interested in the (standardised) regression weight that I calculated above. The fact that it is small is not a big deal. May I still interpret the regression weight, especially because I am so close to significance? 

&#x200B;

This is ""only"" a bachelor's thesis, so my statistical knowledge is rather limited and English is not my first language. Please explain to me in simple terms whether this is a total disaster or whether I can overlook the overall non-significant model.",12eusb7,simplySchorsch,1680892567.0,0,0.5,"[""What you do is to report the results of the model and discusses them. I wouldn't get hung up on significance - the more important considerations are your very low r2 value and your b1 coefficient. In other words, the model does a poor job at explaining the relationship between the variables and would not be very effective for predicting your dependent variable based on your independent variable.\n\nWithout more information about your data and model, that's all that can be said.\n\nI'll add that your intercept seems to be effectively zero which is surprising. Did you specifically force the y intercept to pass through zero? That's usually not a good idea."", ""It looks like you regressed outcome Y on 'meanGPSK', which will just give you the mean of 'meanGPSK'.  Not much to interpret.  You should add more variables for a model since this is just saying the mean of ''meanGPSK' is 0.162\n\nThe default p value for R is for 2 tails, so essentially, what your result is saying is that there is a 5.3% chance that 'meanGPSK' is not different from 0.  Usually p value of 0.05 is the standard maximum for hypothesis testing.  Ideally it should be < 0.01. Any higher isn't wrong per say, just not great.\n\nedit: ok I'm feeling like I'm missing something, what exactly is this meanGPSK variable? and what is the weight that you're talking about?  If you want to do weighted regression, there's an option for it in lm(): weights= your data weight""]"
[E] What are the top UK universities for applied maths?,"I assume at the top is Cambridge and Oxford, closely followed by Warwick and Imperial because everyone says those are the best for maths. But how do they compare for applied, and what other unis are best for applied?

I'm asking you guys at r/statistics and statistics is the main part of most applications of maths but I'm interested in stuff like economics too.

So far it seems like Oxford allows you to specialise in statistics more quickly and I remember hearing somewhere that Cambridge focuses a little more on pure while Oxford has good applied.

Warwick seems to have more applied focused courses like MORSE.

Other than that I really don't know anything.

Thanks!",12ets83,leMonkman,1680890603.0,3,0.71,[]
[Q] Anyone knows a simple algorithm to find the start and end point of a peak?,"I have a room impulse response (RIR) sample audio file and trying to find the start and end point of said impulse. This type of sample has relative silence except for the single impulse.

I can find the init point of the impulse by looking at the maxima, but then, how do I find the last from the tail.

When taking the absolute of the RIR, this looks like a time series for which I'm trying to find a peak.

&#x200B;",12eroo9,AlternativeDish5596,1680886562.0,8,1.0,"['Your options here are going to be extremely highly dependent on problem specifics. This could be as simple as checking if change is greater than some threshold, or it could be an almost entirely ill defined question where youll have to make some arbitrary definitions of what start and end mean.\n\nWe would need a lot more information to help you.', 'You can fit smoothing splines and find where the derivatives are zero and the 2nd derivative is zero.  Check out these R functions: smooth.spline, splinefun, ksmooth.', 'Any function to look at derivatives and their trends and when they go positive to negative and vice versa is a good indicator, as well as anytime second derivative is 0 will be a peak', 'Imagine an impulse function (or function that models the peak you want to find) centered at time *T* laid over your time series.     Your goal is to find the value of *T* that makes the impulse as similar to your time series as possible.     If you choose covariance as your measure of similarity, then this becomes an application of cross-correlation analysis.', ""not sure if this is what you're looking for but scipy has some useful functions whose algorithms you could use \n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html"", '[deleted]', ""Depending on how serious the application is (ie for work or a home project), I would have started with something super simple:\n\n- Get a threshold for the noise to be true from the data, in order to get a rule. Maybe abs(x) > 0.7 (throwing in random number). Maybe take a given percentile of the signal amplitudes, but this might not be as universal a method as it seems as different files with different amount of noise will give different threshold.\n- Transform the serie as y(t) = mean(x(t to t-k))\n- scan the serie. When you find y(t1) > 0.7, put a start flag at t1-k. Then when y(t2) goes below 0.7, put an end flag at t2+k  \n\nSuper crude, but seeing how this approach fails can give you hints about a proper method. \n\n---- \n\nYou can also work with fourier series (both numpy and scipy can compute fourier series of an audio signal if I remember well, at least I know I've used one of them) if you want to discriminate *noise* vs *no noise* based of frequencies instead of amplitude. For example if the noise you're looking for allways has a similar pitch, it becomes super easy this way :)"", 'Sorry, I have updated the question.', 'Got it. Will take a look! Thanks', 'Thanks.\nGive that context you would probably be better suited to ask this question in a DSP subreddit (digital signal processing).\n\nThat said, heres a first idea. Presumably you have some minimal known time in the signal before an impulse could have possibly occurred (maybe 1 second). Use this portion for background estimation. If you cant make this assumption, and even if, it might be valuable to make a dedicated background measurement. With the background measured you can find an construct a stopband filter that attenuated out the background signal giving you a relatively clean impulse signal. Then you can use some sort of simple threshold rule on the relatively deterministic impulse signal.']"
[Q] How to conduct Indirect Treatment Comparison (ITC) and subsequent Cost-Effectiveness Analysis (CEA)?,"Hi all,

I'm trying to compare the cost-effectiveness of two treatments against each other. However, there are no randomized control trial (RTC) data that directly compare the two. However, there are RTC data of each of these treatments from separate studies comparing each to sham treatments. Therefore, from my understanding, I can implement an indirect treatment comparison (ITC) to compare these two treatments and then perform a cost-effectiveness analysis (CEA) from the ITC results. Please correct me if I'm wrong about this.

Can anyone provide resources/citations that will explain to me how I can go about doing this?

Also, is it adequate for me to use the data from just two RCT (one for each treatment) to conduct this analysis?

Thanks so much for any help with this!",12ehn7o,pantaloonsss,1680865837.0,0,0.5,"[""2 RCTs is definitely not enough. You're probably looking at conducting a systematic litt review and a bayesian network meta-analysis.\n\nThe cost-effectiveness analysis would be derived from that."", ""So the terminology is different from what I'm used to but this sounds a lot like average treatment effect/average treatment effect on the treated.  \n\nOne way of going about this is using difference in difference regression.  Regression outcome on fix effects, time (in this case dummy variable for before and after treatment), and fix effect * time dummy.  Read Card and Krueger (1994) for the foundational paper on this method\n\nOr just google difference in difference, this is a pretty common method.  Wikipedia have a pretty good overview, although it's a bit technical.\n\nhttps://en.wikipedia.org/wiki/Difference_in_differences""]"
[Q] [D] Best textbook for advanced biostatistics/survival analysis?,"Hi r/statistics, I'm a PhD student in epidemiology with an undergrad in maths and stats. I find the survival analyses very interesting, and I'd be interested in a biostats postdoc. The thing is, my knowledge of the maths underlying all the stats is still at an undergrad level.

1: What textbook would be best to bring my biostats understanding up to a level competitive for a postdoc?

2: Also (this might just be impostor syndrome talking), is an epidemiology PhD likely to count against me, or would it probably be viewed as close enough to biostats?",12ehjvd,Acting_attempter,1680865624.0,43,0.98,"['I love Frank Harrells book: Regression Modeling Strategies with applications to linear models, logistic regression, and survival analysis\n\nIm not qualified to answer the second question since I havent been in your shoes, nor have I been on the other side of the interview table, so take this with a pinch of salt. From what Ive seen though, many people perceive biostats as an easier and more hackable subfield of statistics. This is absolutely not true. Biostatistics often has just as rigorous training in statistics as statistics PhD programs. However, it really depends on the supervisor. With your background, I suppose youd be seen as more suitable for applied work with advanced existing statistical methods than developing statistical methodologies, so maybe aim for that? Im sure other people will have better advice, but in general, it never hurts to highlight your strengths, compatibility with the field youre applying to. Wish you best of luck!!', 'Although you mentioned your math background I am going to recommend two basic but very well written references. I enjoy reading Kleimbaum https://link.springer.com/book/10.1007/978-1-4419-6646-9. When is something I struggle to understand the easiest reference was Allison https://www.amazon.com/Survival-Analysis-Using-SAS-Practical/dp/1599946408. \n\n2. Depends on your experience during your PhD and your dissertation topic', ""Kleinbaum is good but a bit dated (he's an epidemiologist btw), I'd suggest Moore's book:\n\nhttps://link.springer.com/book/10.1007/978-3-319-31245-3\n\nWhat's wrong with an epi PhD? Feel free to come over to r/epidemiology and ask us."", 'With regard to 2) I think it really depends what you are interested in doing --- if you want to do theoretical work in survival analysis, it could be hard. If you want to engage on the application-side (or more applied methods development), then I think it could be a great fit! I think there are a lot of biostatistics faculty who are engaging with collaborations (and more applied methods work) involving the analysis of time-to-event data, and would love to have a postdoc who is interested in thinking deeply about the data and study design. My suspicion is that it would also be important to be very comfortable with programming.', '1: Bendix Carstensen, Epidemiology with R, https://bendixcarstensen.com/EwR/', ""Frank Harrell is great in some ways, but also kind of a doofus in others. I think one should definitely read his book, but don't take it as dogma."", "">Thanks - I'll also post on epi"", 'Well obviously once you reach the level of crossing over into philosophy territory, there will be different schools of thought. Until then, its better to expose yourself to many different ways of thinking. But I dont find Frank Harrells book discussing unconventional theories. Its more like the discussion is opinionated in favor of or against certain methods, which you can clearly tell. As Emily Dickinson once said, the shore is safer. So if you dont wanna buffet the sea, post doc in biostats might not be the right next step', 'It sounds like we are in agreement? Frank\'s book is just very classical, and on a number of topics he is both opinionated and uninformed. Now, I am often in favor of simpler models, and find that in many problems Frank and I have very similar approaches. That said, he would not be my go to as a first source for someone learning the material: Your early sources do tend to have an outsized influence on your philosophy. By the same token, I would also 100% tell people not to read anything Mark Van Der Laan or his ""disciples"" have written when they begin learning survival/causal stuff, because it is written in an uniformed and super biased way in the other direction. That said, once a person has some grounding, Mark\'s stuff has some interesting and useful ideas.', ""On second thought, yeah I think we're in agreement.\n> Your early sources do tend to have an outsized influence on your philosophy\n\nI couldn't agree more. Looking back, I think I was introduced to dry and boring, trying-to-sound-unbiased textbooks that almost killed my interests in statistics. I just like textbooks with personalities, what can I say.""]"
[Q] How to represent large categorical data?,"I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.

e.g.

&#x200B;

|Date|Value|Category|
|:-|:-|:-|
|1/1/2010|1.11111|Alpha|
|2/1/2010|2.11111|Beta|
|3/1/2010|2.00009|Alpha|
|4/1/2010|0.00000|Charlie|

But the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.

I couldn't process all the data as its too large.

What would be the best way to sample each dataset? I'd like the sample containing a fair representative of the 3 categories.",12eghmf,runnersgo,1680862996.0,2,0.67,"['Perform stratified sampling based on 3 categories?', ""[Q] How to represent large categorical data?\n\n> I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category.\n\nI dont quite follow what you mean here. My best understanding of this also doesnt seem to agree with what your example data looks like. Is your issue here about reading in data from this format?\n\n\n> I couldn't process all the data as its too large.\n\nWhat type of processing did you run into problems with? Answers are going to be completely different if this means you cant even load the data into memory, or if instead you mean you cant fit models to these data."", ""For each10 dataset, we need to get representatives.   \n\n\nWe are using a new algorithm to process them but we can't process all data. So we would like a representative for each dataset.""]"
[Q] Probability symbol confusion,"Hi all, I am new to statistics and am confused with the ""<"" and "">"" symbol.

Say if ""The probability that an individual score is above 22..""

Which one is correct?  P(X < 22) or P(X > 22).

Thanks.

kel",12ebyc6,limkel,1680848083.0,0,0.31,"['The symbol < itself is larger on the right than on the left. So it means that the number on the right is the larger of the two. Its opposite for >, which is larger on the left :)', 'Pac man eats the larger number', ' ""<"" and "">"" are not probability symbols, they\'re inequality symbols.\n\nhttps://en.wikipedia.org/wiki/Inequality_(mathematics)#Properties_on_the_number_line\n\n(also covers  and )\n\n They should normally be well covered in mathematics classes before you\'re about 14 or 15. When I went to school they were first discussed in 6th grade, and then covered in more depth a couple of years later.\n\nspecifically, ""<"" is ""less than"" and "">"" is ""greater than"". For both symbols, the small side of it points to the smaller value, and the  wide arms encompass the greater value.', 'Thanks guys! Much appreciated it.', 'P( X>22)', 'I mean it seems simple enough but I have to admit i confuse it too sometimes', 'Quite simply 3<5 \n\n5>3\n\n\nIf p X is the probability and it should be greater than ( >) 22 , think for yourself what would be the answer and verify by checking the below answer.\n\n\n\n**Click to reveal**\n\n>!P(X > 22) is the right answer!<', 'In probability, you have a known value and an unknown value. \nFirst, suppose that the unknown value is X. Now, the question asks the probability that the unknown value is above 22. So, you would need X to be greater than 22. In algebraic symbols, this would be X>22. \nSo, P(X>22) would be the required probability.\n\nIf the question asked for the probability that the unknown value is below (or less than) 22, we would need X<22. The associated probability would then be P(X<22)', 'We always drew it as an alligator mouth that is hungry for the larger number.', 'Does it mean < 22 if it eats the larger number', 'I think he knows \n\nHes just understandably confused as it the same as whats up and down \n\nFor left to right languages it is simple enough \nBut with right to left languages ( although it doesnt matter much ) it is not that intuitive \n\nThere is no standard as to put higher or lower numbers first ( i.e on the left in Left to right languages )\n\nLike 3< 5 and also 5>3', '# OM NOM NOM', 'The show Numberblocks on Netflix does a similar thing for inequalities. Its a monster that only wants to eat larger numbers based on the direction of its mouth, which is an inequality symbol.', 'The bigger number is in the mouth. Is X bigger, the individuals score, or .22? Is their score bigger than .22? Whats the prob? P(.22<X)']"
[Q] Scoring higher on a test by virtue of taking the test more than once - what is this called?,"For example, a group of students takes a test at 8AM. At 12PM, these same students take the same test. There was no education between these two times, and the 12PM average was higher than the 8AM average.

Is there a term for this?",12ebafg,LoopyBullet,1680845834.0,2,0.75,"[""It's a testing effect or a practice effect in my books."", 'The learning effect is defined as a significant increase occurring in cognitive test scores as the number of repetitions increases until the score no longer changes and achieves stability.', 'Usually that\'s called ""waking up""', 'This is a paired sample and we use the wilcoxon signed-rank test or sign test to determine if there is an actual effect and the effect size. I love me some nonparametric statistics.']"
[Q] What data can I use for a cost-effectiveness analysis (CEA)?,"Hi all, thanks for taking a look at my post!

I'm trying to conduct a cost-effectiveness analysis (CEA). To keep it general, I'll try to describe my study with a generic example.

Say I'm trying to determine the cost-effectiveness for a new treatment for back pain (treatment ""A""). I would like to compare this treatment with the gold standard, treatment ""B"". I would like to use data that is already published in the literature. There are no studies that directly compare the effectiveness of treatment A vs. treatment B. Instead, there are case series, randomized control trials, meta-analyses, etc. that compare one of these treatments to either a different treatment, no other treatment/sham treatment. Also, all these studies have different follow-up periods. 

Because there is no published data directly comparing treatments A and B against each other, can I use all of those individual studies (where treatments A and B were analyzed separately from each other) to conduct a CEA?

If so, how do I go about combining all of these different studies together to generate quality-adjusted life year (QALY) values? Do I need to weight the numbers from these heterogeneous groups of studies somehow?

My last question is, can I use Visual Analog Scale (VAS) in order to calculate QALY? I believe I can, but I just wanted to double check with y'all.

I really appreciate any help you can provide. Even if you don't have the answers, just a point in the right direction would be very, very helpful (and appreciated!)

Thanks so much for your help!",12e5fh5,pantaloonsss,1680831731.0,9,0.91,"[""You need to find, or conduct, a network meta-analysis. Of RCTs only, non-randomised trials will be chock full of unavoidable biases.\n\nThen you'll need to build a decision-analytic model, which can be quite simple or very complicated, depending on the clinical context.\n\nQALYs are constructed in all sorts of ways, some justified, some less so. You may need a patient panel to help you weight the different outcomes if the work has not already been done well.\n\nThis isn't really something you can do alone. You need patients, clinicians, information specialists, reviewers, and health economists at a minimum, possibly also psychologists, mathematical modellers, etc. It's a six figure funding job, with the first number depending on how complex the question is.\n\nThe [HTA series](https://www.journalslibrary.nihr.ac.uk/hta/#/) has a lot of monographs reporting this sort of research. You could have a look through to find out more about methods."", ""Oh dear. There's issues here, but you're definitely a student so I'll be helpful.  \n\nI'll give you one term to help you on your way: indirect treatment comparison."", ""I really appreciate the response--very, very helpful. This is supposed to be a clinical research study for a new treatment for pain management. We're not looking to submit it for government review, etc. I work in healthcare and am at an academic institution, so we're trying to put together something to submit for publication. There are a handful of other studies in the same realm and though they seem complex, it seems like they were able to do the analysis without a huge team. What do you think?\n\nAlso, are you able to recommend any open source/free software that is capable of conducting these analyses?\n\nThanks so much for your insight so far!"", ""> We're not looking to submit it for government review, etc.\n\nIt doesn't matter. You still need to do the work properly and you need to have collaborators who already know the answers to these questions. The team doesn't need to be huge, it does need to have all the skills required to do the work. It's not something you can do with advice from strangers on the internet."", '100% agree with D-Juice here. If you work at academic institution and are planning to published this research, then consider reaching out to health economists, statisticians, and clinicians affiliated with this university.']"
"[Q] Creating a scoring system with different weightings…the closer a number is to 0, the more weighting it should be given…How do I do this?","Sorry if this is a silly question. I come from a non math background but have to do something for work.

Essentially, I have data that tracks the number of maintenance events an item/equipment underwent. The best score the item can get is 40, whereas the worst is 0. (It is 40 because the other 60% of the score is made up from something else)

The MORE maintenance events an item underwent, the LOWER it should be.

BUT the weightings shouldnt evenly distributed. For example, if the maximum number of maintenance events an item can go thru is 12, I cant just do 40 - (40/12 * [number of events for that item]).

Ideally, the MORE maintenance events an item goes thru, the FASTER it will approach zero. (Logarithmically approaches 0).

I really like doing this sort of task and creating a scoring methodology with my data, but have no clue how to do this (or even what to search - things I tried dont seem close at all to what I am talking about).

Any tips on how to achieve this? I think I have included the relevant information but just ping me if not.

[I think I basically need a formula for thismaybe. But also not sure if there is a better way, hence the post](https://i.imgur.com/lvYwMYR.jpg)",12e3m3p,DoctorQuinlan,1680827572.0,22,0.87,"['Something similar to this\n\nx=40+ 40\\*log(1/(12-y))\n\nIn other words,  the more maintenance, the higher the score, but it logarithmically grows, rather than linearly.\n\nMin score is 1  \nMax score is 12', ""Just gona rewrite this stuff I wrote before in case if anyone else is interested:\nFormula for this particular problem is essentially a  production possibility frontier curve:\nY^2 + (x*40/12)^2 = 1600\n\n1600=40*40. what ever your maximum is, square it. 12 is the maximum number of maintenance event. \n\nY does drop faster as X -> 12. Slope as X->12 approaches infinity. You can increase this 12 to make the oval flatter.\n\nIf you want Y to drop faster:\n\nY^2 + (x*a)^b = 1600. Change 'b' to a higher number and solve for 'a' such that x=12 and Y=40. As b increases, the relevant portion of the curve would start to look more like an edge/ horizontal then vertical line. Y^2 fixes the Y axis at maximum of 40."", 'A simple equation that meets your criteria is a parabola.  \n\nY = 1 - a*x^2\n\nAdjust the constant *a* to match your taste.  \n\nYou could also just code your graph into a spreadsheet and use it.', 'You could take a linear combination with some normal density function that drops off as quickly as you want it to. This is how spline smoothing works.', '[deleted]', ""Why do you need it to turn down at the end?\n\nIf there's some threshold at which the machine is fucked and needs servicing, is 20 events significantly worse than 30 events or are they about the same degree of fucked?\n\nBecause then you could just do something easy like 40/x, or if you wanted to be picky 41/(x+1) so you never have a divide by zero error."", 'Seems like the equation for a circle but setting some constraints so that it only works in the quadrant you want', 'Makes me think of a Type 1 Survivorship Curve.  Think of a graphed curve that starts high, but gradually falls faster and faster as x increases. Like a function representing the height of a free-falling object over time, or the expected survivorship of an average human vs age. Really comes down to how steeply/quickly you want this curve to ""drop,"" then find a similar function to represent it.  \n\nTry....\n\ny = 40(1 - x^2 /144)  \n\nwhere x is the # of events (12 max), y is the score.  \n\nI chose this one to fit your max score / max event requirements. We know at least that your y-intercept should be y=40 and zeros out at x=12.  \n\nHope this helps! Enjoyed thinking this over my morning coffee.  \n\n--  \n \nEdit: If you want to adjust my equation to better suit your expected scores, try toying with the exponent in:.  \n\ny = 40(1 - (x/12)^n ). Try plugging in 1.5 for n, then 1.2.  The closer you get to 1, the closer it gets to being a perfectly linear function. The first equation I gave you up top, I used n=2.  If you need more help, try supplying a rough sketch of the curve you\'re looking for.  It seems to me your graph should truthfully look like descending stairs of 12 steps, being there is no x= 6.75, for example. Just 12 rectangles in decreasing height to represent the 12 possible scores. Perhaps look into step functions.', 'I think u/doctorquinlan should use the Weibull distribution to satisfy his requirements.  \n\nhttps://en.wikipedia.org/wiki/Weibull_distribution', 'I think that is not quite it. As X increases, I want Y to approach 0 faster and faster....\n\nCheck out the image I attached above. I expect the graph to be flatter for lower numbers of X, and then take a sharper drive down to the X axis as X increase', 'Oh just saw you edit. Want still a bit different. The more maintenance, the LOWER the score should be.\n\nI graphed what you said on [desmos](https://www.desmos.com/calculator) and it seems wrong still.', 'That could work! Is there a science to picking the right a coefficient (if that\'s what we should call it)? Or is it random?\n\nIn other words, is there a ""more"" correct way to do this?', ""Well what I don't quiet like about it is...\n\nIf 12 is max, and X=12, Y=39. Ideally I would want it to be 0 probably"", 'Think of it like this.  A machine that undergoes 8 maintenance events should be considered more than just ""twice as bad"" as another that underwent 4 events.', ""Wouldn't it be something like 1 - exp(x)?"", '>  As X increases, I want Y to approach 0 faster and faster....\n\na linear function of x^(-k) for some k>0 that works the way you want?', ""Nonlinear least squares? That's what I would use first."", ""My bad, I didn't think through.\nAnyway, your graph looks like a standard production possibility frontier with diminishing return, so something like this:\n\nY^2 + (x*40/12)^2 = 1600"", 'That part I get, but if the threshold for ""fuck it, it\'s trash"" is 15 events, what\'s the practical difference between 25 and 35? The curve I proposed degrades quickly, so you get to ""fuck it, it\'s trash"" territory quickly. Specifics on how deep you are into that threshold doesn\'t matter - just whether you are in it at all or not.', 'what do you mean by exp?', 'Or k^-x with k>1', 'Okay nvm. This actually kind of works! \n\nOne other problem tho.if something has X=10, it still has too high of a Y. How would yo suggest fixing it? \n\nY should drop sharper as X increases and I would want something where x=10 to be much closer to 9 but not quite. Maybe like 8 or so.', 'That could work too. Is there a common formula people usually make for stuff like this? Or is it a bit random in how it is picked?\n\nFor example, the 1600? Why use that (other than it be 40 x 40). ?', 'well in their example, the maximum number of maintenance events per machine is 12, at which it should receive a score of 0 out of 40.', 'https://en.wikipedia.org/wiki/Exponential_function', 'e^x', 'Exponential function', ""This is a equation of a circle, well more like an ellipse.  It's often used to create a PPF curve with diminishing returns and your problem can be thought of as a version of that.  General formula is Y^2 / a^2 + X^2 / b^2 = c^2\n\n1600=40*40. what ever your maximum is, square it.  12 is the maximum number of maintenance event. Mess around with with an online graphing calculator to see what I mean.\n\nY does drop faster as X -> 12.  Slope as X->12 approaches infinity.  You can increase this 12 to make the oval flatter.\n\nIf you want to drop it faster:\n\nY^2 + (x*a)^b = 1600.  Change 'b' to a higher number and solve for 'a' such that x=12 and Y=40.  As b increases, the relevant portion of the curve would start to look more like an edge/ horizontal then vertical line.  Y^2 fixes the Y axis at maximum of 40.""]"
[Q] blme package and p-values,"I recently tried to run a binomial mixed model using lme4 but couldn't because some of the levels had complete separation so I adopted the approach recommended by Ben Bolker [here](https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html#digression-complete-separation) by using the package blme and fitting a bayesian binomial mixed model. It worked well but I get p-values as one would when adopting a frequentist approach. I am so confused by this! Does anyone know why this is a bayesian mixed model but the output is identical to a frequentist one, i.e., to one generated by lme4?",12e1uc5,majorcatlover,1680823492.0,3,1.0,['See [this](https://stackoverflow.com/questions/58065947/extract-p-values-and-estimates-from-blmer-list-of-regression-models) stack thread for a comment on the p-value reported by blme.']
"[E] Comparing Funded MS Programs, R1/R2 and Other Factors, MS","Hi all,

Hope you're all doing well. I've found myself in an extremely fortunate predicament, and was hoping this sub might have some advice for me. I'm going for a MS in Statistics this autumn, and have two fully funded offers, one from a R1 and one from a R2 University.

The offers themselves are fairly similar (tuition waiver, stipend, health insurance, etc.) for TA-ing 20 hrs/week. I'm currently undecided on a PhD. I'd imagined going into industry after an MS and am still leaning that way, but want to leave the door open in case I really get enamored with a topic while in school.

The R1 has a PhD program and seemed to be pretty open to letting people continue on (obviously assuming good academic/scientific standing) and have a traditional qualifying exam after the first year. The R2 does not have a PhD program.

The R1 is quite a bit larger (\~31k grad/undergrad) than the R2 (\~9k grad/undergrad.)

I should be clear that I'm not asking for ""Which Program Should I Pick?"" more, which factors should I be considering in making a decision?

Any additional information I'm happy to provide, and thanks again. I'm feeling extremely excited and fortunate with whatever happens, I just want to ensure I'm considering the right variables & factors.",12e0uwu,eifjui,1680821298.0,1,0.67,[]
[E] Thinking about Masters in Statistics with focus in Quantitative Finance,"Hey all,

Im a currently graduating senior at FSU and I will be obtaining degrees in Economics, Statistics, and Political Science. I also ran a business (pseudo hedge fund) where I had friends who had similar backgrounds investing with me and creating AI trading strategies to generate alpha. I got As in really anything involving statistics application but I only really got Cs for Calc 1-2 and linear algebra. Is this bad if my focus is application and not necessarily mathematical theory? Im also worried if it will hold me back from what Ive accomplished up until this point when it comes to applying to masters programs.",12dx1kj,lokilego,1680813206.0,2,0.58,"['How can someone study 3 degrees and run a business on the side? (and then consider to add another degree)', ""I have a bachelors in pure math and decided to do a Masters in Statistics. I am graduating in a few months, and did just fine with some C's in my bachelors. (However, linear algebra is extremely useful, so maybe refresh this knowledge) I found that I did better in the Master because it was closer to my interest, so I studied more and worked harder. If you are prepared to do the work, you will be fine."", 'Business grew out of my interest in investing from a young age. My dream job is to be a quant and wanted to build my own experience if I couldnt get internships. Started with Economics and Statistics but added poly Sci since I felt that it would help with modeling seasonality eventually when I code my strategies as well as other reasons.', ""It's a pain in the ass but some of us are dumb. \n\nI'm currently doing a PhD in criminology, a college program to be a research analyst, and just took a mental health break from a 45 hour work week compromising of 5 jobs (bunch of different contracts). \n\nCan't wait till I finish college at the end of the summer, and finish my data collection for uni....then I just have to try working till I can save enough to buy property lol"", 'Evidently, by sacrificing grades in some areas. (Impressive nonetheless)', 'Wait poli sci to help model seasonality?', 'Thats my plan if I dont start out with a big job and stay in school. The Tallahassee real estate market is gonna explode since the school is only getting bigger with more applications for undergrad each year. Im in the same boat with starting my life finally but I think I still have more gas in the tank for a masters since itll be key for starting pay and such. Also, Ill get to understand more advanced statistical tests and methods when it comes to the side business I created.', 'Indeed! And thank you!', ""Good on your for focusing on stats mate. It's so much more valued in the field than what my undergrad taught me. I would've gone straight into my analyst program instead of doing a PhD and would've been working by now (29). \n\nAh well, never too late. Good luck with the programs and remember to pace yourself. We need a healthy you before anything else."", 'You regret doing your phd?', ""Yup. \n\nAcademia is very toxic, my research area is very triggering for me (green criminology), and I have to basically work contracts for like a decade before I can get tenure. Plus loads of unpaid work expected (publishing articles, extra curricular crap in departments, etc...). \n\nOnly reason I'm sticking with it, all I have left is data collection, analysis and then i defend my dissertation. \n\nBut if I could go back, I would have done my analysis program after my master's instead of the PhD"", 'So should I reconsider a phd in stats?', ""Different ball game my friend. My PhD is in sociology/criminology, the skills I learned there aren't the most transferable outside of academia. \n\nA PhD in stats, not sure what the program entails but it sounds like you'll be doing big data analytics. Plus the one course that is always in demand for professors is stats profs. With a stats PhD you can easily be a professor, but also loads of govt and private jobs where you'll be the lead guy for analytics. \n\nHaving said that, I don't think you need a PhD in stats to be a valuable asset for data analytics in private sector. A master's or grad certificate in data analytics can do that. \n\nI'd say only do a PhD if you wanna teach in uni's or colleges one day.""]"
A priori Power Analysis for GLMM? [Q],"Hey, I am a MSc student in Psychology and a little overwhelmed by the a priori power analysis for my experiment. The experiment is repeated-measures within-subjects design. The outcome variable is binary, so are two predictor variables. Based on that, I am planning to conduct a GLMM in the likes of glmer(response \~ variable1 \* variable2 + (1 | subject) + (1 | stimuli). Since my supervisor is currently working on a similar analysis, he provided me with a script he came up with. It basicially creates a dataset and simulates for every trial the outcome based on probabilities set in the beginning of the script. Then it sets the model and runs a power simulation based on the powerSim() function. Now my issue is, that I get the weirdest results when plotting the outcome: Basicially I get a power spike for n= 10 before dropping until n = 17 and then rising again at n = 20. I can not find a rational explanation for this effect, however, I do also not see a flaw in the logic of the script. Therefore, I wanted to ask wether there is any possible explanation for this?",12dqfb8,paulschal,1680800190.0,9,0.85,"['[This](https://i.redd.it/ix8wacrzkasa1.png) makes my issue a little clearer, I guess.', 'I dont see how its possible for power to decrease as n increases, all else being equal.', ""I've been using simr and the powerSim() function without any issues for a while so I doubt this is related to a bug in the function. That said, I've never used it for a glmm. If you don't get an answer here, you could try over on github: https://github.com/pitakakariki/simr"", 'It can happen with very skewed datasets. https://garstats.wordpress.com/2017/11/28/your-power-is-lower-than-you-think/', 'Right? I am so confused by this? But then - even with simulated data - this would not be possible, so I assume the error to be in the power analysis function.', 'I do have a heavily unbalanced sample set for my binary outcome variable. So this could actually in part explain the issue.', 'Interesting', 'That would be my guess. Not trying to be negative, but unless you have data from a closely related experiment, it is unlikely you will get an accurate estimate of power for this design. For example, it seems to me it would be hard to know in advance how much the results will differ for different stimuli, a key factor influencing power when stimuli is a random effect.', 'Thanks for your explanations. Based on other experiments in the literature i do have some very well educated guesses. As for stimuli, this is more of an control for differences in task difficulty, which i honestly do not expect!', 'Glad you have a basis to estimate parameters. If stimuli is just used to control differences, you might want to model it as a fixed effect. If you want to generalize to the population of stimuli from which they were sampled, it should be modeled as a random effect.']"
[Q] Optimal pseudo-random algorithm for mutation in a Genetic Algorithm,"Hey. So lately I've been working on a problem for which I'm trying to apply a genetic algorithm to find the optimal solution.

The thing is the (binary) chromosome is very large (hundreds of thousands of bits at least, millions maybe) so some aspects of the algorithm can become a problem if they don't scale.

One bottleneck I found is mutation - so generally the algorithm is to go over each bit, generate a random number between 0 and 1 and if the mutation\_rate (e.g. 0.05 = 5%) is less than that we flip the given bit.

Of course this for such long chromosomes becomes a computationally heavy operation so I've been thinking about replacing it with maybe something less truly random but faster.

The approach I currently have in mind is to simply pick the number of genes to mutate at random (random number in *\[0, chromosome length)*, and then generate that random number of random numbers between *\[0, chromosome length\]* which would be the indexes (genes) to flip.  But I'm not 100% sure how to incorporate the mutation rate - so I would need the number of genes to mutate to be picked from some normal distribution with *mutation\_rate \* chromosome\_length* being the average (maximum chance)?

Does anyone have an interesting idea how to replace the naive O(n) algorithm with something better but still being in line with the idea behind mutating the chromosome so it doesn't get stuck in a local optima?",12dkm49,topcodemangler,1680788196.0,17,0.91,"['The approach you suggested has merit, and I can suggest some further refinements:\n\nIncorporating the mutation rate with your approach can be done by using a binomial distribution to determine the number of genes to mutate. The binomial distribution allows you to model the number of successes (mutations) in a fixed number of independent Bernoulli trials (flipping bits), each with a constant probability of success (mutation_rate).\n\nTo implement this, you can follow these steps:\n\n- Determine the number of genes to mutate by sampling from a binomial distribution with parameters n (chromosome length) and p (mutation_rate). In Python, you can use `n_mutations = numpy.random.binomial(n, p)`. \n- Sample the selected number of unique indices from the chromosome without replacement:  `mutation_indices = numpy.random.choice(range(chromosome_length), size=n_mutations, replace=False)`.\n- Flip the bits at the chosen indices: \n```python\nfor index in mutation_indices:\n        chromosome[index] = 1 - chromosome[index]\n```\nKeep in mind that this approach has an average-case time complexity of O(k), where k is the expected number of mutations. Since k = mutation_rate * chromosome_length, the complexity is O(mutation_rate * chromosome_length). However, in practice, this should be significantly faster than the naive O(n) approach, as the mutation rate is typically small.\n\nYou can do the same thing in r with `rbinom(1, n, p)` and the `sample()` functions too, if thats your preference.\n\nGood luck with your project!', 'You can check how many mutation there will be for 1 millions chromosome and then select randomly which one will be changed.', 'Let X be the number of mutation.\n\nE(X) = n*p\nVar(X) = n*p*(1-p)\nYou can compute a random value following this normal distribution.\nSo you have the number of mutation you will got.\nNext you will randomly select them.', 'Just a side note: the optimal algorithm depends on your environment/data, as well as your mutable structure. Check out the no free lunch theorem.', "">  so I would need the number of genes to mutate to be picked from some normal distribution with mutation_rate \\ chromosome_length* being the average (maximum chance)?\n\nAssuming you want each gene to have the same chance, and the genes mutate independently of each other, the total number would be binomial, not normal.\n\nIf the number of genes is very large, so that the expected number to mutate is not too small, then you can use a normal approximation for it, but why not simply generate the binomial directly? I assume you have access to a decent stats library in your language of choice; the binomial can be obtained via an inverse regularized beta function if you don't have a binomial quantile function.\n\n> then generate that random number of random numbers between [0, chromosome length] \n\nIn R that could be done by something like \n\n     Ng=100;p=0.05;sample(Ng,rbinom(1,Ng,p),replace=TRUE)\n     [1] 41 67 85  5 47 25\n\n\nOne problem is that you have a chance of choosing some of the same numbers more than once (with 100 genes, this happens about 1/9 of the time); sampling without replacement would avoid this doubling up of some genes. \n\nAgain in R:\n\n     Ng=100;p=0.05;sample(Ng,rbinom(1,Ng,p))\n     [1] 26 34 23 25  2 13\n\nwould suffice; there's approaches to coding sampling a vector of indices without replacement that are easy to implement that would probably do for your purposes."", 'Thanks for the detailed answer! Will try out doing it like you said, hopefully the rand libraries for the above are present in Rust :)', 'Thanks, ChatGPT.', 'Perfect solution.', 'You know this, but fabulous comment. Literally what I was going to tell op, but better!', 'No worries! Its absolutely doable in Rust, but its been too long since Ive used it to be able to recall the exact functions youll need. Let me know if you have any other issues with this :)']"
[Q] Is it okay to 'split' a 3x2 table where you would normally do a chi square test into three 2x2 tables?,"Hello,

I have a question about chi square tests. I want to compare some nominal variables, with 2 answer options, between three age groups. For example: I want to compare the 30-day mortality (Yes or No) between group 1 (ages < 70), group 2 (ages between 70-79) and group 3 (ages 80-89). This gives a 3x2 table that I wanted to compare with a chi square test.

Fortunately not many people died but this means that I have multiple expected cell counts lower than 5. This is the table for the 30-day mortality: 

||< 70 years old|70-79|80-89|
|:-|:-|:-|:-|
|30-day mortality NO|41|40|39|
|30-day mortality YES|2|3|5|

I can not group the data more than this as we really need to compare these 3 groups.

&#x200B;

What is the best way to compare these groups?

I had some ideas but I don't know if these are correct.

1. Dividing the 3x2 table into three 2x2 tables (compare group 1 with 3, compare group 2 with 3 and group 1 with 2). This would also give me the answer to between which groups the significant differences are but I fear that I won't be able to extract all the information from it as if it were a 3x2 table. Is this a correct way to analyse this data or will I miss things?
2. Use a chi square test anyway. Since some posts on the internet say that the rule of cell counts having to be above 5 is not necessary and that statistical programs nowadays are good enough to calculate the significance even with lower numbers.
3. Use a different kind of test. I don't know which one as there are many different opinions to be found online.

Are any of these options correct? And if so, what would be the best option?

Thank you for the help!",12dhlw2,Emmannuel_C,1680781461.0,5,1.0,"[""I would recommend using Fisher's Exact Test to analyze your data, as Chi-squared is likely inappropriate given your low cell counts. \n\nThis test should provide an accurate assessment of the relationship between age groups and 30-day mortality rates, allowing you to make valid inferences from your data. \n\nRough steps: \n\n- Create three 2x2 tables for pairwise comparisons between age groups:\n- For each 2x2 table, calculate the Fisher's Exact Test p-value. R/Python/SAS have built-in functions for Fisher's Exact Test - in R, its `fisher.test(table)`.\n- Apply the Bonferroni correction to control the family-wise error rate. To do this, multiply each p-value by the number of tests performed (in this case, 3).\n- Compare the corrected p-values to your desired significance level (e.g., 0.05). If the corrected p-value is less than or equal to your significance level, you can reject the null hypothesis and conclude that there is a significant difference in 30-day mortality rates between the age groups being compared.\n\nGood luck!"", ""Fisher's test would be most appropriate here - it's like a chi square test but more appropriate when you have low cell values like this.\n\nYou also might be able to try a logistic regression on mortality - I expect your event rate is too low, but it's worth a shot."", 'Thank you for the help!', ""Thank you! I'll try both""]"
[Q] Assign predictor to extreme measures in Bayessian Regression,"Hello,

I am using Bayesian regression to model the distribution of a certain quantity depending on some predictors. This is a quantity that is supposed to be 0, but it is not due to measurement errors of the predictors, so the purpose of the study is to identify abnormal measurements and where could the problem be among all the predictors.

So far I have the regression model, and I have a prosterior calculation to obtain the likelihood of a new measurement given the model. However, the step of assigning the abnormal measurement to a certain predictor has me stuck. This is for work, so right now we would like to at least get a ""toy"" method to show the clients what we have is on the right track (they do not have the background to understand regression models sadly so I need to come up with some examples to make them see what we are going for).

My only idea/hope right now is to do tests on some simple datasets the client can provide, and then if a extreme measurement happens, try to manually adjust the predictors and rerun the model to see if the measurements are more plaussible with this modified model. However, since this is just my intuition I am still not completely sure if modifing the model like that would indeed show what I expect.

Any comments, discussion or just references is greatly appreaciated, I am learning a lot of new tools on this project and with Bayesian regression I am discovering a lot of things I didn't know you could do. Thanks in advance!

&#x200B;

edit: (wrote this on a comment too)

To give more detail:

I have a system with a material that goes in the system, out the system and sometimes the system itself uses some of the material for mantainance or other reasons. Then, in theory, if I measure all the material that goes out, goes in and gets used and then add it, it should be 0. However, it is not, due to measurement errors in the measure stations (or sometimes it could happen that there is a leak and then there needs to be immediate action). Abnormal measurements are interesting because it means some measurement device is working out of the expected errors and a check up will be needed, or that there is a leak and it has to be repaired. So the are the measurements I will look at, not just outliers.",12dgrom,Chus717,1680779424.0,2,1.0,"['Apologies if Im missing something, but if you already have your posterior distributions, then can you not just take any given abnormal sample, and input this data into the posteriors to see which predictor gives the lowest likelihood?', 'What do you mean by ""the quantity is supposed to be 0""?\n\nIs an ""abnormal measurement"" interesting by itself or just an outlier to be discarded?', 'Am I understanding the model correctly. It is, essentially, p(y+e(y)-x1-e1-x2-e2-x3-e3)+(1-p)(y+e(y)-x1-e1-x2-e2-x3-e3-leak)=0? Where each e is a measurement error and p is the long run probability of no leak. You need to know P(Leak>0ly, x13)?', 'I am the one that is not an expert on Bayesian regression, so I might be missing something, but my understanding is that the prior is just the probability of a new measurement of the quantity given the model and the new predictor values, how would you work from that to assigning the uncertainty to a specific one?', ""To give more detail:\n\nI have a system with a material that goes in the system, out the system and sometimes the system itself uses some of the material for mantainance or other reasons. Then, in theory, if I measure all the material that goes out, goes in and gets used and then add it, it should be 0. However, it is not, due to measurement errors in the measure stations (or sometimes it could happen that there is a leak and then there needs to be immediate action). Abnormal measurements are interesting because it means some measurement device is working out of the expected errors and a check up will be needed, or that there is a leak and it has to be repaired. So the are the measurements I will look at, not just outliers. \n\n&#x200B;\n\nMaybe I was not too clear about it so I'll edit this to the post."", 'The errors are proportional to the x_i, so it would be more like y= sum(x_i*e_i), and this e_i are the parameters in the bayesian regression, and we give them normal priors to both y and e_i\n\nEdit: and what I need to know is given a new value of y and new values of x what is the probability of that y given the new x and the model, ie, the data used to obtain the model distributions', 'Can x_i(observed)>x_i(real)?', 'Yes, the uncertainty comes for the measurement instruments, which do not have a specific tendency to over or under measure. From the specifics of the problem we are more interested in when the overall Y(observed)<0 because that means losses for the client.', 'Is a leak the only important source of loss?', ""There could be several causes for an abnormal measurement: there could be a problem in the measurement instruments, there is a leak or there was a human error are the ones we would be considering. It's not about determining which one is it yet, right now is trying to identify abnormal measurements(which is already done with the posterior) and then the next step would be to pinpoint the source of thr abnormalities."", 'I asked about the leak for a different reason. If x_i is always present and z is the loss to a leak, but leaks are uncommon, then they have to appear in the likelihood with an attached probability p of occurring. Otherwise your posterior is wrong. \n\nAlso, you said some input is used for internal purposes. Is it a continuous usage or does it periodically take what is needed.', ""Leaks if they appeared in the historical data they will be noted and taken into consideration by the model (another variable that would affect the desired quantity Y), but the data we have right now to do testing has no leaks so we are not worrying too much about it. \n\nThose internal consumptions are sporadic and usually related with a big change in other predictor's value. For example, if an output wants to be changed from 2 to 10 units, then the machines used to do so will consume some determined quantity from inside the system. So there is big correlations between some predictors."", 'So let me begin with what I would do. Your concern is one of your xs is mismeasured. So you are really working in the sample space. I would run a Frequentist regression with a null hypothesis that every slope is one. It honestly sounds like you have additive errors. If the Ftest rejects it, then the beta that a t-test rejects would be my bad item. \n\nIn a Bayesian construction, I would do the same thing except I would create two models. One model would have no intercept. The other would have an intercept. I would give very low prior probability to the model with an intercept. That is because its highly engineered, so it should be zero in aggregate. If the intercept is the probable model, then you have a systematic problem. \n\nAlternatively, you could construct the posterior predictive distribution for the marginal values of each x_i under a model with no intercept. That would give you an idea of what an extreme value would look like for each x_i, potentially under a range of conditions. You could either marginalize out the other x_i values or condition them to interesting values. That would give you a sense of what a normal extreme value should be. \n\nThe weakness in this is that you have not modeled the internal consumption mechanism. Its likely not linear and may be zero inflated. That will distort the results if true as it will be misspecified.']"
Smashing my head against Kruskal-Wallis [Q],"Smashing my head against Kruskal-Wallis

So I'm doing a masters thesis which includes 1 dependent sample taking 2 questionnaires, which I am comparing. A biostatistician at work told me that I should use Kruskal-Wallis to replace the one-way ANOVA (which I thought was for independent group) for this type of data but stats is far from my area of expertise so I've hit a bit of a wall with this one. 

Can anyone give me any guidance?",12dejek,zonatelake23,1680773286.0,5,1.0,"[""The Kruskal Wallis is a non parametric test to check if two samples originated from the same distribution, while the one way anova is a parametric test of equality of means. \n\nTo use anova you have to check your assumptions first: are response variable residuals normally distributed? \nAre the variance of the populations equal? \nThe third assumption independent and from identicall distribution i am not sure if it holds anyway, (if the same person answers two questioners are the questioners independent? I think not but i need to look it up).\n\nLets say that the above assumptions dont hold so you are forced to use Kruskal Wallis.\n\nWhat exactly is the issue?\nYou don't know how to implement the test? How to interpret the results?"", ""What's the research question you're trying to investigate?"", 'Im not sure about Kruskal-Wallis being appropriate given you have paired samples.\n\nSince you are working with paired data (1 dependent sample taking 2 questionnaires), you need a test which accounts for this. If your data is normally distributed and meets the assumptions for parametric tests, you can use a paired t-test, which compares the means of two related samples. If your data does not meet the assumptions for parametric tests (e.g., not normally distributed), you can use a non-parametric alternative like the Wilcoxon signed-rank test.', 'What is the scale of the responses? Are they binary (correct/incorrect), ordinal/Likert ( disagree, neutral, agree), or something else?', ""I asked the statistician again to clarify and he stated that I would split my sample into 2 by the documents the questionnaires are based on. So, 20 volunteers for doc A and the same 20 for doc B, is this correct?This guy has a PhD in stats but every comment here says otherwise so no idea what I should do. I've already wrote 1000 words on Kruskal-Wallis and the more I wrote the more confused I got as it didnt seem to match."", 'Likert', 'Im not sure what you mean by splitting the sample into 2.\n\nSo you have 2 vectors of data, Q1 and Q2, representing answers to the respective questionnaires. Here, every pair (Q1_i, Q2_i) represents the same person answering 2 different questions, so we need to account for this paired nature of your data. \n\nWhat is the research question? Or the general idea behind it? Im struggling to see why you would throw away this additional explanatory power from your data.', 'The idea is does improving the readability of an informed consent form improve comprehension. \n\nSo I took an original ICF which is about 30 pages long and then I made a revised version of the same document based on current literature and guidelines but much shorter with a reduced reading age. \n\nThen presented both original and revised ICF to 20 volunteers along with a questionnaire for both ICFs which include quantitative and qualitative questions to see if the hypothesis is correct and which style of ICF is preferred.', ""Okay great, thanks for sharing that. \n\nSo, Kruskal-Wallace still assumes independence of observations (just like ANOVA), which doesn't align with your study design, as each participant evaluates both the original and revised ICFs.\n\nI would echo my previous comment that a Wilcoxon Signed-Rank test is likely your best bet.\n\nIt's possible that the biostatistician might have overlooked the fact that you have paired or related samples, but Im pretty confident that a non-paired test would be wholly inappropriate here. It might be worth asking them directly: \n\n>Given that my study involves comparing two related samples (original and revised ICFs) based on the same group of participants, would the Wilcoxon Signed-Rank test be a more appropriate choice than the Kruskal-Wallis test you previously suggested?"", 'Yeah so, I had done the Wilcoxon Signed-Rank and had reached out to see if there was anything else I can do and he recommended Kruskal-Wallis. I asked him to clarify about an hour ago...\n\nHe wrote, ""For the Kruskal-Wallis test, if you\'ve got ordinal data, make the two groups as original and revised and compare those two, essentially looking to test if they\'ve come from the same distribution using bar/histogram to visualise the difference."" \n\nI also have a secondary part of analysis which takes 5 years worth of ICF readability scores across 4 readability formulas and measures them against a recommended score. I\'m not sure if that has confused him? Or if I\'m way off.']"
"Why is learning statistics so ""annoying?"" [Q]","Dear All,

I am currently a student learning statistics. it's nothing advanced in the field, just a prereq for other courses I need to take.

however, as I take this course, I'm noticing more and more issues overall with statistics. many things are intuitive and good, such as some equations. others, not so much.

perhaps it is simply due to how I am being taught, but I feel as though many parts of this field are incredibly obtuse to learn. things that already have names are given new ones to fit the field, or sometimes an equation will use 30 different greek symbols instead of using a normal latin alphabet (which the majority of chemistry equations use, and they're easy to process to me).

I consistently ask myself ""why did they make this that way? why did they explain it in such a horrible way? why can't they just say what it is, instead of using so much jargon every time they need to explain something?"" every time I crack open the textbook to solve a single HW question.

I was wondering if anyone else had these sentiments about the redundant and annoying nature of statistics as a whole.

like, why do equations use the same symbol in very different contexts, instead of using a similar method to chemistry (such as the equilibria constants having different subscripts based on the type, or why don't we just use a single letter to always mean a certain thing, like p=population or r=radius)

why is a left skewed graph on the right side, when skewed means to swerve in a certain direction? would it not make more sense for a left skewed graph to have the data skewed to the left, rather than to the right?

why do they use explanatory/predictor variable in the same way as independent, despite them not meaning the same thing?

there's many questions I ask about this field, and it's increasingly annoying to learn as they never seem to have any set way to solve something...

maybe I'm just too used to chemistry, which mostly uses X, Y, Z, A, B, alpha, omega, and delta.

&#x200B;

edit:

another example:

I am being taught about Center of Population Distributions. The question is not dealing with geographic centers of population distributions. it then goes on to immediately say that "" The center of a population distribution is the mean of the population."" Why in all that is holy would it not just say ""population mean"" instead of using a term, then defining the term in an application that is not meant for that word?",12d9d5d,Artist_mugi,1680756383.0,0,0.3,"[""It probably depends on your background whether you find the language statistics uses intuitive or overly complicated. I've always appreciated the nuance it allows for"", 'I am glad that you find statistics intuitive. Frankly, not many people enjoyed statistics and usually found it counterintuitive (such as probability theory). In terms of Greek letters and some jargon, you just have to know the language we speak here (the same goes for math). When I was an undergrad, I sorta had the same issues with statistics - why are the wordings in statistics so convoluted? Why not just accept something instead of failing to reject it anyway.\n\nBut as long as you pass that invisible ""doorstep"", you will appreciate how rigorous the statistics are. Statisticians can reject an idea, but we never provide a ""concrete"" answer - it always ends with ""It depends"" or ""We need more data"" - We adore the beauty of uncertainty.\n\nOne day, you will find yourself speaking statistics and you will realize ""Darn, now why am I turning into this pain in the neck person"" when you see your colleague scratching her/his head and trying to process what you just concluded based on the results you provided.', '>  things that already have names are given new ones to fit the field\n\nThis is true of every field. Your problem seems to be that you\'re used to certain conventions in *chemistry*, and are having to learn new ones in statistics. This isn\'t statistic\'s fault, it\'s just hard to have to learn new conventions in general\n\n> why can\'t they just say what it is, instead of using so much jargon every time they need to explain something?""\n\nStatistics, like all mathematical fields, work with objects and concepts that have extremely precise definitions. Jargon is inevitable, and saying what something ""is"" generally does involve a lot of specialized terminology.\n\n> I am being taught about Center of Population Distributions. The question is not dealing with geographic centers of population distributions [...] Why in all that is holy would it not just say ""population mean""\n\nThis is like complaining that a ""group"" in mathematics isn\'t the same thing as a collection of people, or that a mathematical ""ring"" doesn\'t have anything to do with benzene. Words mean different things in different fields. The ""population"", in statistics, is a distribution, and the mean is a measure of central tendency. All of those terms have perfectly clear definitions, and it\'s perfectly fine to say that the mean is the center of a population distribution. It\'s just a matter of becoming comfortable with what those words mean in statistics.\n\n>  why do equations use the same symbol in very different contexts\n\nIt\'s hard to say without you giving an example, but there are far more contexts than there are symbols, so it\'s natural that some would get reused. There is still a fair bit of consistent terminology, though.\n\n> would it not make more sense for a left skewed graph to have the data skewed to the left\n\nThat\'s exactly what left skewed means (at least, intuitively, and this will be correct most but not all of the time). A left skewed distribution has a longer left tail, which pokes off to the left.', 'nothing is intuitive when you are first learning about it. Knowledge builds upon itself over time. Im sure a seasoned statistician with no exposure to chemistry would be saying the same thing about the naming convention in chemical compounds. Or why is the chemical symbol for gold not G. Etc.', 'I feel you, as a student this can be rough. I am a QA and Food Safety professional where stats is only a part of what I need to apply to my job so as you end up applying the same small part over and over you lose some knowledge. Hell half the time it is better for me to use reference materials provided by ANSI than it is to try to apply them myself. That being said its no reason to hate on the words for the class. Knowing them for the class will help you pick them back up if you ever have to relearn some things you dont use every day.', 'I think answering these questions might help you answer the questions you asked about skew: what happens to the mean when you have skewed data? In other words, which direction does the mean swerve?', 'I have a recent blog post about the meaning of ""skew"" and other examples where a common word has been given a technical definition that clashes with its ordinary meaning: https://www.allendowney.com/blog/2023/03/16/what-does-skew-mean/', 'I felt this way about frequentist stats. I found bayesian stats kinda pleasant to learn']"
[Q] Statistical significance of a data set from a poll I conducted,"I recently ran a poll online where I asked if you agree or disagree with some statement. I asked respondents to indicate ""yes"" or ""no"" and then their age. I want to test a few hypothesis, and do this in Excel. I think I would use a T-test but I am unsure, it's been a while since I did this. 

E.g. data is like this:

Age 59+: Yes - 23 No - 41

Age 43 - 58: Yes - 33 No - 74

Under 42: Yes 53 No - 106

I wanted to test the theory that ""older folks"" would be more inclined to say yes than younger folks, and that overall, more people would say NO than YES (sorry, in this case for example, old is the 43 and over group). 

How would I do this in Excel?",12d1bbh,z1ggy16,1680736268.0,0,0.5,"['Generally, I think its a really bad idea to split continuous variables like age into arbitrary categorical levels. Lots of lit on this topic. Probably avoid that if that was how you were gonna analyze', 'I would do a logistic regression predicting Yes or No dichotomous outcome from continuous age IV', 'CHISQ.TEST() function will test if the probability changes with age window using the 3 by 2 table you have above.  \n\nIf you have the responders actual age you could also do logistic regression; this would have more power and get at the direction of the affect with older subjects.', 'If Im understanding your set up correctly, you really wouldnt conduct a statistical test on this data. Its a volunteer response sample and you cant draw conclusions about a population from such a sample.', ""You have to assume that your sample is representative of the population, or your inference is only applicable to this one sample.  Anyway, read this:\nhttps://stats.stackexchange.com/questions/113602/test-if-two-binomial-distributions-are-statistically-different-from-each-other\n\nor you can use Fisher's exact test :\nhttps://en.wikipedia.org/wiki/Fisher%27s_exact_test"", ""I had no choice bc the poll structure didn't allow for individual age responses. The limit by the platform was like 15 variables."", 'I only have the have the bracket they fall into. I did it by generation (boomer, gen x,y,z)', 'Totally fine if your inferences are about the population that would take a voluntary survey.', 'Ah, then Id model it as an ordered factor in a regression model. I would not ignore the ordering']"
[Q] Question about regression analysis with numeric variable as Dependent and ordinal as Independent Variable,"\[Q\] Hello there! I'm trying to conduct  a mediation analysis with mixed variable types (ordinal and numeric)  and I'm at my wit's end. This these are the three regressions I am making and the first one is the procedure in question:

1. **Variable 1 (numeric) <- Variable 2 (ordinal) <= This is the analysis in question.**
2. Dependent Variable **<-** Variable 2 (numeric) + Variable 2 (ordinal) <= Already performed.
3. Dependent Variable <- Variable 2 (ordinal) <= Already performed.

**Variable 1**  is  a numeric variable (factor from factorial analysis, range: \~ -3 to  1) and the Dependent Variable and Variable 2  are ordinal variables (scale: 0 to 3, ordered).Previously, I've conducted partial proportional odds models successfully, but this particular  procedure (1) has left me puzzled.

* Should I  transform the numeric **DV** **(V. 1)** for this analysis? / Is dummy coding a viable option?
* Can I convert the ordinal **IV (V. 2)** to numeric for the sake of completeness?
* Or is there a procedure / package I've overlooked?

Any suggestions on how to measure the impact of IV2 on IV1 are appreciated. Thank you!

Edit: Clarification of the question.",12cy3sm,NightmareAkin,1680729103.0,19,1.0,"['In brms you can wrap the ordinal predictors in the monotonic mo() function.  I think Paul Buekner has an accompanying paper explaining but too lazy to look it up.', 'Structural equation models to the rescue?\n\nYou should provide more context why you conduct this analysis.', ""Found it! I'll look into it."", 'Thank you for your response! The research question is ""do values affect the relationship bettween childhood adversities and violent behaviour"". I have questionnaire data und there was a problem with mulitconlinearity so i did a factorial analyisis with the 10 Items and got 2 well-fitting factors out of it (values). After the FA they are numeric and range from negative to positive values.   \nCurrentyl I am thinking to just do last two PPO and measure their difference with and withouth the potential mediating/moderating value. With that I can circumvent the modelling of the first IV1 <- IV2.']"
[Q] Sample vs Population - Do I have this right?,"I am trying to solidify my understanding of sample vs population. For the most part I think I understand. I get that a sample is just a subset of the population I am looking at. For example, If I wanted to know whether or not there is a relationship between the height and weight of people it would be really hard to get accurate data on every person on earth. Instead, I would take a random sample and infer that my sample is a representation of the whole population.

&#x200B;

My question is when I wouldn't take a sample because I can get the whole population. For instance, let's say I work at a company and I wanted to know within that company if there was a relationship between a person's gender and whether or not they leave our company. I wouldn't use a sample I would use the whole population because I have access to that data. 

Would this change the way I would do statistical testing? 

Would I be doing inferential statistics on a population?

Do I have to reframe my thought on what a population is?

&#x200B;

Just trying to nail down the difference and my thoughts on the conclusions of running statistical tests.",12cr9z6,AKing2713,1680714962.0,3,0.8,"['> For instance, let\'s say I work at a company and I wanted to know within that company if there was a relationship between a person\'s gender and whether or not they leave our company. I wouldn\'t use a sample I would use the whole population because I have access to that data. \n\nIn that case, you are almost certainly interested in whether your companies culture/administration/policies affect the tendency of a particular gender to stay/leave. You\'re probably not interested *only* in the specific people who happen to work at the company right now, or in the past. Even if you examine everyone who has ever worked at your company, you\'re only observing a sample of people who have experienced the specific factors at work in your company.\n\nThe ""population"" is the data generating process. If your research question is about a specific group of people, then that set might reasonably constitute the population, but for questions of the form ""Is X related to Y"" (e.g. does my company discriminate based on gender), even the entire group of people who have ever experienced X is still only a sample of the underlying mechanism relating the two variables.\n\nAs an example, if your company flipped a fair coin in order to determine who to fire, this process is absolutely non-discriminatory. But it is very unlikely that the *empirical* distribution of firings would be exactly equal -- e.g. if 1000 people worked at your company, by chance it is likely that slightly more of one gender have seen heads than the other. Your interest is in the *data generating mechanism* (the coin flip), not the sample of people who happen to have worked at the company.', 'Thank you for the comments. The main thing that I was hung up on was the population through time. It makes sense that we would still consider the data as a sample of what could be in the future.', '> I wanted to know within that company if there was a relationship between a person\'s gender and whether or not they leave our company.\n\npresumably you want to know that because such knowledge could influence some kind of company policy (e.g. \'women leave because they feel we aren\'t promoting women, we should see what we can do to address that\'). In that case your population - the one you want to make conclusions about - is not ""people in the company at the time of sampling"", but across a sequence of times where the policy were to be in place.\n\n(Of course you can\'t sample those populations, but they\'re the ones you want to carry your conclusions over to.)']"
[R] t test for ratios,"I am a biologist. I have some data that fits the description of this website.
https://www.graphpad.com/guides/prism/latest/statistics/stat_paired_or_ratio_t_test.htm
As a result I want to use a t test for ratios. But I don't really find a lot about t test for ratios online except on this Graphpad website. Is it a commonly used method? Is there any alternatives? Thanks a lot, people.",12cmwew,maemji,1680706354.0,2,1.0,"['I have never encountered the term ""T test for ratios"", but perhaps some other redditors have. \n\nThe name Is tricky, because there are many tests regarding ratios, but the test you\'re talking about would be generally seen as a T test for transformed samples. That might be why you\'re having trouble searching for it.', 'That term isnt commonly used but transforming to logs and testing differences in geometric means and computing confidence intervals on ratios of geometric means are well understood. [This webpage](http://www.jerrydallal.com/LHSP/ci_logs.htm) has a good explanation.', ""I've also noticed that Prism is the only place this test is described this way. u/dmlane is correct though that the underlying logic is common.""]"
[Q] Interpreting Coefficient of Logistic Regression with Independent Variable Expressed as Percentage,,12cklen,Barzoic,1680701724.0,0,0.5,[]
[Q] How does the hypothesis formulation influence the statistical analysis?,"I have a few questions regarding the formulation of hypotheses.

For example, here are 2 hypotheses:

1. Age will influence the choice of food individuals order at the restaurant.
2. Younger individuals are more likely to order fried food at the restaurant.

These 2 are almost the same, yet different. Does one have an advantage over the other? How does the stats analysis change, if at all? Which one would I be better off choosing?

Another example could be a hypothesis on gender and safety.

1. Gender impacts the individuals' feeling of safety when walking home at night.
2. Women are more likely to feel unsafe when walking home at night.

Thank you for your help.",12cgyym,csira_allapot_5876,1680693658.0,0,0.5,"['The way you state the hypotheses will define the nature of the data: age could be ratio, ordinal, interval or nominal. That will influence the type of test. It also matters if the hypotheses is stated directionally or not. The second one is directional, I guess. So it determines whether you should be considering one tailed or two tailed test.', 'Thank you']"
[Q] What are your favorite data visualization packages in r?,"Just like the title says! Ive experimented a ton with everything ggplot and plotly has to offer, and Im familiar with a lot of other libraries, but what are some libraries that you guys have been using, whether with or without shiny, that display data pretty well?

Side quest: what are the best ways to display regressions on top of existing data? Ive only ever used base r for lm visualization",12cfbkr,Mcipark,1680689441.0,64,0.97,"['Ggplot2 is really the end-all be all for me. Combine with GridExtra and ggally for anything thats missing. Specific packages often come with specialized visualizations for model outputs like LASSO regression and I often use those rather than reinventing the wheel. But anything other than that is ggplot2. \n\nAs far as plotting, simple models can be fit with geom_smooth. More complicated models, I fit separately and store the predictions in a new data frame and plot separately.', ""Man this is a much easier question than the Python equivalent.\n\nBesides ggplot2, I like:\n\n* ggforce.  It has some nice miscellaneous functions.  I use facet_zoom a lot, which shows a plot with a zoomed-in portion of an existing plot.\n\n* ggrepel.  Creates readable labels that automatically dodge each other, which is more readable than what geom_text and geom_label do by default.\n\n* ggridges is great for showing a bunch of distributions in a relatively small amount of space.\n\n* gganimate is great for demos.  It lets you easily add animation components to a static ggplot object.\n\n* leaflet is great for 3D mapping.  You can use OpenStreetMaps or similar as a background.\n\n* ggsurvplot for plotting survival objects more flexibly than R's default plotting."", 'https://r-graph-gallery.com/\n\nEnjoy the rest of your day trying these out :)', 'I use ggridges and bayesplot for plotting distributions in Bayesian data analysis.  ggplot2 pretty much can do anything so usually better to go deep on that package than wide with others.\n\nSide quest answer:  ggplot2::geom_smooth(method=lm)', 'Not surprisingly ggplot2 is my go-to for most projects.', 'I use base R.', 'I am lazy, so I recently started using ggpubr (based on ggplot2), which gives you publication-ready plots without the need for further formatting. :)', ""Check out the ggeffects package. It's built on ggplot2 (which basically everyone uses), so adjusting the plots is super easy. I use it all the time to plot models, it has really nice defaults so it's super fast"", 'Plotly is dangerously based. Gives very impressive visualizations.', 'ggplot2', 'Ggplot2 is my go-to too, but there are a couple of task specific ones that are maybe less aesthetically appealing, but still useful. In particular, I like sjPlot for plotting model summaries, and gratia for summarising generalised additive models.', 'Plotly is another great visualization tool, but it doesnt export out of r nicely. However great for markdown reports if you dont care about pdfs and such.', 'igraph for network plots. Not the best visuals but super easy to build and work with data', 'Ive noticed a ton of libraries expand upon ggplot2, are there any more interactive ones that youve played with that you would reccommend? I liked how with plotly you can hover over elements of s plot and it will tell you information at that point', ""Just want to add to this great list ggmap which let's you use stamenmaps as basemaps without an API key, login, or any real headache"", 'YOU CAN MAKE CIRCULAR BAR PLOTS??? Tysm, this will definitely keep me going for the next few weeks', 'Based', 'Sounds like my speed lol', 'You have the esquisse package, they have an interactive ggplot builder, where you can drag and drop variables in x and y axis, choose plot types, colors, adjust legend... and then export the code.\n\nTo make an interactive ggplot like in plotly, just call the ggplotly funtcion on your plot made with ggplot2.', 'Generally, if Im doing interactivity Im using another tool, like a BI tool for work. You could play with Tableau community edition maybe? Ive used AWS Quicksight as well.\n\nThe honest answer is that interactive visualizations are kind of hard to share and a bit too much hassle for the payoff for me. Ive never gotten familiar with those libraries because ggplot2 is so fully featured that I havent needed much else. If you want to highlight certain points, sometimes its better to just to geom_annotate.', 'You might want to look into d3-based libraries like networkd3 then.  You will get pretty interactive graphs just be prepared for some headache as its a much different paradigm from ggplot.', 'Baser']"
[Q][D] A problem that requires your creativity! Can it be solved?,"Let's assume you are a platform (called X7) that connects distributors to supermarkets. 

You have two services: 

1) X7-orderNow: as a supermarket you order a basket of goods from different distributors. 

If a supermarket uses this services, only a fraction of the supermarket's purchases are being captured, many purchases are done outside the platform.

2) X7-PointofSale: as a supermarket you sell to retail customer, and all your purchases from distributors are automatically registered.

If a supermarket uses this service, all its purchases are captured.

Therefore, we have 4 types of supermarkets:

1) uses X7-orderNow only (73% of all the supermarkets)

2) uses X7-pointofSales (2%)

3) uses both (2%)

4) uses None (23%)

&#x200B;

which means:

4% of the supermarkets have there purchases being fully captured.

73% of the supermarkets have a fraction of their purchases being captured.

23% of the supermarkets have none of their purchases being captured.

&#x200B;

Statstically, scientifically, practically, hypothetically, whatever approach you want to take, would I be able to answer the following question:

What were the total purchases of a given product (i.e. across all supermarkets) in a given month?

If yes!!! How would you approach this?",12cedgj,Riolite55,1680686431.0,0,0.33,"['Okay, well you can never know the total purchases of a given product exactly, but you can develop a pretty good estimate with statistics if youre willing to make a few assumptions. \nHowever, it is not known whether the fraction of the 73% using X-7 OrderNow includes this product, ideally it does. And I also dont know if this fraction changes store-to-store such that it sometimes includes the product, and sometimes doesnt. So, Im going to assume it sometimes includes the product. \n\nSome assumptions:\n1) theres no systematic difference between stores that use this program and stores that dont (I.e., the 4% is representative of the population of all stores).\n\n\nLets start with what we do know:\n1) We know all of the sales of this product from at least 4% of the stores, and at most 77% of the stores. \n\nMy approach would be to find out how many stores we have sales-data on for this product, and to also know this as a proportion of all of the supermarkets (e.g., 4%?).\nThen Id record how many sales of this product each of those supermarkets made within the month. \nId find the mean-average of the distribution of sales for this product, and then Id calculate the standard error of the mean (using the finite population formula). \nUsing this, Id then find the margin-of-error, and then the 95 and 99% confidence intervals for my mean. \nFinally, to get an estimate of the amount of this product that wouldve sold, you would multiply the mean by however many supermarkets there are altogether (even the ones not using the programs). However, Id also do the same with the lower-bounds and upper-bounds of the confidence intervals to gauge how confident you can be in your estimate.', '""find out how many stores we have sales-data on for this product"", regardless what type of store, i.e. a store using the service orderNow, poitOfSale or both?', 'In my analysis, I assumed that there was no systematic difference between stores that use one program, and stores that use another. \nSo, this assumption means that it would make no difference. \nHowever, if you expect a difference, then just do the exact same thing, but break the data down by the program the store uses. Sadly, you would also be left with no information about stores that we have no data on though. \nAs well as this, other factors like region and population density would be important to consider, but werent considered here.']"
[Education] Excited to learn more statistics,"Excited to learn more Statistics

Hello,

Im an adult student studying math and computer programming. Im a total beginner in the computer stuff, but I took Calculus in high school 25 years ago (damn Im old).

Im remediating my math skills, working up through algebra and trig, and starting Calculus again this summer. I also took Elem statistics last fall and enjoyed the real world aspect of it. It might be the first real applied math Ive ever taken. 

Id like to learn more stats, but The Internethas told me that learning more is going to take a lot more math skill first. Im planning on taking Calculus 1-3 and linear algebra at my community college. What should I master so that I can learn more stats, and what stats should I try to learn once I can do it?

Thanks.",12c9qkf,RitardStrength,1680672792.0,10,1.0,"[""> Im planning on taking Calculus 1-3 and linear algebra at my community college. \n\nThose are just the right things to start with, if you want to learn the basic theory (which will get you quite a way) that will suffice to get you through something like the harvard probability course\n\nhttps://projects.iq.harvard.edu/stat110/home\n\n(The book there is free as a pdf, there are video lectures and exercises etc there)\n\nThat will be enough probability to tackle a typical math stats subject (but you can probably leave the later chapters until a bit later). There are many suitable texts that cover basic theory of inference (e.g. Larsen and Marx, or Mendenhall, Wackerly and Scheaffer ... or any number of others). From there you can pick up many other topics in statistics, but I'd suggest a regression text next, one with a theory component as well as coverage of the practical side (I don't have a particular recommendation for that)."", 'Thanks for your help. I have Mendenhall, and I also have a book called Statistical Inference by Casella. Which is better?']"
[Q] Most suitable test to compare changes in word frequency in Twitter after a treatment,"Hi, I'm having real hard time finding the correct test for statistical significance.  I'm trying to measure if a statement by an influential figure had an effect on a particular discussion on Twitter, one of the ways I do this is by comparing frequencies for certain keywords before and after the statement, i.e. the treatment in my case. Keywords are also grouped under certain themes, such as violence, ethnicity,rights-based etc. So my data looks like this

&#x200B;

|Keywords|Group|Pre Treatment - Group Frequency|Post-Treatment Group Frequency|
|:-|:-|:-|:-|
|\[indian, arab, white\]|race|150|100|
|\[killing, beating\]|violence|120|140|
|\[civil rights, human rights, law, legal\]|rights|50|80|

&#x200B;

So far I've been reporting the simple percent change for group frequencies, which feels lacking. I've been suggested to take the means of each keyword group and use dependent/paired samples t-test, but I'm hesitant as I can't claim pre-treament and post-treatment tweets include the exactly same users. Is it fine to do that, or is there a better way to test the treatment?

I've read that ANOVA is also not recommended as the independence assumption is violated ([https://stackoverflow.com/questions/66423466/anova-test-on-time-series-data](https://stackoverflow.com/questions/66423466/anova-test-on-time-series-data))",12c86li,otobab,1680668553.0,2,1.0,"['Do you have placebo (non-treated) users in the post period? Without them its difficult to distinguish whether the changes in word frequencies come from the treatment or just trend.', 'Hi, you could also try doing counterfactual inference. So you we expect that there will be bias in doing the straight comparison using a t-test between your pre and post treatment groups because we believe that tweet keyword frequencies change over time. So we try to remove the change over time bias by fitting a time series model to the data up until the treatment time. Then you use your model to forecast the group frequencies from the start of your treatment time to the end of your treatment time. This counterfactual answers the question what would the keyword frequencies have been if the treatment never happened. Once you have this counterfactual prediction you can simply take the difference between the true post-treatment group frequency that you measured and the predictions that the model made and this gives you your causal impact of your treatment. Now to determine ""significance"" you have two options, you can either do all of the above in a Bayesian framework (recommended) where you will have uncertainty estimates for your causal impact estimate or you can generate a bootstrap confidence interval. If your confidence interval/highest density interval does not contain 0 (or some value of relevant equivalence that is reasonable) then your results are not due to random chance.']"
[R] I need textbook or resource recommendations to learn more about more advanced statistics,"Im getting deeper in my thesis work and I need to be more than just familiar with Generalized linear mixed model and Multivariate statistical analysis. Are there any textbooks that cover these topics that you can recommend. Most college textbooks just stop at Anova.
Id appreciate textbook recommendations or any other form of resource. I dont have a very strong stat background but I happen to enjoy statistics a lot and I understand the topics fairly easily. YouTube videos havent been as intuitive for me to understand.",12c5tso,brianomars1123,1680662635.0,1,0.67,"[""Statistical inference by Cassella and Berger should cover everything you'll need or at least provide the bridge to understanding stat theory if you'll need to dive into that."", 'You may benefit from checking out this [reading guide for an independent study program in statistics](http://stanfordphd.com/StatisticsTutor.html) (2nd half of the page).', 'Wow. Thanks for this list. Would be very helpful indeed.']"
Use cases for Bayesian time series? [Q],"Im currently in a time series course where we are learning about classical models: white noise processes, AR, MA, ARMA models etc. I was wondering when one would use the Bayesian version of these models, or in general when does Bayesian time series come about? Ive heard of Bayesian structural time series is this a Bayesian way of decomposing a time series into things like seasonal, trend components? What do we put priors on etc?",12c5azg,AdFew4357,1680661411.0,18,0.96,"['you can easily build expressive models with latent variables and put priors on them\n\neg HMM, DLM and state space models more in general', 'In macroeconomics bayesian vector autoregressions are huge and DSGE models are often estimated using bayesian methods.', ""Filtering. The Kalman Filter and its variants are Bayesian inference on a time series.\n\nYou have a hidden state x\\_t, x\\_t+1,.. and observations y\\_t, y\\_t+1.. . There is a model for the evolution of the hidden state (e.g. physics of an asteroid moving through space) and a model for the observations given the hidden state (how an image of the asteroid is produced given its position and velocity). There is also a prior for the hidden state, which comes from the previous time step.\n\nBayes' theorem will give you the distribution of the current hidden state given current observations. Continuing the example, the posterior is a distribution over the position and velocity of the asteroid, which gets updated given new observations of the asteroid as time evolves."", 'The use cases for Frequentist and Bayesian are different. They are not substitutes for one another. They are orthogonal to one another, literally. \n\nFrequentist probability works in the sample space. The sample is randomly chosen from the sample space. That is not true in Bayesian probability. The parameters are fixed, the sample is random in Frequentist probability. \n\nBayesian probability does not, strictly speaking, use the parameter/sample split. Instead, it speaks of observable and unobservable things. Parameters, generally, are unknown, so are random. Randomness here is in the sense of uncertainty not chance. However, if a parameter is known, it is treated as observable. Data is generally known and has no randomness to it. However, missing data is no longer observable, so it is treated as random. Likewise, future data is unobservable at this time. So a known parameter is treated as data and as missing value is treated as a parameter. \n\nYou place prior distributions on unobservable things. Priors, generally, should be proper and informative when information about their location exists, such as prior research or theory. Priors are actually important, and SHOULD bias results. That bias is desirable, in this case because, if done correctly, should contain information about the unobservable thing. \n\nBayesian methods are generative, not sampling based. The best Bayesian model will match natures way of creating data. If the best Bayesian model is AR(2), given a large enough sample size, then you can be reasonably certain that the Bayesian model is the true model if there are no omitted variables. \n\nFrequentist methods are about recovering as much information in the data as is possible, subject to the conditions and constraints in the model, such that it has good properties on repeated sampling. Frequentist time series tend to be lossy. It is impossible for a Bayesian method to lose or double count information. A Frequentist model can have equal or less information in it than a Bayesian model. It can never have more. The Bayesian likelihood function is always minimally sufficient. \n\nA Frequentist method is superior when you need large scale social agreement as to the results. Everybody will have a different prior so everybodys Bayesian results will differ. A p-value has an agreed upon meaning. \n\nA Bayesian result is superior if you need to gamble or if you need to find something for noncollective reasons. \n\nFor example, let us imagine that you are a COVID researcher back near the beginning. You will want unbiased estimates of its rate of spread because the results will be politically consequential. \n\nNow imagine that you are, instead, running a hospital and you are seeing a wave hit New York City and you want to know how long before you are overwhelmed in your hospital in Wyoming. You can capture all the information from other areas in your prior. It is as if New York gave you a Wyoming sample before it happened. You can greatly improve the quality of your predictions. In effect, you are getting a larger sample size. \n\nIf you are placing money at risk in a competitive situation or need to find the value of something that you cannot see, use a Bayesian method. If WE need a result from you as a community in a manner WE can all agree on, use a Frequentist method. \n\nBayesian methods cannot be stochastically dominated, minimize the average loss created from getting a non representative sample, they cannot be arbitraged and their predictions minimize the K-L Divergence. They are ex post optimal methods. \n\nFrequentist methods minimize the maximum amount of risk, can be unbiased, and give 100% weight to the data and nothing to outside information. They have an agreed upon meaning. They are ex ante optimal methods.', ""Take a look at fb prophet. It's a bayesian ts framework and it's open source as well. It's fairly well documented so you may be able to absorb some of the motivation as well as practicalities of bayesian ts modelling by going through the docs and then the source code."", 'Today in industry, anomaly detection on time series is built primarily off of the Facebook prophet package, which is a Bayesian time series model.']"
[E] Youtube recommendations,"
Hello! 
I am looking for some cool educational YouTube channels on stats. 

For example, 3blue1brown is great for math, Essentials of Linear Algebra is elite for me.

What are some channels with similar quality that teach stats related topics that you'd recommend?",12c2l7c,apoptosis100,1680655166.0,39,0.95,"['StatQuest', 'I recommend [statisticsmatt](https://youtube.com/@statisticsmatt). He has been helping me in my statistics courses (probability, inference and regression).', 'there is an channel that posts their lectures for MBA stats and analytics I know of.  https://www.youtube.com/@OnlineBusinessAnalytics', 'In addition to statquest I use this @marinstatlectures and @bbrcourse6203', 'jbstatistics is goated, he also has really nicely organized playlists.', 'Bam!!', 'I would classify his videos as edutainment. Might be a good first pass through a topic though, just to get thinking about it.', 'DOUBLE BAMM!!!!']"
[D] Need help creating a formula using multiple variables,"Hello. I have a bunch of data. I think the easiest way to explain is that I have a bunch of possible independent variables (like, 15<) and 1 dependent variable. I don't know for sure if all of the independent variables even have an effect on the dependent variable. I'm looking for a way to develop a formula that has a very strong relationship with the dependent variable. Imagine not knowing that velocity is the change in speed divided by change in distance. Is there a way to compute/process columns of speeds, distances, and velocities that would output the equation v = ds/dt? But in a way that could process 15+ different independent variables rather than just two?


I was thinking, since all of this data is on a spreadsheet that I could find the P-value between each individually independent variable and the dependent variable? Is that a first step somewhere? I have no idea, maybe this is useless.

Please help!",12c1v4b,yaggirl341,1680653521.0,2,1.0,"['Look up LASSO regression.', 'So you want to get this out of the spreadsheet and into a statistics environment like R, Matlab/Octave or use some Python. \n\nThey have built in functions that, to put it simply, just consume the data and spit out all the answers you are looking for. Keep in mind those answers are technical and you need to provide some information about the variables which you may are may not be aware of; if you provide the wrong info the answers that you will get will be wrong. \n\nYou just cant do that with excel. And even if you could, you would still get technical answers and require knowledge of the variables you are using.']"
[Q] Determining statistical significance,"I'm conducting a market research project where I have obtained satisfaction ratings (1 - not satisfied at all, to 10 - very satisfied) from the general population across a series of brands. Respondents can provide ratings for multiple brands if they have experience with each.

I have summarised the response data into means and distribution plots by brand but am having difficultly deciding which statistical test to apply to determine which brands have an adequate sample size to be confident in its aggregation.

My summarised data looks something like this:

Brand | Responses (n) | Mean Rating
:--:|:--:|:--:
Brand A | 854 | 7.43
Brand B | 18 | 7.26
Brand C | 1,034 | 7.11
Brand D | 228 | 6.43

What would be an effective test for determining if ""Brand B"" received enough responses for their mean rating to be fairing compared against the others. I'm not sure if I'm overthinking things by reading into ANOVA scores or if it's just a simple p-test? Just need someone to point me in the right direction.

I'm using R for my analysis if there's any recommended functions to apply to the observation sets. 

Many thanks!",12bvvb1,lolard,1680640944.0,6,0.87,"[""If by fairly rated compared to the others, you mean there are no significant differences between the means, then an ANOVA will suffice if you can't reject the null (which would imply not enough evidence for any differences across all means). You could also do t tests if you want in a pairwise fashion, but you ought to do multiple comparison corrections. So you could compare category B pairwise to all the others one by one."", ""(long discussion removed for now because I missed some of the information in your question)\n\nOne thing that worries me is that you don't have information for all brands from all people. That's going to make it a little bit harder to do well, but it won't stop you doing something, as long as you can reasonably assume that it's missing at random (rather than in some way connected to ratings, for which, unfortunately, there are some potentially plausible mechanisms). \n\nPresumably you would want to treat the individuals as having  random intercepts in a linear mixed model.\n\nFailing that you could treat the individuals as blocks I guess, but that's less suitable."", 'Thanks for your help. \n\nTo clarify what I mean by ""fairly rated"" - I want to establish if there are enough responses per brand to determine if a single brands mean response rating is a valid rating.\n\nThink of it like looking for a pizza restaurant on google maps, if there\'s a pizza place with 300 reviews and a 4.7 rating, you know that place is probably pretty good, but what about a pizza place with 8 reviews and a 4.8, I might not trust that rating because the sample size is too small. If I wanted to say which is the best pizza place in my town, what would be the minimum number of reviews per restaurant to be considered in the top 10?', 'Thanks for the long original response - and for re-clarifying.\n\nI do have all the original response 0-10 ratings for each respondent for all brand. For this single question I would have ~3,000 responses across something like 25 total brands.', ""Conceptually what you are talking about is a simple t test of means, whether there is a significant difference is a function of the difference of means and the (sample) variances. In your pizza example, I would do a test with mu = 4.8 with the null hypothesis that it is equal to 4.7. Statistically speaking, there is no sample size that is too small to detect /any/ difference; the smaller than sample size, the greater the variance in this case, which will make your t test value smaller as it is on the denominator. Hence, to detect a significant difference, you would need a larger difference in mu vs the null hypothesis. The idea that you're close to is the power of your test, the ability to detect differences given a sample size, and you absolutely can calculate the power of tests you're going to do, but it is a little more involved.""]"
[Q] Which stats test to use when I have unequal variance & skewed distribution?,"Cant do a students t-test, or welchs, nor can I do a wilcoxon. What to do?

Thanks!",12bsqev,Free-Cellist-1565,1680634622.0,6,1.0,"[""1. What is you response variable? What values can it take? (NB not what values are in the sample, but what is able to happen)\n\n2. What is the working hypothesis or research hypothesis? (/what *exactly* are you trying to find out?)\n\n3. Why can't you do the things you say you can't do?"", 'Anova is generally conservative with skewed distributions although it is often the case that a transformation such as the log (to reduce positive skew) increases power. One alternative is a randomization test which can be done on [this website](https://www.lock5stat.com/StatKey/).', 'Try some nonparametric tests....order statistics should be robust.', '\n>to accompanyStatistics: Un*lock*ing the Power of Data\nby Lock, Lock, Lock, Lock, and Lock\n\nTIL: this book exists and these authors must have had lots of fun coming up with that book title', 'Yes, one of the greatest book titles ever and from a family of statisticians.']"
[Q] Breaking up a dataset into groups based on frequency of occurrence. How to determine what the groups are.,"Background: I am a data analyst dealing with a dataset of about 33,000 chemicals which are broken up into about 800 classifications. My task is to break those up into the least and most represented classifications. I am using R.

Problem: The top 15 classifications range from 500-1200 occurrences, the next 50 from 100-500, the next 300 from 10-100, and the rest are below 10 with 300+ occurring once or twice in the dataset. Those ranges/groups are rough and I do not expect them to fit.

For representation, I was going to make a log bar plot displaying the top vs bottom x but that is obviously not a good idea since the bottom 100 are all the same value (1). Creating groups based on similar frequency may point me in the right direction. How do I figure out what ranges/groupings are statistically significant in order to organize the data?",12bsmwo,TrippLaP,1680634429.0,1,1.0,"[""My first approach wouldn't be to do this computationally; it would be to get a more qualitative sense of which kinds of chemicals are in certain ranges. It may be the case that they share something in common. Maybe make a stacked bar plot of which chemicals occur at which frequencies (bin them, perhaps, if that helps)."", 'First question is are you sure you need to group them. It is probably fine to give/show examples of classifications with few occurrences. If they are all at 1, its not your fault, you cannot say that 1 is less important than the other.\n\nIt would be possible to group classifications together, but the grouping can only be done based on the business context ie. you need to decide which classifications can be grouped together because you know they share some intrinsic similarities, not because of a statistical reason. You can then count the number of occurrences in each new group and order them.']"
[E] Should I take Mathematical Statistics before applying to grad school?,"Ive heard that its harder than real analysis at my school, and I dont want to tank my GPA. I also want to be prepared for graduate school. Are there similar courses that go into stats theory? Or should I just take it and hope for a B.",12bq5jl,ewoodnc,1680629396.0,8,0.84,"['Having gotten an A (amazingly) in a Math Stats course, yes. Its not easy, but my prof made the grade attainable  be prepared to work REALLY hard, use outside resources (papers sometimes very, very rarely), and repeat the mechanical aspects of the class a LOT.', 'You would be shocked at how much easier math stats will be easier to you than real analysis truthfully. Most people who take math stats dont have any proofs classes before it. Math stats doesnt require proofs, but seeing something as rigorous as real analysis helps to an extent.', 'Grad-level mathematical statistics is likely a lot harder than undergraduate. My advice is to take both real analysis and undergrad mathematical statistics -- when you end up in grad-level mathematical statistics, it will be a lot of work but it will be familiar and your chances of success are much higher.', 'Have you taken probability already?', 'You can always tank your gpa in grad school and no one will care. If youre going to the same school for grad school then do it, real recognizes real', ""YOU MUST LEARN STATISTICS IN ORDER TO SURVIVE IN THIS CENTURY.\n\n&#x200B;\n\nI can't emphasize more on the fact that you HAVE TO, MUST, OBLIGED to learn it. It doesn't matter if you're an engineer or medical practitioner or business person, you are going to need statistics at some point in your career. If you know it already, you just nee to review the concepts. If you don't know, dude you're FUCKED UP. Cause you have to learn everything from scratch. Not that it won't be possible, I already am in the middle of the way, but it took me a considerable amount of energy and time to achieve what I could have with a single course in Uni."", 'What are you wanting to go to grad school for?', ""It depends, really. Are you planning on doing further graduate work afterward? If so, your GPA will matter to some degree if you're going to switch schools. You'll need to be competitive.\n\nIf not, your GPA matters only so much as to meet the minimum requirements for your degree. Obviously, it's still good to perform well.\n\nI'm basically mathematically inept and had to take an archaeological data analysis course which is, of course, all stats. And then, I proceeded to do an entirely statistics-based thesis. I think perhaps that some background on stats, how to apply them, how to interpret them, would have been useful. If the course teaches that, then yes, it would be helpful, but not necessary.\n\nIt's a bit more difficult in archaeology because statistics in general favours pristine datasets. My discipline, by nature, uses mostly fragmented datasets with poor sample populations that require some creative mathematical solutions.\n\nGet a sense of what the course will help you accomplish and decide for yourself if it will be worth the time, money, and effort.\n\nEven better is reaching out to your potential supervisor and getting their advice."", ""So you would rather get a B in undergrad or fail it in graduate school? Because if you do not take it in undergrad, you'll have it for the 1st time in grad school."", 'Im confused by this comment. Im trying to get into stats and all of the textbooks are chock full of proofs. Especially at the grad textbooks Ive looked at. \n\nWhat books do you recommend?', 'A little late to the party, but can verify that grad-level math stats is a lot more difficult than undergrad-level math stats or real analysis. Currently being brutalized by grad-level math stats.', 'Yep!', 'If stats related, yes. If pure math, probably not. If applied math or a natural science, yes.', 'I took math stats in undergrad and grad and I can say both of mine definitely required proofs (although my undergrad program also required us to take a proofs class first).\n\nWe didn\'t have to be absolutely 100% rigorous in the ""proof"" aspect, but they were definitely a big part of the class.\n\nI think textbooks are bound to be full of proofs though - if you understand the proofs behind theorems then it\'ll be a lot easier to apply them in specific cases', 'Well at a grad level its casella and Berger. Undergrad probably at the level of degroot and schervish', 'Yep, for stats', 'Then 100% as much math stats as you can']"
"[Q] In hypothesis testing, how can we just reject/accept the Null Hypothesis by taking the mean only once for example from a sample and then calculating it to get the p-value?",Doesn't it make sense to calculate the mean from the sample multiple times to know if that mean value that was calculated initially actually occurs repeatedly?,12bo5s9,-S-I-D-,1680625344.0,0,0.5,"[""We don't generally perform experiments multiple times -- it's too expensive and too time consuming. So we're stuck with one sample, and the best we can do is ask if the sample mean we have observed is consistent with a specific population mean."", ""It might be helpful to play with some toy examples.\n\nFor example, if you sampled the weight of apples from an orchard, with a good sampling procedure, and the weights are (10, 11, 12, 13, 14, 15, 16, 17), how likely is it that the mean weight in the orchard population is 5 ?  It's pretty unlikely.  It's *possible* that you just happened to pick 8 really large apples, but you can have some confidence in the conclusion."", 'Thats the whole point. In a perfect world yeah Id just measure the entire population but thats not practical so we take a sample and draw conclusions. Thats why a power analysis is done prior so you know how large your groups need to be.', 'What you\'re asking about is already, to some extent, baked into the construction of the hypothesis test.\n\nFirst, assume that we\'ve collected as much data as is reasonably possible - because of course, if you can get more information then you should. Then using that data, you want to work out whether the evidence points to ""nothing special is happening"" or ""something special is happening"".\n\nAs a matter of course, the default assumption is ""nothing special is happening"", because we\'re usually trying to prove that something *is* happening, meaning the burden of proof lies on us to demonstrate it. We call the ""nothing is happening"" case the null hypothesis, and the ""something is happening"" case the alternative hypothesis.\n\nSo we build our test statistic. And we feed our data into the test statistic, and it tells us one thing:\n\n* Assuming the null hypothesis is true, are these results weird? How weird are they?\n\nAssuming we\'ve constructed the test statistic properly, the ""how weird are they"" part includes a measure of how likely we are to be wrong, which is a function of whether we\'ve collected enough data to have any confidence in our results.\n\nSo you *could* have a test statistic that involves splitting the data into parts, calculating individual means (or whatever is appropriate) from each part, and then looking at whether the spread of those values suggests that they give a consistent result. But you can take that to the logical extreme of considering each data point individually, and then finding a statistic that summarises the whole spread of the data into a single measure, and that\'s pretty much what most standard test statistics already do.\n\nFor example, if I\'m doing the classic ""test whether this coin is weighted"" test, my test statistic is likely to be something like ""the proportion of heads flipped"". And when we perform the hypothesis test, we calculate the probability that we would observe a proportion at least as large on a fair coin - and that probability is inherently dependent on the number of flips.\n\nIf I flip a coin 10 times and get 6 heads, there\'s a better than 1 in 3 chance I\'d get a result like that from a fair coin and that\'s pretty reasonable so we wouldn\'t reject the null hypothesis. But if I flip a coin 100 times and get 60 heads, there\'s only a 1 in 35 chance I\'d get a result like that from a fair coin, so I should probably be pretty suspicious that something\'s happening. And if I got 600 heads out of 1000 flips, then that\'s something like a 1 in 10\\^10 chance that it would happen with a fair coin, which seems like pretty damning evidence against it.\n\nSo yes, clearly more data > less data, but assuming we only have the budget (or patience) to flip the coin a limited number of times, we set up our hypothesis test in such a way that we have a clear measure of how ""weird"" our results are under the null hypothesis, and we only reject the null when it seems hard to justify doing otherwise. That\'s also why the language is usually ""reject the null / fail to reject the null"" - when the results are not weird enough we\'re not saying that the null is unconditionally true, just that there isn\'t enough evidence to say otherwise, in a sort of ""innocent until proven guilty"" situation.', 'You could break the samples down into subsamples if you wanted and have enough data, generally though the bigger the sample the more confident it represents the population so the tighter your confidence i terval gets.', ""> Doesn't it make sense to calculate the mean from the sample multiple times to know if that mean value that was calculated initially actually occurs repeatedly?\n\ncan you clarify the circumstances you're asking about? Why would a sample mean occur repeatedly?"", 'Ah ok, so knowing this would you be confident with the alternative hypothesis if we were to reject the null hypothesis?\n\nI would be a bit skeptical tbh.', 'Thanks for the detailed explanation. Really clarified this concept.', 'Well wouldnt it be better to get the mean from multiple samples rather than just one sample which could have all outliers for example cause we making a claim from just one sample.', ""The confidence derives from the test itself. Your p value indicates the probability that your sample could have come from the population, given its parameters.\n\nConventionally, 0.05 is used, but if you don't think that's good enough (and many don't; especially in really prolific domains where thousands of grad students are testing everything under the sun), then you're always free to set your alpha value at something stricter. 0.01, 0.005. It truly is up to you."", ""It's all about a middle ground between observing every instance (which gives you the true mean) and inferring from just an observation (which may not be that different from a random guess). \n\nSo yeah, it could be better to have a bigger or better sample, or multiple samples, but are the resources spent gathering that extra samples worth the marginal confidence you gain in your assessment?"", ""If the sample is all outliers, you can hardly call them outliers (they seem to be more typical than the non-outliers), unless the sample is extremely small, in which case that's the risk with extremely small samples; you would avoid that problem by *not* taking small samples. Much more likely is that you could get one or two outliers in a small sample; that's a realistic situation to worry about.\n\nIf you know how to deal with the potential for (say) one weird contaminating observation in a small sample you can also do it with several of them in a large sample (e.g. by using robust approaches both times).\n \nConsider instead ... if you're truly taking random samples of the population of interest what's the information  difference between one large sample and several small ones?\n\nIt's not quite  a trivial question if you dont just combine the samples into one. For a mean you can combine information from  smaller samples  efficiently,  but for say a quantile you cant - you can lose information if you dont combine samples but instead average estimates, say.\n\n(If your samples *aren't* random samples then you have other issues and multiple samples won't fix that)"", 'Hmm makes sense.', 'And we use ""power analysis"" to identify the ideal sample size right?']"
[S] SPSS/R and Manual Calculation yields different Chi-square Results,"Checked many times that there is no data entry. Compared the Expected Counts in my manual calculation (abbv to MC) and SPSS. They yield different chi-square result. SPSS is 12-ish and MC is 13-ish. 

Downloaded R and loaded the same data. SPSS and R yield the same chi-square value. 

I am using the formula provided by my Statistics book: Summation of (o-e)^2 divided by e. 

What am I doing wrong with my MC? I want to prove in my thesis that even when MC-ing the data, it will yield the same results. 

Advice?

EDIT: SUCCESS! Rounding-off was the issue. Thank you very much to u/SalvatoreEggplant and u/bubbleberry1!",12bmlxg,Rain_Deer7004,1680622278.0,12,0.88,"[""Can you give an example of the contingency table and what you're manual calculation is ?"", 'Are you using the chisq.test function? Because if you are then there\'s a correction that R does automatically. If you want your answers in R to match your answers by hand you need to add the argument ""correct = FALSE"" inside your chisq.test function.', 'IV is Sex (Male and Female). DV has 4 categories. \n\nMale:\nColumn 1: 25 (26.2)\nC2: 24 (29.6)\nC3: 12 (6.7)\nC4: 2 (0.5)\n\nFemale:\nC1: 73 (71.8)\nC2: 87 (81.4)\nC3: 13 (18.3)\nC4: 0 (1.5)\n\nExpected Counts are enclosed within the parentheses.\n\nSPSS/R Chi-square: 12.821\n,MC: 13.247', 'The difference is due to rounding. \n\nFor instance in the first cell (male,column1) the expected frequency is exactly 26.1610169491525\n\nIt may look like a small difference but when squared and then summed over all the cells, it adds up. *(chi-square joke)*', ""The expected values look correct.  \n\nIn R,  I'm getting \n\n*X-squared = 12.821, df = 3, p-value = 0.00504*"", 'Yes. I also manually acquired those Expected Counts and they check out with the E. Counts in SPSS. But upon acquiring Chi-square, it yields a different result.', '*(O - E)\\^2 / E* \n\nlooks like\n\n*\\[,1\\]      \\[,2\\]     \\[,3\\]     \\[,4\\]*  \n*Male   0.05152553 1.0702234 4.250872 4.025962*  \n*Female 0.01876363 0.3897345 1.548005 1.466102*\n\nAnd the sum of these is 12.821']"
[Q] What design is a 1 way between subjects ANOVA?,I have a one way between subjects ANOVA 1 IV (3lvls) and 1 DV. Can i call it an experimental 1x3 Between Subjects factorial design?,12bk4gq,bbycharlie,1680616977.0,3,1.0,"[""No. It's just called a one way between subjects ANOVA. Not that it would make sense, but 1x3 would be two way with a 3 level and 1 level factor."", 'Thank you very much! :)']"
[E] Why the Cross-Lagged Panel Model Is Almost Never the Right Choice,[https://journals.sagepub.com/doi/full/10.1177/25152459231158378](https://journals.sagepub.com/doi/full/10.1177/25152459231158378),12bjjpg,Stauce52,1680615681.0,16,0.92,[]
[Q] Analysis of factorial design experiment - how should a discrete numeric predictor be treated?,"Hey everyone,

I'm  looking for some literature on (or a somewhat simple explanation of)  the proper way to treat predictors which are continuous (e.g.  temperature) but manipulated at discrete values (e.g. 10C, 20C, ...,  40C) as part of a experiment with a factorial design, where this  predictor is one of the factors. My hunch is that this predictor should  be treated as discrete when fitting a linear regression or GLM, because  (1) it's not known a-priori whether the effect is linear, and (2) the  model with a linear effect would have ""too many"" degrees of freedom, and  would therefore be likely to lead to an inflated probability of type 1  errors.

My PI wants to fit these  as linear, because he's interested in testing a specific hypothesis  about the effect of e.g. temperature, which is that it has e.g. a  negative, linear effect, but I figure I can just test the linear effect  by using polynomial contrasts and post-hoc tests, e.g. with the  \`emmeans\` package in R. Intuitively, this seems like a better approach,  but I haven't been able to find any papers or posts explaining why  specifying a model of experimental data in the way my PI suggests is a  problem, or even whether it is a problem.

Thanks!",12b1go6,sad_house_guest,1680563601.0,2,1.0,"['It makes sense to do polynomial contrasts for linear and perhaps quadratic as well. There will be essentially no difference between a linear regression and a linear component of trend. Keep in mind that a significant linear component of trend just means the function has a linear component and if the function is monotonic the test of the linear component of trend will typically have the most power. Incidentally, I think the question about the predictor is categorical versus numeric rather than discrete versus continuous even though most software requires you to specify it in terms of the latter.', ""You seem to be conflating 'discrete' with 'categorical'"", ""Hmm, so there isn't a problem with fitting an equally spaced ordinal experimental factor as continuous in terms of statistical power - it's just a choice of which hypothesis you want to test? Seems like the linear model would be more likely to reject the null than the linear component of a polynomial contrast, no?\n\nAnd I'm using R / glmmTMB to fit a Binomial GLMM, so unfortunately the syntax for specifying the model is the same regardless of how the predictor is coded, and the terminology is none of the above, but numeric, factor, or ordered factor :/"", ""No yeah, I know the distinction between categorical / ordinal  vs. continuous predictors, and discrete vs. continuous random variables, even if I'm being loose with the terminology in the post... the question is whether treating an ordinal experimental factor as continuous in a (G)LM(M) is a bad practice, and if so why."", 'By definition, an ordinal variable cannot be assumed to be equally spaced. In a regression analysis (OLS) without higher-order terms, there is no practical difference between the null hypothesis that b=0 and that there is no linear component of trend. I would guess the same for the power would be the same to about 2 decimal places. I suspect the same for your analysis but Im not certain. I would choose numeric but some purists may prefer ordered factor.', ""There are higher order terms - second and third order interactions - since it's a factorial design, so I think the coefficient tests spit out by the model aren't very useful anyway. I'm doing post-hoc pairwise comparisons for the main effects, assuming there aren't significant higher order interactions""]"
[Question] How to set in-process control limits for the mean?,"We have a manufacturing process in which the finished products have the following requirements: individual unit must have a weight within  5% of average weight (test with 10 random samples).

The in-process controls tests so far is:  
Test 1: Collect 10 products. Determine the weight of each product and the average weight. Individual unit weight must be within  5% of average weight (so same as finished specs).  
Test 2: Collect 10 products, determine the average weight (not the individual weight). The average weight must be within ??% of theoretical weight.

Test 1 and Test 2 are done separately.

My question is: In test 2, which % should we set the control limits? From my limited understanding of statistics, I'm thinking since we're dealing with averages, should it be the standard error? (i.e 5%/sqrt(10) ~ 1.58%). If anyone can point me to any related documentation I'm be very thankful.",12atpam,gnad,1680547606.0,1,1.0,[]
"[Discussion] How seriously do you take errors, in general? (sorry if this isn't the right tag)","I took a statistics class in DOX methods this semester to ""feel out"" the field of biostatistics. I was considering a medical degree as an undergraduate but hated the culture for various reasons, like elitism, caring more about their image than the patient, prioritizing memorization over critical thinking, among others.

I also hate when doctors make mistakes because they're too dismissive or egotistical to care. When my dad was a resident, he warned his supervisor that a patient seemed to have a lot of symptoms of this really rare disease, and that a biopsy, while it was the recommended route for more common diseases with similar symptoms, would be deadly in this case. The supervisor chewed out my dad, and the older residents laughed at a first-year resident correcting a seasoned physician. The next day, the whole hospital floor was quiet, because they did a biopsy on the patient, and her room was splattered with blood, empty blood transfusion packets everywhere. She was dead because of that doctor's arrogance. No apology or admission of poor judgment followed.

So I wanted to do biostatistics because with metascience is emerging as a field, we've observed that over half of scientific studies have statistical errors, and a majority cannot be replicated, with some fields like psychology approaching an 80% failure rate. Doctors go off of faulty data, and then their mistakes and biases are amplified, giving patients bad care but will treat them like idiots if they question a professional.

Thing is, my professor has made a few mistakes on the whiteboard this semester, and I also found three misprints in the book, confirmed by my professor. She also gave us a study guide for the first exam, and said that I should focus on learning Minitab than worry about what's going on in the equations. Then she changed gears completely, saying I should do more handwritten work, and then the day before the exam she changed the tested chapters from 1-3 which I'd been studying intensely, to chapter 4. I found two misprints in this section, but only after I bombed the exam. Everyone else also bombed, unless they asked the professor so much that she caved and gave them the answers.

I'm just really confused about the work ethic and culture here. I scored in the 97th percentile for college entrance exams and am attending a school that's in the mid-50s, but I'm still the village idiot in the class because I literally cannot change my professor's mind about my intelligence, and I'm not a math major. My motivation is completely shot because it doesn't matter how hard I work, she'll just change the goalposts. She accused me of cheating once because I just did too well to match her expectations of me. To compare, I'm a model student in Calc II and have a 101 average and my professor really wanted me to do an MS in math instead of what I wanted to do.

I'm so frustrated. Is this at all typical? I don't want to go into a field that's this chaotic.",12asj2g,SoggyHuckleberry5,1680545260.0,3,0.71,"[""The professor sounds like a worry.\n\nNot sure what to tell you, but I will say that errors in lectures  are very common (though often not too serious  if the person teaching is ready to correct them), even among people that know their stuff. \n\nMany  books are quite poor but even good ones will have a few typos. When learning stuff I like to supplement a book with several others so I'm not relying too much on any of them\n\nI can tolerate obvious typos (for all that I'd rather they were not still present 3 editions later!) It's the books that just flat out state superficially plausible nonsense that are the bigger worry.\n\nWhat text do you have?"", ""Misprints in books and lectures are a nuisance. I had a professor who used my class as guinea pigs to proofread a book he was writing (and he did not write well.) But I don't think it is routine.\n\nIn every field there are conscientious researchers and sloppy researchers. But the great thing about statistics is we are the field that *studies proper management of error rates* --- if you want, you can attach a cost to a false positive and a false negative, and design the analysis method that minimizes the expected cost of errors.  That's the sort of thing I *wish* my doctor might do when deciding whether or not to run expensive tests on me, rather than just ordering every test in the book."", ""Hi, thanks for the response! I'm getting back to you late, but it's Design and Analysis of Experiments (10th edition) by Douglas Montgomery."", ""\n> I had a professor who used my class as guinea pigs to proofread a book he was writing \n\nI've had a similar experience though the class was postgraduate, so less of an issue"", ""Montgomery? It should mostly be good, but damn you'd hope that almost all errors were gone by the tenth edition, since an author should only have to be very worried about new sections by that point."", 'I encountered that in a MOOC and figured oh well this is free. Would have been less chill if Id paid big money']"
Why don’t we always bootstrap? [Q],"
Im taking a computational statistics class and we are learning a wide variety of statistical computing tools for inference, involving Monte Carlo methods, bootstrap methods, jackknife, and general Monte Carlo inference. 

If its one thing Ive learned is how powerful the bootstrap is. In the book I saw an example of bootstrapping regression coefficients. In general, Ive noticed that bootstrapping can provide a very powerful tool for understanding more about parameters we wish to estimate. Furthermore, after doing some researching I saw the connections between the bootstrapped distribution of your statistic and how it can resembles a poor mans posterior distribution as Jerome Friedman put it. 

After looking at the regression example I thought, why dont we always bootstrap? You can call lm() once and you get a estimate for your coefficient. Why wouldnt you want to bootstrap them and get a whole distribution? 

I guess my question is why dont more things in stats just get bootstrapped in practice? For computational reasons sure maybe we dont need to run 10k simulations to find least squares estimates. But isnt it helped up to see a distribution of our slope coefficients rather than just one realization? 

Another question I have is what are some limitations to the bootstrap? Ive been kinda of in awe of it and I feel it is the most overpowered tool and thus Ive now just been bootstrapping everything. How much can I trust the distribution I get after bootstrapping?",12arj39,Direct-Touch469,1680543292.0,117,0.98,"['Bootstrap distributions for statistics dont always converge (quickly) to their true distributions. For example, consider bootstrapping for the sample maximum of a uniform distribution. You can show with some simple calculations that as your bootstrap samples approach infinity, the bootstrapped sample max is not a good estimator for the sample max', 'The big one computation efficiency.\n\nIf you have an analysis that takes 24hr to run, 10k bootstrapped samples is not feasible.', ""I've wondered the same thing - bootstrapping is kind of a cheat code. Over time I've concluded:\n\n1. Historically, statistics as an academic discipline evolved in an environment with low compute power, so a lot of theory was built to construct probability distributions from first principles. Now all this theory is sort of taught out of habit, even though lots of practitioners will just go straight for more compute intensive approaches like bootstrapping.\n\n2. The core idea of bootstrapping shows up in disguise more often than you think: for instance, you can think of the random forest model in machine learning as a sort of bootstrapped decision tree. It's a similar story for lots of other ensemble models.\n\n3. There are a lot of cases where it's not appropriate: if your data is skewed or biased in some way, bootstrapping can give you a false sense of security."", ""1. The bootstrap isn't always consistent (see propensity score bootstrap lit)\n2. Computationally intensive. Sometimes fitting a model once can take weeks...now what about thousands of times? Note that the parametric bootstrap has problems too."", ""The bootstrap has only large sample validity.  It isn't magic.\n\nIt also has assumptions and conditions, which most users don't know about and never check.\n\nAlso the bootstrap for regression cannot be totally nonparametric.  To get a handle on conditional distributions, you need to bootstrap residuals.  And those residuals involve parametric estimates (they are wrong if the model is wrong).  This is all explained in books on the bootstrap.  So again, it is not as simple and magical as you are thinking."", 'For hypothesis testing, bootstrapping is generally a bit less powerful and not more conservative, so its great when you dont have a parametric alternative, but if you do its never the best choice.', ""Yes it's a good tecnique I can agree. It doesn't add so much in information content in the majority of cases,  it is useful if you want to check how robust you estimations are given the data. Roughly."", ""Good question.  For that matter, why don't we always do Bayesian parameter estimation?  \n\nI think a combination of reasons are involved in terms of why it's not more commonly used.  For some applications, it can be time consuming, require more expertise, etc.  Inferences via bootstrapping can also be potentially misleading, particularly when you're working with a small sample size.  Conversely, when your sample is already sufficiently large, bootstrapping might not be necessary.  Finally, bootstrapping can tempt some into thinking they're getting more information then they actually are.  It doesn't help you to better understand population parameters; rather it is only helpful in better understanding your sample data."", 'Just FYI: if you want a ""whole distribution"" instead of a point estimate, i.e. p(|x), your only option is Bayesian inference. Bootstrap gives you p(b(x)|), which is the distribution ~~of the data x~~ of the estimator as a function of the sample x, given a fixed parameter .', 'Often other replication methods such as the jackknife or balanced repeated replication (BRR) are preferable because they get you a useful variance estimate with much less computation. Large government surveys rarely use the bootstrap for this reason.\n\nFor one-off analyses, you might ask Who cares? Computers are so powerful nowadays, why not just bootstrap? But government agencies that publish large survey datasets (on the order of 100,000 to 5 million records) dont want to publish a matrix of bootstrap weights with dimension 5 million x 5,000. Jackknife and BRR can give good results with a matrix of weights with many fewer columns.', 'I agree with a lot of whats been said here. But at least within my area (pharma/medical device consulting) there isnt enough statistical literacy among middle management. Or, depending on how you look at it, there is some minimal literacy, but they are so risk averse they dont use it even when the FDA has been prodding the industry in that direction for a decade.', 'Sounds like a fun class. Which one are you attending?', 'Because oftentimes we have more powerful ways to talk about sampling/posterior distribution. For example, Monte Carlo simulation can be much more powerful for small sample size (before asymptotic properties kick in) hypothesis testing than bootstrapping would be.', 'There are statistics that cannot be bootstrapped.\n\nIIRC, the maximum can cause issues.', "">For computational reasons sure maybe we dont need to run 10k simulations to find least squares estimates\n\nyou wont do bootstrap to estimate the parameters of a linear model. You would do bootstrap to obtain more accurate confidence intervals or hypothesis testing, in case you suspect that distributional assumptions doesn't hold. If distributional assumptions are violated by things like misspecification of the model, bootstrap wont solve that. If model is well specified (and we seldom can be sure of that) and indeed residuals distribution is different from normal, it seems that the CI's and hypothesis testing are some what robust to that.\n\nIn other cases like heteroscedasticity or other distribution besides normal, we just have tools to address that like GLM's , GLS and GLMM.\n\nAll in all, I believe the gains to do bootstrapping are just not so big and when you have to do a data analysis you just go for the classical approach."", 'You can and absolutely should bootstrap your estimates. What is relevant, however, is good theory. Otherwise you are doing statistics for the sake of it and are simply polishing a turd.', 'Note the parametric bootstrap sucks', 'Because purist fucks cant help but inject themselves into every aspect of the pragmatic persons life. Fuck em. Bootstrap everything and ignore the haters.', ""We should probably always bootstrap for the general linear model. \n\nWhy doesn't everyone bootstrap every analysis? It's probably a combination of laziness, ignorance, and not perceiving a need. \n\nWhy doesn't everyone always report confidence intervals around every statistic point estimate? See above."", ""I have no theory to back me up, but I think bootstrapping doesn't bring much benefits to large size dataset and costs too much processing time."", 'Which book are you guys covering in your class?', 'Sometimes the theory behind the parametric sampling distribution is fairly sound (like regression coefficient estimates following a t-distribution). So, using a bootstrap wouldn\'t be wrong, but it\'s not really necessary.\n\nAlso, if you\'re comfortable calling the bootstrap sample a ""poor man\'s posterior distribution"" in OLS, you must also be okay with calling the estimated t-distribution the same thing (it\'s fully defined by the mean, standard error, and degrees of freedom, all from standard output).\n\nThat said, there are lots of applications where I\'m _not_ at all comfortable with the theory behind the distributional assumptions of a sampling distribution (or maybe none exist yet). In those cases, I often look to things like the bootstrap. With the caveats others raise that the bootstrap doesn\'t _always_ work, I often like to prove (even just to myself) the bootstrap approach works ""properly"" for novel estimators using simulations.', "">You can call lm() once and you get a estimate for your coefficient. Why wouldnt you want to bootstrap them and get a whole distribution?\n\nWell, sometimes you don't need the distribution. You do for constructing confidence intervals and testing hypotheses. But for simple parameter estimation you don't\n\nMoreover, the distribution (say, of an estimator) obtained via bootstrapping may not really be accurate. If you model your data with a parametric model, it is often the case that, theoretically, your estimator follows a known, explicit distribution (e.g. t-distribution), either exactly or asymptotically. So you use that, because it's arguably much more accurate, provided you have grounds to believe your model is adequate"", 'You didn\'t want ""quickly"", the bootstrap distribution does not converge to the sampling distribution of the estimator *at all*.\n\nUpvoted anyway.', ""Maybe I'm missing the point, but this seems more like an issue of using the wrong estimator than a problem with bootstrapping itself."", 'What about things like cross correlation coefficients in time series?', 'Regarding #1, I dont necessarily think that its only taught out of habit, but also because it can be important context to understand (relatively) new approaches and how and why they were developed. Including all the historical context probably makes the student a better practitioner, since it helps cement a lot of the reasons for why things are done the way they are today. \n\nYou see it in ML too, with models and approaches that have completely fallen out of favor. Youre taught decision trees and their flaws so you can understand why random forests and boosted trees are an improvement (AdaBoost might even be a better example, since its not a building block like individual trees). What sigmoid and tanh (and now ReLU) were trying to achieve, and how the new activations get around the shortcomings of their predecessors. How LSTMs solved some of the main issues with vanilla RNNs, even though they have been completely replaced with transformers.', '10 years ago I asked my Computational Stats prof about this issue and his response was almost exactly your #1.', "">There are a lot of cases where it's not appropriate: if your data is skewed or biased in some way, bootstrapping can give you a false sense of security.\n\nCan you elaborate on that? are you taking about the form of a distribution or that if data doesn't come from a sample survey (i.e not an i.i.d)?"", 'Interesting, is there anywhere else I can read more about those assumptions? What books?', 'So should it only be used to estimate standard errors for estimators that have no closed form?', ""I am not sure about what you are saying... Bootstrap doesn't give approximation to the likelihood, but rather to the distribution of some estimator. Two different things. Also bayesian statistics are used to derive a posterior distribution, which is a parametric approach, and this is the first cause of using bootstrap, because you don't know the exact distributions behind your data. \n\nAgain, correct me if I am wrong but bootstrap does not give you p(x/beta) directly, rather p(T(x)/beta) where T(x) is some estimator"", 'What your describing is the likelihood function. Thats not what the bootstrap gives an approximation to. Its for the sampling distribution of an estimator.', 'Its funny that often when people say the FDA isnt gonna like it theyre making excuses for their own resistance to change (see also the FDA being happy with you not using SAS for years)', 'What about cross correlation coefficients in time series? Ive seen things like block bootstrap that attempts to preserve the dependency in the data', '> you wont do bootstrap to estimate the parameters of a linear model\n\nI beg to differ! It is quite common to use bootstrapping if you want to report an estimate for how the parameters of your model are distributed - this is usually the best way to do it unless you know a lot about the distribution your data is drawn from. (Of course you might not bother if you only care about the predictions of the model, but sometimes you really do care about the parameters.)', 'Elaborate please', 'Give me a reason why would you bootstrap a GLM given that when you have around 300 datapoints, a small model reasonably is approximated by the large sample distributions?', 'Statistical computing with R by rizzo', 'So when should one actually use the bootstrap? And when can it be a mistake or sometimes lead to misleading results? For example, what if I want to know the cross correlation at a given lag between two time series, I would like to see a distribution of these correlation coefficients rather than a single point estimate if possible. What can we bootstrap, what can we not?', 'Added quickly because in general, bootstrap estimators may converge, but at a slow rate. Youre right that in the case of the sample max, it doesnt converge', 'Does the bootstrap distribution converge to anything in that case?  Does it have a known bias, etc.?', 'You cannot blindly bootstrap any statistic is the point.', ""Well, both of those phenomena could be a problem:\n\n- If the distribution itself is highly skewed, then bootstrapping will probably give bad answers, or at least take a long time to converge. If you're trying to estimate something about the wealth distribution in the US and your sample consists of mostly average income people together with one billionaire, bootstrapping won't help much.\n\n- If your dataset is biased, due to bad empirical methodology or whatever, then you can't bootstrap your way out of it - your only hope is to model the bias. If again you're trying to say something about the wealth distribution in the US, you're just going to have a hard time if you only survey homeowners in San Fransisco, for instance."", ""For a gentle introduction I really like Hesterberg's [What Teachers Should Know About the Bootstrap](https://arxiv.org/abs/1411.5279)"", 'The standard undergrad/masters level textbooks are [Efron and Tibshirani](https://www.amazon.com/Introduction-Bootstrap-Monographs-Statistics-Probability/dp/0412042312/) and [Davison and Hinkley](https://www.amazon.com/Bootstrap-Application-Statistical-Probabilistic-Mathematics/dp/0521574714/).', 'You can always start with Wikipedia https://en.wikipedia.org/wiki/Bootstrapping_(statistics)', 'Yeah. Thats right. But also Jerome Friedman, the guy who was behind the bootstrap, states that it can be a approximate nonparametric non informative posterior distribution for the parameter. So the guy above you is wrong.', "">which is a parametric approach,\n\nWhile it's very common to use parametric Bayesian methods, it's definitely not required that the model be parametric."", 'The estimator is ultimately a function of the sample, and what bootstrap does is resampling from the sample. My point was just that bootstrap does not give you anything interpretable as a posterior of .', 'Im sure you will find it not at all surprising Ive heard those exact words more times than I can remember. The one time Ill never forget was by an upper mid level manager regarding the using of Bayesian statistics, and like the literal next slide I had was an image of the title page of the FDAs guidance on it for device trials.', ""I don't know if cross correlation coefficients can be bootstrapped, but at first glance I don't see why not.\n\nSome details on why the maximum can cause problems https://stats.stackexchange.com/questions/9664/what-are-examples-where-a-naive-bootstrap-fails/9722#9722"", ""I am talking about point estimates and those don't require distributional assumptions because OLS properties. I am thinking in the GaussMarkov theorem. \n\n>if you want to report an estimate for how the parameters of your model are distributed\n\nConfidence intervals and hypothesis testing is based on this."", ""It's parametric and almost always wrong."", 'Smaller error values.', 'The (parametric) large sample standard errors can sometimes be wildly incorrect if the glm distribution is misspecified, e.g. Poisson Regression but where data are not actually Poisson.', ""The sampling distribution from a parametric assumption is no less a distribution than the sampling distribution from bootstrap samples. Yes, the MLE is a single point estimate, but that's why you usually see things like standard errors as well, together those represent a whole sampling distribution, not just a point estimate.\n\nOne thing I think you're confusing is thinking bootstrap samples are giving you a Bayesian posterior distribution for the parameter, they're not, they're giving you a Frequentist distribution of the estimator (not the same thing). One big difference, is that as the sample size increases you'd expect the sampling distribution to get tighter and tighter around the point estimate.\n\nAs for, cross correlation at a given lag between two time series, I'm not sure, that's not in my area of expertise (my focus is in survival analysis). But, \n\n* Can you assume the **estimator** for the cross correlation follows a known distribution (e.g., Gaussian)? \n* Can you estimate it's standard error? \n* Does it take a long time computationally to get an estimate? \n\nThose are the types of questions I'd ask myself before assuming a parametric distribution for the estimator, rather than using bootstrapping."", 'It converges to a random *discrete* distribution (the location of the atoms of the distribution is random) and this is completely wrong since the true asymptotic distribution is continuous.\n\nIn order to get the right answer you have to know that the true rate of convergence for this estimator is n^-1 rather than n^-1/2 and then use the [subsampling bootstrap](https://www.amazon.com/Subsampling-Springer-Statistics-Dimitris-Politis/dp/0387988548/).  Deriving the correct asymptotic distribution of this estimator is [problem 1 on this homework](https://www.stat.umn.edu/geyer/5102/hw/h2.pdf) (there are a lot of hints).  So this is a problem where the ""usual asymptotics"" of maximum likelihood break down (because one of its assumptions, that the support of the distribution does not depend on the parameter, is false).  For an explanation of how the subsampling bootstrap fixes the problem and how the ordinary bootstrap fails miserably, see [Section 4.1 of these notes](https://www.stat.umn.edu/geyer/5601/notes/sub.pdf#page=3) and [the accompanying computer examples](https://www.stat.umn.edu/geyer/5601/examp/subboot.html#extreme).', 'How do you know which statistics are things you can bootstrap?', 'ok, for the first I would say is a sample size issue more than a failure inherent to bootstrapping. Agree on the second.', 'Tim Hesterberg is 100% awesome. Great link thanks for sharing.', 'How is Jerome Freidman the guy behind the bootstrap?', 'You misspelled Bradley Efron.', 'But OP didnt say anything about the posterior\n\nBootstrap approximates the sampling distribution, not a posterior.', 'Love that. Yeah I had the FDA guidance on statistical software on my cube wall.', '> Confidence intervals and hypothesis testing is based on this.\n\nBut how do you actually in practice test the hypothesis that, say, a certain coefficient in a GLM is nonzero? You might be able to manufacture some sort of test statistic if you know a lot about your data, but in general it is not at all obvious how the coefficients should distributed, even if the residuals obey all the usual assumptions.', 'Reducing the error value by a magnitude of 10e-3 order is not a reason to fit the same model 2.000 times.', ""Sure. But I don't see how would bootstrapping fix that."", ""Understanding tbe bootstrap's assumptions and the sampling distribution of whatever you estimate. \n\nhttps://stats.stackexchange.com/questions/491668/should-you-ever-use-non-bootstrapped-propensity-scores"", ""Often we don't have control over the sample size! The main competitor to bootstrapping is to postulate a class of distributions to which you believe the true distribution belongs, and this approach often beats bootstrapping for skewed data. For instance, if you use a sample to parametrize a Zipfian distribution in the wealth modeling case, you will be much less surprised by outliers than if you use bootstrapping, even for a fairly modest sample size."", 'Oh yeah my mistake Friedman was responsible for decision trees not bootstrap', 'My intent was just to caution OP against the false interpretation of bootstrap as anything resembling the ""whole distribution of the coefficient"", which is what OP seems to be looking for.', 'Nonparametric bootstraps estimate the sampling distribution, and will provide consistent standard errors even when the mode is misspecified.', 'well, parametric models are generally more powerful than non-parametric ones, but I understand your point.']"
[Question] Should I use two-way or repeated measures ANOVA,"I am trying to determine whether I should analyze my experiment using two-way or repeated measures ANOVA but am not clear on the definition of a repeated measures ANOVA. I ran my agricultural experiment as follows:

A population of 20 individuals of the same plant was subjected to two factors of treatments: temperature treatment (non-stress control vs. elevated temperature) and chemical treatment (untreated control vs. chemical).   
To break this down, here were the treatments:  
\- Non-stress control temperature + untreated control without chemical (5 plants)  
\- Non-stress control temperature + chemical treatment (5 plants)  
\- Elevated temperature + untreated control without chemical (5 plants)  
\- Elevated temperature  + chemical treatment (5 plants) 

Measurements were taken every 7 days for a duration of 35 days, which means that there were 6 data collection dates.

I am specifically looking to determine the significant effects of the chemical on plants under each temperature condition; therefore, I am comparing non-stress control temperature + chemical treatment to the non-stress control temperature + untreated control (no chemical). For the other group, I am comparing the elevated temperature + chemical treatment to the elevated temperature + untreated control (no chemical). \*\*\*I am not comparing all four groups to one another\*\*\*

I originally evaluated my data through two-way ANOVA using the general linear model procedure in SAS with the intention of defining the significant differences between chemical treatments for each temperature condition. Someone who reviewed my work wants to know why I did not use repeated measures ANOVA. Would repeated measures ANOVA be the proper way to analyze this experiment, or did I already analyze my data correctly?",12apkrg,Dr_SmartyPlants,1680539312.0,1,1.0,"['The repeated-measures ANOVA will let you test Day and the Treatment x Day interaction. The tests of the effects of your treatments will be the same.', 'Sounds like you need repeat measures ANOVA', '2 x 2 x 6 with temperature, chemical, time as factors', 'Seems fine. At 20 reps you have 480 df.']"
[Question] Parametric modulation in fMRI GLM - log transformation?,"I'm getting more confident with statistics but still very unsure of myself, so would really welcome any input anyone has here. Thanks in advance for your answers!

I have a task-based fMRI dataset which includes ratings for each trial, where participants ranked how they felt about the stimulus they saw. These are 1-5, and I'm interested in whether the brain activations 'map on' to the rankings across the two conditions (https://andysbrainbook.readthedocs.io/en/latest/PM/PM_Overview.html). One condition is likely to receive a pretty low ranking and the other higher, but more variable. This tracks with how the histograms are looking for each individual, with a very aggressive right skew for one condition and a slightly less aggressive but much more individually variable left skew for the other condition. 

If we want to include these ratings while including as much variance as possible (ie not doing a median split and doing cross-condition comparisons that way) is the best thing to do to log-transform these? And then add them as a parametric regressor to the GLM? I've never done log-transformation before so not sure if this would be correct.",12amr4p,fluorescent_flamingo,1680533356.0,1,0.67,"['The participant responses are the parametric regressor for the HRF in your fMRI GLM. Regressors dont need to be normally distributed in a GLM. Thats a common misconception. Its about whether the residuals are normally distributed. I would not log transform', 'What glm? Why do you want to transform?', ""Thank you, that's really helpful to hear. So would plotting the residuals to check if those are normal be the next step, rather than the plots I have for the responses themselves? And that would be at the single-subject level as well?"", ""My supervisor wants to transform so that the scores in response to the two conditions are more normally distributed, I think because they're going to be included as parametric modulators/regressors and he thinks they need to be normal for that to be the case. The GLM would be the standard for fMRI, so including the sections of the tasks, the other regressors (heart rate variability in our case), and then the nuisance motion regressors. Does that make sense?"", 'Tbh, as someone who does fMRI analyses and has attended a multitude of courses and workshops on fMRI analysis I cant say Ive ever heard of people plotting residuals or checking assumptions. Im not even sure how you could do that. My best guess is youre probably fine People use Likert ratings as parametric regressors in fMRI all the time (although frankly, some being conventional does not mean it is the best approach as may be apparent with many statistical practices)\n\nI think it would be tricky to plot residuals because how fMRI GLM works is youre running a GLM on every single voxel for first level analysis. So does that mean you check thousands and thousand of GLM residuals? Probably impossible. And I dont even know how youd get those residuals tbh', '> so that the scores in response to the two conditions are more normally distributed,\n\nWhy?\n\n> he thinks they need to be normal \n\nWhy would they need to be normal?\n\n>  Does that make sense?\n\nSorry, not so far. I still don\'t know what we\'re even discussing for sure. \n\nFirst thing: when you write ""GLM"" you mean *generalized linear model*, right? or do you mean something else, like say *general linear model*?', ""That's reassuring! I think my supervisor is concerned about the parametric approach but isn't giving me a clear answer on why - his sense seems to that an alternative would be using the modal rating as a subject-level regressor (just our symptom correlates). He prefers a median splitting first and entering as different conditions (ie Own[higherNow] vs Own[lowerNow]) which doesn't make sense to me. Even though the means/SDs are different by condition (own/other) and there's less variance in one group (other) I can't see why that would be a big problem, because that's just what people answered. Log-transforming it would effectively eliminate the condition differences and would introduce a false level of variance within condition, but perhaps the worry is that the low level of variance would mean that including responses as a parametric regressor would reveal/indicate nothing."", ""As I understand it, GLM in fMRI work refers to general linear model https://andysbrainbook.readthedocs.io/en/latest/fMRI_Short_Course/Statistics/04_Stats_General_Linear_Model.html. \n\nI think my supervisor wants the scores to be normally distributed in order to meet parametric assumptions, but I don't have a straight answer from him on this. I would guess that the intention of log-transforming the scores would be to increase the range of said scores, because in one condition they are likely to converge around 3/4/5 and in the other condition they are predominantly 1/2. This assumption seems wrong though, given he is suggesting doing a median split and having a 'high response' and 'low response' grouping for the two conditions. \n\nGenerally speaking, the inclusion of this rating as a parametric modulator is to try and see if people's subjective ratings align with the intensity of activation relating to the task, so I don't understand why he would suggest reducing the dimensionality (if that is the correct phrasing?) by binarising the continuous variable. It seems to me that he thinks the scores need to be normally distributed to be included, unless I've missed something. \n\nI'm speaking to him again tomorrow so will try and get a clear answer, but it's been extremely helpful for me to outline it here and try to reframe it in my mind without his suggestions."", 'The means SDs being different by condition is not a problem. In fact, isnt that what you want?\n\nMedian splitting is almost always a bad idea and worse than leaving your Likert or ratio or interval data as is\n\nAggregating to the subject-level doesnt strike me as a super good idea either. See Elliot et Al 2022 and a recent Nature paper on brain wide associations to see how unreliable fMRI can be aggregated to the person level. FMRI strength is within person, and by aggregating to the person level youre basically throwing away its strengths\n\nIm not sure what you mean by entering as conditions completely though.\n\nAnyways, dont want to question your advisor too much or cause problems but then again, in my experience in my PhD, Ive been continually stunned by how little some advisors know or how clueless some advisors are on technical things. Maybe your advisor tends to know what they are talking about, you have more context. But I tend to recommend to students that its ok not to be completely deferential and compliant as advisors and PIs are fallible and in fact can often be wrong', ""I think you're right that my supervisor is not someone whose statistical opinion I necessarily rate, and there is some learning for me to do around how to hold my ground even though I am more of a newcomer to statistics than he is. There are other students in the lab who feel this way and are much more confident in their opinions, so maybe it's a case of finding my statistical confidence elsewhere!\n\nThe idea he has is to chunk the responses into high and low with a median split across the two conditions, so for each participant we'd have four values. I agree that keeping as much variance as possible is better and I don't know why a median split would be preferable, unless in a situation with moderation/mediation where we need a binarised variable. 1-5 is such a short scale that I don't see the value in reducing that even further.  \n\nThanks for the paper, that's really helpful to see. If there's any others off the top of your head which relate to this I'd be super grateful. Really appreciate your help!"", 'This is not fMRI but just in general keep in mind median splits is generally not a good idea\n\nLot of lit on this topic but heres one\n\nhttps://people.duke.edu/~gavan/bio/GJF_articles/McClelland_et_al.pdf', 'Amazing, thanks!']"
[Question] Measuring effects between variables (Binary/Likert),"Hi all,

I have a question about what tests to use to analyze the data of my experiment.

My main question is: How can I estimate effects between variables that are measured on a Likert scale. And effects of binary variables on Likert scale variables. I am somewhat familiar with OLS regressions, but I am not sure if I am allowed to use those with Likert-type data.

Your help would be much appreciated!

P.S. I am not allowed to add an image, so please dm me if you want me to send an image of my conceptual model if that helps.",12altgl,BozzInc,1680531362.0,3,0.81,"['By ""Likert scale"" you mean single Likert-type items, not several items averaged into a ""scale"" per se ?\n\nIn general, the results of Likert-type items can be treated as ordinal responses.  The general approach is to use ordinal regression.  \n\nThere are also several tests that can be used with an ordinal response in simple designs.  Like Wilcoxon-Mann-Whitney, Kruskal-Wallis, Friedman, Kendall correlation, and others.', '\nSome are single-item Likert and others are averages of multiple-item Likert scales. Should they be treated differently? \n\nCan I use ordinal logistic regressions also if the IV is binary or also measured on a Likert scale?', ""When composing a scale, since you've already summed or averaged the responses, the result can be considered interval in nature.  (Although some will argue that it's still ordinal in nature. I don't really understand this argument.)\n\nResponses from single Likert-type items are probably best considered ordinal. (Although I could make arguments that they could be considered interval in some cases).\n\nYou can definitively use ordinal regression if the IV is binary.  Or for any combination of binary, nominal, continuous IV's.\n\nHow ordinal IV's will be handled really depends on the software.  For simplicity, often people treat them either as interval or as nominal.  But some software, like R, will explicitly treat them as ordinal, basically equivalent to treating them as nominal categories and testing if there are linear or quadratic trends across the categories."", ""Routinely in behavioral science, averages or sums of Likert scales are treated as interval data. \n\nJust do a t-test. Or 2x2 anova if there's 2 factors.""]"
[Research] Need help analysing survey data,"Hi everyone,

I am currently attempting to explain how I will analyse my survey data and I am struggling with what method to use and why.

I am creating feedback forms for sessions. There will be a feedback form for every participant after every session (10 sessions in total with up to 30 participants).

The feedback forms have been made using the Likert scale (strongly agree to strongly disagree). The aim of the research is to see if the intervention as a whole as helped participants with their numeracy skills (completely made up topic).

So, on the feedback form there are a range of questions. Some are specific to that session (e.g the learning material of session 1) and others are standard questions that we are using to see a trend across the sessions. For example, ""I feel confident in my numeracy skills"" will be on every feedback form in hopes we will see a change in answers across the number of sessions (participant starts with a ""strongly disagree"" and by session 10 is a ""strongly agree"").

How should I analyse the results to see the change in responses over time? What is the best method and why? How should it be conducted?

Any help would be appreciated thank you!",12ag8mb,peachybeachy088,1680518177.0,12,0.93,"[""If you want to look at the impact of an an intervention, you could do a before and after survey and use a paired t test to see if there's any significant difference."", 'What if you take the mean & standard deviation of the scores prior to the sessions, then do the same after & compare the two numbers? Simple but effective. Then say that the scores after the session were x% higher/better. Maybe use a line graph & map out one line with the before numbers & the other with the after to really highlight the impact it had on the scores (hopefully they will be significant enough to look more impressive if you choose this route!)\n\nSorry if my answer is super simplistic, but I think this could be pretty straightforward & you wont have to work too hard to interpret the data this way :) good luck, sounds fun & interesting!', 'Hey there!\n\nIt sounds like you have a well-designed survey and a clear research goal. To answer your question about the best method for analyzing the results, one approach could be to use a repeated measures ANOVA. This would allow you to test for significant changes in responses over time, while controlling for individual differences among participants. \n\nAnother option could be to use a linear mixed effects model, which would also account for individual differences and allow for more nuanced analyses of the relationships between variables.\n\nUltimately, the best method will depend on the specific research questions and data at hand. It may be helpful to consult with a statistician or research advisor to determine the most appropriate approach for your study. Good luck with your analysis!', 'hi,\n\nyou could combine both approaches from Haleodo & from Regina to utilize all the insight from designed questions. I presume the key metric under evaluation here is ""I feel confident in my numerical score"" (dependent variable) and there are few other questions (materials, length of session, etc.) (independent variables). To utilize all the questions designed, you could analyze step by step:\n\n1. To see change in responses over time: approach as suggested by Haleodo  \n\n2. After that, there\'s a high chance you\'d receive follow-up questions like ""What factors/elements from the session that drive participants confidence?"". This could serve as findings for improvement. At this point, you can run linear regression (cuz the likert scale gives numerical variables) or logistic regression if the questions are binary ones. Then pick up elements with high coefficient to the dependent variable to highlight in your report. Eg. if Length of session and Confidence score has significantly negative coefficient then you could recommend reducing the length of the session to improve confidence score.\n\nhope this helps!', 'Hi thank you for this! I was originally thinking of doing this but its a voluntary intervention and we are expecting some individuals to not complete all of the sessions. This is why we decided to do feedback forms at the end of every session to see the impact of those who complete none, some, and all sessions.', 'Thank you! This was very easy to understand as you can see from my question statistics isnt my strongest point! Ill definitely take this into consideration when discussing the final analysis tool!', 'I am happy I could help! Only have taken stats 1342 so I know there are probably many 10000x more competent than myself, but I wanted to give the simplest idea? Are you a psych major? My partner is an MA in psych lol']"
[Question] Binary Logistic Regression Alternatives,"Hi all, I have data from my survey/cross-sectional study and I want to look at some questions (with binary responses) with predictor variables (mostly categorical, a couple continuous). Now I have been able to run the BLRs okay but my R values are really low for each model (<.1).

Should I just simply report this or look at alternative models from: [https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-21](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-21)

Thank you!",12a1bp6,KAHomedog,1680476294.0,13,0.93,"['I wouldnt say low R2 is a good reason to try alternative models if you think logistic regression was the right model for the data. That seems like phishing or phacking', 'You can use a linear probability model which is ols with a linear target variable.  Just be aware your residuals will be heteroscadastic so youll want to enable robust errors in your stats package.  A low r2 would only be of concern if you were making a predictive model.  If your goal is casual analysis it really depends on what youre doing.', ""Are you performing a prediction study or an association study? In other words, is your goal to predict who gets what outcome or to see if the outcome is associated with some particular predictors? If the former, there are a lot of strategies you can use to maximize 'accuracy' in whatever metric makes the most sense. We could maybe help if you give more details on your study. If the latter, (psuedo) R and other similar metrics are all irrelevant because your goal is to get an unbiased estimate of a particular effect and R won't tell you anything about that. A reasonable strategy for the latter question would be to think about all confounders for the relationship to your particular predictor and outcome and then use logistic regression with your predictor and those confounders. That would give you the (hopefully unbiased) log odds ratio for that predictor. If you have several predictors that you are interested in, this may require several different models based on which potential confounders should actually be included for each predictor.\n\nHopefully that at least helps you get started!"", 'What are you using for (pseudo) r-squared for logistic regression ?', ""R square means nothing.\n\nWhy don't you to a 2x2 table in which you like at observed data vs. predicted, and check which % you are correctly predicting? That's a lot more useful than looking at R-squared."", 'Dont bother with Rsquared. Focus on AIC or BIC.', 'I agree, I just reported my results to my supervisors and one noted the low pseudo R values may make interpreting the specific factors within the models difficult, and whether I had any ideas on this.\n\nI am more inclined to just report the weak model fit and the potential explanations for it but I was just wondering if there was any alternatives before going that way. I 100% agree with your observations re p hacking/phishing as well.', 'I have both Tjur and McFadden as outputs (I am also not sure about which one to use, as there seems to be some debate on which is superior). But they are both low with Tjur slightly higher and closer to .1 for all my models', ""This is absolutely correct - look at sensitivity and specificity. Then you could use a metric like ROC AUC to evaluate your model.\n\nYou'd also want to look carefully at your feature selection methods. As previously mentioned, you could use AIC or BIC as your criteria / stepwise selection, etc. More features aren't necessarily better in GLM. \n\nYou might also want to look into a ML ensemble tree method - random forest or XGBoost if you're feeling ambitious. You'll just have to deal with those tricky hyperparameters."", ""McFadden values are often low for logistic regression.  Perhaps see: [https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation](https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation) . \n\nIf you have access to the predicted or residual values (on the original scale), you might try to calculate Efron's pseudo r-square, which is pretty simple to calculate and understand.  \n\nBTW, a low pseudo r-square value is not necessarily a bad thing.  In some fields (biological systems, human behavior), it's typical that not much of the variation in response can be predicted by easily measured factors.""]"
[Question] Can anyone explain to me how the concepts of Standard Deviation and Coefficient of Variability are used to interpret data in the archaeological context?,,129sog8,Lmaodead007,1680458155.0,0,0.4,"['same as in the other contexts', ""\n\n*Dammit Jim, I'm a statistician not an archaeologist.*\n\nWhy would we know about how archaeologists choose to use statistics? It's not like we'd be regularly reading their journals, I've never seen what textbooks archaeology students might be taught their 'stats' from (I bet it's none of the many intro books I have read), and no archaeologist has come to me to ask to collaborate on any research. Aside from popular science articles I haven't had a lot of contact.\n\n...  is this homework?"", 'I mean, theres a lot going on in archaeology (I assume). Do you mean like something is this old, plus or minus X number of years? Because thats something theyre probably getting from carbon dating which is more of a physics question. I imagine other phenomenon like that are similar.', 'I dont even know what quantitative data archaeology generates, tbh']"
[Question] Event study with GARCH model," Hi, I have to do an event study, I analyze the US inflation and the returns of the biggest market indexes. Could you help me, I used GARCH modelling to forecast my estimated returns in the event window, than I got CAR, but how should I calculate SCAR? I am a bit confused, I learned it with only market model, but sure here it is not working :( Can anybody explain it?",129shjs,Weird-Secretary9255,1680457757.0,1,0.57,"[""That is the answer of ChatGPT:\n\nTo calculate the SCAR (Standardized Cumulative Abnormal Return), you can follow these steps:\r  \n\r  \nCalculate the cumulative abnormal return (CAR) as you have done already.\r  \n\r  \nCalculate the average (mean) and standard deviation of the normal returns (not the abnormal returns) over the same time period as the CAR.\r  \n\r  \nDivide the CAR by the standard deviation of the normal returns to obtain the SCAR.\r  \n\r  \nHere's the code to calculate the SCAR:\r  \n\r  \npython\r Copy code:\r  \nimport numpy as np\r  \n\r  \n\\# Step 1: Calculate CAR\r  \nWindow\\['CAR'\\] = Window\\['Epsilon\\_star'\\].cumsum()\r  \n\r  \n\\# Step 2: Calculate mean and standard deviation of normal returns\r  \nmean\\_return = Window\\['Return'\\].mean()\r  \nstd\\_return = Window\\['Return'\\].std()\r  \n\r  \n\\# Step 3: Calculate SCAR\r  \nWindow\\['SCAR'\\] = Window\\['CAR'\\] / (std\\_return \\* np.sqrt(Window.index.size))\r  \n\r  \n\\# Print the results\r  \nprint('Mean Return:', mean\\_return)\r  \nprint('Standard Deviation of Return:', std\\_return)\r  \nprint('SCAR:', Window\\['SCAR'\\])\n\n\r  \nNote that in Step 3, we are dividing the CAR by the standard deviation of the normal returns multiplied by the square root of the number of observations (i.e., the size of the window). This is done to standardize the CAR and make it comparable across different time periods.\n\n&#x200B;\n\nWhat do you think, correct?""]"
[Question] A question about justifying time-series analysis,"This question concerns the application of time-series analysis for the social sciences. Any insight, thoughts, or feedback is greatly appreciated.

Is there any rationale, besides theory, for determining if its worth examining the relationship between two series? In other words, is existing theory the only rationale justifying time-series analysis? Or is examining a scatter-plot of two series that appear related a prima facie justification to begin a more formal analysis, even when there is no existing theory to go on?

An example might be useful. Say we are interested in examining the relationship between two series, A and B. Both are monthly data with 100 observations. You plot each series over time. Both series seem non-stationary and appear to follow a deterministic trend. Without knowing any theory about the relationship between these two series, wouldnt the fact that they both appear to follow an increasing trend over time be enough justification in and of itself for a more formal analysis?

With that said, I recognize that its problematic to correlate two non-stationary time-series since they are not independent observations. A random time-series with a trend component will appear related and correlate very highly with another random series with a trend, and simply adding a trend induces one time-series to be related to the other. This is the spurious regression problem that Yule (1926) identified nearly 100 years ago. Indeed, one would need to determine if the series are non-stationary using unit-root tests, and then difference them until they are stationary. These are the beginning steps of a more formal analysis to determine if there is truly a relationship between the series, if they are co-integrated and have a long-run relationship, etc.

But is examining a scatter-plot of the two series really not telling us much? Is there any value in doing this? Say that you plot series B and it appears to be stationary instead. When you examine the two series over time, they appear unrelated. Without existing theory, couldnt the fact that the series appear unrelated be justification for not conducting a more formal analysis, especially since a stationary series cannot be driven by the same data-generating process as a non-stationary series? I do recognize that one cannot simply deduce if a series is stationary just by plotting it over time, as a series that appears stationary could in fact be non-stationary (unit-root tests are needed to verify this). But my point is that by plotting the series to get a sense of their patterns over time, doesnt this provide important information in and of itself that can be used to determine if future analysis is warranted?

Besides existing theory, what else do researchers have to go on when determining whether to conduct a more formal time-series analysis between two series? If theory is all that matters, and we really cannot infer anything from series plotted over time due to the possibility of serial dependence of the error terms, then how are novel areas of research discovered when there is no theory to go on? As a final caveat, I do recognize that in practice, there is always at least *some* prior theory to go on. So perhaps theory is a necessary but not sufficient condition to begin formal analysis, where you need a plausible theory that explains the mechanism, but also a preliminary examination of both series to determine if there is a potential relationship between them. Thoughts?",129qx2m,warwick607,1680454469.0,3,1.0,"["">Is there any rationale, besides theory, for determining if its worth examining the relationship between two series? In other words, is existing theory the only rationale justifying time-series analysis?\n\nOf course not. You can look for patterns and use statistical methods to help your find patterns that might continue to hold in the future, without knowing why.\n\nBut thinking about why some relationship comes about is an additional, and in practice extremely important, check against falsely concluding the relationship will continue to hold.\n\nIt may seem like statistical methods can tell you this (whether the pattern will extrapolate) for free, but they can't. You invariably need to assume something about how the data comes about (and will continue to come about in the future) for the methods to have their advertised properties. Theory is a key source for motivating such assumptions."", '>  wouldnt the fact that they both appear to follow an increasing trend over time be enough justification in and of itself for a more formal analysis?\n\nThe fact they they both increase with time is a good reason to exercise caution. \n\nhttps://en.wikipedia.org/wiki/Spurious_relationship#Examples\n\n(... Otherwise you might conclude that global warming is due to there being fewer pirates, or that increasing priests in a town leads to more fires.)', 'It seems like you are not aware of the empirical cycle. There is only one reason to do time series analysis: you are testing a theoretical prediction and that prediction is in the form of a time series. \n\nIf you are not using a qualitative model to test a prediction, you are not doing science.', 'You can regress time series A to time series B. The trick is, you have to analyze the residuals. If the residuals look like random white noise, then its okay. If the residuals have autocorrelation or are non-stationary, then you have a problem with spurious correlation. Look at the Engle-Granger method of cointegration', ""Great reply. I agree with you, it's helpful to hear it explained by someone else. Thank you!"", ""I already mentioned spurious regression in my post.\n\nEdit: I understand your point. But your examples...\n\n>global warming is due to there being fewer pirates, or that increasing priests in a town leads to more fires\n\nboth use *theory*, which is exactly my point. What if we don't have any theory? What if we have no information about the series? Can a scatter-plot demonstrating their over time patterns be used as prima facie evidence and rationale to begin more formal time-series analysis (i.e., unit-root tests)?""]"
[Q] Basic question about assumption of normality for error terms in OLS,"Hi all, Ive asked this question on SE but im still confused.

I had this question from a quiz in my stats class:

Which of the following is NOT a correct way to specify the assumptions needed for inference about the parameters of a Simple Linear Regression Model?

A. The experimental units are randomly selected, the responses are Normally distributed, and there is a constant variance of the points around the line.

B. The responses are random and Normally distributed with a mean of zero and constant variance.

C. The errors are independent and normally distributed with a mean of zero and constant variance.

D. iid N(0,)

Now of course B is the correct answer since those assumptions are wrong. But, would C and D not also be correct answers? I was under the impression that the error terms do not need to be independent nor normal due to Gauss-Markov Theorem?",129qs94,bernful,1680454177.0,1,0.6,"[""The question specifically asks about inference, which may be intended to describe the traditional set of hypothesis tests (though it should really be more specific). If that's the case, then the assumptions about the residuals are required.\n\nAlso note that option A is false too - the responses don't need to be normally distributed. There is practically no assumption about the distribution of the response, or any of the predictors."", ' You don\'t need any of those assumptions (nor any of the assumptions in Gauss-Markov) if all you\'re doing is estimating a least squares line; the relationship doesn\'t even need to be linear.\n\nThe assumptions here presumably relates to hypothesis tests (or possibly CIs, both count as inference, but then so does estimation). Specifically, for tests, what is intended by ""assumptions"" is usually the basis on which the null distribution of the test statistic is obtained, since that\'s the basis for getting the desired significance level (and consequently, also p-values).\n\n\\* though they should definitely have specified what inference they meant, since it can certainly matter.\n\n(edited for clarity)', 'Conducting a test with the correct asymptotic level does not require Gaussian errors. You are correct. They need only be mean 0 with finite variance. In finite samples type 1 error probability can be inflated without normality, but the same is true of the test of a single mean, using a t-test.', 'C and D say the same thing!', 'Oh I missed that but thank you.']"
[Q] How to best handle ordinal independent variables in causal mediation analysis?,"I'm writing a statistical analysis plan, and one of the RQs aims to estimate the effect of a treatment (binary) on Quality of Life that goes through a specific mediator, which is a symptom and is ordinal in nature (1-5 Likert scale). 

Since it's not an RCT, I'm using causal inference; and I made a DAG (using expert knowledge) which has 5 causal pathways I need to block. Some of the variables I need to include in the analysis to disentangle the effect of interest, are other symptoms; which are also ordinal in nature.

I'm aware about some options to deal with ordinal (likert) IVs, such as treating them as continuous or as categorical variables. However, I don't want to assume going from 1-2 is e.g. the same as going from 2-3 and I don't want to lose the ordinal nature either. And I don't know if looking at model fit is the right choice either.

What is the best way I can deal with ordinal IVs in case I'm behind causal inference? 

I would appreciate if someone can push me in the right direction. Many thanks in advance.",129ocqf,Shoddy-Barber-7885,1680448946.0,1,1.0,"[""I am not an expert in causal inference, but I've learned that ordinal IVs should be entered as monotonic variables, which takes into account their ordinal nature:\n\n[https://cran.r-project.org/web/packages/brms/vignettes/brms\\_monotonic.html](https://cran.r-project.org/web/packages/brms/vignettes/brms_monotonic.html)"", 'The key with ordinal variables is that you cannot necessarily assume that a 1 point increase has the same meaning across the levels of the ordinal scale. \nBecause of this representing the ordinal variable as a continuous term with only linear effects is particularly unlikely to be reasonable, and so it is especially important to model the effects of the ordinal variables flexibly. You have a lot of choices for doing this. One nice choice is the default in R which is to encode them as integers, but consider high order polynomials of those integers. This allows the effects to vary nonlinearly across levels.', 'Very helpful, thank you so much', 'Thank you!']"
[Q] Standardizing between 3 measured variables,"For data like below, would it make sense to take the average of the triplicate measurements as one x value, while using the average and stdev of all 9 values for standardization?

424 = average of 24hrs, 348 = total mean, 89 = total stdev

Standardized x for 24hrs = (424 - 348) / 89 = 0.85

Sample 1

&#x200B;

|24hrs|463|394|418|
|:-|:-|:-|:-|
|48hrs|414|343|347|
|72hrs|171|260|326|

I am planning to graph the standardized measurements of each variable's x average for a total of 4 samples (if it even makes sense to do so)",129f7wu,TumbleweedFresh9156,1680424891.0,6,1.0,"['I dont really know what your research question or design is, but Ill just eyeball it. \nIt looks like you have 3 groups of time measurements (24hrs, 48hrs, 72hrs), if these measures are correlated (Im guessing they are) then youll want to work with the mean and standard deviations of each group, respectively. So, no dont use the grand mean and grand std deviation. \nIt will still be fine to graph the standardised measures altogether, as of course, they are standardised, so you can interpret between sets of data.', 'See cross-posted question: https://www.reddit.com/r/AskStatistics/comments/129f5nh/standardizing\\_between\\_3\\_measured\\_variables/']"
[Q] How can I know the mean and the variance of the population before even doing a sampling on all the population?,"Hi! I'm a student in Aerospace engineer and I'm trying to get a grasp on statistic.

I saw [this](https://www.youtube.com/watch?v=F2mfEldxsPI) video and there's one thing I don't get.  
In the video it's showed a code that is used to demonstrate why unbiased variance is used instead of biased variance, but that is not the focus of my question. For the question we can just focus on the biased variance (graph on the top left): the code produces a casual sample of 50 new elements at every click, so at every click our ""total"" sample increase by 50, at the sime time for every click the code calculates the variance of the total sample (red dot), and a pseudo variance (black dot) which is calculated using the population mean instead of the total sample mean. Then the code plots the results with the ""true variance"" (The horizontal line) for the comparison.  
I know that the creator assumes a population with flat distribution.

My question is: how the heck does he know the true mean value (used to calculate the pseudo variance) and the true variance (horizontal line), in other words the mean value and the variance of the population? Isn't the population composed of all the samples, so first the codes needs to know all the samples?  
In my head I immagine that if I assume that the population is 500, all the samples before 10 clicks will be less than 500 elements so the variance will not coincide with the true variance, finally when I reach the 10th click I can really calculate the true variance because only now I have all 500 elements that compose my population.

Hope I made it clear. If you know what it's wrong in my view please let me know, I really want to understand this. .",129ewm0,ahahfilip,1680423958.0,16,0.88,"['You have stumbled upon the difference between real world statistics and simulations. In simulations we assign actual values to unknown parameters to show that the methods we are working with actually do what we claim they do. Youre right, in practice population parameters will never be known, otherwise there would be very little reason to be doing any statistics.', ""He is sampling from a distribution with known mean and variance -- i.e. using a random number generator with specific mean and variance. What that distribution *is* is impossible to say, since he doesn't show his code."", 'https://en.wikipedia.org/wiki/Expected_value#Random_variables_with_density', 'But he did calculate the ""true"" value of variance. Is there a way to calculate the variance assuming a mean value, a range of data and that the distribution is flat?', ""Sorry, I didn't suggest to check the comment section to find the [link](https://www.khanacademy.org/computer-programming/unbiased-variance-visualization/1167453164) to the code.In the code he is simply imposing the true values and generating casual values specifing only the range (1-200) as you can find in line 72.\n\n`var truePopulationMean = 100;`  \n`var truePopulationVariance = 3366.7;`  \n`var generateNewSample = function() {for`   \n`(var i = 0; i < sampleSize; i++)`  \n`{newSample[i] = (floor(random() * 201));}`  \n`numberOfSamples++;};`"", ""He's sampling from a discrete uniform distribution over the integers 0:200. The variance is (n^2 - 1)/12 = 3366.667. See [here](https://en.wikipedia.org/wiki/Discrete_uniform_distribution).""]"
[D] Does scale invariance explain Benford’s Law?,"There   are a handful of proposed explanations for why Benfords law crops up   in so many natural datasets. One of the explanations listed by [Wikipedia](https://en.wikipedia.org/wiki/Benford's_law#Explanations)   and other sources is scale invariance, meaning that the law applies   regardless of what units the data are in (pesos or dollars, meters or   inches). They note that *if* the distribution of the leading digits in a dataset are scale invariant, *then* they must follow Benfords law.

My   question is - how is this an explanation for why we see Benfords law   so often in the world? The main argument I have been able to find goes   like this: Wouldnt it be weird if the leading digits were very   sensitive to units? These are ground truths about nature, so we should   expect them to be scale invariant (and thus, expect them to be   Benford). However, I dont think this appeal makes sense, because I   dont agree that we should have this intuition that the leading digit of   our data should be insensitive to units. Indeed, there are plenty of   datasets where scale invariance doesnt hold.

I have one idea about how scale invariance might actually *explain*   the preponderance of Benfords Law and I havent seen it put forward   elsewhere. I'm curious what you all think: For all we know, there is a   really striking distribution of leading digits that emerges from a   common natural process, but the distribution is only visible when we   measure in yards and base 7. Maybe there are many such distributions   that if only we measured in certain units and bases, wed be in awe of   them and theyd have their own Wikipedia entries. Since Benfords Law is   scale (and base!) invariant, it happens to be the one we see all the   time because it doesnt require that we measure in a particular way.

What do people think of the above explanation? Are there other ways in which scale invariance **explains** the preponderance of Benfords law?",129cbe4,plumb_tuckered,1680416407.0,19,0.92,"[""The best explanation I have heard of Benford's law has to do with logarithms:\n\nConsider data of the form y=a^x. with x spread somewhat uniformly. The lead digits of x will be relatively evenly spread (x is uniform), but what about the lead digits of Y?\n\nNote that x=log(y). log(20)-log(10) is bigger than log(30)-log(20) etc.\n\nNow, note that Benford's law only works when you have data that spans many orders of magnitude: What kind of data does that? if a is 10 and x goes from 2 to 20, then y goes from having 2 digits to having 20 digits. So X only spans one order of magnitude and Y spans 19 of them.\n\nHence, this kind of phenomenon shows up in situations where you have some relatively tame distribution A and another distribution the logarithm of which is A. This is often seen whenever effects are multiplicative rather than additive, which is what causes the eccentric distributions where you will see Benford's law."", ""Scale invariance in statistics is a property of some probability distributions that makes it so the *distribution itself* doesn't change if you change units. This is only true for a very small family of power-law-like distributions like Zipf's law, where X ~ X^-d. (neglecting the constant of proportionality). \n\nYou can see how this would lead to scale invariance while something like a Gaussian distribution (X ~ e^(-X^2)) would not: let's say we have house prices measured in either dollars or pesos. If the prices are distributed like a power law with parameter *d*, then the number selling for $10k divided by the number selling for $100k will be ($10k)^(-d) / ($100k)^(-d) = (0.1)^(-d). Now let's translate the prices to pesos instead of dollars. One USD is about 18 Mex$ at the moment, so we should be comparing Mex$1.8M houses to Mex$180k houses. We don't need to do anything at all to our distribution to do this comparison: (Mex$180k)^(-d) / (Mex$1.8M)^(-d) = (0.1)^(-d), still! If we tried to do this with Gaussian-distributed prices, it would not work without changing the distribution: e^(-$10k^2) / e^(-$100k^2) is not equal to e^(-Mex$180k^2) / e^(-Mex$1.8M^2).\n\nBecause scale invariance implies a power law distribution, the power law and Benford's law are closely related. Whenever you see a power law distribution, Benford's law will hold (if the data covers enough orders of magnitude). So the question then is **why are power law distributions so ubiquitous?** As /u/tuerda said in their reply, this seems to be a consequence of multiplicative effects. More specifically, power laws seem to emerge from the combination of exponential growth with normally-distributed growth rates. (And there are very good reasons for things like growth rate to be normally distributed!) To me, this is a satisfying explanation for why Benford's law appears everywhere: it's a consequence of power law-distributed data, and power laws appear everywhere because they are a consequence of random, exponential growth.\n\nSo to your explanation:\n>I have one idea about how scale invariance might actually explain the preponderance of Benfords Law and I havent seen it put forward elsewhere. I'm curious what you all think: For all we know, there is a really striking distribution of leading digits that emerges from a common natural process, but the distribution is only visible when we measure in yards and base 7. Maybe there are many such distributions that if only we measured in certain units and bases, wed be in awe of them and theyd have their own Wikipedia entries. Since Benfords Law is scale (and base!) invariant, it happens to be the one we see all the time because it doesnt require that we measure in a particular way.\n\nI think this makes sense, but kind of misses the point. Yes, any combination of a specific data distribution & specific units should yield a specific distribution of leading digits. But Benford's law is interesting because the only things it requires (power law-distributed data covering several orders of magnitude) are actually very common."", 'This gives some good intuition:\n\n\nhttps://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/', ""I'm not sure I buy your proposed argument. The universality of Benford's law is the interesting part, not the specifics of the function log(d+1) - log(d).  (Well, the fact that this function is not constant is also part of the interestingness.) If you found some first digit distribution that only holds for specific datasets in base 7, then that is inherently less interesting than the fact that Benford's law holds for a wide variety of diverse datasets in any base.\n\nBut you are correct to observe that scale invariance is not by itself an explanation: it is not obvious that any universal pattern should exist at all, let alone that it should be scale invariant.\n\nI think the most convincing explanation has to do with the statistics of sampling from families of distributions. A given normal distribution does not itself satisfy Benford's law, but if you repeat the experiment of choosing a random normal distribution and then randomly sampling from it over and over again, the resulting numbers will satisfy Benford's law. You might be able to find a proof of this fact which emphasizes scale invariance; I'm not sure."", 'I think the relationship to power functions establishes the scale invariance.... Notice your proof remains valid for any choice of *a* coefficient.   \n\nI think scale invariance emerges as this feature of mathematical proofs where they remain valid after any linear transformation of the parameters.  In the physical domain, results are valid whether you measured in degrees Celsius or Fahrenheit.', ""Thanks, I appreciate the well-thought out reply. I agree with you that the power law explanation is the best explanation of Benford's law. I was under the impression that people put forth scale invariance as a separate and independent explanation. But you seem to be saying that the power laws explain scale invariance too and so the scale invariance in and of itself doesn't do explanatory work. \n\nI guess one reason that people might want a second explanation for Benford's law is that it's not always obvious why the data-generating mechanism that produces Benford's law should be a power law. For example why are the numbers in a newspaper generated by a power law? Or the lengths of rivers?"", ""I agree this is a good explanation of Benford's law, but my impression was that people (like the Wikipedia page for example) put forth scale invariance as a separate and independent explanation for why we see Benford's law in the real world. What you and u/tuerda seem to be saying is that scale invariance is merely another consequence of the logarithmic behavior that generate Benford's law. So scale invariance in and of itself doesn't do explanatory work. \n\nI think the reason people seek multiple explanations is that it's not clear why all of the places where Benford's law crops up should be the result of a data generating process like the one u/tuerda describes. For example the numbers in newspapers or the lengths of rivers."", ""Some people think mathematical relationships like that power formula are the ground truth, and other people think that mathematical relationships are tools studying the ground truth.    I think your impression from the wiki page (which is common everywhere people use math to understand natural phenomena) is based on the first way of thinking.\n\nI'm more inclined towards the second way of thinking, which says:   It's not that Benford's law *exists* because of scale invariance.  Rather that scale invariance is a useful tool for studying things, and Benford's law one of the things that manifests when you look at the Universe through the lens of a power-law relationship.  In other words, Benford's Law *is proven* because of scale invariance."", ""I think I'm reacting to a certain type of reasoning I see floating around discussion of Benford's law. On Wikipedia, in the section on [Explanations](https://en.wikipedia.org/wiki/Benford%27s_law#Explanations) they write\n\n>in a list of lengths spread evenly over many orders of magnitudefor   \nexample, a list of 1000 lengths mentioned in scientific papers that   \nincludes the measurements of molecules, bacteria, plants, and   \ngalaxiesit is reasonable to expect the distribution of first digits to   \nbe the same no matter whether the lengths are written in metres or in   \nfeet.  \n\n\nTheir argument seems to be,\n\n1. It is reasonable to expect the leading digits of certain types of   \nreal-world data to have the same distribution regardless of what units   \nwe use.\n2. *If* you have scale invariance of leading digits, *then* it must be Benford's law.\n3. Therefore, we should expect to see Benford's law in real-world data.\n\nBut I don't buy the first premise. Why is it reasonable to expect this? For example, the ending digit in numerical data is highly sensitive to what units we use. Imagine there were exactly 1000 rials in every dollar. Then all of the   \nnumbers in our financial data would have 0 as an ending digit. So why   \nare we supposed to buy premise 1 just on intuition?\n\nI don't mean to be touting Wikipedia as an infallible source, but this line of reasoning seems common and I'm wondering whether its flawed or not."", '**Benford\'s law** \n \n [Explanations](https://en.wikipedia.org/wiki/Benford\'s_law#Explanations) \n \n >Benford\'s law tends to apply most accurately to data that span several orders of magnitude. As a rule of thumb, the more orders of magnitude that the data evenly covers, the more accurately Benford\'s law applies. For instance, one can expect that Benford\'s law would apply to a list of numbers representing the populations of UK settlements. But if a ""settlement"" is defined as a village with population between 300 and 999, then Benford\'s law will not apply.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', 'There is no good reason 1 should be intuitive. This is why my explanation did not include it in any way.']"
[Q] What does it mean when two random variables are stochastically independent ?," I dont understand the difference with linearly independent Also, how does stochastic independence show on a plot ?",1293qd3,haidoooo,1680394136.0,2,0.67,"['Colloquially, independence between the rvs X and Y means that knowledge of an event about one of them does not affect the probability we would assign to events with the other.\n\nFormally, P(X&isin;A, Y&isin;B) = P(X&isin;A)\\*P(Y&isin;B) for all events A and B.\n\nYou can\'t really plot random variables, but if you have two random samples of them (realizations of the random variables), a scatter plot of them would look like a ""cloud"" of points, with no obvious relationship between them.', '> What does it mean when two random variables are stochastically independent ? \n\nhttps://en.wikipedia.org/wiki/Independence_(probability_theory)\n\nSpecifically, you probably want this:\n\nhttps://en.wikipedia.org/wiki/Independence_(probability_theory)#For_real_valued_random_variables\n\nin short -- the joint density is the product of marginal densities; the joint cdf is the product of the marginal cdfs.\n\n>  Also, how does stochastic independence show on a plot ?\n\nThe conditional distributions are all the same. Or, equivalently, the conditional distributions match the corresponding marginal distribution.\n\nRoughly speaking, take a thin slice (vertical or horizontal) anywhere, and the distribution you get\\* is the same no matter where you slice it. One way to think about it is to consider transforming the marginal distributions so that the margins are uniform, you have independence when all the conditional distributions are also uniform; equivalently, (for two variables) the bivariate distribution becomes uniform when you transform the marginal distributions to be uniform. (Mathematically, independence implies that the [copula](https://en.wikipedia.org/wiki/Copula_%28probability_theory%29) is uniform.)\n\nThese notions are illustrated in the plot here:\n\n https://i.stack.imgur.com/IM5si.png\n\nOf course there are some small differences (most noticeable in the histograms) but those differences just due to random sampling; the two variables are actually independent here. \n\n---\n\n\\* assuming you have a large enough sample that Glivenko-Cantelli ""kicks in"", i.e. that the sample cdf is essentially the population cdf', ""The image in this wikipedia article is quite informative. Only the center top image reflects a random sample from stochastically independent variables. The horizontal and vertical cross-sections have basically the same distribution regardless of where the cross-section occurs. Also kind of the center-center image, but the variance of y being zero causes some mathematical problems.\n\nLinear independence means there's no linear relationship. That's true for the entire bottom row as well as the center top image. There is a clear relationship between x and y for the bottom row, but none of them can be captured with a linear equation.\n\n[https://en.wikipedia.org/wiki/Correlation](https://en.wikipedia.org/wiki/Correlation)""]"
"Help Needed, Appropriate Test for a Research Study [Q]","I am conducting research on diet changes in a population of mammals. The crux of the study is comparing data gathered over a 10 year period in the 80s and 90s to data recently collected in 2020-2022.

I am comparing counts of plant species eaten in the old study and those in the new study. Some species have been phased out of the diet or new species have been added. 

I initially did a t-test but my mentor told me to do a chi square test, and wilcoxin signed rank test instead. But the data is largely numerical so I'm concerned about the validity of the results. 

I am not confident enough in my reasoning for using certain stats tests so it is difficult for me to make suggestions with my mentor who has been doing this work for decades. 

What do you think would be the best statistical test for the type of data I have below. This is an example of the raw data.

 

|Plants|Recent Data Collection|Old Study Data|
|:-|:-|:-|
|Banana Trees|650|0|
|Coconut Trees|0|3000|
|Mint Trees|750|5000|

or 

&#x200B;

|Parts of Plants|Recent Data Collection|Old Study Data|
|:-|:-|:-|
|Coconut Leaf petioles|300|600|
|Mint Pods|500|450|
|Banana Leaves|500|250|",128t6vb,Bitchbettahvmyhoney,1680370851.0,2,1.0,"[""What type of data do you think a chi square test would be used on? \n\nWhat you've laid out here are contingency tables, which typically are exactly what you want for chi square tests. That's testing the null that time of collection and plant (or part of plant)  are independent of each other.""]"
"[E] Is ""Probability and Statistics - by DeGroot and Morris"" a good text book to learn statistical theory from?","Coming from a background in Psychology and Molecular Biology, I've learned about how to use off the shelf statistical tests - in SPSS; and to an extent -  building my own models in R. However, I'm working as a data analyst now, and I would really like to understand the fundamental concepts of statistics, sort of from first principles, I guess.

I've had Probability and Statistics by DeGroot and Morris on my PC for a while now. And while I can sort of keep up with the examples and tasks. I do wonder whether it is even a good book to be using? If not, could I get some suggestions? I hear Casella's book is good.",128nn19,Relevant-Dog6890,1680358440.0,11,0.88,"[""Honestly, if you're coming from a psychology background, reading [this classic paper by Edwards, Lindman & Savage](https://doi.org/10.1007/978-1-4612-0919-5_34) will build up invaluable intuition about probability before diving into the mathematical details with texts like DeGroot & Morris."", 'statistically speaking, it probably is.', ""I use Walpole & Myers, but I don't know if it's exactly what you are looking for. It's pretty common on science and engineering courses"", ""DeGroot is fine.  Theres dozens of other texts at a similar level if its not to your taste. \n\n\nYes Casella &  Berger is a good text  on the theory of inference but I would not start there unless you're mathematically pretty comfortable"", 'I used this textbook in my undergrad (stats). the book does get kind of multi variable calc heavy towards PDFs and such. but otherwise I found it to be a solid book and liked it.', 'Another good option is Wasserman All of Statistics', 'I will certainly give it a read. Although, I forgot to mention I did maths at A level, so I have an introductory understanding of calculus, and some rudimentary statistical methods.', 'Okay, thanks, I will definitely take a look.']"
[Q] What to do if residuals are not normally distributed (linear regression)?,"Hello folks,

I would like to calculate a simple linear regression in RStudio. To test the assumption of normally distributed residuals, I have calculated a Shapiro-Wilk test. Unfortunately, it shows a significant result, so that the prerequisite is not fulfilled.

Due to my sample size (N = 144), can I invoke the central limit theorem at this point and still reliably interpret the results of the hypothesis test? In principle, my hypothesis could also be tested using a simple correlation, for which I would not need normally distributed residuals, but the supervisor of my thesis seems to be a fan of using regressions.

English is not my first language, but I hope that one can still understand what I am trying to ask.  


Edit: Thank you to everyone who wrote a comment! Nowhere do you get advice on statistical questions as quickly as here! :)",128nhhi,simplySchorsch,1680358085.0,55,0.99,"['OLS is still the best linear unbiased estimator regardless of non-normal residuals as long as the Gauss Markov assumptions are met: https://en.m.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\n\n(Note: the Gauss Markov assumptions + normally distributed errors gives you the Classical Linear Model assumptions)\n\nIn practice, these assumptions are rarely met, but OLS tends to be relatively robust to modest violations of these.\n\nAlso, OLS never requires the _residuals_ to be normally distributed but instead that the _errors_ are normally distributed (under the CLM). We do often inspect the residuals though as they are our estimates of the errors.\n\nJeffrey Woodridge suggests that at sample sizes as low as n=30 we can rely on the Gauss Markov assumptions to guarantee the asymptotic normality of the test statistics. We can use a t-distribution instead of a normal distribution to compute p-values to be extra careful.', 'Generally speaking, significance tests of whether an assumption is exactly met are not very informative because the assumptions are never  exactly met. Often a Q-Q plot is more informative. Incidentally, the CLT applies to means not individual data points. As pointed out  by General-point801 you could try transforming the data. Tukeys ladder of powers is sometimes a good simple alternative to Box-Cox.', 'Of more concern than non-normality is heteroskedasticity for normal standard errors. At a sample size of 144, youre probably reasonably well off with normal standard errors unless the distributions are extremely heavy tailed or highly skewed due to the central limit theorem. I would suggest looking at residual diagnostics to assess rather than a Shapiro-Wilks test, which is generally not particularly useful.', ""There's a convincing argument made by [Allen Downey](https://twitter.com/AllenDowney) that [testing for normality is generally a bad idea](https://www.allendowney.com/blog/2023/01/28/never-test-for-normality/)."", 'Try to transform your response variable, usually log or square root if theyre non negative.', 'I believe running a box cox transformation test should give you some options for transformations.', 'Honestly, if you have a reasonably large sample size and the residuals dont look completely whack youre probably fine.', 'Did you do the SW test on the residuals of your model? Cause you should.\nIf you have only one predictor variable, you try a Kruskal-Wallis. But if you want to try a linear model, look if your response variable does not warrant Poisson, Gamma, Binomial or other error distributions, cause then you could go glm. But first, try log transforming your response variable to see if this improves your normality issue.', 'Normality doesnt matter. More important is that your functional form is correct, like is linearity (in the parameters) satisfied. If its not then that alone can throw off the residual distributions too.', '[deleted]', 'You have enough data to try a restricted cubic spline. Also, there is a function called boxcox in the mass package that will suggest an ""optimal"" transformation of the response variable based on maximum likelihood estimation, I think.', ""I'm not so expert and if anyone would like to correct me I'll be fine, but:  \n\\- SW test with decent sample size tend to be significative even for modest deviation from normality  \n\\- seeing the plots you provided, as others noted, you could try a bootstrapped model and compare to a classic one"", '1. You can\'t interpret the shape of the distribution of the residuals unless the other assumptions are pretty close to correct.\n\n   There\'s some discussion of this point here: https://www.reddit.com/r/statistics/comments/1242kkk/q_how_to_straighten_qq_plot_for_regression/jdy18mi/\n\n   It talks about looking at a Q-Q plot rather than a test but the issue being described is the same\n\n2. Formal tests of assumptions is not typically particularly helpful in general and really not very useful in this particular case. In large samples you will be sure to reject quite trivial non-normality with little appreciable effect on the type I error rate. Indeed as sample size increases, for a given effect size non-normality has less and less effect on type I error, but the p-value of a test gets more and more sensitive to this less and less important issue. \n\n  With power, you don\'t get that same ""impact of a given effect size becomes less important with n"" effect, but it\'s still roughly constant with effect size, so again p-values aren\'t giving you what you need. A plot that shows the sort of things that the power of the test was sensitive to would look at the right sort of thing but, a test still doesn\'t.\n\n3. ""can I invoke the central limit theorem at this point"". Well, the classical CLT says that in the limit as n goes to infinity the distribution of a standardized sample mean (-)/(/n) goes to a standard normal distribution.\n\n  There\'s a version that can apply to a regression coefficient (given some specific conditions), but you\'re relying on more than that (you don\'t know , for one thing, so you have a gap to fill there), and the theorem says nothing specific about any finite sample size and certainly not about n=144.\n\n  The thing I presume you\'re actually worried about whether the true type I error rate of the test of the slope coefficient is close to the chosen type I error rate. That depends on a number of things including how non-normal the errors are (and not necessarily in the way any particular omnibus goodness of fit test statistic is detecting).\n\n   To know the frequentist properties of your test, you\'d need to know what population error distribution you\'re dealing with. Obviously you can\'t tell that from a small sample (and there\'s reasons why it can be a bad idea to use the same sample you want to run a test one to decide that), but you *can* investigate the properties of your test, in particular, given your x-values, how sensitive the test is to difference violations of the assumptions (including normality of errors). \n\n   For example, simulation can be used. One problem I frequently see is that people use simulation as a form of confirmation bias, not seeing what the least things are that it takes to ""break"" the thing they want to do. A second issue is only treating the potential kinds of violations as acting completely independently, when typically they tend to interact (so there\'s also some need to investigate that).\n\n4. ""*my hypothesis could also be tested using a simple correlation, for which I would not need normally distributed residuals*""\n\n   The actual requisite assumptions for both tests are identical, this has been clear (including explicit statements in the literature) at least since the 1920s (of course, if it appears in the stats literature ... almost nobody outside stats pays the least bit of attention to it). A lot of non-mathematical books state older (and considerably more restrictive) assumptions for Pearson correlation -- but some books get even that assumption somewhat wrong. Beware what you rely on.\n\n   In short, the assumptions I assume you\'re thinking of will be harder to satisfy, not easier, but they\'re overly restrictive. \n\n   ""*the supervisor of my thesis seems to be a fan of using regressions*"" -- in this case they\'re not leading you astray.\n   \n5.  With *simple* regression you can perform a permutation test for the slope, assuming the (unobservable) errors are exchangeable (if the other assumptions all hold that should be the case), avoiding the need to worry about near-normality at all.', '**[GaussMarkov theorem](https://en.m.wikipedia.org/wiki/GaussMarkov_theorem)** \n \n >In statistics, the GaussMarkov theorem (or simply Gauss theorem for some authors) states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', 'Subtle point here, if errors are normal the residuals **will also be normally distributed**. Non-normality of residuals is sufficient to falsify normal errors.', 'Ols does not require normally distributed errors, you can even see this in the wiki snippet below.', 'Op will want to enable robust errors in their stats package when their residuals are heteroskedastic', 'CLT + asymptotic linearity in this case does mean that hypothesis tests/CI are asymptotically correct even without normal errors (regression estimates are, to first order, sample means).', ""Thank you! I tested for heteroskedasticity by looking at the residual plot and doing the breusch-pagan test which showed no heteroskedasticity.  \n\n\nCould I -generally speaking- avoid all that by simply calculating the correlation? I'm only interested in the standardized regression weight and its significance. That way, I would not need to fulfill all these requirements. Or is that completely wrong?  \n\n\nThis is for my bachelor's thesis, so it problably does not need to be 'high mathematics' but of course it shouldn't be wrong :D"", 'Either this  or check if you can add other significant IV to your model... Non-normal residuals could mean the you still have some ""signal"" that\'s not accounted for by the model.', ""Is N = 144 reasonably large? :D but yeah, I figured it doesn't have to 100% accurate, especially for a bachelor's thesis"", 'Normality tests are simply not the right tool to be using here. Its not good advice to suggest a less powerful test. If the normality hypothesis were actually the hypothesis of interest to test more power would almost certainly be better.', ""... That's what I said.\n\nSorry I thought it was clear I was referring to the CLM assumptions still."", 'I would not use correlation instead. Im admittedly fuzzy on the exact details of the correlation tests because I literally never use them for anything, but I believe they require substantially stronger assumption than the regression tests do.\n\nI would say you are more than likely good with what you have done so far then, just check qqplots to make sure your residuals arent heavy tailed.', 'Wouldnt this usually present some sort of trend in your residuals? ie. heteroskedasticity', 'Hard to say. Depends how non normal it is. Really skewed distributions can send the CLT to shit pretty quickly.', 'Nah youre comment says that OLS requires the errors are normally distributed.', '[https://imgur.com/a/DgwQJ2Y](https://imgur.com/a/DgwQJ2Y)\n\n&#x200B;\n\nThis is everything I did to check this assumption. Every other assumption for the linear regression was met successfully. Could you have a look at it?', ""But if you don't know the omitted variable, you may not be able to see the heteroscedasticity by inspection of the residuals alone."", 'I uploaded my results here: [https://imgur.com/a/DgwQJ2Y](https://imgur.com/a/DgwQJ2Y)', 'Theres a bit of a heavy tail, for a published analysis I would want to probably run a bootstrap type test as well to check for robustness of the results. For a Bachelors thesis I would say, as long as your advisor is also okay with it, you can probably go with what you have and just acknowledge that theres some heavy tail behavior that may introduce a small risk of slightly incorrect confidence intervals.', ""Those are light tails, not heavy tails. That, coupled with the sample size, suggests it's fine to use inference based on the assumption or normally distributed error terms."", ""Perfect, I'll definity discuss the results with my advisor anyway. Thank you so much!"", 'This is correct, my bad misreading graphs before my coffee.']"
[Q] Univariate vs multivariate analysis,"Hey there, novice here.

I'm doing a cohort study about a certain medical procedure that has fail/success outcomes, and its risk factors for failure. 

When I did a univariate analysis for risk factors for failure I used Chi-square. After that, I used a multivariate regression model to analyze the effect of these risk factors and to account for confounding effects.

Some of my results did not reach significance in the univariate analysis, but they did reach p<0.05 in the multivariate. 

Did I do something wrong or biased the analysis when I included the non-significant factors of the univariate analysis in the multivariate one?",128la81,sholopinho,1680352958.0,16,0.94,"['Side note: When you say ""multivariate"" you really mean ""multiple"" or ""multivariable"". Multivariate means that there are multiple dependent variables (think PCA).\n\nGenerally, you shouldn\'t do the univariate analyses at all and certainly not for screening variables to be included in subsequent multiple regression models. A paper that goes into detail is [here](https://www.jclinepi.com/article/0895-4356\\(96\\)00025-X/fulltext). Ideally, the selection of variables is based on subject matter knowledge condensed into a DAG.\n\nFinally, it\'s no surprise at all that the p-values in the multiple regression model do not ""match"" those in the univariate analyses. After all, the reason to use a multiple regression model is to asceratin the effects of variables after taking the effect of other variables into account.', 'I understand why it makes sense to not univariate screen for significance. But what if you are discussing topics in which you cannot predict based on subject material knowledge. For example predicting prolonged stay after hospital. The reality is for each procedure their may be different patient demographics that predict versus some surgeries it may be more intraoperative or postoperative factors. \n\nIn a more general sense truly most the variables we know from subject matter to be potentially important are ones we have previously determined usually via single variate analysis to be important just in past studies. \n\nJust something I struggle with at times as it relates to this.', ""If your goal is prediction instead of explanation/confirmation, then I'd apply a technique that is known to perform well for the task, such as LASSO, ridge or elastic net (or some even newer ones).""]"
[Research] Attrition Rate - Critical Appraisal of an Article,"How do you guys tally the drop outs? Do you include those subjects who withdrew or got excluded before the group randomization process or only those who were removed after the randomization process when the experiment/intervention is already commencing  

Any help would be appreciated. Thank you!",128a20a,Travis_41,1680320427.0,2,1.0,"['When Ive done projects/studies where sample sizes have decreased based on inclusion/exclusion I usually state those inclusion or exclusion criteria and then also the n (number of people) excluded because of them. Almost always report the largest sample we stared with and then say X was our final (reduced) study sample and heres who fell out and why.', 'The study that Im appraising totally did what you stated. But Im trying to determine if the study has high or low attrition rate. The study has 64 subjects consented but 26 of them either withdrew or got excluded, so 38 went on with the randomization process. At the middle of the intervention phase, 2 withdrew, so only 36 subjects were included in the final analysis\n\nIf I only count those 2 that withdrew after the randomization process, then I could say that they have low attrition. But if I include those 26 who withdrew or got excluded before the randomization, then I could say that they have a higher attrition rate. Thats where I am stuck up right now ', 'Oh wow! That is high attrition if you could the study starting with consenting phase. Nearly half! Overall though, low attrition because the study phase and data collection phase was started after randomization and then intervention. Or thats how Id see it', 'Thats what I am leaning on as well, that those who got excluded before randomization should not be considered as dropouts. Thanks a bunch!']"
[Q] - Odds of a baseball card existing based on sale data of other cards in the set.,"# Summary

I am putting together a set of baseball cards from 2022 Topps Inception. In the set there is a base autograph card, which is white and ranges from 75 copies per player, to 299 copies per player. While searching for them over the last year, I have been able to find 80 of the 83 players for this specific card. **In that time, I have not seen a single base card for 3 players: Juan Soto, Sammy Long, and Ian Anderson.**

# Set Information

There is a white base card and 7 colored parallel cards. Green, magenta, aqua, red, orange, blue, and black. Black data is excluded since only 1 black card exists per player. If a white base card is rarer than the other parallels, the parallel does not exist. Example: Shohei Ohtani has only 75 white base cards, so the green, magenta, and aqua do not exist. The others are Fernando Tatis Jr (/125  so no green exists), and possibly Wander Franco (/130  no green sales have been seen).

Topps sells these in cases or boxes. Each Box includes 1 pack with 7 cards in it. Each case has 16 boxes. There are different types of card in the Inception set. See here for details on other cards:

[2022 Topps Inception Baseball - Beckett](https://www.beckett.com/news/2022-topps-inception-baseball-cards/?utm_term=&utm_campaign=Performance+Max-BGS&utm_source=adwords&utm_medium=ppc&hsa_acc=5407491096&hsa_cam=18643815275&hsa_grp=&hsa_ad=&hsa_src=x&hsa_tgt=&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQjwiZqhBhCJARIsACHHEH8qbCwjGvSasuQHKGFrnGiEE8zwOuOc7t44pQ5GmWPT6fEy6u4YYjAaAmm3EALw_wcB)

# Data

The data provided below is from eBays Terapeak, which gives insight into sales over the past 2 years. Since this set was released in early 2022, it only shows data from April 2022 until now. The data provided shows how many sales exist for each colored card for each player.

The search used looked for generic terms about the set, including the player and the number of the parallel. Common terms that would return incorrect listings were specifically omitted. Example Search:

    2022 inception auto -break -silver -signings -patch -relic -hat -rpa -glove -digital -sock Gavin Sheets 299 

# Question

**I am trying to figure out if Topps omitted these cards in the set, or they exist and just happen to have 0 sales. Is there any statistical evidence they did not include them in the sets?**

I think it is nearly impossible that these cards exist, but all have 0 sales when compared to the other sale data.

&#x200B;

Assumptions that were made:

1. All players have a white base card
2. The same card can be sold more than one time. (data reflects this)
3. Soto is somewhere around a /99 since there is an aqua color, but no magenta.
4. Sammy Long is likely around a /299
5. Ian Anderson is likely around a /299
6. The sellers listings were accurate and included what the card was numbered to. (A scrolling QC of the pictures was done, and any photos that differed from the players card was excluded.)

# Data

||Number of sales per card||||||||
|:-|:-|:-|:-|:-|:-|:-|:-|:-|
||**BLUE**|**ORANGE**|**RED**|**AQUA**|**MAGENTA**|**GREEN**|**BASE**|**Notes**|
|**Player**|**/10**|**/25**|**/50**|**/75**|**/99**|**/125**|**/299**\*|\***unless otherwise noted**|
|Sammy Long|1|3|6|7|10|19|**0**||
|Juan Soto|2|8|0|19|N/A|N/A|**0**|magenta and green do not exist since there is a low print run. See Ohtani& tatis|
|Ian Anderson|0|4|10|3|19|12|**0**||
|Aaron Ashby|1|1|3|8|12|22|37|/249 base|
|Alec Bohm|6|4|8|9|25|19|29||
|Adolis Garcia|1|9|10|75|24|26|30||
|Andre Jackson|0|7|6|5|8|19|14||
|Alejo Lopez|1|5|2|5|6|15|20||
|Austin Riley|2|9|10|20|28|25|34|/199|
|Alfonso Rivas|2|1|5|5|8|13|26||
|Angel Rondon|1|3|4|11|7|11|17||
|Alex Verdugo|2|10|10|14|7|17|37|/249|
|Andrew Vaughn|1|2|8|11|15|29|18||
|Alexander Wells|0|2|3|6|10|8|19||
|Bryan De La Cruz|3|2|1|13|13|27|26||
|Brandon Marsh|1|0|0|12|0|0|14||
|Charlie Barnes|2|2|5|7|8|16|26||
|Casey Mize|1|7|5|15|15|18|16|/150|
|Cristian Pache|1|0|3|8|7|18|27||
|Cal Raleigh|0|2|13|9|18|22|33||
|Curtis Terry|5|2|1|3|8|9|15||
|Dylan Carlson|1|1|5|14|18|31|30||
|Drew Ellis|2|1|7|19|9|14|5||
|Deivi Garcia|1|3|3|2|18|7|22||
|Ernie Clement|2|3|4|12|7|4|16||
|Emmanuel Rivera|2|6|5|7|2|7|20||
|Freddy Peralta|0|6|7|15|3|15|28|/225|
|Fernando Tatis Jr.|2|3|15|21|18|N/A|20|/125 base (no green exists)|
|Greg Deichmann|1|2|4|4|8|9|22||
|Griffin Jax|0|5|6|7|13|13|19||
|Gavin Lux|1|2|7|9|6|11|23|/225|
|Glenn Otto|1|1|9|15|6|6|26||
|Gavin Sheets|0|8|12|6|15|6|25||
|HOY Jun Park|2|1|2|16|8|10|24||
|Jake Burger|2|5|6|11|20|26|22||
|Joe Barlow|0|2|10|13|13|14|18||
|Jazz Chisholm Jr.|4|1|14|13|17|12|32|/225|
|Jake Cousins|1|3|6|8|14|13|34||
|Jarren Duran|1|3|2|1|0|1|19||
|Josiah Gray|2|4|4|8|11|9|31||
|Jarred Kelenic|0|1|15|12|18|21|28||
|Jackson Kowar|1|3|1|9|10|11|36||
|Josh Lowe|0|2|9|12|13|8|46||
|Jake Meyers|1|4|4|9|10|18|34||
|Jake McCarthy|3|4|14|11|15|20|35||
|Joe Ryan|2|2|7|10|24|26|41||
|Jose Siri|0|9|3|5|15|22|26||
|Kyle Muller|1|5|14|14|18|15|28||
|Logan Gilbert|3|5|9|21|21|23|37|/249|
|Luis Gil|3|3|3|8|7|13|40||
|Lars Nootbaar|3|7|8|2|12|6|39||
|Luke Williams|3|3|6|3|15|24|28||
|Marcos Diplan|2|4|6|5|3|0|39||
|Max Kranick|1|1|8|1|6|3|10||
|Matt Manning|1|3|4|11|9|15|20|/225|
|Zach Thompson|1|4|2|3|8|9|8||
|Matthew Vierling|4|2|2|6|14|14|23||
|Nick Madrigal|1|4|11|15|10|10|30||
|Pete Alonso|1|9|9|8|22|34|15|/150|
|Randy Arozarena|2|8|3|11|24|28|36|/225|
|Riley Adams|1|4|12|8|13|13|32||
|Reid Detmers|1|10|15|15|8|19|45||
|Romy Gonzalez|0|3|3|5|18|16|19||
|Reiss Knehr|1|1|2|10|13|6|33||
|Ryan Mountcastle|0|5|20|21|23|22|36||
|Ryan Vilade|0|3|3|10|1|4|14||
|Seth Beer|2|5|6|10|23|13|47||
|Shane Baz|3|1|15|12|14|21|48||
|Shohei Ohtani|0|9|7|N/A|N/A|N/A|16|/75 (no green, aqua, magenta)|
|Stephen Ridings|2|4|11|8|13|19|26||
|Sixto Sanchez|2|1|9|15|10|8|32|/249|
|Spenser Watkins|5|2|9|9|6|6|40||
|Trey Amburgey|0|2|9|5|4|20|31||
|Tyler Gilbert|1|2|5|10|7|11|15||
|TJ Friedl|0|5|2|9|6|13|23||
|Tylor Megill|0|4|8|17|9|14|30||
|Trevor Rogers|1|4|6|8|11|13|9||
|Tony Santillan|1|1|3|7|12|10|25||
|Vidal Brujan|3|4|0|4|18|8|30||
|Wander Franco|0|1|3|4|5|0|3|/130|
|Yordan Alvarez|4|3|10|20|24|28|45|/150|
|Yonny Hernandez|1|1|8|5|7|6|17||
|Yohel Pozo|1|4|3|9|12|7|15||",1283ic5,r_stra,1680304964.0,20,0.93,"['You doing bunt too?', ""Edit: Wait, had a mistake in my spreadsheet. I'll fix it. Edit2: done\n\nIf every existing card has the same chance to show up in your dataset then it's very likely that these three cards were left out somehow, but your data sample itself is suspicious in some aspects.\n\nAmong the colored cards you expect 31224 cards in total (83 players for blue, orange, red, 82 for aqua, 81 for magenta, 80 for green). Out of these you have 3871, or 12.7%.\n\nIf you expect 12.7% out of 125 cards (lowest for Sammy Long and Ian Anderson) then the chance to find 0 cards for one player is just 40 in a billion.\n\nHowever: The chance to find 45 or more base cards out of 150 (Yordan Alvarez) is just 20 in a billion. If we use the maximal 299 base cards then the chance increases to a reasonable 13%. We still have the very unlikely 20 out of 75 aqua (0.03%), 4 out of 10 blue (0.5%), 24 out of 99 magenta (0.05%) and 28 out of 125 green (0.09%) all for the same player. Something seems to make their cards much more likely.\n\nIf something can make some cards more likely to show up in the dataset than others then we can't tell anything without a quantitative understanding of what causes this bias."", ""Tangent question: I amassed a massive baseball card collection in the 80's and early 90's.  I quit after the strike.  I recently saw packs for sale for $10-20 while standing line at Walmart and was shocked.\n\nJust curious, how much did this experiment cost?"", 'Bunt as in digital cards? No', 'Let me dig into yordan and see what numbers have been sold.', ""So I have only looked at sale data for these. I didn't open these, only bought singles on eBay or traded for them. The box with 7 cards (only 1 auto per box) costs $165. So it's a huge waste. Most of the cards are under $10 to buy separate except for some high end players. \n\nI wanted to get the set of all 83 white cards for my collection. Its run me about 1.5k, and most of that has been from buying 3 cards. Those 3 could probably be sold right now to cover the whole set. But I'd like to hold now."", 'https://www.reddit.com/r/baseballcards/comments/10c9qsn/2022_inception_auto_set_4_more_to_go_fairly/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button\n\nHere is my set. Waiting on 1 redemption, and there is a Pete Alonso not shown here which I do have now.', 'Bunt is cool.']"
[Q] Methods for handling offset variables,"I have built a BRT model with about a dozen input variables of mixed types and a binary outcome variable. There are 105 observations in my 80% training set (trust me, I wish it were higher, but thats the reality). 

I am getting promising results in terms of P/R and AUC. However, one of my variables is an exposure measure in the form of minutes during each observation. 

When I try to shift this from an input variable to a log-transformed offset variable, the model essentially breaks. 

Precision goes from 0.9 to 0.7

Recall goes from 0.9 to 1

AUC goes from 0.9 to 1

The model does not predict any positive cases, even with a decision boundary of 0.1. 

Using the raw value (not log-transformed) yields similar results but the AUC is 0.6 instead of 1. 

I dont think its valid to use an exposure variable as a predictor, but Im having trouble figuring out how to best incorporate it. Any help appreciated. 

Thank you.",127vmvm,thanks_paul,1680289582.0,1,0.67,[]
[Q] Inference: Which explanatory variables should I include in the model?,"Hi all!

I need your help. Suppose that the aim of the analysis is hypothesis testing, point and interval estimation of the unknown regression parameters. I am not interested in making predictions. Which explanatory variables should I include in the model? For the sake of simplicity, suppose that there is no multicollinearity (both perfect and approximate multicollinearity).  Moreover, suppose that there is no prior information on the phenomenon. If I include irrelevant variables in the model, I will get wider confidence interval for the parameters. If I exclude some relevant variables, I will get biased estimates. Is it correct to start with the full model (the model with all possible explanatory variables) and iteratively remove the most insignificant variables according to the p-value until all variables left in the model are significant? Can anyone point me to a good reference on this subject?

Thank you a lot!",127udio,Neverstop50,1680287096.0,14,0.9,"[""> Is it correct to start with the full model (the model with all possible explanatory variables) and iteratively remove the most insignificant variables according to the p-value until all variables left in the model are significant? Can anyone point me to a good reference on this subject?\n\nNo! No! No! No!\n\nFirst, p-values are a terrible way to prune features/variables.  That's not how you use p-values.   Secondly, after you prune features your p-values will be biased.  Finally, what you describe is called backward stepwise selection, and is now considered a terrible way to do feature selection (e.g. https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/, https://statmodeling.stat.columbia.edu/2014/06/02/hate-stepwise-regression/, https://twitter.com/f2harrell/status/1276575455982825473?lang=en).  A better choice is something like lasso regression, but if your goal is hypothesis testing then lasso might not be the best choice because the selection procedure of the lasso will also bias the p-values.  \n\nIf your goal is hypothesis testing, then you need to either choose through domain knowledge the variables to include, or include everything and be careful with your interpretation.  Or maybe someone else has other ideas.  But under no circumstances should you do what you proposed."", 'By far the most important reason to include or exclude variables depends on something you haven\'t said anything about. What are you analysing? And with what? \n\n&#x200B;\n\n>Suppose that the aim of the analysis is hypothesis testing, point and interval estimation.\n\nIf you *really* care about the hypothesis test achieving nominal coverage, standard errors and intervals not being misleadingly small etc,  you have to either do no model selection (pre-specify the model and the variables included, without doing any analysis of how the variables predict the outcome whatsoever), split the data, doing model selection on one and estimating the selected model on the other half (and once you have done it, you cannot change it anymore). Or google advanced ""post model selection inference"" papers and follow their approach', 'if a linear combination of two highly correlated predictors explains the output then in stepwise regression youd likely drop at least one of them and end up with the wrong model', 'You should commit to a statistical analysis prior to receiving the data, based on your competing hypotheses. Adjusting the statistical model according to what makes the data look interesting will lead you to phoney conclusions (""p-hacking""), even with the best of scientific intentions.', 'No that is stepwise regression which is bad and biases everything. \n\nWithout any a priori information on the relevant variables and functional form (like interactions) I would argue you cant really do this analysis for meaningu\n\nThe most you can get is feature importances, essentially.', 'Useful summary article on why stepwise selection should not be used: https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df', ""The model should be based on theory. If you want unbiased coefficient estimates, it's generally better to keep all variables in the model - unless some of them are likely to be colliders, in which case they should be thrown out."", ""Thank you for the detailed answer!\n\n&#x200B;\n\n> or include everything and be careful with your interpretation. \n\nSuppose that I have 10 explanatory variables available and that I include all of them in the model. Suppose that only one variable out of 10 is statistically significant. Why should I leave the other 9 insignificant variables in the model ? Wouldn't that just enlarge the confidence interval of the only statistical significant variable?"", 'True, but as I said in the original post, I supposed that there are no highly correlated predictors', ""You're thinking about this completely the wrong way, and it seems like you're confused about what the parameter hypothesis tests are actually testing.  The parameter hypothesis tests test whether the parameters are significantly different from zero, not whether they are useful in the model.  That's a completely different question.  Hypothesis tests are *not* the way to choose a model.  There are much better choices out there such as AIC, BIC, lasso, ridge, etc.  \n\nAdditionally, if you had read the links I provided above you would see that \n> It yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\n\nSo by doing your proposed procedure, you're biasing your coefficients and rending your final hypothesis test useless."", 'The thing is that if you throw out the other variables the resulting confidence interval might shrink but that is not a good thing because the reason that the interval is smaller is simply because its wrong. So getting a small confidence interval using a bad model means that it might look nice but at the same time its useless because  its not actually the confidence interval you are looking for but something else. \n\nAlso a variable being not significant does not mean that it is useless or that it doesnt belong in the model. It just means that there is not enough evidence to say that it is useful.', 'nonzero correlation impacts your pvalues', ""> doing your proposed procedure, you're biasing your coefficients and rending your final hypothesis test useless. \n\nI think OP might be interested in estimating the parameters / coeficients themselves, not estimates of predictions. Hence their interest in confidence intervals around the coefficients. It's hard to tell and they're not being explicit, despite being asked for clarity. \n\nI agree with all your comments, but your advice seem geared towards helping OP develop a predictive model. As opposed to doing some kind of research where the coefficient itself is of interest. Which would warrant different advice. But again, hard to say based on OP's lack of clarity."", 'You can perform stepwise regression based on AIC and you can perform stepwise regression based on p-values. None of the link you provided specify how stepwise is performed. So it is not clear to me which stepwise regression they are talking about. Is it based on AIC? Is it based on p-values?', 'I am not interested in making predictions. The interest is in the value of the unknown true regression parameters (estimate, confidence interval, hypothesis tests)', ""It doesn't matter, stepwise regardless of the method is bad.  Even when I was taught it in the early 00's people knew that stepwise with p-values was bad and AIC was preferred.  But then later the field realized that stepwise in general is a bad idea."", 'Thanks for the clarification! \n\nI disagree with u/timy2shoes that stepwise is a poor choice in this case. However, I agree with his comment that iteratively removing predictors so that your confidence interval shrinks is not okay. If you\'re doing that, you should use a held-out dataset, as he suggests. Otherwise you\'re p-hacking.\n\nIf multicollinearity is not an issue, as you state, AND you don\'t expect the predictors to be related to each other in specific ways, AND all the other assumptions of multiple regression hold, then I suggest you compare two models using a likelihood ratio test: one including EVERYTHING EXCEPT your predictor(s) of interest, and the other including ALL predictors. If your predictor(s) of interest add any additional predictive power, the likelihood ratio will show that. Google: ""compare two models using a likelihood ratio test"" or ""conduct chi-squared test comparing two models using multiple regression."" Something like that.', ""> It doesn't matter, stepwise regardless of the method is bad.\n\nThe context matters a great deal. Stepwise is a poor choice for selecting variables for a predictive model. But it is not a bad choice if you're trying to learn something about the predictors themselves. This is an entirely different research context, and stepwise regressions--as tools for constructing likelihood ratio tests (with -2 log-likelihood)--is totally acceptable.\n\nEdit: I understand the issues with stepwise regression. I'm not talking about following some arbitrary algorithm to end up at a predictive model, which is the main issue with most stepwise regression. Perhaps it is just semantics, but it seems like comparing two models using a likelihood ratio test--a reasonable approach when justified with prior theory or as a conservative estimate of the predictive power of select predictor(s)--could be considered a limited form of stepwise regression."", 'Comparing 2 models is ok, and the likelihood ratio test will give you that for nested models. However, the issue arises when you do that more than once. Now youre introducing bias through model selection. Theres a whole field of statistics that studies this problem called selective inference. Its interesting stuff, you should check it out.', 'As the links I provided above detail, stepwise regression biases both the regression estimates and their p-values. And therefore is a terrible choice for learning something about the predictors.']"
[Q] measuring user persistence,"I am working with a data set that tracks a daily distinct count of user_ids that meet a specific criteria. There is some noise in the data and I want to differentiate groups that have high and low persistence.

An example:

Let's say that one group has an average daily unique count of 100 IDs with relatively low variance. After 30 days I have a total unique count of 200. If I use DAU/MAU I get a stickiness of 50% but that feels like a poor measurement of how persistent each user is. If over the month I had the same 100 users every day I would see a total monthly count of 100 vs if it were a different 100 users I would see 3000. I normalized over this range and came up with 1 - (monthly_count - daily_average)/(daily_averagex30 - daily_average) (not simplified on purpose)

In the example above that means that every day on average, ~97% of my daily users are repeats. The remaining 3% is either noise or net growth.

Is this calculation commonly used anywhere and are there pitfalls to relying on it?",127thqk,tangent1001,1680285360.0,5,0.86,[]
Help Needed [Q],"Ive never had to take a stats class, and Ive run into an issue that I think someone with stats knowledge can help me on.  

Im having a competition at work between my teams, and I dont know how to apply equal weight to the points.  The winning gets 2 days off work, so Im trying to make it fair.  

I have 5 groups, all of different sizes (ranging from 2-21).  Each individual is given a set number of points in various categories.  I am trying to identify which group has the highest totals.  I can easily determine the average number of points per group, but because the groups are different sizes and some people have zero points, the overall averages seem skewed.     For example, the group of 2 has one person with 60 points and the other has zero, their average for the group is 30, but the group of 21 has 2 people with points  10 points each and the Rae have zero, making the group average .95.   

How to I make it more equally weighted?  Im using an excel file.  What type of formula would I use?  What term(s) can I Google to teach myself? 

Forgive me if my terms are confusing or incorrect.",127pkt5,Sdado2,1680277578.0,0,0.5,"['If the purpose is to determine a winner, Im curious as to why youre trying to average at all. Wouldnt total points make the most sense?', ""In general, an approach is to trim the means.  That is, say, discard the highest and lowest scores for each group, and then use the mean.  This obviously won't work if your group is as small as 2.\n\nTo figure out what statistic you want to use, you might create some toy data, and see how you ***want*** to rank the groups.  That may give you insight into what algorithm you think you should be using."", 'Can\'t you give the days off to the individual people with the most points instead of doing it by team? Doing it by team is extremely unfair because (a) difference in sizes, (b) your example of the 2 person team! One person is among the one with most points and the other has no points! So person with many points has no chance simply because the coworker is lazy or doesn\'t want to participate.\n\nThe whole thing was set up very poorly. No way you can come up with a fair ""formula"" or anything.\n\nAre individual people\'s points dependent on support they are getting from their team? Or are they truly individual points? If they are truly individual, why give anything to the team?', ""So in my humble opinion is the game was unfair. There is also no fair way to weight/handicap teams with more people vs less people. If I were you id probably not give something as major as days off at work over a silly game either. If you are going to do something this big you'd probably need to design the game to be fair. Equal teams, equal departments, etc. Anywhoo this seems like you are begging for a lawsuit if its real."", 'I thought about that, but the teams are different sizes, so if all else is equal, the larger team will always win because each person in the group receives points for the different areas.  \n\nIm probably explaining it poorly.  \n\nThere are 18 different categories in which each person can acquire or lose points.  Team A has 2 people whereas Team B has 21.   They are being evaluated on the same criteria.  So if Team A has one person receive 60 points in one area, and the second person receives 0, then the average is 30.  Team B, same criteria, two people receive 10 points each, but the remaining 19 get 0, the average is only .95.   \n\nIm trying to level it so the team size doesnt give one group an advantage over another.   Idk if this makes sense, sorry for the confusion.', 'Interesting.  I didnt think of that.   Ill look at my data and see what the results look like after trimming the means.  Thank you!', 'Its a team effort.  Its designed to bring the teams together, which it has.  The teams are performing better now because they each have individual ownership of their contributions.   They have been motivated to do the little bit extra to bring their individual scores up, therefore contributing more to the team score.  \n\nThe concept is working perfectly and doing exactly what I need it to do.  I just need to figure out how to do the math.', 'Yeah, not a game.  Its a sustained team exercise that takes place over the course of three months.  Points are awarded based on specific performance metrics, personal and professional development, community involvement, etc.  Its leadership 101 type stuff.   Raises morale, improves work performance, and builds confidence, and creates a sense of camaraderie as well as ownership of actions.', 'But in those situations, some people also have incentives to be the free rider, particularly in big teams. Why even put effort if you can have points =0 and still get the prize?\n\nMaybe you need to find a way to penalize teams that have individuals with 0 points. Unless there is a problem with the way points are awarded. Are any of the 60 points of team N=2 thanks to the person with 0 points? Is that person getting 0 but should they have points? If people that got 0 is because they didn\'t contribute, then the mean might be fine; their team gets a 0.95 because only 2 people did something.\n\nI think what you are seeing is that individual performance went up, because like you said, now there is ""individual ownership"" and not necessarily ""team performance"".', 'Hmm, sounds like a game to me, at least by definition. You are assigning points based on certain tasks and attributes, not to mention luck associated with who you get on your team. This is incredibly unfair as smaller groups have it so that dead weight drastically affects them, while also making it so that its easier to doctor dead weight as well. Large groups may be more difficult to control but also hide dead weight easier.\n\nMight be more fair if you let people choose their team mates.', 'Individuals that dont contribute receive negative points towards the teams total in whatever category they cannot meet the minimum performance goals.  Every one of my 60+ employees have increased performance in multiple areas since creating the incentive.', 'The groups are task assigned.  Certain individuals are put in their work groups based on their specific specialties and functions can only be performed by certain individuals.   Individual team selection will not work in our industries dynamic.   It also defeats the overall purpose of the competition, which is to bring each group closer through self improvement.because they understand their individual effort effects the whole of the team.  Again, the concept is solid and it works.  I need to know how to make the math work, which Im admittedly not good at.  \n\nYour dissecting the source of the data if this was outlined a something else instead of a team building exercise with a generic background, I dont think there would be any comments that could help be figure out how to calculate what I need.', 'Mmmm well one of the important parts of stats is to know what the source data actually means/its validity. You can plug stuff into distribution functions and think about probabilities, but it doesnt mean anything unless you understand what the data you input actually means.\n\nAnyway admittedly my strong suit is more about sampling as I am a QA professional. Just having trouble seeing any level of consistency to judge the groups on.\n\nMaybe this time don\'t give an award and think of a more fair way to judge them next year like a KPI on leadership/team work for each individuals. While it is important for people to understand how they affect each other on the team, it is equally as important not to punish someone for something like a ""toxic hire"" they had no control over for example.', 'Good points.  Thanks for your input.']"
[E] Post Hoc analysis literature suggestions,"Hello! Could someone recommend literature on post hoc analysis? Specifically, on Bonferroni, Tukey's, and Scheffe's method.

I'm struggling with my Variance Analysis course and it will be my fourth time taking the final exam. We are using the book ""Applied Regression Analysis and Other Multivariable Methods"" Fifth Edition by Kleinbaum et al. However, the post hoc analysis section of the book only covers the basics and easiest types of problems, nothing more advanced. The exam problems are far more complex than what is taught.

Thank you in advance.",127ob6l,Ilikedishwashing,1680275076.0,5,1.0,"['There may exist better books, but when looking through my the texts this is what I found. \n\nA thorough review on Tukey can be found in Modern Mathematical Statistics (Devore and Berk). Moreover, Statistical Inference (Casella and Berger) provides alot of detail on both Bonferroni and Scheffe. Also, Applied Linear Statistical Models (Kutner et al) contains some brief info on all of them in its 5th edition. All of these discussions are in the (One-Way) ANOVA sections.', 'Following!', ""Thank you for your suggestions! I'll have a look at them.""]"
[Q] Help me with a problem that I've been thinking about for years,"Let's say I'm going in my car at 50 mph and I crest a hill and spot a red traffic light 100 yds in front of me. I know the light is red for 1 minute and green for 1 minute. There's a limitation on how fast I can accelerate and decelerate my car.

How should I accelerate and decelerate my car in order to maximize my speed through the whole interaction.

I used specific numbers but I really want variables.

I know that on average there's 30 seconds until the light turns green, but should I decelerate linearly? That's not clear to me.",127ayur,fastornator,1680239588.0,1,0.67,"[""The ideal strategy will depend on what exactly you want to optimize, but it's likely that an ideal strategy will be very difficult to find.\n\n- minimize the total time loss, measured when you accelerated back to full speed after the intersection?\n- maximize the speed at which you cross the traffic lights?\n- maximize the average speed until you reach the traffic lights?\n- maximize the lowest speed you reach, to reduce fuel consumption?\n\nFor all options you have to further specify if you want to optimize the best case, the worst case, the median, the expectation value, or something else.\n\nThe second option has an easy mathematical answer, although it's probably violating traffic rules: Stop well ahead of the traffic lights, so you can accelerate back to full speed ahead of them once they turn green."", 'The variable is when did the light turn red as you crested the hill. This is going to change all variables. If you hit the apex, still in full motion, as you saw the light turn, then the simple-ish answer would be to increase to at least 65mph to make the light just as its phased over. (60mph = 1 mile per minute or 5280ft per min, which is 88ft per second.) I say 65, because I dont know how long it could take the car to reach 5 more mph. One vehicle could take 1.5sec, while another could take .5 sec. If you wanted to ensure you made it, then increasing to 70 would ensure you make the light. But, if you hit the apex and the light was already red, you now have to wait until its green to start your timing. While not extremely important, one of the other variables is the slope angle of the hill. Now with knowing the slope, I would want to know the car. 50mph cresting one hill could leaving you soaring with little to no control for many feet, placing a great strain on the braking system, while another slope could leave end the most modest of vehicles cruising about. I could add in aerodynamics, but while it could help with acceleration/deceleration, I figure that it would be on extreme levels of variation that deal more in long linear equations than this one. Lastly, are you being monitored (police/intersection camera) at the light or on the street. If so, then decreasing is the only option. This then goes back to the variables of when the light changed to red to gauge whether to cruise or continue to brake.', 'Unfortunately you would be riding your brakes, but that would decrease enough for 1min to roll through the light.', 'Maximising speed (without direction) might suggest no gradual decreases. If necessary break hard and reverse direction at speed repeatedly until the lights change, or until a minute is nearly up.', 'I want to get to my destination as quickly as possible.', 'Yes, I meant to say that when I crested the hill I saw a red light but I knew how often the light turned from red to green.', ""No, I'm not being monitored There's no police involved. I think I need to decrease my speed but by how much is the question."", 'Not driving backwards (or off the road) is probably implied in the problem.', '4.1mph']"
[Q] Any spatial stats person here? Need help understanding how to choose a model for variogram [R language],"Im new to spatial statistics. How do I decide which model to use to fit my variogram (vgm function).
Do I just visually look at the experimental variogram and decide on the model or use or is there a more concrete way to make the decision like the way I can use Anova to decide between linear models in traditional stats.",127ahpn,brianomars1123,1680238140.0,33,0.96,"['A statistician might answer that you should select your variogram model based on some theoretical considerations about processes, not based on the data themselves. But this is usually unrealistic, and you have to use a criterion for model fit such as Akaike.', 'Common practice in applied geostatistics is to simply fit a variety of variograms models to the experimental variograms and pick one that looks best. This has the downside of underestimating uncertainty in the resulting model predictions, and also has an element of added subjectivity, but often does work reasonably well in practice.\n\nAlternatives do exist though. AIC or cross validation methods can be used to select from a candidate variogram model, or alternatively a flexible variogram that accommodates several others (especially the matern model) can be used.', 'Sorry, not to answer, but more out of curiosity -- for variogram, do you use coordinates as inputs?', 'There is the eyefit function in a package thats escaping me now an alternative is to use maximum likelihood to compare the fits of different covariance functions and skip the variogram', 'library(automap)']"
[Q] Any good sources for treatment of errors and error propagation? Like a for dummies version with more discussion on concept and basic math?,"I've been having a difficulty time understanding error propagation and covariance matrices. I've looked online for resources for these, but they're scattered university websites and clips, or they're designed for math/stat major students with just lines of derivations and equations. I'm looking for more like maybe a course textbook, that starts out with simple stats and error analysis, and gets to more complicated topics later, but with plenty of discussion on what the math is doing rather than just pure lines of derivations. I wouldn't mind more advanced math like linear algebra down the line (I don't know if it's possible to describe more advanced stats without linear algebra terminology), but I'd like the book to also break down the more complicated math and explain it (rather than just assume you know what the terminology means).

Any suggestions?",1275292,DrBobHope,1680224091.0,3,0.71,"['A good place to start with error propagation is reviewing your Calculus I, especially the Related Rates section that the professor always blows through because he\'s done teaching new derivatives rules and is anxious to get started on integrals. That, combined with remembering the basic rule for combining variances, will take you far: Var(aX+bY) = a^(2) Var(X) + b^(2) Var(Y) if X and Y are independent (add an extra 2ab Cov(X,Y) term if not.) If you know the rate at which one quantity changes with respect to another, you can estimate how much error in your output will be caused by an error in your input.\n\nSimple example: the volume of a sphere is V = 4pi/3 r^(3). dV/dr = 4pi r^(2). A sphere with radius 10cm has volume 4189 cm^(3). If that 10cm radius has a 1cm uncertainty on it, put in 10 cm for r and 1 cm for dr and get dV = 4pi(10)^(2)(1) ~ 1257 cm^(3).\n\nNow Check your answer by seeing that when r=9cm, V=3054 cm^3 (1135 less than 4189), and when r=11, V=5575 cm^(3) (1386 more than 4189). You can see that, on average, your estimate of the uncertainty in V was close to correct, but that the nonlinearity begins to matter when you have as much as a 10% error in r.\n\nSame general idea with two variables: the area of a rectangle is xy. What if x and y are both uncertain measurements? dA/dx = y; dA/dy = x. Var(A) = (dA/dX)^(2) Var(X) + (dA/dY)^(2) Var(y) = y^(2) Var(X) + x^(2) Var(Y). \n\nSuppose we have x = 201cm and y=502cm. Var(A)=50^(2) 1^(2) + 20^(2) 2^(2) = 4100: sqrt(4100) ~ 64. We estimate our area is 100064 cm^(2). That\'s a narrower bound than you\'d expect from looking at 19x48=912cm^(2) and 21x52=1092cm^(2) since we\'d be unlucky to have both errors be large and in the same direction -- but a wider bound than if there were only one source of error: 19x50=950, 21x50=1050; 20x48=960, 20x52=1040.\n\nSave the linear algebra on giant variance-covariance matrices until after you can solve the simple questions of the ""I measured the height of that flagpole with trigonometry, but I had a 0.2 error in my angle measurement and a 5cm error in how far I was standing from the flagpole"" variety. \n\nWhen you are comfortable there, you can tackle the general form. Given some big ugly function f(w,x,y,z), and a variance-covariance matrix describing the uncertainties in w, x, y, and z, you multiply:\n\nVar(f) = [df/dw df/dx df/dy df/dz] [V-Cov matrix] [df/dw df/dx df/dy df/dz]^(T)\n\nThere will pretty much always be a string attached, that the errors in each input must be ""small"".', 'Honestly, I have been in this field somewhat for years (propagating model errors and uncertainties through time and coupled domains) , and I wish I had a more fundamental understanding of some concepts. I hope someone has a good resource suggestion for us. \n\nI think the field of GD&T (geometric dimensioning and tolerancing) could have the right fundamentals. I have tossed around the idea of exploring that field anyways.', 'Thank you for this breakdown, but any recommendations for a book that goes through this?', 'I wish! It is often skimmed over very fast. A phrase to look for the index of a stat book is ""delta method."" Rice (the book my grad program used) spent about 5 pages on it. Bickel and Doksum give it nine. Wasserman\'s *All of Statistics* gives it two. Rossi gives it one (plus a problem set.) Freund, a popular undergrad book, does not cover it explicitly at all. \n\nBut at least if you work on industrial/practical problems you\'ll probably use it more often than half of the rest of the book put together.']"
[Q] Question about Medical statistics,"Hi guys, I am stuck for past 2 days on following problem.

I need to figure out treatment and period variable. any help would be appreciated

I have dataset with 3 variables Group Air Co A clinical trial was designed to assess the impact of low levels of exposure to carbon monoxide (CO) on exercise tolerance in patients with ischemic heart disease. A total of 30 nonsmoking patients with documented obstructive coronary artery disease and a history of exercise-induced ischemia were enrolled in this 2-period crossover study. The study period consisted of 3 days : a training day and 2 exposure days during which patients were exposed to either air or CO in an environmentally controlled chamber. On all 3 days, patients followed a bicycle exercise regime in which exercise was conducted at increasing work loads until angina, fatigue, or hypertension occurred. On the first (training) day of the study, all patients conducted a training exercise on the bicycle. Patients were then randomly assigned to one of two exposure sequences, exposure to air followed by carbon monoxide (Air:CO) or the reverse (CO:Air). The outcome variable measured was the difference between the duration of exercise (seconds) after the exposure condition and the duration of exercise recorded on the training day. The data for each patient are provided, Group 1 was exposed to CO first and then to Air, and group 2 was exposed to Air first and then to CO.",1274dzu,mhbb250,1680222351.0,2,0.75,"['> I need to figure out treatment and period variable. any help would be appreciated\n\nIt looks like you already identified the treatment variable. Which consist of two conditions. (Although I\'m surprised they didn\'t include a control (only air) condition): \n\n> Patients were randomly assigned to one of two exposure sequences, exposure to air followed by carbon monoxide (Air:CO) or the reverse (CO:Air).\n\nAs for ""period"" variable, I assume that is defined by the 3 study days you refer to:\n\n> The study period consisted of 3 days : a training day and 2 exposure days during which patients were exposed to either air or CO in an environmentally controlled chamber.\n\nSo you measure your outcome at day 1 (baseline), and then have 2 days of outcomes after treatment.', 'Review your class notes.', 'Would this study benefit from 4 conditions: co:air, air:co, air:air and co:co? I guess it depends on the sample size?', ""I have, I am stuck so that's why asked. thanks anyway""]"
[Q] Panel Regression,"Im doing panel regression where y is the correlation between bitcoin and 11 cryptocurrencies and x is market capitalization. Im trying to figure out whether market capitalization effects the correlation between cryptocurrencies. As data is over 5 years, 5 increments of market cap is used (55 overall), however there is only 11 correlation coefficients. Should I measure the correlation between each year? Additionally, can the regression tool on excel be used for accurate results specifically to this?

Any assistance would be appreciated, many thanks ",1271m71,Jookygamble,1680215807.0,7,0.77,"[""Let me try and rephrase this to see if I understand.\n\nYou have 5 time periods, so T=5. Within each time period you have 11 observations, one for each cryptocurrency, so N=11. Your outcome measure for each is the correlationn bewteen their... what, value in dollars? and bitcoins value in dollars or something... is that right? It does seem a little weird that bitcoin - a crypto currency itself - is singled out for calculating the outcome. \n\nthe model sounsd like it's y\\_it=apha+beta\\*x\\_it+epsilon\\_it. \n\nWith panel data, people usually assume epsilon\\_it are not independent of each other, because within currencies there's likely correlation over time. There's lots of ways to model that correlation and try to account for it. One way is to assume epsilon can be decomposed into a a currency-specific term plus another term, as in epsilon\\_it = c\\_i +phi\\_it, where c\\_i is constant within currency over time and then you assume phi\\_it is independent over time. In that model, there are a couple estimators you can use, such as the fixed effects estimator or a first-difference estimator. Both can done in excel with some transformations of your data. For example, for the first difference estimator you'd transform your data to be the change in these things from one year to the next:\n\n(y\\_it - y\\_it-1) = beta \\* (x\\_it - x\\_it-1) + (phi\\_it-phi\\_it-1)\n\nWhere the c\\_i and the constant drop out. You'd now have 4 years, because the change is not observed in year 1. \n\nThere remain several challenges though: you have a small sample and your outcome measure - a correlation - is by definition not normally distributed. You therefore really can't trust any standard errors you get out of this model. You would want to have many currencies per year - over 50 would start to make people more comfortable about the standard errors. \n\nAdditoinally, x is not randomly assigned, so it's probably a leap to say you would estimate the degree to which market cap actually effects the correlation. Rather than estimating a causal parameter, you'd simply be describing the correlation between market cap and this correlation measure.""]"
"[Q] I’m a novice R user. I have a dataset of events of varying duration that take place over the course of a year. I’d like to predict the likelihood of an event occurring after one days, two days etc. Can anyone suggest tutorials or methods for doing this?",,126r715,Danzadeviejitos,1680191976.0,3,0.67,"['To make your life so much easier I would suggest to get some basics on R first', 'https://rpubs.com/riazakhan94/arima\\_with\\_example', 'You can describe the data (column names and values) to chat gpt and ask for it to write a starter code.', 'Remove the escape slashes on the underscores BTW, otherwise link is broken', 'Thank you!', 'Do you use Relay?', 'no just clicking on the link', 'Works fine for me.', ""oh, might be because I'm using old reddit"", 'Yep, tried it both ways -- breaks old reddit, works on new.\n\nYet another way reddit helps to make life miserable for moderators']"
[Q] Sources for Self-Study of Application?," I'm looking for sources to gain knowledge in applying statistics. I am in finance in the manufacturing industry, and my work includes corporate finance as well as operational/integrated forecasting models. I finishing up my Masters but most of my exposure to statistics has been formula based and the application and method are already matched for you in the problem. That doesn't help me in the real world when I need to create a non-financial model and then measure its behavior vs. expectations. I have plenty of experience modeling at established companies with mature processes, but my current employer is very green so I am starting everything from scratch.

I have full access to Cengage and plan on renewing it for a year past graduation. Is there a textbook anyone could recommend for self-study? Or another learning source, paywalled or not? I am specifically looking for how to apply statistics, how to choose the correct statistical method for a situation, etc.",126obem,NotBatman81,1680185464.0,13,0.94,[]
[Q] Need help with looping/repeating syntax in SPSS,"Hi all.

I'm looking to create a loop for the following syntax, so the syntax will not only create a new variable for the year 2013, but for the years 2010 through 2022. Everything with \_2013 should thus be replaced in the loop by the correct year for the loop (i.e. 2010 through 2022'. I'm using SPSS 25.

compute position\_2013=0.

if any (var1\_2013,90,93,99) or var2\_2013=999 position\_2013=6.

if any (var1\_2013,31,32,33,34,61) position\_2013=5.

if any(var2\_2013,200,310,320,400,500) position\_2013=4.

if var2\_2013=100 position\_2013=3.

if any(var3\_2013,-2,-1,0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,20,21,22,23,24,25,26) position\_2013=2.

if any(var3\_2013,9,16,17,18,19) position\_2013=1.

exe.

value labels position\_2013 1'test1 2 'test2' 3'test3' 4 'test4' 5'test5' 6 'test6'

Thanks in advance!",126hz2i,StirredEggs,1680169638.0,1,0.67,[]
[Q] Masters Statistics Berkeley vs UChicago,"I got into both Masters programs in Statistics.  
I plan to take the PhD level courses regardless of which program I go to.  
On one hand, I want to go to UChicago because I believe there are a lot more course options and I could learn more statistics. If I go to Berkeley, there's only around 4 thereotical courses and 2 applied courses. Chicago has tons of optimization, applied math, and thereotical statistics courses.

  
I love learning statistics and doing thereotical math, but I don't know anything about research, so I'm afraid that the thesis at Chicago will be a pain in the ass because I hate writing. I don't care too much about the time or cost differences. What program would you go to? Is it really true like I believe that Chicago has better courses?",126dm6b,Ass_Ripe,1680156207.0,1,0.99,"['UChicago', ""If you got into both programs, you must be a pretty strong student to start with. My advice is to go for the PhD -- try to switch gears once you get there. After you graduate, the first and very possibly last thing people look at on your resume is the highest degree completed. A master's from a strong program might not outweigh the lack of a PhD when you apply for jobs -- you don't want to chance it. \n\nI know you said you don't like writing, but once you've done the work for a dissertation, the writing is a lesser problem. Most people just muddle through it, and once you're done, you'll be glad you did."", ""They are both very good programs. You could go either way and it'd be a good decision. I think you need to focus on the other part of this, which is:\n\n(a) level of support in the department; is there a potential mentor? Are grad students happy? If you stayed for the PhD program directly, how is the funding? (I'm assuming UChicago would be much better stipend). Do you have an opportunity to work with a professor while you do the masters?\n\n(b) living; would you prefer to live in Chicago or the Bay Area? Berkeley is expensive and you might have to live further from campus and commute, while in Chicago you can take the train and it'd be an easy commute if you don't live close to campus."", 'Hey, would u be willing to discuss more about your thought process while applying for masters? Ill apply this year to get an admission next year, so Id really appreciate the help if possible. \nAlso, all the best for your future :)', 'Alright thanks, I heard Chicago is a feeder to good phd programs so thats probably it.', 'I did USNews, looked at the T20, but this is because I had good stats. I applied to 7 programs which matched with me by either location and theoretical phd coursework available to me as a masters. I looked at the coursework, and chose what I thought was best. I didnt do Stanford as I was too late.', ""Well, yes, but Chicago is already a good PhD program, so what I'm thinking is you can just stay there. (Same for Berkeley, what the heck.)"", 'Could you share your stats for your undergrad and GRE? Did you have prior research or internship/ work ex?', 'Did you apply to MS or PhD programs? Also were they funded MS?', '169 gre quant when I applied. 162 verbal. \n4.0 math and stat double major. \nInternship in big bank. T100 Public state university.', 'Applied to 6 masters stats and Columbia phd. Got rejected from Columbia phd  and Wisconsin masters. Got into a &m, nc state, Chicago, Berkeley, ucla. Nothing funded so far );', 'Great stats, thank you for a response and all the best with whatever decision you make', 'I see. Yeah I got into no phd programs. 0/13.', 'Looked into your profile. Did you get into masters in applied at Michigan?', 'Yes, and MS statistics program at a smaller school, which is where Im going. I got into michigan but got no funding, but I got it at the smaller school']"
[E] Undergrad grades for Stat PhD,Im a 3rd year stat undergraduate student taking differential equations even though its not a requirement for my major. Im on track to get a C in the class unless something changes dramatically but the highest I could get even if I try really hard is a B. Is it worth trying to improve my grade or should I put the class on Pass/Fail. Does that look worse than the GPA hit? Im involved with research and I have a good relationship with my universitys former graduate admissions director for stat (I do research under him).,1269ew6,Foreign-Ad-5118,1680145188.0,5,0.78,"['You really dont want a C in a math class when youre in stats, especially if youre going for a PhD', 'I would REALLY try to get a B. I have two Bs on my transcript from when I was a freshman/sophomore (aka an idiot), but turned out just fine applying. \n\nDiff Eq can be tricky  this sounds like bad advice, but do textbook problems till youre blue in the face.', 'I failed an app cycle for PhDs but people who gave me real responses instead of a canned email were people my advisor knew. So I will say you can take the C and be unscathed but you have to make it up in an equally difficult class like real Analysis, or a graduate course that uses Diff Eq heavily (stochastic Optimization Courses come to mind). \n\nWhen it comes to apps rely on professors your advisor has connections to as they will get honest consideration and not get GPA filtered as easily.', 'Take a W', ""It is never a good idea to quit a class you're passing because of a grade. Especially with the price of tuition nowadays.\n\nGo to office hours, do extra homework and you should end up better than a C. One class doesn't make or break your GPA. Eventually its denominator is 120 credits in the semester system. Linear algebra and vector calculus are the most important courses for grad work. Being involved in research is most important."", ""Don't worry, a couple of C's won't harm your chance of admitting to a Stats Ph.D. program.\n\nYour research experiences and aptitude matter the most.\n\nCheers!"", ""If the course ends in about a month, can't you ask for extra credit? Students do all kind of annoying stuff to improve their grades, so talk to the professor during office hours. Do extra exercises, etc.\n\nDon't put it on Pass/Fail. That's worse than having a B. \n\nAlso, the grade is only relevant in context, which can be explained in recommendation letter. If everyone gets an A and you got a B or C, ok, that's bad. But if it's a difficult class where almost nobody gets high grade, then a B is not bad."", 'This is a lie OP. Take my advice as someone who just got rejected from all 12 phd programs in stats with a 3.5 gpa with no Cs and mostly As and Bs. I had Bs, and was told the same shit about how I would be fine, but well here I am.']"
What job titles am I looking for?? [C],"Hopefully will be a PhD in Statistics, on track to defend in November. I like experimental design and Im a statistical consultant at the university currently. I enjoy statistical modeling, inference, and helping translate clients research questions into statistical processes. I dont want a job where one of the main responsibilities is scripting or using SQL to interact with databases, etc. Obviously Im aware of jobs with the title Statistical Consultant, but what other job titles should I be investigating that are maybe less known? 

Also for the love of god everyone and their dog are data scientists nowadays.",1266k4o,PostCoitalMaleGusto,1680137734.0,12,0.84,"['Biostatistician and clinical data analyst are a couple that come to mind', 'Quant', 'Follow the money', 'Being a statistician does not make you automatically a data scientist. Just be aware of that pitfall many people tend to make', ""You just want to do the best part of the job and leave the actual data and programming work to grunts? Stay in academia. There's a steady stream of students to use."", 'Both of those jobs are 80%+ data wrangling.', 'I mean, as far as modern statistics go, I think you get fairly close', 'Youre just oversimplifying the post and being confrontational. Incredibly unhelpful.', ""That's a gross overgeneralization. My official title is biostatistician and i'd estimate that more than 50% of my week is spent consulting, reviewing protocols and papers, designing experiments, and presenting results to clients. The rest is wrangling and modelling. The higher the seniority of the position, the less programming is typically involved."", 'Not to mention mostly regulatory writing.', ""No. There's dedicated programmers for that.\n\nNot that you do zero programming as a statistician, but it's not generally an expectation it's the majority of your role. 10-30% total."", 'Depending on how you define data scientist :)', ""I'm honestly just excited for you to get into the real world and realize that 99% of stats projects are either well-designed studies that don't require complex modeling or crappy studies that use regression without even checking assumptions and are then P hacked to get the desired result."", 'I do consulting and Ive worked, and youre hitting me with the clich, well wait for the real world youve gotta be better than that.']"
[research] Help with Confidence Intervals,"I understand the basic idea of confidence intervals and was wondering if you could help me make sense of some data. 

Correlation analyses on the same sample, testing for moderation. So we did a median split on our data, and did a correlation for the high on this and low on this group using two variables. 

Our output didnt give us p values, it gave us CIs. Heres an example of the data:

Low group: r = -.54, 95% CI [-.81, -.16]
High group: r = .11, 95% CI [-.55, .45]

Interpretation: Is it safe to say that this is a significant finding? As in, low groups r is outside of high groups CI, and high groups r is outside of low groups CI. 

Is this how to interpret?

Thank you.",1265ve4,hopelesslyhopeful4,1680135993.0,2,0.75,"['The best way to understand this is to draw it out. Plot your point estimates and the related confidence intervals. How do they look?  \nThen review what a CI means and how you can interpret your data.', ""I'm missing something. The low groups r is 0.54 but the CI is -0.81 to -0.16. Shouldn't the r be inside the CI (and roughly near the center?). Are you sure the low group's r isn't -0.54 ?"", "">  So we did a median split on our data, \n\nsee the links offered by Stephan Kolassa here:\n\nhttps://stats.stackexchange.com/questions/43608/which-test-to-use-for-a-set-of-data/\n\n\nor see p128-129 of:\n\nweb.archive.org/web/20120412171759/http://www.unt.edu/rss/class/mike/5030/articles/makefriends.pdf\n\n(or indeed any of literally dozens of other references which point out problems with this disturbingly widespread notion; I don't have my old references on this, but if you need more I can probably dig a few up)."", 'As a general rule of thumb, when two CIs overlap, you should explicitly test the differences. Its not sufficient to say correlation two is outside the CI of correlation one therefore they are different. The reason for this is that there is uncertainty with both estimates; when you just compare to the CI, you are basically assuming the second correlation is estimated with certainty. \n\nTest it! You can do this in a regression by interacting x with a dummy variable for the high (or low) group.', ""They are not significantly different because their confidence intervals overlap. A confidence interval doesn't give you a rejection region. It says with given probability (95% in this case), the true parameter lies in the interval.  Determining if two confidence intervals overlap is equivalent to a two-sided hypothesis test of two group differences."", 'I previously drew it out, and came to the conclusion that they are statistically different because they are within each others 5% rejection region. Its my first time using CIs in reporting data so I wanted to make sure that this was correct.', 'Thank you for the correction. Edited now. Yes -.54. Do you have suggestions as to how to interpret?']"
[Q] Anyone have experience with ChatGPT-4 and statistical analysis?,"I'm in the process of performing statistical analysis on a large datasets using spss for the first time. I don't know a lot of statistics as this is my first time doing it, and its a pretty steep learning curve.

In the process of analyzing I got the idea to give GPT-4 the context of my study and just literally copy-paste the data from Spss, and it seemingly was able to analyze the results, in the context of my study..

If GPT does this accurately its a pretty amazing learning tool. I actually find it really fun doing different analysis and chating with GPT about what it means in the context of my study, I just don't know to what degree I can trust it.

Any statistics pros that have experience with this?",125yvdy,cooolaids,1680119782.0,40,0.73,"['Use GPT as a super-Stack-Overflow. Do not use it as you.', 'it couldnt calculate pearsons r given two lists of numbers. I am begging you, do not do this.', '> Anyone have experience with ChatGPT-4 and statistical analysis?\n\nOn statistics, my only experience is in correcting its errors when other people  use it; the errors can sometimes be hard to spot, and sometimes deeply consequential - its a confident idiot. \n\nThis repeated experience has been more than enough to convince me not to trust it on anything that needs to be correct.\n\n(edit: minor clarification)', 'Please dont do this', '""I want to do something, but don\'t care to understand why or how it works.""', 'How will you confirm the results you obtain from this procedure?   In other words, how will you validate the code written by the AI?', '> If GPT does this accurately its a pretty amazing learning tool\n\nIf GPT does this accurately, why are you needed?', 'Its predecessor made some pretty jaw dropping errors on not particularly tough topics (I asked it help me explain to a non technical audience). It got better with time but Id for sure be wary of what it has to say.', 'I pasted some spaghetti code someone wrote to just see what it could do, and was able to break it down and somehow understand the context of the 2 letter variable names and what kind of statistical analysis it was doing. Pretty wild, and saved me a ton of time trying to decipher the code myself', ""From what I seen, GPT-4 can only be used for questions where you know, or can check, the answer. It will give you confident bullshit (including made-up references), and double down when you push it.\n\nI've seen it produce correct R code, along with incorrect output, and when I asked it about the discrepancy it claimed to be correct and that I had probably messed with my data before running the analysis.\n\nSo go ahead and ask it to write code for you, but don't use it before you're sure you understand every line. And never ask it to do data analysis directly, as it's extremely unlikely to get it right."", ""When you are comfortable doing it yourself, why not use it as a shortcut before correcting the analysis manually, to provide a skeleton of the analysis.\n\nIf you are studying, don't. The point of studying is to learn how to do it yourself.\\\nIt is the same logic that says you shouldn't make someone else do the work for you."", ""I've been using it to teach myself and show me possibilities within R and Python. It's impressive but not completely accurate"", ""please don't put fuel on the fire of ai nonsense posing as sophisticated, reasoned analysis. it's going to happen either way cause it's cheap and most companies care about validating their preconceptions more than being correct, but they don't need our help doing it. you shouldn't trust it any further than you can spit, it doesn't know statistics, it just knows how to arrange words similarly to us"", 'gpt-4 (apparently) got around 85% on the ap stats, so even if your questions are  of that level, expect around 15% to be convincingly worded BS', ""I use it for R coding because it is faster than reading stackoverflow forums, it gives me the codes and I can test it (I don't take everything it says for granted)."", ""GPT is a NLP model. Remember this. It's literally trained on the entire internet, but whatever analytical concepts it might have, are limited to what it's been trained on. It doesn't understand it. It can just put together an output that best fits your question based on your input. It doesn't have any analytical or statistical insight as such, it's merely trained on all the language used. So yes, it can give you very thorough and detailed answers, descriptions etc. They can also definitely be correct, however, it's a language model. Remember this.\n\n\nIt can be useful for helping with the coding part, but I'd be very, very, very careful (I genuinely just wouldn't) using it to analyse my findings. If you already don't understand what you're doing, this seems like an even worse idea. You'll have no way of verifying what it spits out. Just don't do it."", 'It\'s better to look for examples of syntax that do what you want. It\'s more sensible to think of ChatGPT as an average of all answers, including wrong ones. What you get will not represent your thoughts and your needs, just some consensus that might or might not fit.\n\nAlso, when building up commands from menus, always use the ""Paste"" button, so you can see what SPSS is doing. You will learn about its command structure (mostly logical, but sometimes maddeningly not), and see what it is actually doing. You can save and recycle pasted code as well, keeping the syntax (.sps) file.\n\nHope this helps somehow.', ""Its a language simulator that predicts likely words. It doesn't know what it's doing. Don't use it to do analysis."", 'Something worth bearing in mind is data security. Even once AI can actually do this work for you, uploading to a giant public database carries risk if its sensitive information. I imagine you could breach data protection laws if it was a private data set.', ""Yes it can help with the topics that you don't understand well!"", 'You can use ChatGPT to write your results', 'Just like you explore different things when you are looking to learn, chatgpt also an option. The best option is always your teacher', '>\tit **seemingly** [emphasis added] was able to analyze the results\n\nThats the whole point. Its not trying to analyze results. Its just trying to make you think that its analyzing the results. Thats a really big difference.', 'I could offer you a free license to this, if you are interested.   \n\n\n[www.statgenius.io](https://www.statgenius.io)\n\n&#x200B;\n\nGood luck with your journey!', ""Do not use to do your statistics.   \n\n\nI show here how it can't get the Sample Standard Deviation correct most of the time and that it is simply not reliable.   \n\n\nhttps://youtu.be/7RmzXz-rR\\_A"", ""What confidence level do you need in your analysis?  And what's your proposed process for double checking ChatGPT's work?  This may be sufficient for a quick and fast analysis, but may not be suitable for peer review study or for making fiscal/business decisions without additional collaboration."", 'I dont if you can feed you me spss dataset into ChatGPT. It wont take on variables metadata even if you are able past cases and few variables. I use gpt to 1) let me know right statistical analysis to implant  2)structure, prepare & do the analysis using spss syntax - gpt can write syntax for you. You can ask specific question related to spss, it will be able to answer.', 'Our company has warned us that ChatGPT stores anything you ask it, you may want to be careful what you are sending it. Depending on your data, you could be violating confidentiality rules by asking GPT to analyze the data for you.', ""This discussion is really interesting and I hope is going on in workplaces and university departments everywhere. I have used chat GPT to help me decide on what model to use given a study design and its answer matched exactly what I had been planning to do - my use of it is an interesting little case study though:\n\nI'm a programmer/bioinformatician and often I do not \\*fully\\* understand the math behind the statistics I am applying to problems because I don't have a degree in math or statistics (though I have taken many courses related to/required for my degrees' fields). So what do I do? Like tons of others, I use stack overflow, R package manuals, vignettes, etc. to put together workflows. I definitely try to learn and re-learn the critical basics, but I am going to miss nuance etc. and there is definitely room for mistakes to be made when the human user does not fully understand the math behind what they are doing.\n\nWhich is just to say...yeah, I bet chatGPT is going to make some mistakes, but I have no reason to believe those mistakes will be worse than the mistakes someone who thinks/needs to use chatGPT would already be making. If you couldn't do your job without chatGPT...that job is probably not the right fit."", ""100% agreed.  I use it as an r buddy but do not trust it's statistical output"", 'This is the best way to think about it in general.', ""This is what I've been doing recently and its been hella nice. It's like a personal TA you can bug without guilt"", 'And cross validate everything it says. Especially if you dont know statistics yourself.', 'I dont use it to calculate anything', '""confident idiot."" \n\nPerfect.', 'This has been my experience as well and it deeply frustrates me how even professors have gone the route now of ""I don\'t need to learn R, I can just ask chatGPT to code the analysis for me"".', ""It's pretty good at summarizing the chapters in the text book, and providing the standard format of the formulas, explaining the variables, but I tried to walk it through a simple linear regression, 10x values, 10y values, and after solving for sum _xy all the numbers stopped matching the source data. Not much RAM per instance I think."", 'ChatGPT would a lot less known if every other answer was hmm dunno, let me get back to you', 'or even whether it works', ""This sums up the whole field of AI, doesn't it?"", 'need to input the question mate', ""It's better than Google for getting clarification on points you are learning, cautious acceptance of the answer though"", 'Ive used it to translate Python into R, which was very handy.', ""That's a much lower  bs ratio than I have in most exams..."", 'This is dangerous advice. GPT-4 routinely produces plausible-sounding bullshit, so it should only ever be used for things you do understand - or at least, things where you can easily verify the information.', 'I dont use it to do my statistics. Spss calculates, gpt tells me what it means(hopefully). That being said Idont use it as my primary source, and I take what it spits out with a grain of salt. Also im fully aware that it sucks at math', ""It's not for a peer review no. Personaly i want to be 100% sure. Its just a larger paper for my studies."", 'Yes it probably makes mistakes, I just posted an example of how I use it in the comments section.  Although It makes mistakes, i find it hard to believe it makes more than me, basically got introduced to spss and statistics a week ago.', 'You said in your original post that you paste data into GPT. It is ambigous what you mean here. Do you mean the SPSS output?', ""I'm a STEM professor at an R1; I don't know of any professors that actually do this, though I know a ton (myself included) who joke about doing this. My guess is the professors you've heard are saying it in jest."", ""2023: you don't need to learn R.\n\n2025: fukk R, it's broken"", 'It\'s just not outputting any math. It\'s a *language* model that spits out the next word or symbol based on the probability distribution it created from its training data.\n\nAt some point, the values and operations it\'s spitting out lead it down a cul-de-sac where it hasn\'t seen them enough times to be confident of the next combination of characters. But it can\'t stop mid stream to say ""Im not sure"", so it just puts whatever it\'s next output is without even hiccuping.\n\nIt would tell you 5 + 5 = 7 without flinching if it hadn\'t read ""5 + 5 = 10"" enough times.', 'Meta', 'Pretty much my life since K-12', '[deleted]', 'Yup and it uses tidyverse instead of using the base R, really smart', ""You really shouldn't use it to interpret the significance of a test either.   \nThere are plenty of sites that can explain how to interpret your results."", 'I wouldnt rely on ChatGPT if you need 100%.  But its great if you need preliminary results to test a theory.', ""I meant that i litearly paste the output data into GPT4\n\nFor example: Run regression > copy coefficients output > paste into gpt4 > Ask it to analyze.\n\nThe following is a snippet of our last conversation \n\nMe:\nHere is the coefficients table: Coefficientsa\t\t\t\t\t\t Model\t\tUnstandardized Coefficients\t\tStandardized Coefficients\tt\tSig. \t\tB\tStd. Error\tBeta\t\t 1\t(Constant)\t83,708\t56,084\t\t1,493\t,136 \tMann_DL\t229,803\t48,318\t,089\t4,756\t<,001 \tMannsdominert_Br\t-24,118\t53,051\t-,008\t-,455\t,649 \tMEAN_tot.kapital\t,005\t,001\t,097\t5,355\t<,001 \t Antall ansatte\t39,341\t2,231\t,319\t17,635\t<,001 a Dependent Variable: MEAN_EBIT\n\nGpt:\n1. CEO Gender (Mann_DL): The standardized coefficient (Beta) for CEO gender is 0.089, and the p-value is < 0.001. Since the p-value is less than 0.05 (or even 0.01), we can conclude that there's a significant relationship between CEO gender and the mean EBIT. The positive Beta value indicates that male CEOs (coded as 1) are associated with higher mean EBIT values compared to female CEOs (coded as 0). This result contradicts your hypothesis that companies with a female CEO perform better than those with a male CEO.\n\nAnd so one with the rest of the independent variables"", ""I hope it's in jest... but maybe it isn't because this often comes from humanities/social science profs who often have little to no CS education."", 'Its mot necessarily just memorizing though. It recognizes patterns and can extrapolate past whats in its training set (not saying you should trust everything it says)', 'Managers have been trying to do the ""just shove all the data in the machine and let it replace the statistician"" ever since the first version of Tableau came out 15+ years ago.\n\nI am not expecting them to succeed this time around either. I expect ChatGPT to be good at anything a random person can do successfully after reading a wiki article, but not to be good at anything requiring more care.', 'Yes im not going to be sloppy, but I do think it has some value. But its realy enoying that you can never trust it.', ""Do you know enough to know if that's a correct interpretation of the results?\n\nIf not, don't trust ChatGPT.\n\nIf you do, I'm not sure that using ChatGPT to draft the text and then checking it for accuracy is any faster than just writing it yourself."", 'I have worried about those folks. Hopefully publication requirements are changed so that all journals require any code for analyses to be uploaded.', "">  can extrapolate past whats in its training set \n\nThis is a particularly dangerous activity in stats, I would contend. If statistics was especially amenable to intuition, it'd probably work pretty well, but it's really not."", 'You cant trust it. YET.  Its new, and Im sure itll be highly reliable in just a year or two.']"
[Q] Mediation analysis shows increase of strength between the relationship of two variables...I've only seen relationships reduce. Is there a way to explain this?,"After running a mediation analysis using PROCESS by Hayes, the relationship between two variables increased from .25 to .29.

I'm not really great with statistics in general, and I've only seen relationships reduced after a mediation analysis. Is there a layman's way to explain what this means?

My analysis shows that a specific learning style mediates (in this case, increased) the relationship between a specific personality trait and test scores...",125xt4o,Beer_ReviewedArticle,1680117348.0,3,1.0,[]
[Q] Is coefficient estimate only valid above the intercept value?,"Let's say we have data with age and alcohol consumption. In the data we have age from 12-90, but below 18 consumption is negligible, and so is above 80.
Let's say the intercept is 16, after which consumption increases by 1 glass per year of age. If the intercept is 16, can we say that consumption increases by that much if age is above 16?

Also, let's say people above 80 are not allowed to drink and they generally follow this rule, apart from some that still drink a bit. The scatter plot for this data would look like a triangle (because there is big variability, some 30yo drink a lot while others don't, but in general the older you are the more you drink, until you're 80), with a sharp drop at 80. Is this still a linear relationship? How do we account for this drop?",125v0tk,AdKooky3754,1680111685.0,1,1.0,"[""Give more details.\n\n1. It is apparently clear that age is measured in years. What is the measurement unit for alcohol consumption?\n2. which one of these 2 are the dependent variable? Again, I am assuming it isalcoholconsumption.\n\nIf assumption 2 is correct, and we assume the usual simple linear regression model (E(Y|Age=x)=b0+b1x) then the intercept is just the average consumption of alcohol for people with 0 years, which is totally useless since you don't have data in that age range.\n\nIf you want a meaningful intercept, you can do a little trick: centering the independent variable (age) around its mean. Your model would look like this: E(Y|Age=x)=b0+b1(x-mean(x))for this model, the intercept would be theaverage consumption of alcohol for people withaverage age. This would be more helpful and useful.\n\n&#x200B;\n\n>Also, let's say people above 80 are not allowed to drink and they generally follow this rule, apart from some that still drink a bit. The scatter plot for this data would look like a triangle (because there is big variability, some 30yo drink a lot while others don't, but in general the older you are the more you drink, until you're 80), with a sharp drop at 80. Is this still a linear relationship? How do we account for this drop?\n\nNo, this would be outside a linear relation. You may use several techniques for nonlinear regressions. Or simple exclude those people for the analysis since they behave totally different.\n\nEdit: improved notation"", ""If your data is heteroskedastic, then you are violating one of the assumptions for linear regression and may need to find another way of modeling this data. A triangle shaped scatter plot can be an issue. \n\nA good model doesn't just describe the central tendency, it also describes what kind of uncertainty you have. As age increases you become less and less sure how much someone drinks, right? You need a way to describe that.""]"
[Q] What issues/challenges you face in current tools for data science/analytics?,,125pjix,lightversetech,1680099760.0,25,0.97,"['Stan is too slow.', 'my backlog of things I want to play with is growing too quickly', 'I have no budget and a bunch of inane requests.\n\nIt\'s all well and good to say that you want to automatically qualify leads, but when we don\'t pay for API access to the actual data I can\'t do anything.\n\nIf I have to use the same ""user-friendly"" interface that computer-illiterate salespeople do, you may as well just ask me to cut the crusts off your sandwiches, because we\'re trapped in a Fisher-Price ecosystem.', 'chatgpt is too slow', 'font is too tiny in the windows and no obvious settings to make it bigger available', 'Perceived sexiness. \n\nI do some internal stats consulting within a company. Many of the people I collaborate with, particularly associate scientists, view coding as a way towards career advancement. So they are adamant on learning and using Python for basic stats because coding and Python are sexy. \n\nFor some people, it works and its useful. But for the vast majority, I think it would be better if they used their time and energy to understand basic stats, and use the most user-friendly tool available (yes, with a GUI) that allows them to do their work with the flattest learning curve.\n\nBasically, people want to learn tooling at the expense of stats fundamentals when it makes no sense, and it bothers me.', 'I have a bunch of client touchpoints like them calling our company, chatting, surfing the web, and what theyre doing and the transcripts.\n\nI have a list of when the customer bought our products.\n\nBusiness leaders keep saying we need to find the story in the data.\n\nTheres no research questions or anything just tons of data and a need for actionable insights. If I pose a research question and do some analysis, its not holistic enough. \n\nJust not sure what models to run without specific hypotheses to test.', 'Im currently a MSc student in biology and we dont get much training in R, so Im trying to teach myself to run Bayesian mixing models on isotope data with Chatgpt as my supervisor', ""As I'm sure you're aware, sampling is a fantastically inefficient way to do inference; even if Stan were improved, that would be a fundamental limitation. My world of dreams for a long time has been to increase the scope of exact inference, and fall back on approximations as necessary, trying the most efficient applicable approximations first, and falling back on sampling only as a last resort."", 'now that you mention it, could you please cut the crusts off my sandwiches? thanks.', 'Got timed out this morning for using it too much and I had to wait an hour to resume lol. ChatGPT is only subpar with r though', ""I've looked into alternatives like inla but it is just not flexible enough for my needs. An alternative which would ameliorate the issues would be to have automatic within chain parallelization.""]"
[Q] Cost Benefit Analysis on maintenance of offshore wind turbines with random breakdowns,"Hello everyone, this is my first time posting, so forgive me if Im breaking any of the rules. 

Im performing a cost benefit analysis to assist in the decision making process for choosing a boat to buy for the maintenance of offshore wind turbines. 

I have data for; the number of turbines, the assumed yearly breakdown rate for the turbines, the number of transfers for maintenance session per year, the number of days each boat is available to travel per year and the cost per day of downtime due to inability to travel to perform maintenance. 

I am unsure how to model this to calculate the potential benefits of purchasing a more expensive boat that can travel through more days of the year  (which therefore leads to less costs from the downtime of the turbines). 

I am unsure how to assign the days available to travel per year with respect to random breakdowns (i.e flat turbine breakdown rate of 5%, 1 boat can travel 215 days and another 260 days etc etc). 

Can anyone help me with this? Please let me know if I need to provide further info. A point in the right direction would be great!",125m6l3,lucasanderson123,1680091628.0,1,1.0,"[""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657479\n\nI learned about this today, but it seems to work for your problem, but may also be overcomplicating things.\n\nNormal or benefit analysis ranks expected lifetime cost and expected lifetime benefits. You can use probabilistic outcomes in this. For example if there's a 5% chance in a year that a particular wind turbine breaks down, you can just assume that every year you will have to service 5% of the turbines. Run this for multiple years and the randomness all balances out by the end."", ""Seems very intense (with my limited knowledge of this area). But it's all I've got at the moment so will be giving it a try. Thanks for your reply."", 'How would you normally do a cost benefit analysis?\n\n[I discovered Markov decision processes from this video, which is nice and clear.](https://youtu.be/2iF9PRriA7w)']"
[Q] Can I define my own estimator function?,"Since, estimator is a function on a statistical(or any?) data to estimate an **unknown** parameter, will it be valid (or is one allowed to) to define your own estimator, like your own parameter..",125k2mr,greedygarlic69,1680086632.0,5,0.73,"['You can define literally any estimator you like.\n\nYou would (presumably) want it to have good properties, but there are tools for examining the properties of estimators - consistency, sufficiency, bias, variance, MSE, relative efficiency etc etc. So you can define one and then investigate how good it is. \n\nOf course, things like efficiency are not the only consideration (e.g. simplicity may be important in some circumstances, or the ability to update ""on-line"", or robustness etc etc). I use very simple but not fully efficient estimators quite often (getting a ballpark estimate or a starting value for something else for example). They can be very handy in a variety of situations.', ""Depends where you live. In some countries it's illegal without a license."", 'You can estimate things however you want, but most of statistics is about defining what estimators are good, which is why people follow the theory.', 'Yes.\n\nWhy would you want to?\n\n\nThink of the following problem: you want to estimate the unknown mean of a distribution.\n\n- you could draw many observations from that distribution, take their average, and use that as your estimate of the mean.\n\n- or, you could draw **one** observation from that distribution, and use that as your estimate of the mean.\n\nNote that both methods give you unbiased estimates of the mean.\n\n\nNow, why would you prefer the second method to the first?', 'This is het exactly what happens in machine learning, especially in neural networks.', 'Yes! Your estimator will then have a number of properties that you would probably be interested in: bias, variance, consistency, etc. That\'s where the pain comes; calculating that stuff is definitely doable for a lot of estimators but not always easy. \n\nMeanwhile, for a lot of known distributions, a variety of estimators for their underlying parameters already exist and are studied for you. For example, within the class of unbiased estimators, most parameters in most distributions either have a known minimum-variance unbiased estimator or a known unbiased estimator with variance close to some known lower bound on the variance. \n\nOutside the class of unbiased estimators, in most interesting or useful problems, it\'s not known how to (or impossible to) define estimators that are minimum MSE (mean squared error: the sum of the square of the bias and the variance) without knowing the underlying parameter(s) you are trying to estimate in the first place.\n\nPart of the branch of statistics called ""decision theory"" is devoted to figuring out which estimator(s) to use for a given problem.  Might be something to look into if defining a new estimator makes sense for a problem you\'re trying to solve.', 'So could I do this in my data analysis and justify those properties with simulation studies? Could this mimic the feature engineering step in data science?', '', ""If you understand the issues with simulation well enough to use it wisely\\*, sure you can make use of simulation. Ideally, you need to combine it with a little algebra -- e.g. you can't compute the ARE of an estimator (relative to the CR bound say) if you don't know what the CR bound for the distribution is. \n\n---\n\n\\* In slightly different contexts, I've seen plenty of people convince themselves of false things because they used simulation unwisely (essentially, by failing to try sufficiently hard to break their biases/intuition, whereupon it became a tool for confirmation bias).""]"
[Q] What are the problems you face in your data science workflow?,"Hi All,

We are a team of data analyst. We are doing a survey on the problems data scientists face while doing their work. Please comment if you have anything to share from your experience.

In our personal work, we have found that sharing our work across our team and saving a history of our work progress is challenging.

Thanks",125jcda,lightversetech,1680085019.0,2,0.75,"[""Receiving shoddy data is probably my most annoying and frequent problem. (Shoddy both as in 'not sufficient for what they want me to do with it' and as in 'untidy, riddled with spelling errors and formatting issues, missing or incomplete cases etc')"", 'Low quality data (and a lot of it). Specifically problems with compliance/ownership over data calculations/features. Working data analysis for financial institutions is gruesome work.', 'The description of your problem to track progress and share data in your team sounds like the perfect use-case for [DVC](https://dvc.org/). It is a tool that has helped me enormously.\n\nI like it because it is not opinionated about your workflow. You can integrate pretty much anything in it.']"
[Q]Chi-square or T-test for finding statistical significance for market share of brands?,"Hi,

As the title suggests, I'm calculating the statistical significance of 2 brands within the same market using their market share. I have the market shares of the two brands for the past 12 months and am weighing the cons/benefits of using either a chi-square or a t-test. 

If anyone has any inputs or other statistical techniques that could be of use, it'd be much appreciated. Thank you!",125fd4h,niccolo52,1680071583.0,0,0.5,"[""Chi square and t-tests don't work on the same data, so there is no need to decide between them.\n\nYour research question is nonsense anyway.  \nIf you want to do null hypothesis significance testing, you need to start with the basics of knowing what a hypothesis is, what significance is (and more importantly isn't) and only then start looking into tests."", 'If I understand your question correctly, the null hyppthesis is that the two distributions are equal, correct?\nAssuming I got that right, then either a Wilcoxon or a Kolmogorov-Smirnov test are due. Only if both distributions in market shares are normally distributed you could go for a more potent T-test.\nI would leave the Chi Square test out of the whole matter to be honest.']"
"[Q] How do I compare lotteries' chances of winning jackpot, when they differ in the maximum number of plays?",">[However, there is one way to boost your chances of winning the lottery, says [Dr. Mark] Glickman: Your odds do improve by buying more tickets for each game.](https://www.cnbc.com/2019/05/31/harvard-prof-on-odds-of-winning-multiple-lotteries-like-these-people.html)


#### 1. How can I deduce which lottery's jackpot is easiest to win, when they differ in the maximum plays? 

#### 2. For example, which has the highest chance of winning : 10 plays of Lotto 649 vs. 5 plays of DAILY GRAND (assume I pick $7 million lump sum) vs. 2 plays of DAILY KENO?

## Data for 3 lotteries from the Ontario Lottery and Gaming Corporation

| | [LOTTO 649](https://www.playsmart.ca/lottery-instant-games/lottery/odds) | [DAILY GRAND](https://www.playsmart.ca/lottery-instant-games/lottery/odds) | [DAILY KENO](https://www.playsmart.ca/lottery-instant-games/lottery-daily-games/odds/)
|:-:|:-:|:-:|:-:|
| **Jackpot** | $5 million | $1,000/day for life or $7 million lump sum | $2.5 million |
| **Odds of winning jackpot** | 1 in 13,983,816 | 1 in 13,348,188 | 1 in 2,147,181 | 
| **Matching numbers required to win jackpot** | 6/6  | 5/5 + Grand Number | 10/20 on a $10 bet |
| **Number pool** | 49 | 49 + Grand Number (7) | 70
| **Number of tickets one person can buy** | [10](https://i.imgur.com/rmjU10P.jpg5) | [5](https://i.imgur.com/5b5PMEM.jpg) | [2](https://i.imgur.com/h43DaJV.jpg)",125e0bx,3neth,1680067175.0,0,0.33,"[""Divide the odds by the number of tickets you are allowed to buy.\n\nIf you're trying to choose which one to play, you would calculate the expected value of each ticket. This is the jackpot x the odds.\n\nFwiw, your expected value is better by putting the money into a savings account.""]"
[Q] Looking for advice on what statistical test would best suit a small data set (n=25) with a random grouping factor. LMMs keep resulting in singular fits.,"I'm working on some research examining the impact of different land cover types on bat species diversity in my area. Month was not accounted for in planning for the species surveys and, as some bats are migratory, this may have an unintentional impact on my results. My original plan to account for this was to use a linear mixed effects model using the month the survey was performed in as the random grouping factor. Bat study season in the US only last 4 months, however, so my grouping factors has less than the recommended 5 levels. I've read recently that it is becoming more accepted to use LMMs with grouping factors with as little 2-3 levels as they are still more accurate than fixed-effect models. I did my best to simplify the models to accommodate my grouping factor, but my results are coming back with too many singular-fits to really justify using LMMs to make an argument. At this point, I'm thinking of just using a linear regression and incorporating ""month"" as a fixed-effect factor. Though this is the approach I've read is recommended in my case, I still want to try and assess what, if any, explanatory power the potential influence of the month takes away from different types of land cover. An approach I've been ruminating on is running two regressions, one with month as a factor and one without, then using a chai square goodness-of-fit to assess which model is the best fit for the data. My knowledge of statistics is piece meal, but working through things like this is how I've learned thus far. Open to any advice and/or suggestions.",125cc9u,H4drienne,1680062105.0,1,1.0,[]
[Q] How many rolls in average will it take for a twelve sided dice to roll the same number four times in total.,"I'm working on a tabletop RPG with a less conventional health system where each point of damage is represented by a d12 rolled that has its value recorded and a character dies when they take four points of damage to the same value.

So I wanted to ask if someone knows the answer to how many d12 would have to be rolled in average to roll the same number four times across all rolls.",1259epv,worstGirlEva,1680054157.0,42,0.94,"[""Someone else can chime in with an exact answer, but here's a quick Python simulation...\n\n    import numpy as np             \n                \n    def roll_until_four():\n      rolls = []              \n      maxct = 0\n      while maxct < 4:\n        rolls.append(np.random.randint(low=0, high=12))       \n        vals, counts = np.unique(rolls, return_counts=True)      \n        maxct = max(counts)           \n      return len(rolls)           \n                \n    results = [roll_until_four() for _ in range(1000000)]\n\n[Histogram of results shown here, average is about 17.5](https://i.imgur.com/Ich6hUH.png)"", 'I havent verified this answer, but [here](https://math.stackexchange.com/questions/542200/expected-number-of-tosses-before-you-see-a-repeat) is a stack overflow thread that describes the answer (its a nasty integral). You can follow the persons links to their other answers that show the derivation if youd like. According to Wolfram Alpha, the answer is around 17.5', 'TL;DR E(X) = 17.505 VAR(X) = 23.709 StD(X) = 4.869\n\nThe number is conclusively between 17 and 18 by the following examination of the CDF.\n\nE(X) = the value of x when the CDF(X) = 0.5. By definition this is the expected number of rolls to get 4 of the same kind.\n\nWhen 12 \\* (X choose 4) \\* (1/12)\\^4 \\* (11/12)\\^X-4 = 0.5 the number X will be the number of rolls that will achieve a 0.5 probability of any of the 12 sides appearing 4 times in X rolls.\n\nThis confirms the simulation that I ran to get the exact figures.', 'Are you saying ""how many rolls until the most-rolled number has happened 4 times""?', 'Between 4 and 37, the pigeon hole guy, is partially right.', ""The distribution of the number of rolls needed before getting 4 1's is negative binomial (r=4, p=1/12), with expected value r(1-p)/p = 44.\n\nSo expect 44 rolls before getting 4 of them to be 1s. But after 37 rolls we must see at least one number 4 times (pigeonhole principle, 3 of each of the 12 numbers, then 37th is the 4th) . So somewhere inbetween 37 and 44\n\nEdit: nvm, it's late. 37 is the max so expected # must be less.\n\nEdit2: just coded 100 tries and got about 16.5"", '[deleted]', ""Here's a guess, I'll wait for someone to correct me to give you the right answer!\n\nFirst roll is 12/12 probability, because it can be any number 4 times in a row.\n\nThen 1/12 probability 3 more times.\n\nThis works out to 0.058% chance of occuring in any four rolls. Or once in every 1724 attempts.\n\nProbably not statistically perfect, but gives a pretty good idea of how it will go."", 'The negative binomial distribution gives this. R parameter is ur total number of successes and x will be the number of failures needed to get to R success. So set r = 4 and p = 1/12 and get the E(x). This will give r/p = 48 failures. And check me on this last part but to get total rolls maybe u just add R. So final answer is n = 52.', 'During my computational statistics class, I learned that all of the difficult analytical methods could just be simulated and you can calculate sample properties super easily. Blew my mind! Asked my prof about it and his answer was ""well, if computers would have been around 200 years ago, we would all be doing computational stats instead of analytical stats.""', 'Kind of a weird way to code it instead of just incrementing the ith element of a 12 item array and stopping when it is 4', 'Statistics noob here. What exactly is this histogram saying?', ""The expected value of a distribution is related to the mean, not the median, and since the distribution is not symmetric they won't be the same."", '[deleted]', 'That is the probability of rolling the same value four times in a row. So not exactly what OP is asking for.', ""Lol right. I started trying to math this out exactly with tree diagrams and combinatorics, and then when things started getting hairy I stopped and reminded myself that outside of academia virtually nobody actually needs exact results. Simulations like this don't even have to be *good* code, just any old thing that comes to mind will probably get you the right answer within a tiny margin of error in like five minutes tops."", 'The law of large numbers is such a cool and intuitive result!', 'Agreed..! This was how I did it. Thought it was a more intuitive way to do it, although the results, in the end, are exactly the same.\n\n    import numpy as np\n    \n    N_trials = 1000000\n    N_rolls_results = np.zeros(N_trials)\n    \n    for i in range(N_trials):\n        rolls_hist = np.zeros(12)\n        N_rolls = 0\n        while (np.max(rolls_hist) < 4):\n            rolls_hist[np.random.randint(0, 12)] += 1\n            N_rolls += 1\n        \n        N_rolls_results[i] = N_rolls\n\nThat gave me a mean value of 17.49797 and a standard deviation of 4.88', 'Its the distribution of the number of rolls until one number came up 4 times. The code simulated thousands of trials, recording the number of rolls in each trial it took until a number was rolled 4 times', 'Its the distribution of the number of rolls until one number came up 4 times. The code simulated thousands of trials, recording the number of rolls in each trial it took until a number was rolled 4 times', 'You are absolutely correct, this does give the median. The distribution is still approximately normal, but I def was in lalaland.', '[deleted]', 'Thanks, I misunderstood the reading of the question.', '[deleted]', 'Got it', '[deleted]', ""No offense taken, I appreciate it. I was in academia for several years and have been in government and corporate research science roles for several years after leaving academia, I'm fully aware I would be an abysmal software engineer ""]"
[Q] Instrumental Variables in Gompertz Functions,"Hello!  I have but a mere Statistics minor and am currently in my Econometrics Honours, so I'm not the brightest bulb in the Python Studio.  


I don't quite know where I am going with this question, but I've had these neurons firing at each other for over a week wondering if there's ""something here"" and so I thought, maybe people who take apart data for a living can parse what I am asking!  


I am using an ODE model with a Gompertz function to measure oncolytic virus interactions with cancer cells for my dissertation.  For one of my classes, we are using IVs to measure the usual education/income data everyone does every year.  


But I wondered, could we utilise IVs in a Gompertz model to isolate the actual effects of the oncolytic virus vs any ""noise"" or errors?  But if the ODE model DOESN'T INCLUDE NOISE (cause there's just so many variables in cancer) how can an IV be used to figure out the true effects of the oncolytic virus? Additionally, what would that IV even BE?!  


For some reason my brain says ""there has to be a way to use IVs in this ODE model to truly capture the cancer lysis process"" but if the model doesn't model noise is that even possible?  Would this introduce more bias?  Am I on a track that others have been on and can tell me if they know where my brain is going?

It feels like my brain is missing a tiny piece of the puzzle but wants REALLY BADLY to connect these two concepts if not just to see what happens.

Thank you for putting up with this fool. I'm still learning and I was never good at maths to begin with, so it's been four years of straight new-found passion with 0 mathematical background, so I appreciate the help!",125688k,splithoofiewoofies,1680046238.0,5,0.86,"['I am not answering your question directly because I dont have enough experience with IV methods in general.\n\nI look at it as a problem where you want to use Gompertz likelihood instead of Normal likelihood for your outcome.  An easy way to adapt this is to do it using Stan.  Yes, you need to go full Bayesian, which is probably not where you are interested in going.  But it should be the same stan code for the IV case study with a modification on the likelihood on the outcome vector.  Good luck!', 'If I did a 2 stage with regression on the Treatment and IV and then substitute for the endo in the gompertz in the 2nd stage, would that work?\n\nEdit: would this not have a chance of increasing my standard errors tho?', ""No no actually a lot of this study is based in full Bayesien so thats not a problem. I mean it is in the sense it's painful and not quite perfect for oncolytic virus models but hey, good enough.\n\nAnd holy shit I did ask my supe and your answer was similar to both her and what I found. Regress w my ivs and then insert into the gompertz function... It seems possible and fine??\n\nI'll check out the Stan, not terribly familiar with that funnily, even tho I am knee deep in Bayesien. \n\nI mentioned the heat of the mouse cage as an IV for if the injection volume is too noisy we could find closer effects for the model... And my supe actually said that was not a stupid suggestion at all and she will see if it could be reasonably applied to our study???\n\nI am NOT an idiot???\n\nThank you so much. It's wild to know my brain worm actually did have something reasonable it was trying to figure out!!\n\nEdit to clarify that I have aphasia so my words are always a little off thank you for parsing what I mean."", 'As you are getting into Stan, there is a wonderful forum https://discourse.mc-stan.org/ for you to ask questions.  I am active on there, too.  Good luck again']"
"[Q] Correlation between a variable that is a %, and one that is a whole number?","I'm trying to find the correlation between population density (per square km) and obesity rates. When i graph these two variables in a scatterplot, they look very correlated, almost like a straight line. But when i calculate the R value, after i convert my % data into decimals, the r value is low, 0.43. When i just leave it as 30%, it's really high, 0.9. which is correct? Am i doing anything wrong? would it be incorrect for me to leave the % value as it is when calculating R? (ex. 30% instead of 0.3) Thank you.. i'm really weak in stats ",125475o,fartmaster900,1680041589.0,1,1.0,"[""You're doing something wrong.  Given, e.g., the following, the correlation between A and B, and between A and C are the same.\n\nA = (1,2,3,4,5)\n\nB = (0.10, 0.20, 0.25, 0.30, 0.27)\n\nC = (10, 20, 25, 30, 27)"", 'TBH I can\'t tell what you might be doing from the description but 30% is the same number as 0.3. ""Per cent"" means ""per 100"" or ""times 0.01"".', ""LOL a sorry yes. i'm trying to say, i calculated an r value using decimals (like i converted my percent data to decimals) and i got 0.43. but my scatterplot looks VERY  strongly correlated, so i am confused.... I don't understand why this happened"", ""That's odd since (assuming you're computing the Pearson correlation coefficient. i.e. covariance over product of standard deviations) correlation doesn't change under scaling by a positive number. I would guess you're computing it incorrectly.""]"
[Q] Sample size estimation," I want to know if patients with a particular disease have a greater likelihood of having a comorbidity if their disease is severe (0/1). I want conduct a cohort study of patients with a particular disease from a single hospital site. All patients with the disease from that hospital will be asked to participate. Those who choose to participate will then be assessed for a comorbidity with a device and stratified by severity of their original disease. 

I want to see whether their comorbidity is significantly different by their original disease severity or not

Whats the smallest sample size I should enroll before looking at the findings?

What should I integrate/do to adjust if I look at the findings midway and then decide to enroll and collect more patients after?

Thanks for any and all help thinking through this!",1251j7o,Climbing_Yggdrasil,1680035961.0,2,1.0,"['I think [sequential analysis](https://en.wikipedia.org/wiki/Sequential_analysis) is what you are looking for. The link is just an overview, youll need more details to perform the analysis.']"
[Q] Significance of hedges g?,Can someone explain for me the significance of the hedges g values? Are values greater than 1 possible and what would that infer? Can provide more information in comments if need be.,124zq43,Bmilz7,1680032308.0,6,1.0,"[""Hedge's *g* is similar to Cohen's *d* (which you'll find more literature about).  It's an effect size statistic.   Conceptually, it's the difference in means between two groups, divided by the pooled standard deviation.  Yes, it can be greater than 1.  Simply, if the difference in means is greater than one standard deviation."", ""Hedges g is an effect size.\n\nHypotheses are about population parameters, like population means. You could potentially cast a hypothesis in terms of population effect size but it would be ... kind of odd.\n\nYes, values greater than 1 are easily possible, unless you're dealing with some unusual situation (which would likely make a nonsense of using Hedges g).\n\n>  what would that infer? \n\nYou mean *imply*. *Inferences* are something *you* do. It would imply something, you would infer.\n\nDirectly, it would imply that the means differ by more than one  pooled standard deviation\n\n> Can provide more information in comments if need be.\n\nPlease don't just put any additional information in comments. If you need to add information, make sure you edit it into your post, so that hapless readers aren't expected to go hunting through what sometimes ends up being lots of comments looking for some essential bit of information, or to ask the same clarifying questions over and over.""]"
[Question] What kind of hypothesis testing can i perform on such a table,"Hello everyone , 
Currently on my junior year, after the end of the hypothesis testing chapter , our teacher asked us to lead a study on how people from our country spend their time , the tables  we are working on has 4 columns which constitutes the education level and lines which are the average time spent on each activity : leisure , sleep , religious practice ..etc we have a table for men and one for women . 
My question is what kind of hypothesis testing can be performed on this table , so far i thought of performjng a chi-squared test of independance to see if for example the average time spent sleeping is dependent on the education level , but i don't know how to proceed further since the data i have on hand doesn't have observations but rather average times spent so i don't see how i can perform tests on the mean ,variance ,proportion ..etc",124vytt,Yang-Wenli14,1680024385.0,2,0.75,"[""The usual chi-squared tests on contingency tables are for count data (while the hypotheses would relate to proportions in any of several ways), not means\n\nI don't see any reasonable basis for inference if people are recording *times* (specifically, durations) and all you have is the average. Further, the note that the times in each of the categories are dependent (you can't do more than 24 hours of activity in a day; e.g. more time sleeping means less time on other activities)""]"
[Q] diffusion models - drift rate,"Does anyone have a good example of how one goes from having response times to extracting the drift rate in a diffusion model? How is the decomposition into non-decision time and drift rate done? I have seen it written up in stan code and all, but I was wondering if there is a simpler explanation I could memorise more easily. Thanks",124uqh5,majorcatlover,1680021735.0,3,1.0,"[""There really isn't anything to memorize more easily, and there generally aren't any closed form solutions for the model estimates. The combination of model parameters implies a specific distribution of response times, and the parameters are selected to best match the distribution of the observed data. In certain very specific cases, for very simple models, people have derived closed form approximations for some of the parameters, but these are rarely used in practice.""]"
[S] How to find p-value boundary in Minitab,"Greetings,

I have the following situation in Minitab:

I have a reference population with a mean and standard deviation. 

I'm looking to make an area plot with mean and standard deviation on its axes. In the figure I wish to plot areas and the boundaries where a sample with the mean and standard deviation from the axes values are significantly different from my reference population. 

I've done this before in Excel where it's essentially countless different t-tests (for each mean-stdev pair) but that doesn't give me smooth contours, and I feel like this might be built in somewhere but I just don't know the right name.",124rx0i,audentis,1680016451.0,1,0.67,"[""I'm confused by what you write there -- how were you testing for differences in standard deviation using a t-test?"", ""I'm not testing for differences in standard deviation. This is more akin to a sensitivity analysis. \n\nI'm looking to visualize how much change to the sample mean, sample stdev, or both, it takes for the difference to become statistically significant.""]"
[Q] Help in understanding error propagation,"Hello,

I'm having some difficulty understanding how to propagate errors.

Let's say you solve 2 linear system of equations

    1=A
    0.01=2A+B
    -1=B
    
    AND
    
    1=C
    2=2C+D
    -1=D

Naturally, the above doesn't have an exact solution, so you get an approximate where

    A=1.003
    B=-0.996
    C=1.66
    D=-0.33

With a norm of residuals

    A and B=0.0057
    C and D= 1.154

Say you have some function

    f(A,B) = sqrt(((A-C)^2)+(B-D)^2)

Now I've seen 2 ways of error propagation. The errors can be added,sub,mult,divided 

I.E.

    if addition,  the error in the result of an addition or subtraction is the square root of the sum of the squares of the errors in the quantities being added or subtracted https://faraday.physics.utoronto.ca/PVB/Harrison/ErrorAnalysis/Propagation.html
    errors A-C and B-D=sqrt(0.0057^2+1.154^2)=1.531
    errors A-C^2=2*(A-C)*1.531
    errors B-D^2=2*(B-D)*1.531
    ((A-C)^2)+(B-D)^2) = sqrt((2*(A-C)*1.531)^2+(2*(B-D)*1.531)^2)
    sqrt(((A-C)^2)+(B-D)^2) = 1/2*(1/sqrt(((A-C)^2)+(B-D)^2))*sqrt((2*(A-C)*1.531)^2+(2*(B-D)*1.531)^2)

I've also seen it done via covariance matrix, where the norm of residuals is your variance, and assuming no covariance between the variables, you would get

    errors f(A,B)= sqrt((df/DA)^2*(0.0057)^2+(df/DB)^2*(0.0057^2)+(df/DC)^2*(1.154)^2..

where the derivative of the function can easily be derived

    e.g. df/DA= A-B/(sqrt(((A-C)^2)+(B-D)^2))

Then you can just plug in the values of A,B,C, and D and calculate the uncertantity that way.

These 2 methods give you different results however, and I'm not quite sure which is the proper route or method for propagation of errors. Any help would be greatly appreciated!",124qnl0,DrBobHope,1680014045.0,7,0.82,"['For the physical sciences, the last approach is the ""go-to"". You can find (much) more information from BIPM\'s [Guide to The Expression of Uncertainty In Measurement](https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6).\n\nNote that this last approach is also just an approximation using a first-order (sometimes second) Taylor polynomial.  While you *can* find exact uncertainties, this is often very hard and sometimes impossible. Another cool way to find uncertainties are via Monte Carlo simulations.', ""The general case of a function of some uncertain variables is handled via the change of variables method; a web search for that will find some resources for that.\n\nFormulas involving derivatives are approximations to the exact result which you would get via change of variables -- essentially just making a linear approximation and then applying rules that are exact for linear combinations of uncertain variables. Whether the linearization is a close approximation or not depends on how nonlinear the function is, and that has to be studied case by case.\n\nMonte Carlo methods are a different approximation, which keep the nonlinearity at the cost of giving up a simple formula, and instead yielding just a histogram. (On glancing at the NIST Uncertainty Machine, it looks like that's what they're doing.)"", ""so are both methods valid then? In many cases the 2nd approach derivatives are harder to determine (I usually use finite differences to approximate it). If so, I presume both should give similar values, but I've found in my experience they can give drastically different uncertainties (which is why I was asking this question)"", 'The first method is not valid. When you find the difference, between two random variables, A and B, then the variance of difference V(A-B) = V(A)+V(B) + 2cov(A,B). The variance is not V(A)-V(B). This should also make sense intuitively as that could provide negative variances very easily.\n\n&#x200B;\n\nAgain, for the first method, you do not actually need to think partial derivatives to handle the problem. It is possible to split the problem up to simpler chunks.\n\nFor example, for the problem\n\nY = (A\\*B - C\\*D)/E,\n\nyou can first calculate the uncertainties of A\\*B and C\\*D. Calculate then the uncertainty of the difference F = A\\*B-C\\*D, then you find the uncertainty of F/E.\n\nFor simple situations like these, you can just apply some [simple formulas](https://www.isobudgets.com/propagation-of-uncertainty-rules/).\n\nUncertainty propagation can be very hard, and many people in my field (analytical chemistry + chemometrics) struggle to do it.', 'You are correct, I have subsequently fixed it', 'I did have a follow up however, in my example I have \n\n    f(A,B) = sqrt(((A-C)^2)+(B-D)^2)\n\nWhich the first method is valid because A and C, and B and D, are not correlated to one another in any way (i.e. cov(A,C) =0). However, what if we were to do this instead: \n\n    f(A,B) = sqrt(((A-B)^2)+(C-D)^2)\n\nA and B are correlated, they were solved together within the system of equations. Same with C and D. How would you go about propagating the error for something like this?', ""There are also tools which do these calculations for you. NIST's [uncertainty machine](https://uncertainty.nist.gov/) is probably the easiest and most accessible one."", 'You then find the covariance term and then add it into the equation. It is quite simple but it depends on the operations you are carrying out. Look at the ""example formulae"" on [this wikipage](https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulae)', ""The issue is how would you find the covariance term of 2 numbers? Would it just be\n\n    E.G. \n    A=2\n    error A=0.1\n    B=4\n    error B=0.2\n    cov(A,B)=(2-3)*(4-3)=-1\n    So for the example\n    A+B, error propagation is \n    (0.1)^2+(0.2^2)+(2*(-1))\n\nThis confuses me because this doesn't appear to use the partial derivatives of the function at all (such that I have setup in my post). What are the lower case a and b in the wiki page you linked, are those the partial derivatives?\n\nE.G.\n\n    A+B\n    \n    df/dA^2*varA^2+df/dB^2*varB^2+2*(df/dA^2)*(df/dB^2)*(cov(A,B))\n\nOf course in this example of just f=A+B so df/dA and df/dB is just one, so this would be identical to what I posted above, but I'm just saying in theory""]"
[Q] someone who understands correlation pls explain this to me,"So I'm doing research on impact of Social Media on Eating Behaviour. Out of 60 respondents, 33 had a social media addiction and 25 of those were at risk of developing an eating disorder. There were only 7 participants with ED risk without SM addiction. But when I used Pearson's correlation on excel, I got the score of 0.3 which signifies low correlation, why is this the case when 75% of people who are addicted to SM are also at risk for ED?",124nk38,peskyant,1680007334.0,0,0.33,"['Correlation is for a linear relationship between two numerical variables. You want to do a chi-square test for independence.', ""Aren't your variables categorical ? You should probably not be using Pearson's correlation in that case !"", 'Both *Social Media* and *Eating Behaviour* are dichotomous variables ?  That is, each is *yes / no* ?', 'In response to some of the comments, Pearson correlation for dichotomous variables, treated as numeric, and *phi* \\--- the correlation coefficient for a 2 x 2 table --- will give the same result.', 'Could you do a correlation of scores on one measure with the other, rather than collapsing them into addicted/not addicted?', 'Note to readers:  as far as I understand, the counts for a 2 x 2 table would be: 18, 15, 7, 20.  OP can confirm.', 'How many were at risk of eating disorder but did not have the social media addiction?', 'It is unclear how your correlation really relates to your hypothesis. \nYou can build a 2 \\times 2 table with rows : ED Y/N and columns: SM addiction: Y/N.  \nthen use the chi square tests\nhttps://www.statsdirect.com/help/chi_square_tests/22.htm', ""Would Spearman's rank work in that case as well?"", 'No, the participants responded to a survery and were scored. Those who attained a score over a certain level were deemed addicted/at risk.\n\nWhich test should I use in that case?', ""But a chi-square test of association doesn't address the degree of correlation (or *association*, if you'd rather), which is what the question asks about."", 'But you want to analyze each as *AtRisk / NotAtRisk* ?  Or you want to analyze the scores ?', ""You are absolutely right. I can't say I understood OPs method (it wasn't clearly phrased). I was trying to suggest an alternate method."", ""Honestly I'm not sure, I'll have to ask my prof tomorrow. Thank you so much for your response ""]"
[Q] Writing model based on model parameters,"Just a quick question, did I write the model right based on model parameters or I made a mistake ? 

Thank you 

[https://imgur.com/TNAxrRY](https://imgur.com/TNAxrRY)",124iozp,murdafeelin,1679994373.0,0,0.5,"['Yep', 'Yep I made a mistake or Yep I wrote it right :)\n\nThank you for your answer :)', 'A linear equation is of the form:\n\nY = intercept + parameter*predictor + error\n\nSo yes you got it right.', 'Thank you.\n\nAnd how would I write the model for this ? [https://imgur.com/IF1ochU](https://imgur.com/IF1ochU)\n\nDo I look for ar5 and something else ? \n\nThank you again', 'x\\[t\\] = 0.0081 - 0.4075\\*x\\[t-1\\] + 0.0046\\*x\\[t-2\\] - 0.0206\\*x\\[t-3\\] + 0.0303\\*x\\[t-4\\] + 0.0532\\*x\\[t-5\\] - 0.0537\\*x\\[t-6\\] - 0.0040\\*x\\[t-7\\] + 0.5494\\*e\\[t-1\\] + e\\[t\\] would something along these lines be good or not', ""It's the same.\n\nx_t = intercept + ar1/*x_t-1 + ar1/*x_t-2 + ... + epsilon_t + ma1/*epsilon_t-1""]"
[Q] Are there any good references for linear algebra in statistics?,My class is getting into time series. Would there be any good references for starting in linear algebra in statistics? The reference our professor gave us is really hard to understand. It skips most of the proofs at times.,124ehl5,enderheanz,1679981597.0,43,0.97,"[""In our class we used Searle & Khuri - Matrix Algebra Useful for Statistics. Pretty readable and straightforward. While it's not that rigorous compared to other LA books intended for math majors, it's a pleasant introductory material that you can use to get up to speed w/ fundamental LA concepts useful for higher stats."", 'If you have the time/patience to start from scratch, Linear Algebra Done Right is really intuitive and friendly. \n\nMatrix Analysis by Carl Meyer is from a numerical analysis point of view which I think is an underrated  but really useful skill for statisticians (eg knowing about condition number and QR factorizations etc). \n\nEither way Id recommend to really just hunker down and invest a lot of time in making sure you understand eigenvalues and matrix factorizations and just linear algebra in general because itll make the rest of the statistics journey much faster and enjoyable to wade through. And bonus its also used all over the place in engineering too so itll make you better at understanding that literature too.', 'The Matrix Cookbook has a lot of *super* useful identities. I definitely recommend keeping it as a reference. It can easily be found on Google (or Bing, lol).', ""I don't doubt that LA is significant in time series, but I also don't know where it is featured - mind sharing more details just out of curiousity?"", ""If you are looking for reference materials covering your typical undergrad linear algebra then [https://www.skytowner.com/explore/introduction\\_to\\_vectors\\_in\\_linear\\_algebra](https://www.skytowner.com/explore/introduction_to_vectors_in_linear_algebra) might be worth checking out. It's a series of 52 guides with visuals and full proofs."", ""They are rare, mostly because they're not _that_ useful. \n\nLinear algebra is important in classical statistical analysis (I'm excluding ML) for three things :\n\n- optimization (model fitting)\n- expanding unidimensional methods to multiple dimensions\n- some designs of experiments strategies \n\nWhile this is super important conceptually, 95% of statistician will never have to do those themselves. Methods are predefined and simply applied when needed. Truth is, you don't _need_ linear algebra to _do_ stats,   you need it only to _understand_ the intricacies of what you are doing. \n\nStatistician are down to earth, practical, applied mathematicians. In an industrial setting you just need to solve the problems given to you and move the f* on. \n\nAt the end of the day, it's not as important as your teacher want you to believe. Your better off learning useful applications of statistical models rather than digging into linear algebra for stats. \n\nNote : i still think it's important to understand the basics (matrix operating, spaces, basis, eigenvalues/vectors) , just because It's part of your math curriculum."", 'Book I used to learn this stuff since I was an applied student and really needed to crash this.', 'The power in this book is too much. Could probably use most of these to do the proofs we have', 'Multivariate time series analysis, explaining cointegration, Kalman filters.  But I agree, most time series phenomenon should be probabl;y understood in one variable context (if possible). The additional cognitive load from LA is not worth it.', ""Don't forget Markov type processes, where it's easy to describe transitions from one moment in time to the next, and linear algebra gives us a way to describe the long-term evolution of the system."", 'Great resource, thanks!', '>\tLinear algebra is important in classical statistical analysis (Im excluding ML) for three things \n\nIm sorry, but I really disagree with your post. Linear algebra is the vocabulary of statistics. What language *other* than a matrix do you have to express something as fundamental as a dataset? \n\nI agree that you dont need a full understanding of linear algebra to be a good statistician. Most methods will be expressed in terms of matrices and vectors, though. If you want to learn something new, youll have to find a non-technical explanation that glosses over the linear algebra.', 'This is very misguided. Linear algebra is needed for dimension reduction techniques, which are butter and bread for statisticians, and for penalized regression, which is a staple of modern statistics.', 'Yeah I used the hell out of that cookbook when I was learning advanced longitudinal regression.', 'Relevant xkcd:\n\nhttps://xkcd.com/1838/']"
[Q] How to straighten qq plot for regression?,"Say residuals are not normal and you want to do some transformations to variables. Is there a way of knowing how to transform them and which? Can that be seen from the gg plot? Say you want to bring down the upper tail that is curved. Is there a specific transformation?
Any way I do it, I always break something else. I get the upper part to be straight, and the lower part just curves more. Or the other way round. So are there any rules on how to know what to do?",1242kkk,sunandskyandrainbows,1679955104.0,22,0.92,"['There is an r function (boxcox in the mass package) to help you determine the optimal box cox transformation of the response variable to make residuals as normal as possible. \n\nProf. Frank Harrell advocates making a restricted cubic spline of the predictor variables.', 'Not sure internet strangers can recommend a transformation of your variables that would result in a good model fit without more information. Options are: better/more correct statistical model, data transformation, or maybe the data are what they are. Need more info.', '(made several edits to add information and examples)\n\n> Say residuals are not normal and you want to do some transformations to variables\n\nThere\'s problems with this whole approach, but let\'s start by entertaining the idea.\n\n1. You\'re looking at a Q-Q plot and you see it\'s clearly not linear. Does that of itself mean the conditional distribution of the error term is non-normal?\n\n  No. \n\n  It really doesn\'t. Here\'s two examples. In both cases, each and every error term *was exactly normal*. I know because I generated them that way.\n\n   https://i.stack.imgur.com/MG9Hw.png\n\n   The top row is the Q-Q plot. The bottom row is the residual plot that *shows why the Q-Q plot looked nonlinear*.\n\n  Clearly, we need to impose some restrictions before we try to interpret a Q-Q plot!\n\n  Requirement: the other assumptions of your model have to hold, especially (i) the model specification of the conditional  expectation must be correct (or at least very close to it); (ii) the model specification of the variance term must be correct (ditto).\n\n  Trying to interpret a Q-Q plot without that requirement is a waste of time, you\'ll be chasing phantoms.\n\n2.  Okay, let\'s imagine that we\'re satisfied - by some means - that this is not a problem and that QQ-plots are interpretable. \n\n   Then (speaking very loosely) the apparent relationship in the plot the way around R does it (random quantity on the y-axis, fixed quantity on the x-axis -- i.e. the \'right\' way to plot a relationship) will tend to show how a normal error distribution has been ""transformed"" (and hence, it might seem, how to invert it).  If the relationship looked curved like an increasing quadratic / cubic power / exponential, that might suggest square root, cube root or log, for example.\n \n3. Great, so we at least sometimes have a pretty useful tool, right? If we are satisfied that the other assumptions hold, we can just look at the Q-Q plot and use that transformation and everything should be okay, right? \n\n   No. \n\n   Sadly, everything is not okay. \n\n   It sometimes sort of works ... but it often doesn\'t work (in part for reasons we\'ll see in a moment, but also for other reasons). While it may sometimes give you some sense of a helpful transformation; we\'d also need  to consider additional complicating issues -- like, say the impact of a location shift of the conditional distribution on a transformation - a shift may render the transformation next-to-useless, without impacting the shape in the Q-Q plot).\n\n   Even if you do happen to have a transformation to approximate conditional  normality, your nonlinear transformation *will screw up the residual vs x plot* - the very thing that had to be right in order to correctly interpret the Q-Q plot. Worse, normality is usually the least important of the three (linearity, homoskedasticity, normality) so screwing up the other two things to get approximate normality is likely to be highly counterproductive.\n\n   Take a look at the bottom row of the plot above. You will create *both those issues at once*. Well, not exactly those things, but things like those, you\'ll introduce curvature where you had linearity and heteroskedasticity where you had homoskedasticity. \n\n   Further, typically when you do get a sense of how a normal error distribution might have been transformed, the inverse transformation doesn\'t help the normality either, because the errors are different from the conditional distribution of the data.\n\n   That is, when you *can* interpret the Q-Q plot, transformation is not the right tool to fix your regression.\n\n   Conversely, the only time transformation is of use is when the residuals-vs-x relationship is *wrong* (and it would have to be wrong in just the right way), making the Q-Q plot essentially *uninterpretable*, as we established back in 1. That is, if transformation is to be useful, you should not expect that the Q-Q plot will typically be of much direct help in figuring what transformation to use. It may mislead you because of the interaction between the impact of the non-linearity in residuals, the heteroskedasticity and the non-normality.\n\n   Let\'s look at an example where the linearity and the heteroskedasticity are okay so the Q-Q plot *is* interpretable and indeed, pretty clearly shows us how a normal error was modified before the error was added. \n\n   https://i.stack.imgur.com/zUO3F.png\n\n   Looking across the top row of the plot, we can see that the relationship is linear, so the residuals are flat vs x, and we can see that there\'s no heteroskedasticity -- the spread is constant as we go from left to right. The distribution of errors around the line is skewed, and that shows up clearly in the Q-Q plot. This is a case where the Q-Q plot really does show us the problem with the distribution of the error term. \n\n   The curve in the Q-Q plot in row 1 looks sort of exponential, and indeed that\'s exactly what happened; I took normal errors and exponentiated them before adding them to the line. We can see it fairly accurately. So wouldn\'t we just need to take logs?\n\n   Well, what happens when we try it? As we see in row 2, the y vs x relationship is now curved, and (though it\'s not as obvious) the variance is no longer constant. It\'s actually a bit more spread out on the left end than at the right end. We screwed up the part that was right.\n\n   Worse, we didn\'t even make the residual distribution look normal. We did improve it a bit at the left end but had almost no impact at the right end. This is because the line shifted the noise distribution up and an unskewing-transform is changed when you shift it; in the case the log-transform does less and less the more you shift it up, so the skewness is only a little improved on the left - and hardly affected at all on the right. What we\'d need to do (if only we could) is go back to the exponentiated noise I made and take the log of that, but we can\'t do that. \n\n   Transformation is just the wrong tool in precisely the case where the Q-Q plot most accurately shows you the problem! Conversely, in a case where transformation would help, the Q-Q plot may suggest something like that problem or it might entirely mislead you about it (e.g. you might be able to approximately fix nonlinearity, heteroskedasticity and non-normality all with the same transformation - it\'s not typical but it can happen -- but the Q-Q plot might not show any problem at all, or might suggest a very different-looking problem).\n\n  A better use for transformation is to try to fix residual plot patterns - nonlinearity and heteroskedasticity, though it\'s not the only tool. The Q-Q plot is usually not all that critical by comparison, if sample size isn\'t small (unless efficiency/ power is important)\n\n4. Given that you will generally introduce considerable difficulty in interpretation when you transform y (even when you avoid the problems above), generally my advice is not to use transformation even in a case where it might work quite well, but to choose a more suitable model for the conditional distribution for y from the start, considering the whole thing together. This might be a generalized linear model, for example, or it might be something else. There\'s multiple options depending on what the particular collection of issues may be.\n\n   Wherever possible such choice should be made without reference to the same data you want to use for inference or prediction, for the usual reasons; this means either figuring out a good model without looking at the present data or splitting off part of the data at random in order to make such an assessment -- though that, too can carry some problems of its own.\n\n   An exception to the \'generally, don\'t transform\' advice would be where the transformed scale is particularly meaningful in its own right (e.g. convert times to speeds by inverting, that sort of thing).\n\n\n---\n\nWhy don\'t books, notes, websites (etc) that talk about transformation and regression address these issues more carefully? I don\'t know. I think many people perhaps aren\'t aware of these issues.', 'Just here to say that just because residuals dont look normal doesnt mean you cant use good ol linear regression. \n\nThanks for coming to my Ted Talk.', 'Depends on your goals for the regression model, but quantile regression can sometimes be helpful when dealing with heteroskedastic or non-normal residual distributions.', 'If you wanted to compare your data to another distribution, say a t with 5 df you could do this:\n\nplot(sort(x), qt(((1:n)/(n+1), df = 5))', ""In general, you can transform any distribution to a normal (or other chosen) distribution using the empirical cumulative distribution function (ECDF).\n\nRoughly speaking, this consists of\n\n1) Converting your series to ranks (or, more precisely, quantiles) and then\n\n2) pass the results through the *inverse* CDF of your normal (or other selected) distribution.\n\nAdapting this to a regression context is trickier. You could \n\na) transform all your variables to normal before analysis, or\n\nb) fix the regression coefficients, just transform the residuals, and use the result to create new values of the dependent variable.\n\n**I'm not suggesting that any of this is a good idea, however.** You should think hard about what your goal is before proceeding."", 'Box-Cox analysis is the way to go, but honestly, most of the time log(y) does the trick.', 'Thanks, this is exactly what I had done, but it the messed up the other part of the plot. I then tried transforming an independent variable that has a funneling residuals vs fitted plot, but anything I do, any increase/decrease, makes it worse than no transformation at all', ""I've found that you need to determine your specific lambda values depending on model structure to make the box cox work as intended"", 'That\'s a great explanation and example of why to interpret those plots jointly, thanks. \n\nThose sources also rarely talk about how much wrong a model failing to meet various criteria causes. Like, you described how there is a sort of hierarchy of importance of assumptions, with qq linearity being ""generally"" least important. I\'m not asking you to explain this, or how important ""least"" important is, I\'m just lamenting the same problem you pointed out. \n\nI end us resorting to bad heuristics, like ""huh, assumptions don\'t seem to be met, but it\'s not terrible. Let\'s try a non-parametric. Looks about the same! Great, I\'ll report the parametric because it\'s easier for my colleagues to understand, and still feel confident because I got the same answer other ways.""\n\n--a probably frustrating biologist', ""Wow, thank you. So to give some more info: \n\nhttps://imgur.com/a/bbepP3u\n\n\nI did the res vs fitted plot to check variance, it's fine (I think). I plotted all x's against y, and I think the issue is with the third one down (it looks like a triangle - what does that mean? Is that linear? Exponential?). When doing res vs fitted plot on that one, it's funneling obviously (sorry don't have a screenshot). I tried boxcox which suggested ^2 or ^3 of the y, so I did that and it worked really well on one tail of the qq plot, but it messed up the other tail.\n\nSo if I understand what you said correctly, sometimes it really doesn't make sense to transform? And sometimes not perfect is good enough?"", 'Thank you for the write up! Id like to learn more about the assumptions being made about the model you noted in pet 1, the conditional expectation, would you have advice on where I can learn?', 'The inputs need not be normal. As long as the 4th moments of certain functions of variables are finite, the CLT yields asymptotically normal outputs. Always.\n\nWho said that an ol good linear regression cannot handle heteroskedastic distribution? There have been numerous articles (e.g. 30 years of heteroskedasticity-robust inference from fricking 2011) on the issue that heteroskedasticity is fine, not scary, an the real deal is how to get the best inference to have the test size as close to the theoretical as possible... but no. Like that one paper about vaccines causing autism, there are multiple shitty textbooks making the strictest assumptions possible for the linear regression and scaring their readers into thinking that a tiniest deviation from them would immediately invalidate their analysis.', ""Maybe I'm missing something, but I think the function boxcox in the mass package does that for you?\n\nI am always a little confused if I should use the model that physics dictates or the one that gives well-behaved residuals."", ""> I plotted all x's against y,\n\n1. Huh? *In the name of all that's decent, no.* Plot y vs x.   \n\n         y |\n           |_____\n                x\n\n\n2. Beware trying to interpret marginal y-x relationships when the model is dealing with *conditional* relationships, which can look entirely different.\n\n3. The last plot looks like you have a bunch of values at the bottom of the plot below -40. A big black line of them, that looks weird.\n\n   I'm not clear what's going on there."", ""I don't think it specifies the lambda based on model structure.  I can send the code that I use next week if you remind  me""]"
[Q] Necessary and Sufficient Condition for UMVUE,"A necessary and sufficient condition for an estimator T to be the UMVUE for theta is that T is uncorrelated with every unbiased estimator of zero. 

The prototypical example of using this that I have found online and in books use a uniform distribution. However, I am not really sure how to show this in general. Specifically, I am trying to use this condition to show that for a random sample X1, ... , Xn \~  exponential(theta), Xbar = n^(-1)(X1 + ... + Xn) is the UMVUE for theta. I am using this parameterization of an exponential distribution: 

f( x | theta ) = theta^(-1)exp(-x/theta)I(x > 0)

I know there are easier ways to show that Xbar is the UMVUE, but I want to get some practice using this condition on a distribution other than uniform.",123ubu4,zebrapaad,1679938782.0,6,0.87,"['Okay I think I got it, in case anyone is interested:\n\nLet *T = (X1 + ... + Xn)*, a sufficient statistic for theta, and let *V(T)* be any unbiased estimator of 0. (Yes, this is a function of *T*)\n\n*T* is known to follow a *gamma(n, theta)* distribution, so we can use this to show that\n\n*E\\[V(T)\\] = 0 => E\\[T V(T)\\] = 0* by dividing out constants and differentiating both sides of *0 = E\\[V(T)\\]* w.r.t. *theta*.\n\n\\---\n\nNow, let *U(X)* be any unbiased estimator of 0.\n\nThen,\n\n*0 = E\\[U(X)\\] = E\\[ E\\[ U(X) | T(X) \\] \\]*                           *(by the law of iterated expectation)*\n\nBecause T is a sufficient statistic, *E\\[ U(X) | T(X) \\]* does not depend on *theta* and is unbiased for zero. Lets say that *V(T) = E\\[ U(X) | T(X) \\]*.\n\nSo we have\n\n*E\\[U(X)\\] = E\\[ E\\[ U(X) | T(X) \\] \\] = E\\[ V(T) \\] = 0*\n\nand by the argument above the  ---,\n\n*0 = E\\[ V(T) T \\] = E\\[ E\\[U|T\\] T \\] = E\\[ E\\[ U T | T \\] \\] = E\\[ U T \\]*.\n\nSo *T* is uncorrelated with every unbiased estimator *U(X)* of 0, and therefore \\*Xbar = n\\*\\*^(-1)\\**T* is the UMVUE for *theta*.', 'This post just reminded me why I hated stats grad school so much.']"
[Q] ANOVA Graph,"I have data from a one way ANOVA (3 conditions). I need to create an ANOVA graph to show the mean, variance and standard deviation. Which is the best graph to use? I was thinking box plot? But i am not sure.
And is there anyway i can create this graph in Jamovi? Tia!",123ty3u,bbycharlie,1679937993.0,3,0.81,"[""P.S. In Jamovi, *One-way Anova > Descriptives Plot* will produce a plot of the mean with confidence intervals.  ...  There used to be a couple of good plotting modules, *jjstatsplot* and *flexplot*, but they don't work with newer versions (I guess).  If you find a good module for this, you might share it for others."", 'boxplots don\'t show mean and standard deviation; you can make a display that does of course.\n\nYou can\'t show variance on the same plot as mean or mean +/- standard deviation; it\'s not in the right units (and would be redundant with s.d. there anyway)\n\nWhat\'s ""best"" is more about what you want to achieve', ""I'm not sure about plotting the variance and the standard deviation on the same plot.\n\nBut ignoring that, how about something like this:\n\n[www.biologyforlife.com/uploads/2/2/3/9/22392738/c4ja00347k-f3-hi-res\\_orig.gif](https://www.biologyforlife.com/uploads/2/2/3/9/22392738/c4ja00347k-f3-hi-res_orig.gif)\n\nor this:\n\n[www.biologyforlife.com/uploads/2/2/3/9/22392738/mean-heart-rate-of-participants-with-standard-deviation-error-bars.png?465](https://www.biologyforlife.com/uploads/2/2/3/9/22392738/mean-heart-rate-of-participants-with-standard-deviation-error-bars.png?465)"", 'Thank you! I plotted a bar graph (similar to the first link that you have attached) using Excel. I used SE error bars to depict variance. And the graph has the mean as well. \nIve tried searching but cant seem to tell if the SD is shown on the bar graph. Do you happen to know if the SD is shown somewhere on the graph as well? Sorry for the questions as im still new to statistics!', ""There are options for what the error bars show in Excel. It appears the options are sd, standard error, or percentage.  You used to be able to specify a specific number for the error bar limits, defined in a cell, but I don't see it anymore."", 'Yup! You can just click on any of the error bars in the chart. After clicking, u will see a green + sign on the top right side of the bar. Click on that, mouse over to error bar and slightly move your mouse to the right, youll see an arrow. Click on the arrow, click more options and in the pop up, under Error Amount you can customise the value ', 'Yes.  You can also hot link to the cell values that hold, e.g. the standard deviation values.  My advice would be to calculate the values you want manually (e.g. the means and standard deviation), and use those for plotting.  And double-check to make sure the plotted values to be the correct values.']"
"[Q] When sources about the Mahalanobis distance use the term ""multivariate"", do they actually mean ""multivariable""?","""Multivariate"" means observing more than one outcome (more than one dependent variable). ""Multivariable"" means more than one variable. I don't see how the Mahalanobis distance is related to multivariate statistics, but it's the term that is always used when I see sources describe the Mahalanobis distance. Am I missing something important here?",123s1q0,TaylorBrow,1679934124.0,0,0.32,"['Why are you posting this again? Why did you delete the [last post](https://old.reddit.com/r/statistics/comments/123b1se/q_is_the_mahalanobis_distance_only_used_in/)?\n\n> ""Multivariate"" means observing more than one outcome (more than one dependent variable). ""Multivariable"" means more than one variable. \n\nNot everywhere and not consistently. This terminology is reasonably common in the context of a regression model, but not elsewhere. Pick up any textbook on ""multivariate statistics"" and you\'ll find extensive discussion of unsupervised techniques that don\'t have *any* dependent variables.', ""The data that you are using in the distance is the outcome variable, hence multivariate. This is overly pedantic in my opinion and there isn't really a chance of confusion."", 'Have you heard of https://en.m.wikipedia.org/wiki/Multivariate_normal_distribution', 'I think you mean [https://en.m.wikipedia.org/wiki/Multivariable_normal_distribution](https://en.m.wikipedia.org/wiki/Multivariable_normal_distribution)']"
[Q] How to find an average weekly score in a game I play,"I went to ChatGPT but I probably asked it in the wrong way.

There's a dungeon with 20 sequential rooms, each room has 5 doors. Only one door leads to the next room. Each door has an equal chance of being the right door. If you open a door and it's wrong, you can try another door without that one closing again. (You have a 1 in 5 chance. If you fail, there are only four doors left 1/4 chance. So you're guaranteed on the 5th try). If you open the correct door, you automatically move to the next room. Once you get to the next room, you face 5 new doors.

Every time you open a door, you get points depending on the room you are in. Room 1 gives you 150 points for each door you open. Room 2 gives you 200 for each door you open. Room 3, 250 and so on and so on going up by 50 for each room. Once you're in Room 20, the doors close after you open them so you can keep farming points even if you make it to the final room.

During a week, you can open 28 doors before the dungeon resets. So what is the formula for figuring out how many points on average I get in a week?  


Edit: Just wanted to say thank you in case any of you read this!",123lb7p,rinkley1,1679919457.0,2,0.58,"[""This is quite a complex situation, so it's probably easier to stimulate the situation a few thousand times and observe the distribution of final scores."", ""It's kinda tedious. Get a free trial of [Treeage](https://www.treeage.com/). You should be able to build the model in there."", 'Im bored at work so Ill take a crack. It sounds like an expected value question so you can multiply all possible total points by their probabilities of happening then summing them all up, but I went a different route. How long to expect to stay in a room x the points associated with X number of doors per room. So you have a 1/5,1/4,1/3, etc chance (1/X) of exiting, 5 as the upper bound. Youll expect to leave when youve got the sum (integral) of probabilities of leaving a room =1, so take the integral of 1/x, from 5 to an unknown lower bound, equal to 1.\nLn(5)-ln(unknown) =1, lower bound = 1.839, so 5-1.839 = 3.16, or how many trials youll expect it takes to leave a given room. 28 total trials so 28/3.16 =8.85 rooms, so the summation of rooms 1-8 = 150,200,etc x 3.16 + 85% of 3.16*room 9s value.\n\nTl;dr and Im not sure if my logic is correct: 9710 points to be expected weekly. Doing a monte carlo sim would be an easy nonmath heavy way to check\n\nAlso: say you make it from room 1->2 on your first roll, do you get 150 or 200 points', 'Looks like about 10,200  2400 points.   Youll make it to about 9.1  3 rooms.  Ran a simulation of this 10,000 times, and reported 95% confidence intervals with the .\n\nI wasnt sure on the rules for the last room, but it seems unlikely youll get there often  theres a less than 1 in 10,000 chance youd make it to the last room.', 'I am certainly not sure about this, but my guess would be 10.050 points on average, with 28 doors opened.\n\nAs I see it, there is an equal chance of selecting the door to the next room in the first, second, third, fourth and fifth attempt.\nThis means that on average, you will open 3 doors in each room. This will be the same for the next room and so on.\nSo, in my mind it makes sense to add up 3x""room score"" for each of the first 9 rooms, and then add ""1xroom 10 score"" to get to 28 doors in total.\nThat is 3x150 + 3x200 + 3x250 + ... + 3x550 +1x600 = 10.050.\n\nI am sure that I am missing something here. But it is my best guess, without doing simulations.', ""Since all the stuff is nested I don't think there's an easy back-of-the-hand way to compute that, and it seems like a pain to do by hand. Not complicated, just tedious.  \n\nAre you not content with simulation or do you need an exact answer?"", ""Makes sense. Thanks for the response. I don't feel as dumb."", ""Interesting! Thanks for sharing. I'm kinda relieved to hear it is tedious because now I don't feel as stupid."", ""Interesting! The logic makes sense to me. Thanks for the effort! I might try a few simulations to see if you're right."", 'Interesting! And close to what another person shared. Where did you run the simulation?', 'Probably a bit oversimplified (but maybe not!), but the score is in line with the simulations so you might be right', ""Simulation is fine if the exact answer is too tedious. Figured I'd ask, though"", 'No worries. I procrastinated a bit and simulated your system in R. I assumed that no points were awarded if the door which was opened led to a different room. Unfortunately I deleted the code, but the average score was about 7000, but out of 10,000 iterations no agent ever got to room 20.', 'According to my R sim n=100000 its 10500', 'I just coded it up in Python.  It was about 20 lines.  I assumed the door that led to the correct room also gives you points.', 'Love a bout of procrastination. You do get points if the door is correct, but this is great. I figured getting to room 20 was near impossible.', ""Nice. Below they got 10,200 so I'm assuming it's in this vicinity."", 'Cool! Very cool. And you assumed correctly.', 'Would you mind if I saw the lines?']"
[Question] Linear Mixed Model with missing data? How can I check if its random?,"Hello! I'm doing a linear mixed model in SPSS and as far as I've understood it missing data in this model is not  really a huge issue as long as the missing data aren't in any way missing for a systematic reason. How can I check if the missing data is random or systematic? Also what should I do if the the missing data is not random? I'm a bit stuck so very grateful for *any* help, haha :)

If anyone knows how this is done in SPSS that would also be a massive help! Thanks :)",123f8st,jagedum,1679902265.0,13,1.0,"[""You can't check it - it either is or is not [missing, missing at random, or missing *completely* at random](https://datascience.stackexchange.com/questions/38138/what-is-the-difference-between-missing-at-random-and-missing-not-at-random-data). It's a condition that is up to you to determine.\n\nDepending on the situation, you can either ignore it, drop cases, or impute data."", ""In my experience, data is rarely missing at random. There's basically always a systematic reason that the data is missing; fewer resources/ability available to measure it, participants who don't want to participate for important reasons, too odd to adequately categorize, etc.\n\nBut then, I'm dealing with social data most of the time.\n\nOur models work far better when the data is missing at random. But that's not a good reason to assume the data is missing at random."", 'Thank you for your help! I really appreciate it :)', ""Endogeneity is the biggest problem in statistics.  My field is in economics so I can only tell you the popular method of dealing with endogeneity from econometrics.  \n\nIf you know or think that your data is biased, and you can't get any better data, you can use instrumental variable regression to correct for the bias.  Although getting a good instrument can be just as hard.\n\nIf your data is truncated and you suspect selection bias, you can use Heckman 2 stage regression.  Although you should still use an instrument for this as well.  Without an instrument you'd need to assume some functional form for the bias, which is usually a very strong assumption.""]"
[Q] How to calculate a simple ANCOVA by hand (or what is the step-by-step procedure)?,"I have the csv data below (Gender: Independent variable, Age: Covariate, Height: Dependent variable):

&#x200B;

Gender,Age,Height

M,20,150

M,22,160

M,24,170

F,23,150

F,25,170

F,27,180

&#x200B;

Inputting it in SPSS gives me an ANCOVA, no problem. But I have no idea how it's done. Can someone please give me the procedure? Thanks.",123bgfv,TaylorBrow,1679891624.0,3,0.72,"['Its quite difficult to do ANCOVA by hand, but its just an ANOVA combined with linear regression; You have a categorical predictor and a continuous predictor. Instead of only looking at height across gender, you also include age to look how age influences this effect.', 'you might find a practical way  in some of the old books. From the time when a computer was a person who computes and you had a room full.', ""First - linear regression of age and height. Get the predicted value of height for each ages\nSecond - subtract predicted age from actual age to get corrected ages (this remove the marginal effect of age on height)\nFinally - anova with your categorical variable and the new corrected age variable (note that the actual value don't seem mean much now, but they still hold the partial effect of the categorical variable if there is any)""]"
[Q] Reference request for a quote about residuals,"Hi, this is a bit of a long shot but I am hoping someone here can help me find a quote about residuals. 

If I recall correctly, the quote is an anecdote about a prominent 20th century statistician (perhaps John Tukey). The statistician asks a student why a particular model is useful, the student replies that the model gives a good summary of the data. The statistician says that although that's true, the real benefit of this particular model is that it gives good residuals. Once you take out the simple structure captured by the model, the residuals show the interesting features that were lurking. 

Does this story ring a bell for anyone? I'd really appreciate knowing where I read it. Thanks for your time",1238p8k,micalmical77,1679884902.0,2,0.76,"[""The concept discussed at the end sounds at least very like things Tukey has indeed talked/written about (but also somewhat like things other people have said). However, the story itself doesn't particularly sound like Tukey to me; I imagine its details are possibly either apocryphal or modified-enough in the retelling as to be merely a rough gist of some interaction.\n\nAcross his talks, papers and books Tukey wrote about residuals *a lot*, so tracking down a specific quote might not be so simple.\n\n\n(multiple edits in what follows as I look around for a suitable reference)\n\n\nIt's rather like some of the notions expressed in books he wrote/edited like *EDA*, *Understanding Robust and Exploratory Data Analysis* or *Exploring Data, Tables, Trend and Shapes* for example, but also in several of his other books (whether lone or co-authoured), and in multiple papers.\n\nThe preface of EDA says something very like it a couple of times (e.g. *regard every description (always incomplete!) as something to be lifted off and looked under (mainly by using residuals)*) but I'm sure he's come closer to the notion than that example; he expresses versions of the notion repeatedly. \n\n---\n\nNot particularly closely related to the present discussion, but in looking for some suitable quote, I stumbled again across a display in *Fundamentals of Exploratory Analysis of Variance* that I had quite forgotten. A collection of city effects (essentially levels in a one-way type of model) is plotted *beside* a display of residuals on a common vertical axis, so you can see the relative size of the modelled effects compared to what is unmodelled. If I was doing one now, I'd change some of the details of the display, but the basic concept is informative, concise and simple. The chapter in question is by Schmid (who himself published a boxplot-like display to summarize a set of distributions shown side-by-side, back in the late 40s -- i.e. predating even Mary Spears' range plot), but the display that's used in *Fundamentals of Exploratory Analysis of Variance* is very like other displays that Tukey uses in multiple books; it looks a lot like some of his other displays. A similar display (but now for two-way type models, among others) shows up in chapters."", ""Thanks for looking. I'll take a closer look at Tukey's book. I may be misattributing the anecdote to Tukey. If you think it doesn't sound like him, it could definitely have been someone else (or made up). Thanks again""]"
[E] Thoughts on OSU online MS in Applied Statistics?,"[Link to Program](https://osuonline.okstate.edu/programs/graduate/applied-statistics-master-of-science.html)

I have been looking at different Master's programs for statistics delivered 100% online. My top two choices are:

1. TAMU MS Statistics
2. Penn State World Campus - MAS

TAMU would be my top choice because they seem to be a well-known program. Additionally, they offer a variety of electives, and they are quite cost-effective. That said, I applied once to this program last year and was rejected, so I am gunshy about my chances of getting in when I reapply in the coming months. But I digress.


I also came across the program offered at Oklahoma State University, and it doesn't appear to be appreciably different from other programs on the surface, but it is a much cheaper option than my other two choices. However, I haven't found much online regarding the quality of the program. If anyone has attended this program or knows someone who has, I would appreciate any insight and opinions on it. Additionally, if there are other programs you think I should consider, I am all ears.

TLDR: Title",1237k46,Augustevsky,1679882324.0,3,1.0,"['Id also consider NCSU if its in your price range (especially if your OOS). Looks like I might end up there for a PhD and enjoyed getting to know some faculty a few weeks ago.', ""I had PSU, Ok State, and Colorado State as the three I was interested in. I ended up with CSU because I didn't like the learning style at PSU, it was more rigorous than OkSt, and I especially liked the flexibility with 8 week subterms instead of 16 week semesters.\n\nIf you're okay with the PSU learning style (very notes and reading heavy) I'd pick them over OkSt, they are more rigorous and better known. If you haven't looked at CSU, id highly recommend it too. I had a blast in the program"", 'I agree. Their reputation and curriculum are very intriguing, but at $1600 per credit hour for non-residents, they are outside of my price range. PSU is pretty much my upper limit at ~$1000 per credit hour. \n\nGood luck with your PhD! It seems like a great place to study.', ""Thanks for the insight! I liked the looks of CSU, but they only seem to accept applications once a year on April 15th for the fall. Unfortunately, my transcripts won't be ready by then as I am still finishing up Calc III. Should I be in the same position next year, I will happily apply."", 'Right  gotcha! Im in-state so it works out well. Good luck with yours!', ""I was in the same boat--you can still apply and they'll accept you conditional on getting at least a B in calc III or linear algebra or whatever classes you are missing, as long as you have them complete by the fall. Might be too late now since the deadline is soon at this point :/""]"
[Question] Correlation Coefficient?,"Hello, I'm currently writing a research paper in which I need quantitativly find the correlation between two graphs (same x and y axis). Think GDP or some other economical concept graph which goes up and down a bit. Is a correlation coefficient the right thing to use in that case? My understanding is that pearson and spearman coefficients don't work with graphs that aren't some type of polynomial. How could I go about finding a quantitative relationship for economical type of graphs??",1236gaf,The_Red_Sharpie,1679879759.0,3,0.72,"[""I am not sure that I fully understand your question..?\nGraphs are just visual representations of some data. Don't you have the data available that was used to make these graphs in the first place? Because that would allow to do a standard correlation calculation."", ""Can you use pearson for a graph that isn't linear??"", ""What do you mean by a graph? Don't you have the actual data available?"", 'I do yes', 'Then it should be straightforward to calculate a Pearson correlation of the two data series, as it is just a normalized covariance, which should be between -1 and 1.\nJust make sure that the data is time aligned (on the same x-axis).', 'Okay maybe I misunderstood, what you mean...  \nA correlation factor like Pearson gives you an idea of how two data series are linearly correlated.  \nMeaning, if the series are non-linearly correlated, the Pearson correlation coefficient will be close to 0.  \nFor instance, if Y = X\\^2, then X and Y will be uncorrelated, but are obviously not statistically independent.  \nThis means that just because to data series (random variables) are uncorrelated, does not mean that they are statistically independent. You can perhaps look at [https://en.wikipedia.org/wiki/Pearson\\_correlation\\_coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to get better understanding, of what I mean. There is multiple examples of uncorrelated and correlated .  \nThis page [https://www.freecodecamp.org/news/how-machines-make-predictions-finding-correlations-in-complex-data-dfd9f0d87889/](https://www.freecodecamp.org/news/how-machines-make-predictions-finding-correlations-in-complex-data-dfd9f0d87889/) also gives a good explanation of what to do, when signals are non-linearly correlated.']"
[Q] What are your favorites statistics/probability articles?,title.,1233hhx,camilams,1679872858.0,89,0.98,"['[Parachute use to prevent death and major trauma when jumping from aircraft: randomized controlled trial](https://pubmed.ncbi.nlm.nih.gov/30545967/)\n\n(A funny critique of study design, but good to keep in mind.)', '[The Difference Between Significant and Not Significant is not\nItself Statistically Significant](http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf) Gelman & Stern 2006', '[Probability, Frequency, and Reasonable Expectation](http://jimbeck.caltech.edu/summerlectures/references/ProbabilityFrequencyReasonableExpectation.pdf), by R.T. Cox. \n\nThis is a brief exposition of Cox\'s ""logical Bayesian"" probability theory, which takes ordinary true/false logic as its starting point and generalizes to degrees of belief between 0 and 1 (or, it turns out, equivalently 1 and infinity). \n\nCox\'s stuff is the basis for E.T. Jaynes, ""Probability Theory: the Logic of Science,"" which I also recommend.', 'Leo Breiman - Statistical Modeling: The Two Cultures (with comments and answers by Leo)\n https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full', '[The Garden of Forking Paths](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf)', '[Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-2/Statistical-paradises-and-paradoxes-in-big-data-I--Law/10.1214/18-AOAS1161SF.full)\n\nYou can\'t ""big data"" yourself out of biased data.', ""Unfortunately I don't have any links to the papers, but they are what ended up getting me interested in statistics. \n\nIn no order:\n\nRole of statistics in quality and productivity improvement by George E. P. Box\n\nThe scientific context of quality improvement by George E. P. Box and Sren Bisgaard\n\nTeaching statistics to engineers by Sren Bisgaard\n\nThe quality detective: a case study by Sren Bisgaard, L. P. Fatti, David Cox, and J. Bibby"", '* 2003 - [Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) - Blei, Ng, Jordan', '[Weighting for unequal P](https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/weighting-for-unequal-empemsubemiemsub.pdf)', ""[Abraham Wald's Work on Aircraft Survivability](https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478038)"", '[Is it safe to go out yet? Statistical inference in a zombie outbreak model](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=safe+to+go+out+yet+zombie&btnG=#d=gs_qabs&t=1679941929024&u=%23p%3D3j30ka1Y2FYJ)', 'Good question! As it happens, the author of my favorite stats paper is also the one who inspired me to get a university degree. His name is Edward Thorp, who has also been referred to as the ""father of card counting"". As a newly graduated PhD in mathematics, his curiosity was aroused as to the beatability of casino games, and to an extent with fellow mathematician Claude Shannon, the father of Information Theory and his new boss at MIT, he developed the system of card counting that professional blackjack players use today to tilt the odds in their favor. My favorite book of his is ""Beat the Dealer"", and my favorite paper of his is ""The Kelly Criterion in Blackjack, Sports Betting, and the Stock Market"". Both can be Googled for those who are interested, and his paper is the basis for an intraday stock trading system I\'m developing. In applying his well-proven theory to the stock market, I\'m currently learning how to build a Kalman Filter from which to extract the drift rate (i.e. price trend velocity) and variance rate (i.e. variance velocity) to produce a running count of the ""Kelly fraction"", or the optimal fraction of one\'s account to bet on that stock at any one time. I expect that fraction might fluctuate significantly, or the market might change direction suddenly, so further precautions will necessarily have to be implemented in order to minimize losses if that happens. I hope I\'ve been able answer your question satisfactorily and explain why. Thanks for reading this!', ""It is funny but it is, irritatingly, often miscited to bolster claims that we don't really need to do RCTs even in situations where there are no ethical or practical barriers to doing them. Because, you know, theory (and/or shareholder returns) are enough to just *know* and there's a (Christmas) BMJ article to prove it.\n\nIts arguments apply to those very rare (in the modern era) treatments which can, in principle, be tested in RCTs but are so obviously effective enough to outweigh any other concerns that it's just not ethical to randomise.\n\nThe list is short. Cisplatin for testicular cancer is the only one I can think of (penicillin would count had it been discovered after RCT methods were developed). When you see a change from 90% of your (typically very young) patients dead within a year to 90% surviving the year, you don't need an RCT to prove that the difference is large, even if we can't ever know how large.\n\nBut very few treatments are effective enough to be able to discount any other explanation for observational evidence. There are many, many people who would rather not have to produce solid evidence, for a myriad of reasons. But they're nearly always wrong."", 'My favourite too', 'Also [The Garden of Forking Paths](http://shiraz.fars.pnu.ac.ir/portal/file/?970437/The-Garden-of-Forkling-Paths-Original.pdf), by Jorge Luis Borges.', 'A survey stats classic.', 'better link: https://www.researchgate.net/publication/254286514_Abraham_Wald%27s_Work_on_Aircraft_Survivability', 'Your comments are valid. It was introduced to me as a counter to RCT is the only valid study type; anything less is garbage. I think there are reasonable critiques of RCT and reasonable uses of observational study approaches. This article also alludes to the fact that we need to pay attention to more than p-values/confidence intervals (like a parachute jump test height of .6 meters).', ""There are plenty of situations where RCTs aren't ethical or feasible. But when they can reasonably be done, we should not accept anything less.\n\nIt's a good article. I did say it was *mis*cited.""]"
[Question] Improving random forest model result,"I have a dataset with ~700 rows and 8 predictor variables, 5 of which are multi-class, nominal categorical variables and 3 are continuous. Im trying to predict a continuous output variable. 

Currently, Ive tried a random forest model and have achieved an r squared value of ~0.4 on a test dataset using cross validation with a 80/20 train/test split. 

Looking for suggestions to improve the model fit or alternative models/data pre-processing techniques to try out here.",1233cji,chutkipaanmasala,1679872537.0,3,0.72,['Hard to say without more information.  This might be close to the best prediction that can be made from the information that exists in the data.  Or there may be some opportunities for feature engineering using insight into the problem and the data.']
[Question] Formula to get precision of estimates when running experiments,"I saw a YouTube Video about Monte Carlo simulations and how they can be used to derive Pi. In the video, they used the area of a square and 1/4 of a circle to derive the number of pi by randomly dropping dots in the squared field. 

Later, they state that one has to perform 500,000 experiments to get Pi up to 2 decimals. 3,000,000 repetitions to get Pi up to 3 decimals, and 500,000,000 repetitions to get pi up to 4 decimals. 

I do not understand how they calculated the required number of repetitions and its relationship to the precision of the estimate of pi. Does anyone know the formula or can point me toward a concept I am unfamiliar with (is it statistical error?). Thanks a lot!",122yxeb,4skinLuke,1679863402.0,5,0.74,"['Depends on what specific calculation they did -- there\'s a few possible things (bounds/approximations) they might have done\n\nTheir numbers look high but my first thought  would have been that they\'d be relating it to the margin of error with known population proportion (p = pi/4) and some high coverage (roughly 95% say) ... see the links at the end for such calculations. Or they might have done some kind of worst-case bound.\n\nSo let\'s outline a simple margin of error calculation to try to get 2dp - accurate to say 0.01 or 0.005 depending on what they define as ""accurate to 2 dp"", but lets look at the first - you could look to get the margin of error (MoE) below 0.01.\n\nThe MoE under those specific choices is roughly 2.[pi/4 x (1-pi/4)/n] ~= 0.82/n  (any MoE will be for the form c/n, so once you know one, the others will all use the same c).  To make that < 0.01,  you need n> 0.82 / 0.01^2 or roughly 6750ish\n\nThat\'s way less than their 500K, so they must have done something quite different. Even if they used less-informed calculations (not assuming they knew pi, for example), and stricter accuracy (say 0.005 rather than 0.01, and higher coverage probability) they won\'t get that high -- e.g. we could try the upper bound MoE (start with the worst-case p=1/2), 0.005 MoE and 99% coverage, but that will will still be way smaller than the 500K they got. \n\nFor those numbers we would solve [(2.58 x 1/2 x 1/2)/n] < 0.005 for n ... i.e. n > (2.58 /4/(0.005)^2 ~ 25800\n\nThey have to be doing something else because their numbers are about 20 times as large as that.\n\nIt occurred to me that they might keep simulating it and just stop when some criterion is satisfied, but what exact criterion that could be is not obvious.\n\nNevertheless, the ratio of  accuracies should scale like 1/n. e.g. to get an extra d.p. of accuracy they should multiply their n by 100. Their numbers are going up way faster than that, which suggests they\'re not using binomial proportions or simulation outcomes or indeed any facts about how variance rules relate to margin of error. \n\nMy calculations follow from the standard error of a  binomial proportion, which can be derived from basic facts about expected value and variance. (see the links at the end)\n\n{They may have used some [bound](https://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0124.pdf) / asymptotic bound, or perhaps some other form of [bounds analysis](https://en.wikipedia.org//wiki/Probability_bounds_analysis). It\'s not 100% clear to me which one since the ratio of n\'s doesn\'t seem to correspond to any regular rule I\'d expect. Their number went up by a factor of *just 6* to go from 2dp to 3, and then by a factor of about 170 to go from 3 to 4??!? (170 I could believe, but 6? I can\'t see how that scaling makes sense). I must admit I\'m somewhat mystified by this. Did you copy all the numbers correctly?}\n\nhttps://en.wikipedia.org/wiki/Margin_of_error#Standard_deviation_and_standard_error\n\n\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\n\nhttps://en.wikipedia.org/wiki/Binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Variance#Properties\n\nhttps://en.wikipedia.org/wiki/Expected_value#Properties', 'Thank you for your response. I am a bit new to the topic. I guess what I dont understand is how the error relates to the number of decimals. \n\nIs the correct way to think about it that a standard error below 0.1% would give me a pi up to 1 decimal?', 'Thank you for your detailed explanation! It became clearer to me! I double-checked and all numbers were correct. \n\nI suppose the difference is due to the probability of the position of the dots: In the video, the position of a point was determined using a uniform probability distribution, not a binomial distribution.']"
[Question] Can I use a Poisson Regression here?,"There is a group of 40 people participating in a Badminton Club. 

They play once a week (Player A vs Player B etc) for 60 minutes. 

I am trying to model the number of points Player X is going to get versus Player Y, for the upcoming week.

I have good reason to believe that the # Points can be modelled by a Poisson distribution, so I thought a Poisson Regression would be a good plan of attack.

My dataset contains every match for every player over the past couple of years (with # Points scored).

Now, I have a few questions here: 

1) Can I use Poisson Regression here? The observations clearly arent independent, as different observations will contain the same players.

2) How do I ensure that the most recent observation is far more important to the model than an observation 2 years ago? Do I need some sort of exponential smoothing etc?

3) Is there a way I can combine these single observations in to some sort of running average (over the last X game), and then use these averages as features? For example, I could look at average points over the last 10 game for this player, and use it as a feature in the regression? 

If I do this, can I essentially treat each 10 game cycle as a new data point? Or does that contradict the independence?",122yekn,BasslineButty,1679862352.0,9,0.78,"['> Can I use Poisson Regression here?\n\nThat sounds like a reasonable choice, given what you know already about the game. Another option is binomial (or, equivalently, negative binomial). \n\n> How do I ensure that the most recent observation is far more ""important"" to the model than an observation 2 years ago?\n\nYou might consider including a general ""time"" predictor variable, if you think all players\' points increase over time. If you expected the rates to change differently with each player, you can model that too. I\'m most familiar with hierarchical linear modeling (HLM), which definitely allows you to look at individuals\' growth over time. If you\'re using R, I recommend lme4. \n\nModeling growth using HLM is probably better, since it will auto-assign how much weight to give to your predictions based on when it occurs / players\' specific growth trajectories. However, you can also use whatever weighting scheme you think is best, a priori, and include them as sample weights in your models. \n\n> Is there a way I can combine these single observations in to some sort of running average?\n\nIf possible, I suggest inputting the data you have, and making modeling / design choices that will aggregate the data for you (e.g. as described above). That is, unless you have strong reason to believe that 10 is the right number to use here.', ""1. What is the purpose of this model  . . . What are you using it to do?\n\n  I dont see a suitable way to answer some of your questions well without an idea of what it's for \n\n   > I have good reason to believe that the # Points can be modelled by a Poisson distribution, so I thought a Poisson Regression would be a good plan of attack.\n\n2. Can you clarify what this is? I often see people convinced toward or away from models for mistaken reasons (such as looking at the marginal distribution). I'm not saying you're necessarily wrong - it might well be a reasonable approximation - but it would be good to be choosing it with suitable reason"", 'I am very (very) rusty with stats, but I dont think this fits a Poisson distribution. Poisson distributions work when numbers are random without any trends, and when the denominator for possible occurrences is either too large or impossible to count.  Since the number of games in easily quantified and the score has obvious trends, it wouldnt be an appropriate fit. \n\nIf youre basing it strictly off of previous games played, I would probably look at some type of weighted moving averagegames from the last year with games from the last 2 months given extra weight, for example. I seem to recall theres some way to determine the best time frame and best way to weight the scores for recency to create the most accurate prediction, but unfortunately I dont remember the particulars! Hopefully someone else here can help you with that.', 'Toward question #2, you might want to try searching concept drift. The term comes from data streaming literature. It might give you some ideas about how to regularize stale data.', 'Thanks for this, very useful.\n\nSo for each match/player, I could add a variable which denotes how long ago this certain match was played?\n\nThen my model would infer the importance of the recent observations here?', '1) I am using it to try and predict the Number of Points scored in a match for any given player. \n\n2) Players change abilities & get better/worse over time. Their most recent performances are much more indicative of their next performance (compared to a performance 2 years ago for example)', 'Exactly. There are several ways you can do this. Perhaps the simplest is to use a fixed effects model, where you\'d include a variable for each player, a variable for time, and player x time interaction variables.\n\nAs for how to code time, you can do ""how long ago"" as you suggest, in days, hours, weeks, if those are meaningful measures of time. Or you can set some arbitrary Jan 1st as 0. Just use a system that will be easy for you to interpret.', '> I am using it to try and predict the Number of Points scored in a match for any given player. \n\nThis is not sufficient. Number of points per match is a random variable. What are you predictions here -- the conditional mean of that variable? An interval for the mean? An interval for the number of points itself?\n\nRe 2. I was asking specifically (indeed I quoted it) about where you said ""good reason to believe that the # Points can be modelled by a Poisson"". Nothing in your response tells me why you should have thought that.']"
[E] What is the bare minimum number of stats classes you need to get a job as an analyst? Data scientist? Data engineer?,"Currently working on a bachelors in math with a minor in computer science. I'm heavily leaning towards getting a masters in math to help me get into a math PhD. By the time I'm done, I will probably have an undergrad probability class, and either one or two upper division undergrad stats class or a grad stats class or two.

My question is, how far will this combo of math major + possible masters in pure math + couple of stats classes + computer science minor get me? I really want to get a PhD in math, but there's a significant chance that won't work out, so I'm wondering what I can do if I don't go in that direction. Are there any data scientists out there with pure math masters? I expect not many, but I'm curious what people say.",122v40j,zmzmzmzm1010,1679855580.0,0,0.36,"['For data engineering roles, it probably doesn\'t matter at all. Recruiters will be a lot more interested in whether or not you have proper software engineering skills. \n\nFor the other two, I also don\'t think there is a ""bare minimum number of stats classes"". Sure, it\'s helpful, but it\'s a nice to have I\'d say.', 'dont do pure math bro. i was once a pure math person. never again', '0.', 'More than stats classes, make sure you know Python, R and SQL. You can pick up stats/ML fairly quickly']"
[Question] Minimum Detectable Effect,"Hello everyone,

I just came across the concept of MDE and I can't understand the rational behind certain things like:

1. If I have a baseline conversion rate of 20% and I decide that in order to change my current setting to the experiment setting I need at least 1% increase in conversion rate, then it leads to a 5% MDE. With a confidence of 90% and alpha of 5%, then I need a sample size of 33k.

However, if I decide that I want at least 2% increase in conversion rate, it leads to a 10% MDE which means that I only need a sample size of 8.5k.

Why is that when I want to see a bigger effect (higher conversion rate in this case) that I need a smaller sample size?

Additionally, let's assume that I do run with the 5% MDE and I see that the conversion rate wasn't 1%, but 0,7%. What would you do in this situation?

Thanks for helping!",122mds0,Terrible_At_Parking,1679837976.0,6,0.88,"['>Why is that when I want to see a bigger effect (higher conversion rate in this case) that I need a smaller sample size?\n\nBecause, quite simply, it is easier to detect a bigger effect than it is to detect a smaller effect. A small effect is harder to distinguish from random noise than a larger effect.', ""Think of it this way: it wouldn't take you that many flips of a coin with two heads and no tails to conclude with a reasonable degree of confidence that your coin is biased. But if your coin is only biased 51-49 in favor of heads, then it will take you many, many more coin flips to detect that bias. \n\nThat's an extreme example but the same principle applies. It takes a smaller sample to detect a larger effect. It's not that you *want* a smaller sample in order to detect a larger effect, it's telling you the minimum sample size that you would need to detect the larger effect is smaller than the one you would need to detect the smaller effect with the same degree of confidence."", ""If I understand the calculation you are performing, the effect size isn't the effect size you are looking for, it's the actual effect size in the larger population you are sampling.  It therefore makes sense that you need a smaller sample to see the effect, if that effect is actually large.""]"
"[Discussion] on probability paradoxes in the real world. I.e. Boy or Girl, Monty Hall","There is an average statistical family with two children.We know for sure that at least one of the children is a boy.What is the probability that both children are boys?Correct Answer: 1/3, i.e. 0.33

There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on Wednesday.What is the probability that both children are boys?Correct answer: 13/27, i.e. 0.48 (7\*2-1)/(7\*4-1)

There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on April 18th.What is the probability that both children are boys?Correct answer: 729/1459, i.e. 0.499657, practically 0.5 (365\*2-1)/(365\*4-1)

There is an average family with two children.We know for sure that the eldest of the children is a boy.What is the probability that both children are boys?Correct answer: 1/2

Note  that the more information or noise in these problems, the faster the  answer approaches 1/2. The answers to these problems range from 1/3 to  1/2. Note that 1/3 is obtained only when we are dealing with ""noiseless""  average values, and with each new piece of information, even a tiny  one, the probability immediately approaches 1/2.

Those  kind of probability problems are not only unintuitive because our  brains have evolved to work with natural numbers, but also because  probability theory works with noiseless mathematics, whit no real-world  noise. Our brains don't solve complex differential equations when  catching a ball or thinking about what to buy. Utility maximization and  optimization problem-solving only work in a sterile world where there is  no noise, all unknowns and their corresponding probabilities are given,  and the rules of the game do not change.

In  reality, where there is a lot of noise and the unknown, our brain uses  simple and fast heuristic methods (rules of thumb) to solve problems of  the unknown. One should not think that in the real world of uncertainty,  the one who does not optimize is not rational. The truth is the  opposite: in a world of uncertainty, success lies in simplicity.

One  of our brain's simple heuristics is 1/n, where under uncertainty, we  simply divide into n cases. Now imagine the following situation: I met  Mr. Smith in real life, he said that he had two children and began to  talk about his boy. What is the probability that both children are boys?  My answer, and the answer of a rational person who does not delve into  probabilities would be 1/2! Why? Because we live in a world of  uncertainty, and the more noise in this problem, the faster the answer  approaches half.

The same goes for  Monty Hall's paradox, which says: Imagine that you have become a  participant in a game in which you have to choose one of three doors.  Behind one of the doors is a car, behind the other two doors are goats.  You choose one of the doors, for example, number 1, after that the host,  who knows where the car is and where the goats are, opens one of the  remaining doors, for example, number 3, behind which there is a goat.  After that, he asks you - would you like to change your choice and  choose door number 2? Will your chances of winning a car increase if you  accept the host's offer and change your choice?

From the noiseless point of view of probability theory, the answer to this problem is that yes, the probability does increase and become 2/3. And if I play with a computer or with a statistical design with the rules described above, then I will always change the door.

However, this problem is also counter-intuitive, and our inner voice tells us that there will be no change in the odds, and the probability will be the same 1/n (a simple heuristic), that is, the correct answer is 1/2. And our intuition is right. In this problem too, the more information or ""noise"", the faster the answer statistically approaches to 50%.

This task is similar to the previous one. And in the real world, playing with real people, I would not change the answer and say that the probability will be the same. Why? Because we are dealing with unpredictable rather than sterile probabilistic risk. In the real world, the host (Monty Hall) can play any trick on us, and we will think about hints, and not about probabilities, which is more correct.

So in the real world of uncertainty, trust your intuition, rather than trying to remember and solve complex problems of probability theory.

PS. Also, don't trust the charlatans at fairs, who in both problems, using probability theory in a sterile situation, can get probabilities of 1/3 and 2/3, respectively.

edit. problems with * in markdown",122ku2g,tavad,1679834356.0,0,0.47,"['Sir, this is a Wendys.', 'Those ""correct"" answers in the first three problems really depend on how you got the mentioned information. Let\'s analyze the first one:\n\n**There is an average statistical family with two children. We know for sure that at least one of the children is a boy. What is the probability that both children are boys? Correct Answer: 1/3, i.e. 0.33**\n\nThe reason why 1/3 is supposed to be the answer is because if we list the possible pairs of two children, we get these four:\n\n*A) Boy-Boy*\n\n*B) Boy-Girl*\n\n*C) Girl-Boy*\n\n*D) Girl-Girl*\n\nLet the leftmost one represent the eldest, and the rightmost one represent the oldest.\n\nSo, having said that at least one of them is a boy restricts our sample space to cases A, B and C, from which only in case A there are two boys, and since all of them should be equally likely, that gives us a probability of 1/3.\n\nNow, what some people don\'t realize is that this calculation is only correct if we are sure that when there is a boy and a girl we will be informed that one of them is a boy and never that one of them is a girl. Otherwise, the cases B and C would split in two halves according to which sex we are informed later, and therefore when we are told that one is a boy, we must discard the respective halves in which we would have been told that one of them is a girl.\n\nI mean, if from the start it is guaranteed that they will give you as a clue the sex of one of child, but you never know which one, the possible cases with their probabilities are:\n\n*A) Boy-Boy => 1/4. Here you will be told for sure that one of them is a boy.*\n\n*B) Boy-Girl => 1/4. But it splits in two halves.*\n\n*... B.1) They tell you later that one of them is a boy => 1/8.*\n\n*... B.2) They tell you later that one of them is a girl => 1/8.*\n\n*C) Girl-Boy => 1/4. But it splits in two halves.*\n\n*... C.1) They tell you later that one of them is a boy => 1/8.*\n\n*... C.2) They tell you later that one of them is a girl => 1/8.*\n\n*D) Girl-Girl => 1/4. Here you will be told for sure that one of them is a girl.*\n\nAs in this case you were told that one of them is a boy, that only leaves the cases A), B.1) and C.1) as possibilities. Both children would be boys if we were in case A), that originally had 1/4 chance, and one of them would be a girl if we were in case B.1) or in case C.1), that together also add up 1/4. So both being two boys or one being a boy and the other a girl had the same original probability 1/4, that must represent 1/2 at this point (with respect of the subset of the remaining cases after eliminating those that are no longer possible). So 1/2 is the correct answer this way.\n\nThe probability 1/3 would be correct if, for example, you had asked your informants something like: *""One of them is a boy?""*. That way they would have been forced to tell you that one is a boy even in the cases that there is a girl. But without a restriction like that, the answer must be 1/2.\n\nNow, strictly speaking, the statement ""at least one of them is a boy"" should be calculated as the conditional probability that gives us the 1/3 as result, but in practice I would bet that it is not the intended problem most of the time, because I don\'t think they are pretending that assumption of always being told the same sex ""boy"" whenever it is possible.\n\nThe same reasoning applies to your second problem:\n\n**There is an average statistical family with two children. We know for sure that at least one of the children is a boy and was born on Wednesday. What is the probability that both children are boys? Correct answer: 13/27, i.e. 0.48 (7\\*2-1)/(7\\*4-1)**\n\nThat supposed correct answer is only true if whenever there was a boy that was born on Wednesday, they would have told you that information and never the sex and the day of birth of the other child when those parameters are different. And this can be extrapolated to the third problem as well.\n\nNow, in the Monty Hall problem what occurs is that it is usually assumed as a rule, despite not always well stated, that the host will always reveal a losing door from the two that you did not pick and offer the switch regardless of what you initially picked. It is not his decision to sometimes offer the switch and sometimes not, otherwise is no longer the same intended game.\n\nYou say that in the real world the probability would be 1/2 because the host\'s actions would be unpredictable, but if his desire is to trick the player, he could also use the Monty Hall mechanism in his favor: Imagine that after you make your selection between the three doors you make a bet about if you managed to pick the prize or not. Then the host proceeds to reveal a losing door from the other two, not to offer any switch, but with the purpose that you increase your bet thinking that now it is more likely that you guessed right. But it is a trick, he could have revealed a losing door regardless of if you got it wrong or not, so your chances are still the same.\n\nIn conclusion, all problems that you put here share in common that they start with a list of possible cases, that are equally likely, but later, after we get information, not all of them remain in their whole, but only part of them prevails. That means that the remaining cases are no longer equally likely, which changes their original respective proportions. In the first three problems, that disparity is what ends supporting the intuitive answer 1/2.\n\nIt is worth noting that this has already been discussed previously. For example, The YouTube channel Zach Star has two videos about it, that are the links below. In the first he states the problem, but didn\'t understood it yet, and it is in the second that he explains the solution:\n\n[https://www.youtube.com/watch?v=bDZieLmya\\_I](https://www.youtube.com/watch?v=bDZieLmya_I)\n\n[https://www.youtube.com/watch?v=ElB350w8iJo&t=255s](https://www.youtube.com/watch?v=ElB350w8iJo&t=255s)', 'An important point for the Monty Hall problem is that the host will never open the door that reveals the car. If he could, then switching does nothing because then it means he picks random doors.', 'Real-world information is almost always a bit more slippery than carefully constructed examples. Take your example for instance:\n\n> I met Mr. Smith in real life, he said that he had two children and began to talk about his boy. What is the probability that both children are boys? My answer, and the answer of a rational person who does not delve into probabilities would be 1/2! Why? Because we live in a world of uncertainty, and the more noise in this problem, the faster the answer approaches half.\n\nIf he actually used the worlds ""my boy,"" your answer should be very nearly zero. He used the label ""my boy"" because it identified which child he meant; he would have called the other one something else, probably ""my girl."" \n\nIf he said ""my oldest"", and then referred to him as ""he"", you answer should be somewhat more than 1/2, but less than 1: did he say ""my oldest"" because sex was irrelevant, or because sex wasn\'t going to distinguish them.', 'I have nothing to add, but just wanted to say that I liked your post; particularly the angle about the significance of noise. \n\nHave a great day.', ""Totally baffled by the Wednesday and April 18th numbers. Going to have to take my time to understand the appeal of the models producing these\n\nI agree with the overall thrust of the argument though\n\n>So in the real world of uncertainty, trust your intuition, rather than trying to remember and solve complex problems of probability theory.\n\nI think the key is that in real world you need good models. Probability tells you how to reason within a model, but doesn't tell you what the salient ingredients of a good models are."", ""Well, I was hoping to order a double bacon cheeseburger with  \na side of spicy discussion, but I guess I'll have to settle for just the regression  \nto the mean."", ""plus he should always offer to switch the door. In the real life it's not always the case."", 'Thanks for your response.  \nYou can read Rationality for Mortals by Gerd Gigerenzer and his other works. My thought here were partially influenced by his work.', 'Okay.\n\nPlease define\n\n> average statistical family\n\nCan you expand on how you arrived at the probabilities for the following.\n\n1)\n> There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on Wednesday.What is the probability that both children are boys? Correct answer: 13/27, i.e. 0.48 (7 x 2-1)/(7 * 4-1)\n\n2)\n\n> There is an average statistical family with two children. We know for sure that at least one of the children is a boy and was born on April 18th.What is the probability that both children are boys? Correct answer: 729/1459, i.e. 0.499657, practically 0.5 (365 **2-1)/(365 **4-1)', 'This shows all the boy/girl pairs as well as the possible weekdays on which they could be born. Green represents situations with two boys, at least one of which was born on a Tuesday. Yellow represents at least one boy born on a Tuesday. Red is neither. Hence the answer is green/(green+yellow)= [(7\\*2-1)/(7\\*4-1) = 13/27](https://www.jesperjuul.net/ludologist/wp-content/uploads/2010/06/fullGrid.png).\n\nThe same goes for the next question, but we have 365 tiles in this case.\n\nAlso see this section: [Information\\_about\\_the\\_child](https://en.wikipedia.org/wiki/Boy_or_Girl_paradox#Information_about_the_child).', '**Boy or Girl paradox** \n \n [Information about the child](https://en.wikipedia.org/wiki/Boy_or_Girl_paradox#Information_about_the_child) \n \n >Suppose we were told not only that Mr. Smith has two children, and one of them is a boy, but also that the boy was born on a Tuesday: does this change the previous analyses? Again, the answer depends on how this information was presented  what kind of selection process produced this knowledge. Following the tradition of the problem, suppose that in the population of two-child families, the sex of the two children is independent of one another, equally likely boy or girl, and that the birth date of each child is independent of the other child. The chance of being born on any given day of the week is 1/7.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)']"
[Question] why logistic regression shows all variables are statistically NOT significant when dataset is unbalanced?,"When I balanced the dataset, I could see which variables are statistically significant. However, when I choose to work on the original dataset (where Y binary value is in ratio of 65:35~ish), all of the variables suddenly has P values > 0.995, which means I cant have a logistic regression?",122g31k,waterfall88888,1679819921.0,27,0.92,"['You [shouldn\'t need](https://stats.stackexchange.com/questions/6067/does-an-unbalanced-sample-matter-when-doing-logistic-regression) to ""balance"" a logistic regression. Unbalanced data is more of an issue in non-parametric ML models (e.g., random forests, etc.) where the models typically won\'t have an intercept term to account/control for the overall marginal class probabilities.\n\n(Although small samples CAN certainly lead to issues with logistic regression - for example, it increases the likelihood that one or more combinations of your predictors will perfectly align with your class labels, leading to what\'s called ""[separation](https://academic.oup.com/aje/article/187/4/864/4084405)"", which leads to issues like non-existent maximum likelihood estimators, etc.)\n\nBut in any case, a ratio of 65:35 is frankly not that unbalanced, and I would suspect any attempt to ""rebalance"" is only doing more harm than good. \n\nAnd as another person said, having ""significant"" variables does not make a model ""good.""', 'You have 200 Datapoints for single independent variable male/ female equally split\nIn the dataset,\nMen have a 1/100 chance of label 1\nWomen have a 2/100 chance of label 1\n\nIt should be clear that this sex differences  is likely chance.\n\n\nNow you rebalance the dataset keeping 200 points\nMen now have a 33/100 chance\nAnd women have a 66/100 chance\nNow this sex difference no longer seems random', ""Who told you you can't have a logistic regression with insignificant variables? In any multiple regression scenario, individual p-values are close to worthless."", '""Why do I observe differences when I analyze different data""\n\nGeez I wonder what could be causing this...', 'Calculating standard errors and p-values assumes that each observation (row) is a statistically independent measurement.  If you duplicate rows and artificially grow the data set, you are violating this, and the p-values are no longer valid.', 'Hello thanks for the elaborative answer! So in the event I need to perform logistic and random forest for a particular dataset, Ive to use unbalanced and balanced data respectively instead?', 'As the mean of binary variables deviates from. 5, three variance decreases, which attenuates their observed relationships with other variables', ""In what way is unbalanced data more of a problem for random forests? I don't believe that's actually the case, random forests and gbms are used plenty in many rare event problems."", 'Hello! Thanks for the answer!\n\nMay I ask, when you rebalance, what are you rebalancing? Okay thought when we mean rebalance, it means to make the number of men and women the same in the record? (Which is already the first case scenario?', 'Could you expand on this topic more please?', 'So is there little value in running individual coefficient tests (like a t-test) vs like running an F-test comparing an unrestricted and restricted model', 'See my response to /u/orgodemir below - there are situations where a simple logistic regression benefits from methods to correct bias associated with rare events, but handling that is (IIRC) more often done using some kind of estimator correction than some sort of upsampling or downsampling-type approach.', ""I only meant that it's an issue one might have to pay somewhat more attention to in such models, not that it can't be handled. \n\nObviously such models are widely used in imbalanced/rare event settings, but it's not for no reason that people who work with them spend so much time developing various upsampling/downsampling/etc schemes. \n\nAs another poster pointed out, logistic regression \\*does\\* have it's own bias issues associated with finite samples and rare events. But to me, it seems like less of an actual problem to deal with: ML estimates are biased in finite samples, true, but are (IIRC) still asymptotically unbiased, and that bias seems to be more a function of the absolute numbers of the rare events (e.g., [here](https://statisticalhorizons.com/logistic-regression-for-rare-events/) and [here](https://proceedings.mlr.press/v119/wang20a/wang20a.pdf)) than their relative occurrence. And if you are in a situation where you feel the need to worry about that potential bias, there are some simple penalized estimators available (e.g., Firth's) that seem to work pretty well.\n\nSo that's why I say that it seems -- to me -- that imbalanced data is usually less annoying to deal with in logistic regression than in something like RFs."", 'Pls check my answer under another comment.', ""Usually softwares show you a p from a Wald test. The problem with Wald is that you have to make some assumptions about sampling distributions (among other things) which are not nearly as neat as you would get in a run of the mill OLS. This is going to mess with your p and SE.\n\nThe other problem is that the second you deviate from a full model run on the original data, the p will be more and more complex in meaning. It will depart from being P(data | other variables+hypothesis), and will become a ragtag mix of a monster, being P(manipulated data+specific model tweaks | other variables+hypothesis). All of your choices will be built in these solitary numbers.\n\nThere is also the roles variables play in the mechanism you are trying to model. In most cases we have limited idea of confounders, mediators, suppressors, moderators. Suppressors for one have no goals in reaching a low p value, they are just there to prop another IV up. (If I can confer a conscious behavior on an IV.)\n\nI haven't gone into the topic more, but likelihood ratio tests are thought of as better alternatives for comparing two models than Wald p values. In the end the only thing that matters (given you at least have a model that is better than a null model) is that you understand your variables and their sophisticated relationships.""]"
[Q] Literature review - how to filter out redundant search results from similar search iterations?,"Hey all, I've got sort of an unusual research question. Basically, I'd like to perform a comprehensive review of all the literature of a particular topic. To do this, I'd like to use combinations of search terms. For example, I'd conduct a search using terms ""A"" and ""B"", then I'd conduct another search using terms ""A"" and ""C"", then again using ""A"" and ""D"", etc. The problem with this is that there's a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?

I'm using OVID Medline as the search database, but I'd be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it's possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn't an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.

I can explain or clarify the problem further if that's helpful. Thank you for any help or suggestions with this problem!!",122cq6o,pantaloonsss,1679810222.0,1,1.0,"['Your query isnt one of statistics.\n\nThis said, in Scopus and Web of Knowledge you can exclude items. Perhaps the same can be done in OVID.\n\nDo A + B. Then do A + C - B, which should retrieve all combinations of A and C that do not contain B. Perhaps then do C + B - A.\n\nI suspect all the various combinations will require post-selection filtering in some fashion.', 'Thanks for the suggestion and you are correct that this question isn\'t about statistics specifically. I wasn\'t sure which subreddits have a large enough community that might be able to help with this problem, so I posted it here.\n\nI like your suggestion and will do some tests to see how it goes. One thing I will mention is that ""B"" and ""C"" are subcategories of ""A"", so I don\'t think a C + B - A search will be necessary, but I think this is a great thought. Thanks again for your help!!']"
[Q] total possible tic tac toe combination,"Im bored and dont know how to solve, and google said it is 3^9 meaning that it is possible for a board to be full of Os

Things to note
1. X goes first meaning that its possible for a single X to be on a board, but not a single O
2.game doesnt have to be complete, meaning that the board being completely empty counts and as possibility
3.games cant go on after someone wins, so X cant win twice unless one X completes 2 lines, and that both X and I cant win

If theres something I forgot Ill add it",1226wh2,TonyyZambonii,1679794393.0,2,1.0,"['Going through all options with a computer is probably easier than trying to do this with pen and paper.\n\nIf we treat rotations and reflections as different games then there are:\n\n* 1 option with no filled field\n* 9 options with 1 filled field\n* 9\\*8 options with 2 fields\n* 9\\*8\\*7/2 options with 3\n* 9\\*8\\*7\\*6/(2\\*2) options with 4\n* 9\\*8\\*7\\*6\\*5/(3\\*2\\*2) options with 5\n\nFor 6 fields you now have to subtract all cases where X already won. For 7-9 fields you also need to consider O winning.', ""I'm not a statspert but I believe it's 3^9 /2-1, as like you said there must be an even amount of x and o."", 'On the first move, there are only 3 real choices for X. Middle, Corner, or Edge, since the 4 Corner and Edge positions are just rotations of each other..']"
[Research] Research,"Can you guys suggest me some statistical tools to identify correlation between 10-50 variables? I'm only aware of ANOVA tests. 
Thank you!",121us1n,Relative_Duty_7614,1679769198.0,1,0.57,"['Do you want to compare them against *each other*? If you have 10 variables, you\'re looking at 45 comparisons, if 50 variables you have 1000+ comparisons. \n\nOr do you have one specific variable that you want to compare with the other 49?\n\nDifferent methods are available depending on what you\'re trying to do. Some could be seen as more ""sophisticated"" than others.', 'I have 50 variables and I want to compare them with each other.', ""What type of data are the 50 variables? (continuous, categorical, etc.)\n\nIf they are all continuous, you could do a correlation matrix to get an idea where the correlations are. If some continuous some categorical you might have to parse the different data types and use the appropriate method for each comparison:\n\n* continuous v. continuous\n* continuous v. categorical\n* categorical v. categorical\n\nAlternatively you could look into Auto ML approaches as they seem to be getting popular these days. Though I am not familiar with them, so wouldn't be much help."", ""It is for a project in which I will be assigning marks to my subjects (50 subjects) and check whether  correlation exists between them or not. \nAuto ML won't work for me because I don't have enough time to learn ML.""]"
[Question] Basic knowledge with stationary process,"In a stationary process how do we read interpret the q-stat and prob columns on eviews with the correlogram and also, why dont we seek in the normal law table to find the critical value ?",121ql00,Sweaty-Win-1963,1679760732.0,0,0.5,[]
[E] Can I get into a top MS/PhD Statistics program with a pure math undergrad?,"Background: Graduated top of my program (University Medal) in pure maths from a top Australian University (GO8), and have been working in industry as a quantitative researcher for \~2-3 years now. I'm considering going back to do a a MS or PhD in Statistics; however, I'm concerned that I don't have the background to get into a competitive stats program (i.e. Harvard or Stanford) as I only ever took two stats related classes: a first year econometrics unit and a second year mathematical statistics classes (with rigorous proofs). Back in my undergrad, I did a bit of research in my and co-authored a few papers as at that point, I was considering in pursuing a PhD in pure maths in the US.

&#x200B;

EDIT: My Honours thesis was to do with metric spaces, and I pretty much took ALL math courses (advanced, reading classes, some grad level courses) during Uni as back then I was pretty set on pursuing a pure math PhD.

Hope the information that I provided wasn't too vague as I'd like to avoid doxxing myself too hard, but feel free to let me know if I need to clarify or add on anything.",121fphf,RelevantCobbler5549,1679733657.0,33,0.8,"['Yes, a lot of people have done so. In fact, they would prefer a strong math student to a strong statistics student.', 'Short answer, yes. A strong math background is much more important than a stats background for those programs.', 'It worked for me, went to a top-5 rated program in the US. I also had research experience, which I found very helpful.', 'They are looking for strong math skills; they can teach you statistics.\n\nAlso, many, many graduate programs in many fields will snap you up, if math skills are the limiting factor among potential students.', 'This is just my opinion, I am not an expert on this, but I am a Stats PhD student in the US so thought I would chime in. Applying and getting into an MS is completely different than applying and getting into a PhD with funding at the types of schools you described. I think given your gpa is 3.9 or higher, you could probably get into some good MS programs fairly easily. In terms of the PhD its going to be extremely difficult, even if you had a perfect resume and academic history. The fact is that the Top 3 stats programs in the US or even Top 5 each only take at most 10 people a year. You are competing with thousands of people, some of whom have worked at FAANG, published tons of Stats papers as researchers at these companies and have US based education, which will be viewed more favorable than an Australian education. It also doesnt help that you will be put it the international application bucket, which means you are likely going to be compared directly to students from all over the globe. \n\nThis is also when you have to mention there is tons of luck and random chance involved in the PhD application cycle even ignoring everything else involved.\n\nPersonally, if I were you, I would try and get into a good MS program, do really really well in that and do a program that is thesis based, then apply to a PhD program if you still really want to do it. Applying with an MS and a good GPA will give you a much higher chance of getting into some excellent PhD programs. \n\nBut make no mistake, there is a chance you will never get into a Top 5 PhD program given how competitive they are and how small the departments are. There also just arent as many Stats programs as there as say history or psychology or biology, so every spot inside the T50 in the US is pretty damn competitive, talking under 10% acceptance rate at the highest and as low as 0.5% at the top end.\n\nIt might not be what you want to hear, but I think as someone who went through the process this is very realistic and honest advice.\n\nAgain just my opinion, but I hope this helps!\n\nGood luck!', ""I am currently a stats PhD student at Stanford and have a very similar background. I did pure mathematics for my undergraduate at ANU (Graduated 2020). Feel free to message me or comment any questions - I think you've got a good shot!"", 'MS (US): Definitely doable given your background, but the COL can be problematic in the US (plus the tuition inflation for non-residents)\n\nPh.D.  (US) : It is pretty uncertain at this point. To be frank the top Stats programs are **really** hard to get into nowadays (totally different from what it was 5-7 years ago). I am not saying your profile is subpar or anything - it is quite contrary to that, but luck itself is never predictable.\n\nYou can look into the UCSB stats program. The stats department is quite quantitative finance/risk emphasized and given your background, it could be a perfect fit.', 'Can I pm you more about your work as quant?', 'You would have a shot at the top schools since you have math research experience assuming you have good LOR and write a decent SOP. Standard stats is practical a math program', 'I think you can have a strong portfolio. You have the parts, but sometimes it\'s how you put it together.\n\nYou need to write a compelling professional statement of purpose; not corny stuff like many people write (I want to use stats to cure cancer). Focus on the skills you\'ve developed from what you\'ve done, don\'t just list things. For instance, from the co-authored papers, you now know how to work in a collaborative academic setting, how to write/present a paper, and you know how the peer-review process works (in addition to whatever you learnt doing the research). You can use your work experience as motivation to moving to Stats; it\'s great to have concrete experience that motivates you, because it\'s not vague (e.g. ""GPT chat is the future and I want to do a PhD in Stats"", please no!).\n\nYou will need strong letters of recommendation, preferably from professors who have good academic records. Best letter I\'ve seen for an international student was one that said something like ""My previous student to do a PhD in this area in the US is NAME who is now a tenure-track professor at Ivy League"". They probably know Americans expect different type of letters than Brits, but some professors from foreign universities don\'t necessarily get that.\n\nSome places might accept additional materials, so you could submit your thesis, but I would include an abstract (1 paragraph) and a long summary (1 page), and make clear this is your undergrad thesis. All of that should be in your first page. Also, include links that work to all coauthor publications, maybe even create a google scholar profile for you and add that to your resume.\n\nDon\'t focus only on the top 10.\n\n(a)  If you have an area in stats that you are particularly interested in, look for great places to do that but that are not in the top 10. There are several areas within stats, and you\'ll find a cluster of professors that focus on that in other universities in the top 20.  \n\n(b) Don\'t forget to check universities that have a stats department but they don\'t call it stats department. In my opinion, Caltech is an amazing university for so many reasons and one of my siblings did their PhD there, but their department is called ""Applied and Computational Mathematics"". Now, because of your profile, you might even have very good chances somewhere like that!', 'I think your profile is honestly what the top research departments would prefer over pretty much anything else.', ""You are probably fine in terms of preparations. The difficulty is that top stats departments like Stanford have become crazy competitive as people have discovered the applicability of such backgrounds to fields like machine learning. I'd ask your professors on guidance on how to position yourself in the right way and whom to get recommendation letters from etc. As you were top of your class, you presumably should have decent relationships at your old department. As a backup you might also consider asking them whether a masters program would make sense as a next step (say masters in maths at Cambridge or stats at ETH Zurich or ML at UCL etc.) in case you don't directly get into a top US PhD programme."", 'For sure', ""Yes, but look beyond just top schools. I'd aim for top 30-50, which generally includes flagship public universities and some large private universities."", 'Howd you get your QR position?', ""You should be fine to get admitted somewhere good, but what's the obsession with top 5?"", 'Yes, I have a PhD in statistics from the US with BSc in mathematics', 'Usually require measure theory to understand martingale and a lot of probability, which is essential in statistics. Student with rigorous math background would benefit more. However, if you are very into pure math and thinks too rigorously, then you might find statistics full of flaws that dont suit your interest. Relaxation of rigor is essential for anyone in Finance math ,stat or related fields.', 'May I presume to ask to hear the ""long answer?""  :)', ""I hope you don't mind me asking, but did you do anything 'extra' besides just the pure maths component? i.e., taking stats electives, a second major/minor in stats or something related,  extracurricular projects etc."", ""Tuition inflation for non-residents is only at state universities. I just wanted to point that out. Private universities don't make the distinction. This is why getting in some state schools is harder for international students, as they cost close to 2 residents. Some departments that don't have good funding rarely accept foreign students unless they have Fulbright or they are truly exceptional."", 'Just to clarifyyou mean for PhD programs? If I may be completely honest, for a masters degree however, if it is not a ""top"" school, then I am not sure whether or not it would be worth the opportunity cost to leave my current job and travel half-way across the world paying exorbitant tuition fees. For a PhD program, I\'d be less hesitant.', ""PhD programs want people who can handle the math-intensive coursework. Many stats majors don't take enough the necessary math to do well in those courses. On the other hand, you'll be competitive with any major if you have good grades in the full calculus sequence, linear algebra, and real analysis."", ""I took the stats class that was required for the undergraduate major, 2 semesters. It's what started everything - discovering that I liked it so much more than pure maths.\n\nI was involved in non-mathematical research, had a couple conference papers, and excellent gre scores. Plus I'm American, that might have helped."", 'Oh that sucks :(', 'PhD of course. Go straight for a PhD program if you are applying in the US.']"
Is it worth still trying to pursue a PhD in stats? [Q],"I was essentially 0/13 on phd stats programs this cycle. I got into the one school I applied to which was a funded MS program in stats at a small school. While I should feel excited, I just dont anymore. Because I have to do this intermediate step as a MS, I feel like Ive been held back so to speak and cant start my PhD at the same time everyone else is. Now a process which normally takes 5 years is gonna take me 7, because I have to do this MS, then reapply and then none of my courses transfer so I have to retake the classes. Not to mention, my school I go to is so small that there is no PhD program (R2 school) so I cant transfer in. Thus, with this MS I really am doing everything again for the first two years.

It sucks cause a semester ago I was passionate about research and going down that path but now that its gonna take more time Ive decided its probably best I try and get into industry after my MS and give up on the phd. 

Can anyone here speak to how they felt towards doing their PhD after completing an MS in stats at another school first? Im trying to find good in this situation rather than bad. I figured maybe its a time to figure out if I want to do a phd still. Did an MS in stat first help you focus your research interests more? What was the benefit of doing an MS first.",1219y0q,Direct-Touch469,1679716946.0,8,0.83,"['[deleted]', 'Are you positive the classes wont transfer anywhere? I have heard of some PhD programs requiring people to take a certain class again if their version is specialized or particularly rigorous, but not ignoring all classes completely. If nothing else, you may be able to test out. I would certainly double check on that aspect. \n\nI think a funded MS sounds great! You can build up your professional network and still have the option for professional R&D or a PhD (ETA: at programs that do not offer them; the faculty members I know all have PhDs themselves but are just starting out) at the end. \n\nI might make the decision based on some of the faculty. Not stats-specific, but I know a couple faculty members at some programs without PhDs that are doing really cool research with their students. If this is that kind of place or if there is a faculty member there publishing on topics you find compelling, this may be a great option.', ""I gona tell you my experience (I stopped at MSc) and some friends (they got their PhD). I started to work as soon as I ended my bachelor's and after a year I manage to get permission to do the MSc while working. I have now +10 years of experience and a well pay work. By the other hand, some of those friends that got the PhD have returned to the country and have been struggling to find a good job. Some of them have jobs which have a salary inferior to mine and I find it disturbing. Some others are doing pretty well, they are working as professors abroad. Some of them would like to return home but they just can't because it would imply getting an inferior payment.\n\nThat being said, money isn't all, If you truly want something with passion and conviction you should totally pursuit. If you are really good at whatever you do, you will succeed in the end.\n\nThose are my 2 cents, best of luck ."", "">I feel like Ive been held back so to speak and cant start my PhD at the same time everyone else is.\n\nThis is not a race. There's not an age or time in which you have to a PhD. The goal is to be successful and doing a fully funded MS is going to put you in a path to be successful whether you choose to reapply for a PhD afterwards or not.\n\nWhen you are a PhD student, you don't have a checklist that you simply go through and easily check things off. It's a lot of work and from the view of someone who has taught PhD stats and advised students, some students do need the masters program before going for the PhD program.\n\nAlso, I don't understand why you think a fully funded degree is a setback and not an opportunity. You are not getting into debt. And PhDs and postPhD is a lot of setbacks, rejection from this journal or this grant, and if you are not resilient, I wouldn't advice doing a PhD. That doesn't say anything bad about anyone. You'll just be happier doing something else, particularly if you think you are supposed to do X by this moment in life and Y at this other moment."", '0/13... what are the odds of that?', 'Yeah Im thinking of getting the ms and then just working, I cant even get into a top program for my PhD so a phd isnt worth it for me at that point', '""Life is not a race, and comparison is the thief of joy"" \n\nI needed to hear those words thanks!', 'Im not positive, but the people who I spoke to from that school who are at PhD programs have been telling me how they have to retake everything.\n\nYes, there is opportunities for me to do research which is nice. But it feels like with my MS I dont really have any more seniority in the job market over a BS', 'Thats one of the reasons why I changed my mind during my masters and decided not to go for a PhD, the immense loss of geographic freedom.', 'Exactly right. There is zero reason to do a PhD at a second-tier school.', ""Most PhD programs don't allow courses to as equivalent, particularly if you are going to a higher ranked program. When I taught a required course in the first semester of the PhD program, I denied any requests from students to skip it (the level was not there).\n\n  \nWhere I did the PhD, they didn't even bother asking professors because it was a flat no. The rumor was that they had allowed it to someone who had switched from another top 10 department and that person didn't pass comps, so they stopped altogether. It probably had more to do with the person, but one thing for sure is that skipping required courses is not a good idea because you are alienated from your cohort.\n\nBeing able to get 1 or 2 courses off your list is not going to make you graduate earlier from a PhD.""]"
[Q] How advanced is this statistics question?,"I took an introductory statistics course at college. I just thought of this question and realized I don't know how to solve it:

""Take a random man and random woman from the population. Given the mean height and variance for both groups, what is the likelihood the man will be taller?""

Is it an intermediate or advanced question? In what class are we typically taught how to solve questions like this?",12185gy,ThrowawayHomesch,1679712534.0,12,1.0,"['>Is it an intermediate or advanced question?\n\nIt uses elementary concepts, but it\'s a question students will struggle with. The difficult part of this is to translate it into the more abstract ""sum of two normals"" problem people have been describing itt.\n\n You don\'t learn that by memorising (or even understanding) the rules of how means or variances add up', 'This question is impossible to answer without making an additional assumption about the distribution of these heights. If we assume that both groups are normally distributed, then you can do it as follows:\n\nX is the height of a random man. It has mean m and variance v.\n\nY is the height of a random woman. It has mean M and variance V.\n\nX-Y is normally distributed with mean m-M and variance v+V. The man is taller if X-Y>0, so we need the probability that a normal distribution with mean m-M and variance v+V is greater than 0, which you can calculate in any statistical software.\n\nHow advanced is it? Uh . . . you need to know the distribution of the sum of two normal variables. This is not very advanced.\n\nNOTE: The assumption that both groups are normally distributed is probably quite wrong in this situation. Heights of humans are often approximately log-normal.', 'It is a typical first-semester statistics question.\n\nYou\'ve probably been told to assume both groups are normally distributed. You\'ve probably been given a formula to compute the mean and variance of the sum or difference of two random variables (or more generally, the mean and variance of aX + bY where X and Y are two known distributions and a and b are two constants.)\n\nIt is a bit unusual if your book used the word ""likelihood"" rather than ""probability"" in the question.', 'Imagine a woman of height x. Then calculate the probability that a man is taller than x. But x is random also, so you have to multiply the previously found probability by the probability for the woman to actually be x tall. Now do that for all x and add up the results. \n\nMore formally, you would find the joint probability density of the male and female heights and integrate over the set where the male height is larger. In this case the problem can be simplified thanks to normality and independence, as boblikesbob pointed out.', 'My approach:\nInput the mean height of a woman into the pdf of mens height and find the probability that a man is greater than that value.', 'Found this reference that could be useful.\n\nhttps://softwaredevelopmentperestroika.wordpress.com/2018/01/16/probability-of-a-random-man-woman-pair-where-the-woman-is-taller-than-a-man/', 'Its pretty basic, search on youtube for examples. Statistics seems so hard at first but when you get it you get it, I took an 8 month long course and spent the first six months completely lost. Clicked with me one day and I ended up getting nealy 100% in exams after failing for nearly the whole class. Good luck!', 'Calculating the CDF of the distribution of womens height until the mean of the mans height. This should give you a pretty good approximation.\n\nEdit: so to answer your question, this is first year uni level advanced.', 'I suppose you\'re meant to assume normality for the heights; if so, intermediate at best, basic if it was a math-stats subject rather than a ""explain everything in words"" one. Whether you would have learned the material required depends on whether the course you did covered the fact that the sum (or difference) of two independent normals is normal, and the basic properties of means and variances.', 'thats a moderately advanced probability question. a little calculus will do it.', ""It's a version of a question I use in stats homework, for the first course in probability. I do it with other distributions than normal.\n\nThe question as stated isn't answerable, but if given X\\~binomial/normal/etc it's pretty straight forward, and distributions of sums of random variables are essential in building up to estimators."", 'I thought of the question on my own. I took first semester statistics but they never taught us how to solve something like this.', ""This gives the wrong answer, because the variance of the man's height is less than the variance of the difference in heights (which also includes the variance of the woman's height). Suppose the average male height is 175cm and female is 160cm. For your answer you get the same result regardless of the variance in female height, but obviously the larger the variance in female height the lower the chance the man is taller.\n\nNote, I'm picturing 2 normal distributions here, which is the usual assumption in this problem but OP didn't state it."", 'and just to add on to this to provide some more theoretical justification, suppose X = male height follows normal with those parameters and Y = female height follows normal with those parameters.\n\nWe are curious about the P(X > Y) or other words, P(X-Y>0). You can derive the joint distribution but also iirc, X-Y follows a normal as well (under independence) with mean (\\\\mu\\_X - \\\\mu\\_y) and the variance is the sum of the variance (recall some properties of variance) and if you plug it into R to find the probability that a women is taller than a man:\n\n`pnorm(0, mean = 177 - 163, sd = sqrt(7.1^2+6.6^2))`\n\nyou get 0.07433852 which is close to the simulations! I believe I did everything correctly but feel free to correct me.', 'This comes before the 2-sample z and t tests do: you build the test statistics for those tests using this formula, applying it to the sample means of the two subgroups you are comparing.\n\nPut another way, your question conducts a 2-sample z test with sample sizes 1 and 1.', ""As a statistician named Bob, I thought i should chime in. That looks fine. That gives the area to the left of 0 for a normal distribution with that mean and sd (don't know the function)? Are those values you used public stats in centimeters or something?""]"
[Q] Can I produce Quantiles as predictions,"Hello everyone, I am working on a demand forecasting model. It predicts the demand in the upcoming month. How can I produce quantiles as forecast instead of a single estimate. I need qunatiles to account for the capital availability. If the available capital is low for example take the 50th percentile as the forecast. If it's high on the other hand take the 75th.",120zvv5,Riolite55,1679694446.0,1,1.0,"['Quantile regression', 'You can also look into Conformal Prediction.', 'I am looking for a model agnostic method.', 'Exactly what am looking for, thanks.', 'What does this mean? It\'s difficult to imagine there could be a model agnostic way of producing claims like ""Conditional on information X, the probability that demand is q or lower next month is 0.75""', 'Quantile regression trees (or forests) perhaps?', 'check conformal regression']"
[Q] How could I perform a mediation analysis with mixed binary logistic and linear regressions?,"Hello r/statistics denizens!

I'm an undergrad med student in the UK, currently doing a thesis on the role of social cohesion as a mediator of the relationship between family affluence and mental health in adolescents. I'm using SPSS statistics version [28.0.1.1](https://28.0.1.1) for data handling and analysis.

The variables I'm using are:  


\-pfas\_iii (predictor)  = proportional rank on a family affluence scale (ridit transformation of a family affluence scale to between 0 & 1)  


\-mhc1 (outcome) = binary variable denoting whether or not a respondent experiences multiple psychosomatic health complaints in an average week  


\-soc\_cohesion (mediator) = composite sum scale of 3 questions asking respondents to rate from 0-4 how strongly they agree with statements related to the cohesiveness of their communities

I've validated my social cohesion scale with a cronbach's alpha, and the other variables have already been validated in previous published work on the same dataset.

I've carried out binary logistic regressions assessing pfas\_iii->mhc1 and soc\_cohesion->mhc1. I've also assessed the relationship between pfas\_iii and soc\_cohesion using a linear regression (validated by plotting standardised Pearson residuals against pfas\_iii, and assessing residual mean and stdev). All these are statistically significant at p<0.001

How do I incorporate these different regressions into a full mediation analysis? My supervisor (a senior statistician at my medical school) has promised to send me the necessary equations, but they haven't got back to me yet, and the deadline is fast approaching.

Any help would be much appreciated  


Edit: I have also run a binary logistic regression with mhc1 as the outcome and both soc\_cohesion and pfas\_iii as predictors, confirming that the beta coefficient for pfas\_iii changes by -25,4%.  


I have betas for all the regressions I have performed  


I've also adjusted for some confounders, but this probably isn't too relevant",120yy1a,Comrade-Lannister,1679692531.0,7,0.9,"['I always built my mediations with binary variables in an SEM (structural equation modeling) environment and not SPSS.', 'Have you googled ""SPSS mediation analysis""? The top result yields [this](https://uedufy.com/how-to-run-mediation-analysis-in-spss/). It seems like you have enough of a grasp of regression to interpret the results. Let us know what roadblocks you come up against. \n\nAs an aside, I urge you not to use phrases like ""I\'ve validated my social cohesion scale with a cronbach\'s alpha."" Validation is an ongoing process that is not a property of scales, but of the *use or interpretation* of scales. Cronbach\'s alpha is a measure of consistency within items, but on its own is far from sufficient for making validity claims.', 'Thank you!!']"
[E] Which class for Stats PhD application?,"I want to go specifically into statistical ml research and am wondering if I had to take one, which would be stronger in admissions (or equal): Intro to ML or Intro to Analysis.",120ya9n,seriesspirit,1679691197.0,1,0.6,"['Analysis', 'Analysis', 'Analysis - they are looking for good math skills.  They can teach you the rest.', 'Analysis ALL DAY. (Really, both.)', 'If you are into math, you will definitely enjoy ""Intro to Analysis"". \n\nBesides having analysis fundamentals under your belt can make your stats-ML journal reading experience much more smooth.']"
[Q] could you help me with a Reliability Analysis issue?,"Hi everyone! 
So Ive edited a scale and Im checking the reliability of the subs scales. Im using JASP. 
As I put the questions in, the note came up that Q6,7,9,10 are negatively correlated so I knew I had to reverse them.
I then put those questions as reversed items but still under item-rest correlation I see negative correlation in Q9 and 10. 

So now Im confused! Should those 2 items be reversed or not in the next steps of my analysis? I know its best to drop them as it shows it will improve my cronbach alpha but I was gonna put that as limitation. 

Please help and sorry for my poor english.
Thanks x",120uqht,Disastrous_Hornet_21,1679685284.0,1,1.0,['[removed]']
"[Q] Hoping to go to Grad school for Biostats, should I major in Math or Stats?","I'm currently an undeclared second-year at University of Washington. My original goal was to apply to Statistics (w/ a concentration in DS) this coming Spring Quarter.

This past Winter Quarter, however, I took Adv. Linear Algebra and Intro. to proofs and I have to say, minus the stress, I thoroughly enjoyed both these classes. Beyond these two and Real Analysis I (which I'm taking Spring), there's very little overlap between the Stats and Math major degree requirements, so I need to make a decision soon.

Pros of majoring in Math are that it'll give me a strong foundation and ability to think rigorously for biostats grad school.

Con is that I will potentially suffer alot. I don't want it to consume my life because I have hobbies and obligations outside of school, and I'd have to be extremely careful not to fall behind.

For those who've pursued grad school, what path would you have taken?",120sl2d,AtmosphereKlutzy,1679681124.0,26,0.91,"[""&#x200B;\n\n>Con is that I will potentially suffer alot. I don't want it to consume my life because I have hobbies and obligations outside of school, and I'd have to be extremely careful not to fall behind.\n\nI don't think this is a con. If you want to do well in grad school, you're going to need to learn how to balance competing obligations, take care of your mental well being, and not fall behind. building a strong foundation now will reduce the burden down the line and avoid falling behind (which is how grad school ends up consuming people's lives)."", 'Major in math and minor in stats? Or double major? I imagine that the overlap between course requirements and electives is really significant between math and statistics. \n\nIf you have to pick, I\'d still say go for math, as it will be pretty easy to pick up statistics later if you\'ve got a really solid general math foundation. And who knows, it might expose  you to interesting approaches to data analysis that you wouldn\'t get from a ""classic"" statistics education. Things like topological data analysis are very hot now.', ""Major in stats, take math classes relevant to statistics (real analysis, probability, measure theory). If you try to minor in math, you'd likely be required to take courses with minimal applicability to biostats, like abstract algebra or complex analysis. If you want to take a more challenging course load, it would be better to take relevant grad classes than irrelevant undergrad ones."", 'If you want to do a PhD in stat or biostatistics, major in math instead.', ""If you would be bored out of your mind with stats why pursue it then? At the very least you should take some core stats courses in undergrad to see if you would like it. Statistics uses math but unless you are studying theoretical mathematical statistics, there's a sense of uncertainty or application in it that's not present in math. There's a comfort in studying pure math because of proof. In applied statistical settings you need to get used to the idea of being slightly less rigorous"", '&#x200B;\n\n>I feel like I\'ll be bored out of my mind with stats\n\nThen why would you want to do a grad degree in biostatistics and work on it? I think you need to take more classes and see if you like it. Baby stats classes are boring but then you can get to more interesting stuff, but to get to that point, you need to take more classes and decide. However, if you already think like this, maybe you need to look elsewhere, like maybe computer science.\n\nStatistics and biostatistics are applied fields (unless your goal is to do a PhD and develop new methods or proofs), so you do to spend all of undergrad doing proofs to then do a degree in biostatistics to then get a job in industry? Does this math major have programming?\n\nIsn\'t it better to do statistics as major and math as minor, or a double major, apply for internships, and then either do the grad degree or work for a couple of years and then do the degree? At least you\'ll see if you like it or not by doing a bit of both and doing an internship or working with a professor in a lab.\n\nI guess I don\'t understand your logic. It sounds like: ""I\'m a second year. Stats is boring so I want to major in math and then do a grad degree in biostatistics."" Like????\n\nAlso, University of Washington has one of the top stats department so at least take advantage of the courses there and the professors for potential letters of recommendation. If you are an honors students, often you can take grad level courses if you do well in your undergrad courses (I read you think grad level courses are going to be more interesting, which doesn\'t immediately make sense, but this is a possibility)', 'Choose statsyou want to develop intuition for statistical concepts early on. You can take supplemental math classes but having that extra degree of rigor with math wont give you as much gain as familiarity with stats concepts. I had a math background going in to stats for grad school and although I was a much more excellent proof-writer and my linear algebra was very strong, I didnt need the whole math degree for that. Probably half of my upper div math courses were not directly useful. I would recommend taking stats and then supplementing with math courses, and doing your best to take the statistics graduate sequences in your jr. Or senior year. This will be the best prep for grad school and also a great signal to ad coms.', ""As someone who double majored in Math and Statistics in undergrad and is currently doing a masters in biostats, I think my math major was much more helpful than stats tbh. The stats major basically gave me a feel for what I might want to do with my career while the math major helped me grasp the theoretical underpinnings of stats and provide me critical thinkings skills / logic.   \n\n\nFor MS programs, they want you to have calc 3 / linear algebra which are often core classes for the math major.  But taking those requires calc1/calc2 / maybe some discrete math class. Then having some probability and real analysis wouldnt hurt. And by that time, you're already halfway done with the math major so why not just finish it? And then just take 2-4 stats classes that sound interesting to you to really see if biostats is something you want to do."", 'Tbh both. Linear sucks, proofs are mid, and real analysis can be the bane of ones existence.', 'Do physics.', 'Math.  Take two courses on Probability and Statistics in the Math Department.', 'I majored in math not having any idea what kind of career I wanted and ended up getting a biostats MS a few years later. My math background was extremely helpful for the stats theory classes in grad school because they were very proof-heavy, so if you enjoy math I say stick with that. Maybe wait and see how you like real analysis before deciding though - that class will give you a better idea of whats to come. I remember one of the professors who interviewed me for my MS program said they felt math majors performed better in the program overall than stats or biostats majors.', 'You can do a Masters of Public Health in Biostats which is typically a two year program.', 'I chose to do an undergrad in Stats because its what I was most interested in at the time. When I went to do my Masters in Stats, I had a lot of courses that retreaded the same material (albeit with more depth) that felt a bit wasted to me (it did make grad school feel a bit easier though since I already had exposure to a lot of the topics). I think Id be a better rounded Statistician today if I had done my undergrad in something other than Stats, but ultimately, I dont regret the decision because my interest kept me engaged during undergrad to set myself up for success in grad school.\n\nI think theres a lot of merit to planning the most efficient undergrad, but Im sure youll find success no matter what you decide. In the short term, Id just pick whatever you see yourself enjoying the most now.', 'Hmm thats a really good point', ""> I imagine that the overlap between course requirements and electives is really significant between math and statistics.\n\nOP explicitly said this wasn't the case in their situation"", '> Or double major?\n\nThe time and effort it takes for a double-major is better spent on a Masters, or towards a PhD.', 'What about an MS?', 'How does a math major prepare one better than a stats major with relevant math classes (probability, real analysis, measure theory, etc)?', 'Im probably more ignorant on this, but Stats in undergrad seems like it would be less interesting than it would be in grad school, where the applications to projects and research are more novel than just standard classroom material. Ive taken an introductory stats course, but it was extremely dry and I had a deadpan professor, which kind of muted my interest in it. Unfortunately, most of the rest of stats coursework is restricted to major only, so I cant take them', ""Which math classes did you take that were relevant to helping you grasp the theoretical underpinnings of stats and which weren't?"", ""There's a big different between taking two courses on Probability and Statistics in the Math Department and completing a major in math."", 'Also, I would caution you not to fall into the trap of viewing graduating on time as being ""on track"". If you need to drop a class and take it later on to avoid falling behind, so be it.', 'Still math. Best way to get an ms is to do a phd program (which means no tuition) and drop out after 1-2 years with ms', ""It's less important if you do an MS and you are 100% sure you do not want to proceed to the PhD later."", ""Full disclaimer, I was neither. I will say in stats grad school I had always wished I had a stronger foundational math background (topology, algebra, PDEs, functional analysis). Having a stronger background in proof writing will make the required course sequence way easier. Also, I'm not sure how many stats majors end up taking measure theory in college. I hadn't heard of or met any that weren't also math majors.   \n\n\nAlso, keep in mind that the landscape is shifting quickly. IMO machine learning teaching is not very good at the undergrad level at most schools. It's better to get mathematical depth so you can pick it up more easily in grad school than to spend a bunch of time working at it earlier without having the conceptual background to make the connections to other parts of applied math."", ""statistics isn't like economics or something where it's really dumbed down at the undergraduate level. besides the introductory classes (which are like that because they're a requirement for like every other major) undergraduate and graduate statistics classes aren't that different"", ""> Stats in undergrad seems like it would be less interesting than it would be in grad school, where the applications to projects and research are more novel than just standard classroom material\n\nThis may or may not be the case. In some cases the undergrad classes (particularly if the major is data science) will include real world data and projects while the grad classes will be spent going over things like moment generating functions and measure theory. Those can be interesting in their own right, but they don't lend themselves to research and projects."", 'My suggestion was to major in math and take two courses (perhaps electives) in P&S.', ""I'm not sure why you mentioned machine learning, I don't think OP or myself mentioned it. The point I'm trying to make is that a statistics major with some math classes will give you the math foundation needed for grad school without requiring many other math classes that are irrelevant to statistics"", ""Aside from maybe category theory and algebraic number theory, I'm not sure there are too many fields of math that are irrelevant to research in statistics. Also, my impression is that few undergrad stats classes match the difficulty of your average proof-based upper division math major class and you will be thankful for the experience if you get more practice on harder topics. \n\nIf you want to just do applied research which uses statistics that doesn't involve much in the way of proving things or devising new procedures, then the standard prep sequence that you mentioned is fine. That said, if it's a PhD you're interested in, I think the expectation is that you go above and beyond that level to focus on methods and theory as opposed to just applications.\n\nI mentioned machine learning because in a major stats department, somewhere between 10% - 50% of the research content is machine learning or adjacent nowadays and it's also in high demand on the job market. It's also a subject that stats departments are called upon to teach more and more frequently. I had the option to take an undergrad level course in machine learning which would have been stuff like rule-based systems, perceptrons, support vector machines, etc which was already obsolete at the time. The graduate class was mostly about high-dimensional probability distributions, energy-based models, regularization, and kernel methods. The former would have 100% been a complete waste of time whereas I still use all the knowledge from the latter today. To summarize, this is an example of where the undergrad version of a course is virtually worthless. The latter assumed more mathematical maturity, hence I thought it would be better to take more math ahead of time."", ""> If you want to just do applied research which uses statistics that doesn't involve much in the way of proving things or devising new procedures, then the standard prep sequence that you mentioned is fine.\n\nThat seems to be the case for OP, given that the specifically mentioned biostatistics, which is more applied than pure statistics research.""]"
[Q] Walsh Test,What do you think is the parametric counterpart of the Walsh Test?,120m4sw,Extension-Pizza-2181,1679668354.0,0,0.17,"['""Counterpart"" in what sense?', ""Can you clarify which Walsh test you mean? I've seen at least three things get called that, that I can think of at the moment.\n\nCan you clarify what you mean by counterpart? \n\nAre you referring to a confidence interval based on the test? Or an estimator based on the test? Or something else?"", 'The ogre 2 test is surely a counterpart, or possibly even the yhslaw test', 'I mean the parametric counterpart', ""i meant the parametric counter part. I wanna read more about walsh test but can't find new resources. I'm only using the Nonparametric Statistics for The Behavior Science."", ""I'm actually thinking that the parametric counter part for walsh test is student t-test"", 'Can you clarify which test, exactly, you mean?  In nonparametric statistics, I know of two tests that might get called the Walsh test, but maybe it\'s not even either of those.\n\nDo you mean the book by Siegel and Castellan, *Nonparametric Statistics for the Behavioral Sciences*? If so, I haven\'t seen that book since the 80s (likely 1st edition, and I only read bits of it even then), so my memory of it is not perfect. A Google books search of the second edition turned up no mentions of ""Walsh test""\n\nI did find a mention of a Walsh test based on pair-differences in *Introduction to Nonparametric Statistics for the Biological Sciences Using R* by MacFarland & Yates, and that specific test corresponds to a paired t-test.', ""The test is called The Walsh Test in which it assumes the difference score between two related samples are drawn from symmetrical population. It uses atleast interval scale. And yes i am using Siegel and Castellan's. It's just so hard to find books that discusses Walsh Test . I wanna learn more about it since I have series of questions that are unanswered ."", 'Thanks, that helps, but given I don\'t have that book, can you give the exact test statistic, and explain how they\'re computing the p-value?\n\n**edit**: managed to borrow a copy of second edition of S&C briefly. .... there\'s no ""Walsh test"" in it. Under the chapter on paired and one-sample tests, there\'s the Wilcoxon signed rank (which does have as its estimator for the location difference the Walsh average), and the permutation test (which *also* relates to Walsh averages). I am wondering if they start calling the second thing Walsh\'s test at some later edition.\n\n\n> since I have series of questions that are unanswered\n\nWe could probably help as long as exactly what you\'re looking at is made sufficiently clear']"
[Question] Non-conformances for QC Charts based on Warning Limits and Control Limits?,"Hi all,

I work in a lab and we have a number of QC charts for the methods we use in the lab (e.g. pH 7 is one of our QC buffers).

The mean for our QC charts is calculated based on the previous 60 QC checks, with the UWL/LWL being the mean +- SD\*2, and our UCL/LCL being the mean +-SD\*2.

Recently, we have been getting a lot of non-conformances. We have to raise one if we get two consecutive data points above/below the WL, if there is one data point above/below the CL or if we have 10 consecutive data points above or below the mean.

Does anyone know if the above is standard practice for raising non-conformances with QC charts? I've just been curious because I feel like our QC charts is penalizing us for getting daily QC results that are close to the mean, as this in turn affects every new QC chart we make (after 60 days) by making the WL and CL more narrow, thus increasing the likelihood of us having a non-conformance. Like, if we're using pH7 as a QC chart, in an ideal world wouldn't it be better to get a result as close to 7, rather than the mean established by the QC chart?",120fbw8,hasdanta,1679651026.0,3,1.0,[]
[Q] How to compare two distributions of crash data?,"I am trying to determine if the distribution of crash injury severity is different between two populations. A crash can be coded as one of 4 injury severity levels...fatal, major injury, minor injury, or no injury. Each population...in this case, I'm looking at different census tracts... Can include several hundred crashes.

I thought about just doing a bunch of pairwise tests of proportions... For example, is the proportion of fatal crashes significantly different, and repeat this for each level. But is this really the best approach?",120enfi,Gullible_Toe9909,1679648842.0,5,0.86,"['What you have is an ordered 2 x K table. If you just want to compare the distributions, you could use  chi-squared test, but this would ignore the ordering of accident severity. To take this into account, you might consider a trend test such as the Gamma trend test using concordant/discordant pairs. \n\nThere are probably others you could find online, but you definitely do not want to be doing multiple pairwise tests.', 'Ordinal regression might give you more power to reject than chi-squared and uses all levels of severity at the same time.  Chi-squared is more general, but could also work.  Ordinal regression is also called proportional odds and youd be doing something like a t-test (but not a t-test) for ordinal data.  You get one number  an odds ratio  to quantify group differences so interpretation is easy.  Testing individual proportions could also work, but you might lose power to see differences and youll be making 4 times as many comparisons.', '""Different"" can mean various things. You can check each category separately, but what does it mean if one dataset has fewer minor injuries while the other one has more major injuries and no injuries instead? There is also the look-elsewhere effect to consider if you compare 4 groups (and them presumably focus on the one with the largest deviation in the discussion).\n\nYou could assign a score to each category (3, 2, 1, 0 or whatever) and see if the average score differs.', '[deleted]', 'Yes, either test would give you the first sentence above.  For ordinal regression your outcome would be severity: 1, 2, 3, or 4 and the independent variable would just be group.  As a bonus OR gives an overall odds ratio, e.g. odds of worse severity is 2.3 times greater for group A vs. B.']"
[Q] Understanding variance calculations in scipy.optimise.curve_fit (Python),"Howdy,

I'm trying to use Scipy's `scipy.optimise.curve_fit` to calculate the parameters for a non-linear least squares fit of my data to a function. I have 2-3 response values for each value of my explanatory variable. Each of these response values is assumed to be errorless.

I need help understanding what I should be specifying for the `absolute_sigma` and `sigma` arguments and what effect these have on the output variance-covariance matrix, as I need the matrix for calculating confidence intervals for my fitted parameters and confidence bands for my function via the Delta method.

Any advice would be very much appreciated!",11zzbzu,dam_the_duck,1679609515.0,2,1.0,[]
[Q] Admitted to UChicago and UIUC for MS Statistics - Difficulty Comparing and Choosing,"Hello Statisticians,

I am going into graduate school with the intention of exploring the field as much as possible and then deciding on a PhD or industry job. I have a background in CS & have a couple of projects in ML/Statistical Modeling so I'm naturally inclined towards research position in industry, I'm also looking to explore financial statistics more. If possible, I would defn prefer to work for 1-2 years before proceeding to my Ph.D, if I choose to do it. I'm not very interested in data science job profiles.  


I'm not a fan of UChicago's course structure(9 courses in 9 months, makes me feel like it might be difficult to pick up anything else) but at the same time it has a stellar reputaion in the field. 

Most UIUC graduates look like they end up in data science. I would also have to take a loan for UChicago while UIUC has the appeal of graduating debt free because of RA positions.

Does anybody here have experience with graduates from either of the uniersities? 

I would love to hear about your thoughts and any advice is greatly appreciated!",11zwrt0,evilFoxStrongie,1679604305.0,2,1.0,"['Yeah I was looking into UIUC MS too as I have a BS in Stats. The financial assistance for MS students looked attractive to me, do you know if its easy to get a TA a research position with a prof to qualify there? \n\nBut id go with UIUC for the potential to be financially free basically.', ""> while UIUC has the appeal of graduating debt free because of RA positions\n\nThen this is a no brainer, go to UIUC. Grad school is supposed to be free. If it's not free, you're getting scammed."", 'A close colleague of mine(read fianc) graduated from uiuc in stats. If you need a stress free life, go uiuc. If you want to end up in tech, go uiuc. If you want to graduate with minimal debt, go uiuc. If you want to end up in finance, choose uchicago or uiuc based on the type of finance you\'d like to end up in. I work in risk and can validate that the students are equally good in either place.\n\nIf you end up in finance; as a financial advisor, the first thing you will end up telling anyone is: ""go for a debt free life"" and I would suggest the same!', 'If ""research position in industry,"" especially CS-related, is in your sights, another plus for UIUC is being right next door to the National Center for Supercomputing Applications, the dozens of tech companies at the UIUC Research Park, and Wolfram Research.\n\nAlso would add that, from your second job onward, the name of the school you attended means relatively little in industry and at lower tier academic institutions. Absent a very specific goal of getting a PhD or an eventual professorship at a super-high-prestige school, I would never tell someone to pay extra just to go to a school with a ""stellar"" reputation rather than merely an excellent one.', '2010 UC grad here. The 9 month program was intense. I was able to finish it in 4 quarters (i was able to finish 3 classes the first quarter but had to slow to 2 classes for the next three quarters). I dont regret going. It was a big accomplishment for me. Class sizes were small (10-15 students). Teachers were good to great but the TAs where excellent. They also have a strong statistical consulting  group where you can some good experience. Like others said it wasnt cheap. I was able to get some minimal financial support from the department but had to fund the majority via a grad plus loan.\n\nDid I learn the fundamentals - yes. Did it help me get a job quickly - yes (this if were UCs reputation helped me but some employees care about school names and others dont). Would I do it again - yes. But honestly here is the secret regardless which school you go to -  the majority of your stats knowledge will come through work experiences and self research lol. The school is there to get you through the door', 'Whats cost of both?', ""I'm not sure, I have a couple of friends there and they mentioned that TA is relattively easier to get and for RA you can go to other departments as well."", 'What If its free at a small school', 'Thanks so much! Graduating debt free is definitely a big plus. Could your close colleague (read fianc) provide a little information on the kind of roles people ended up in finance from her cohort?', 'second this big big time! uiuc has its own upsides and downsides. as mentioned, nothing comes close to their CS/algo degrees. and as a recruiter, I always prefer recruiting someone who has a strong technical backing rather than a mere theoretical knowledge.', ""I'm open to research positions being CS related or finance related. Quant research and Algorithmic trading are definitely fields I'm fascinated by and I want to look into. \n\nYour answer is super helpful, thank you!""]"
[Q] Repeat measures ANOVA,"Hello,

As the title says I am struggling very much with performing a repeat measures ANOVA. The data is laid out as follows:

[https://imgur.com/a/sPy75rx](https://imgur.com/a/sPy75rx)

Tried it in JASP, can't figure out how to do the post-hoc tests. Then I used this website [https://www.socscistatistics.com/tests/anovarepeated/default.aspx](https://www.socscistatistics.com/tests/anovarepeated/default.aspx)

but had to do exercise systolic, exercise diastolic, control systolic and control diastolic all separately. None of the results were significant for p < .05 which is what I want but it is ok to present the 4 groups separately or is there a way to do it together? I know pretty much no stats but got stuck with data analysis in this group project because I went to the bathroom at the wrong time. Any advice helps, thanks.",11zuzsr,ArabLlama,1679600616.0,0,0.33,"[""> None of the results were significant for p < .05\n\nFYI, it doesn't change the conclusion here, but in general you should do some correction on p-values when you interpret many in order to account for the 5% random chance of seeing a rare event accumulating over multiple tests. A common way of doing it is through Bonferroni's correction, where each p-value is multiplied by the number of p-values you want to interpret. Then, you can interpret them.\n\n> ok to present the 4 groups separately or is there a way to do it together\n\nI believe it is fine to present them separately. Do the p-value correction before you interpret though.\n\n------\n\nIf you want to do something more fun in data analysis for your project, read below.\n\nIf you have not been told that you need to do ANOVA, then my advice is (if you have time!) to instead fit a Linear Mixed-Effects Model, and don't customize the covariance/correlation matrix, leave it as it is.\n\nI am not aware of many situations where it is not simpler to use than ANOVA. I have not used repeated measures ANOVAs since I learned about it, and neither has my professor (except to teach in class).\n\nLMMs work like most linear regressions but can take into account random variations per e.g. person or group (e.g. student_score ~ intercept + A * time + B * student_age + C * student_height + random_base_value_per_student).\n\nIn your case, I assume `SYS 1` is the SYS score at time 1, same for the rest, and you want to analyze how SYS and DIA are related in the control and test group.\\\nUsing R, you would need to organize your data into columns `ID`, `Group`, `time` (taking values 1 2 3), `SYS`, and `DIA`, then execute (with the right variables) e.g.\n```\nsummary(lme4::lmer(formula = SYS ~ time + group + DIA + (time | ID) + (time | group),\n                   data = your_data_frame))\n```\nor (same model, same results, but different package)\n```\nsummary(nlme::lme(fixed = SYS ~ time + group + DIA,\n                  random = ~ time | ID/group,\n                  data = your_data_frame)\n```\nFeel free to ask your teacher for help to do this and interpret the model. Fitting a model like this is much more versatile than repeated measures ANOVA and allows for more interpretations."", ""If you're doing repeated I take it your groups column isn't important? As you currently have it in JASP, just run repeated ANOVA and create two factors each with 3 levels (Sys 1/2/3, Dia 1/2/3). Place your items into that and run it. There's a posthoc option if you scroll down... Simply place your two factors into the right hand side box in that section to get those posthoc statistics. Note that if there isn't a significant effect of a factor, it doesn't need a posthoc test for it.""]"
[E] Why We Divide by N-1 in the Sample Variance Formula,"Hi guys,

I have made a video [here](https://youtu.be/E3_408q1mjo) where I explain why and when we divide by n-1 instead of n in the sample variance.

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)",11zumf5,Personal-Trainer-541,1679599794.0,101,0.93,"['Dividing by (n-1) gives unbiased estimation of variance. That means that expected value of sample variance is actual variance. If dividing is done by (n), then you get Maximum Likelihood Estimation which is biased.', 'Wonderful video and excellent explanation. Your visual description makes this correction very understandable.', 'I prefer to divide by n+1 (a la charles stein! to minimize MSE among linear estimators)', 'We divide by n-1 to correct for the degrees of freedom. So when we are estimating the variance we are estimating one parameter and estimating that parameter has the consequence of our degrees of freedom dropping by 1. To correct for this and get an unbiased estimate we take n-1.', 'Nice video!  Relaxing too.', 'Years and years have gone by, still no one can explain this in a simple consistent way without resorting to language that is non-understandable to non-mathematicians.', 'I remember a statistician explaining this to me and they started with not all statisticians agree it should be N-1 but here is why kind of lost me at that point', ""The real reason is because it shows up in the formula for the t-test.\n\nFor technical reasons dividing by n or n+1 is probably better. The difference is small so it doesn't really matter for large samples, and that's why we teach n-1."", '* n-k', 'Could you explain this a bit more?', 'There is a big difference between saying *that* a thing is true and saying *why* a thing is true.', 'Thank you!', 'The minimization is only true for distributions with 0 kurtosis, e.g. Gaussian distributions. But in general, shrinkage does reduce MSE of estimators (while introducing negative bias).', 'Whaaat this is totally new to me o.O So many things to learn', 'The parameter in mention is the mean, mu, since a mean is required to calculate the variance, so one data point must be fixed to calculate the mean, hence dropping degrees of freedom by 1', 'Many thanks!', ""I don't really understand why that would lose you. Tons of things are debated in all fields. That's how progress is made."", 'Thats an incorrect statement. Every statistician agrees since n-1 is found by a mathematical proof. See my above comment explaining it', 'Two degrees in stats here, this is proved mathematically by solving that the expectation of an estimator has to equal the population estimator in order to be unbiased; E[hat(sigma)] = sigma\n\nWhen this equation is solved for sigma, you get that it divides by n-1 instead of n. To start this proof use standard expectation theory with the normal distribution: E[hat(sigma)] = integral from -infinity to infinity(sigma* phi(x) ), where phi(x) is the normal distribution function\n\nAlso fun way to think of it is if you have only 1 data point, theres zero degrees of freedom in the variance, since it is always zero, so starting here you can also prove this by induction that degree of freedom = n-1', 'Ideally we would subtract each value from mu, the true population mean. But because that is unknown, we use the sample mean xbar. The problem is that xbar fits the data a little too well and on average (x_i - xbar) is just a little bit too small. The appropriate fix is to divide by n-1 instead of n.', 'That statement is correct.  n-1 is unbiased but the MLE using n has a lower value for bias + variance.', 'My stat professors never properly explained the concept of degrees of freedom. Any tl;Dr in how to think about it? Why is the denominator here the same as degree of freedom?', ""I was thinking DoF, too. Glad I was able to follow your more detailed explanation, it's very good!!"", 'I wish I could give you an award. My stats prof explained this terribly.', 'Yes and there are [other](https://www.tandfonline.com/doi/abs/10.1080/00031305.2012.735209) estimators with a lower MSE.', 'Only true when Kurtosis is three and skewness is zero like a normal distribution', '[Citing wikipedia](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics\\)): ""In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.""\n\n[The wikipedia page on Bessel\'s corretion](https://en.wikipedia.org/wiki/Bessel%27s_correction) has a pretty good explanation of how it comes into play here: The ""inputs"" to the sample variance is the collection of n numbers (x - x, ..., x - x), and these numbers sum to 0. So if you for example increase x, the mean x will change accordingly, and the total sum still sums to 0.\n\nIf set y = x - x, ..., y = x - x, this means that y + ... + y = 0. We can rearrange this like so:\n\ny = -y - ... - y\n\nThat is, y is completely determined by the values of y , ...,  y. Thus there are only n - 1 ""free parameters"" in the formula for the sample variance. Even though we have n ""inputs"" (our samples), the fact that we subtract the sample mean has the effect of projecting the input onto an (n-1)-dimensional hyperplane, that is, there are only (n-1) ""degrees of freedom"" in the input parameters.', 'Nice comment below, and a simple way to think about it is in the variance calculation, requires a value for the mean, mu. So in order for the variance to be a certain number, one data point must be a fixed number that makes mu able to equate the above equation. So only n-1 data points are actually free to be anything']"
[Question] Swedish Transgender Study,"I recently heard from a republican representative about a trans follow up study based on Sweden, I looked online for this study and cannot actually find the study just mentions of it and information about it. Does anyone know where I can find the original study?",11zqtm2,DeliriousTiberius,1679591637.0,0,0.43,"[""Since this is a statistics forum, note that the study is comparing individuals with ***both*** gender dysphoria ***and*** gender change on the government records ***to*** matched individuals in the general population.  It doesn't isolate either of these potential effects.\n\nIt's less a critique of the study, and more of a critique what I imagine to be a likely misinterpretation of the study, but practically speaking, identifying individuals with both a diagnosis and a treatment doesn't lead to conclusions about the treatment. \n\nI don't have any problems with the conclusions of the study:\n\n*This study found substantially higher rates of overall mortality, death from cardiovascular disease and suicide, suicide attempts, and psychiatric hospitalisations in sex-reassigned transsexual individuals compared to a healthy control population. This highlights that post surgical transsexuals are a risk group that need long-term psychiatric and somatic follow-up. Even though surgery and hormonal therapy alleviates gender dysphoria, it is apparently not sufficient to remedy the high rates of morbidity and mortality found among transsexual persons. Improved care for the transsexual group after the sex reassignment should therefore be considered.*"", 'You mean the study that concluded *""Persons with transsexualism, after sex reassignment, have considerably higher risks for mortality, suicidal behaviour, and psychiatric morbidity than the general population....""*, as well as finding that *""...Female-to-males, but not male-to-females, had a higher risk for criminal convictions than their respective birth sex controls....""*?\n\nhttps://pubmed.ncbi.nlm.nih.gov/21364939/', 'Yes thank you']"
[D] Hypothesis Testing For Time Series?," Suppose there is a company and we have their weekly sales over a period of 20 years (from 2000 to 2020).

Suppose in 2010, the company adopted a new policy (e.g. bought more powerful computers) and is interested in seeing **if their weekly sales improved after adopting this policy**.

Normally, I would have approached this problem using ""Standard Hypothesis Testing"" . That is, I would have taken the average of weekly sales before the policy and after the policy - and then used a hypothesis test like the T-Test or the Mann-Whitney Test to test whether these averages are statistically significant or not.

However, one thing that is making me reconsider if this approach is suitable or not, is that I am dealing with data which is not IID. As such, I am not sure if standard hypothesis tests are suitable for such a problem.

While trying to learn more about different approaches that can be used for this problem, I identified the following approaches:

* **Regression Discontinuity** : Regression Discontinuity ([https://en.wikipedia.org/wiki/Regression\_discontinuity\_design](https://en.wikipedia.org/wiki/Regression_discontinuity_design))) might be applicable to test whether the implementation of a policy statistically changed a time series by exploiting a natural cutoff point or threshold in the policy, and then estimating the difference in the outcome variable before and after the cutoff, controlling for other factors that may affect the outcome.
* **Difference-in-Difference** : Difference-in-Differences ([https://en.wikipedia.org/wiki/Difference\_in\_differences](https://en.wikipedia.org/wiki/Difference_in_differences)) might be applicable to test whether the implementation of a policy statistically changed a time series by comparing the changes in the outcome variable over time between a treatment group that received the policy intervention and a control group that did not, while controlling for other factors that may affect the outcome.
* **Change Point Analysis** : Change Point Analysis Detection ([https://lindeloev.github.io/mcp/articles/packages.html](https://lindeloev.github.io/mcp/articles/packages.html) , [https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/](https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/)):
* **Causal Impact Analysis** : Causal Impact ([https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html](https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html)) might be applicable to test whether the implementation of a policy statistically changed a time series by modeling the counterfactual outcome that would have occurred in the absence of the policy intervention, and then estimating the causal effect of the policy on the outcome variable based on the difference between the observed and counterfactual outcomes, while controlling for other factors that may influence the outcome.
* **Model based F-test/ Granger Causality**: Such methods might be applicable to test whether the implementation of a policy statistically changed a time series by comparing the coefficients of a time series model fit before the policy vs after
* **Bootstrap**: Randomly sample (with replacement) weeks before the policy and after the policy - take the average of these randomly resampled sales before/after and statistically compare using T-Test ... repeat this process many times and count the ratio of times they were statistically different vs not.

**My Question:** However, I am not sure which of these methods (or possibly other methods) are well suited for this problem when the data is non IID. Can someone please provide some comments on this?

Thanks!",11zo6o3,SQL_beginner,1679586087.0,12,1.0,"[""I have used the CausalImpact package. As you wrote, you need to have data to generate the counterfactual. That's one thing I'm not sure you have in your case.\n\nI also used Granger causality as part of the model specification for vector autoregressive model (VAR). I'm not sure, but I think this makes more sense when you have one time series affecting another time series (e.g., advertising affecting sales, and sales affecting advertising over time), not something binary like before/after."", ""Get 'Forecasting : Principles and Practice'.  I would suggest fitting a model to the pre-intervention data, and then make forecasts through the post-intervention period.  Graph the forecasted and actual values.  The set up one of more dummy variables, and get estimates."", 'I think at least the first 3 might be the wrong set of methods.  I recommend the chapter in Kutner et al. ""Applied Linear Statistical Models"" (you can find a pdf online at a reputable .edu address with the right search) on Autocorrelation in Time Series Data.  There\'s a statistical test called Durbin-Watson for detecting the autocorrelation that violates i.i.d. assumption and a few methods for managing it.  First differencing, Cochrane-Orcutt, and one more that I don\'t remember.  There is also a way to account for autocorrelation by fitting an ARIMA model to the regression errors with maximum likelihood.  \n\nYou\'d perform this procedure (first differencing, etc.) on the data to remove the severe autocorrelation, then fit a model on the transformed data, and translate the coefficients on the transformed data back to the original according to the theory.  \n\nI only am familiar with these from a textbook perspective, so I would also be interested if a real expert in econometrics could weigh in on classical techniques like this (that don\'t go overboard with DAGs and IVs!).', 'There is virtually no difference in hypothesis testing for cross-sectional data and time series data other than that you have to model time in some way.\n\nIn regression, for example, this is literally including time as a variable.']"
[E] Does econometrics provide a sufficient background for biostatistics/ statistical genetics,"Hi Im currently doing a double degree in science and economics in Australia. My science majors are genetics and microbiology. 


Im interested in going into biostatistics in postgrad but Im worried that my science courses havent really given me a good background in statistics. However my economics has given me a decent statistics background but its all directed towards econometrics. 


Can my knowledge be applied towards biostatistics or are they completely separate fields",11zlkyl,marcopolo2345,1679580611.0,26,0.94,"[""Different fields use statistics to solve different problems, often very different problems, but the fundamental logic and underlying methods are the same. You shouldn't have any problems getting onto a postgraduate stats course with that degree."", 'No problems at all', 'I was in what sounds like a similar boat as you. Science grad, clinical research oriented with heavy focus on stats in medicine. \n\nI did a few stats courses in my undergrad (mixed biology, psych, econ) and become an intermediate in R and applied stats. \n\nI now do most of my own stats and do fine in my research areas.', 'I wouldnt worry- there should be a lot of overlap.', 'youll gain an understanding of statistical logic so youd be pretty prepared and many concepts are similar, but econometrics focuses more on making sense of messier social science data that you learn special methods and interpretation for. you might focus on more lab/experiment methods in science. you might not have to worry about things like endogeneity issues and using funky causal inference methods to work around them as much', 'I would say that it depends on what type of biostats you will be getting into. For observational studies or larger scale epi-style studies, your background will be great as a lot of techniques are shared across the fields. Things like causal inference and time series analysis work well there. Clinical studies probably less so, but if your statistical background is strong in general I bet you could study up and get there.', 'The application of statistical methods for both studies can be quite different but if you are already capable in passing the econometrics class, youd have no problem to do the same for biostatistics (both gonna give you the same amount of headaches, at least for me). All the best for your study!', 'Yes']"
"[E] Going for a Biostats MSc, am I in over my head?"," I am a doctor in training looking to do a masters in biostats. I want to also take courses in ML, decision analysis, and epidemiology.

I am worried I dont have strong enough math background for for the stats and especially ML sections. Its still a 15 months away and I do have time to prepare

What I have: Biology major in undergrad. Calc 101 (not multivariable), stats up to 200 level (probability, regression, bayes), and very comfortable in python/R (including stats packages, OOP)

I am lacking: multivariable calc, linear algebra, ???

Programs Im considering: UPenn and Harvard health data science, basically pure biostats streams (I know people there I can do thesis projects with)

Question: Am I in over my head trying to go for a biostats masters? Will I be overwhelmed? I am honestly very passionate for and committed to learning data science, and I want to learn it properly rather than superficially in an epidemiology course. I just dont want to not be able to keep up with classes.

In the same vein, any online courses youd recommend I take to prepare? I have 15 months

Thanks for any advice you can offer",11zjvn2,Roquentin,1679576873.0,16,0.87,"['From what you\'ve said, you might be over your head. Go look at the entry requirements for the programs you\'re interested in, I think that most competitive biostats programs expect you to have already completed coursework in analysis and linear algebra.\n\nWith that said, there are a lot of programs called things like ""health informatics"" and similar that might be more up your alley. They\'re more data science focused and less hard-core statistics focused. Maybe even look into bioinformatics programs too', ""I'm confused, are you going to stay in the residency (is that what you are doing?) while studying?\n\nI think that, first, already a residency is 24/7 and you have to focus on your career.\n\nSecond,  in a comment,\n\n>I really am trying to learn biostats for an academic research career.\n\nIf you want an academic career in the medical field, you will be writing NIH grants and will use the grant money to pay for statisticians to work for you and also, collaborate with professors in other departments who can do the research design/analysis or that have PhD students who can run the analysis for you. You are not going to have time to do anything with the data.\n\nMy personal opinion is that you need a reality check as to how academic research works and you will be a PI. You will have people that do everything, you will spend time writing grants, you will need to find collaborators, and you will also have to work as a doctor in the medical hospital."", ""Will they even accept your application if you don't have linear algebra? \n\nCan you email someone in admissions and ask them what they generally expect and/or what the average student shows up already knowing? A lack of linear algebra seems like the biggest worry for me (and this is true of any data science-adjacent field: things like PCA are too fundamental to miss). Not having calculus or analysis...not great, but a lot probably depends on the program in question and how pure vs. applied they are. \n\nIndiana University has a pretty solid graduate program in Informatics and complex systems, which you can kind of tailor to suite your particular interests (double majors are pretty common)."", 'Take Calc I-III, Lin. Algebra, Prob. Theory, Statistical Inference, some entry-level theoretical math course, and possibly Diff. Eq (if you wan the full package). My big mistake was not getting all this out of the way before hand.', 'Short answer:\n\nYou will be alright :)', 'Depends entirely on the programs in question. You can teach most of the above without assuming greater knowlegde from the students going in, you can also do it assuming in-depth knowlegde. With 15 months, nothing should be impossible for you to pick up.', 'Thanks. When I messaged the program director, they said Id have to conditionally complete at least online courses in linear algebra and multivariable calc before starting the program, which I can do, but Im not sure if theres a deeper grasp of math required to keep up with coursework in a biostats masters', 'Thanks for a very thoughtful comment, I dont know if youre a doctor but you really seem to understand my situation well\n\nIm nearing the end of residency and things are quite chill, also starting a clinical fellowship which will be relatively chill (enough to take 1 course per semester online I believe)\n\nI fully agree that I expect to always need PhD statisticians and methodologists at every step of the way for ultimately doing this stuff. \nStill, there are reasons for me to do this masters which I hope make sense: 1) I take pride in being able to understand/ communicate things related to data science, which is what my research will be grounded in (eg clin epi or prediction type studies 2) I genuinely enjoy learning this topic, life just never allowed me to and now I have to do a masters in *something*, because my hospital needs that to hire me, why not take that dedicated protected time to learn something useful I figure 3) Other MScs available to someone who wants to go into my field (like public health or policy) are honestly very fluffy, and I am going to be dropping close to 100gs on this so I want to walk away smarter and with hard skills. \nTo your point about grant writing/collab, Ive already been doing research for nearly 10 years and have learned a lot of those soft skills at work. I believe I need to go back to basics and solidify my knowledge. Youre 100% right I can be successful without this, it just wouldnt be satisfying for me, so I am willing to suffer for that lol', 'Theyve told me that if I commit to taking an online course in linear algebra from a reputable university prior to enrolling (which is 15 months away) they can grant me a conditional acceptance, provided I can show them a certificate of completion before starting the program', 'Thats my checklist too! I have taken stats, probability, and Calc 1. Looks like Ill need 3-4 more courses to be prepared. \n\nWhat do you mean by big mistake, did not having the background up front make it more challenging or totally indecipherable?', 'Thanks for being encouraging. My plan was to enroll in Linear Algebra at my local university (online classes), and take multivariable calc online. I just dont know if that will be enough, or if a deeper understanding is required. \n\nFor whatever its worth heres the course list at Harvard data science \n\nBST 210 Applied Regression Analysis (5 credits)  5\n\nBST 222 Basics of Statistical Inference (5 credits)  5\n\nBST 223 Applied Survival Analysis (5 credits)  5\n\nBST 226 Applied Longitudinal Analysis (5 credits)  5\n\nBiostatistics Requirement (intermediate or advantage) (15)\n\nBST 214 Principles of Clinical Trials (2.5 credits)  2.5\n\nBST 238 Advanced Topics in Clinical Trials (2.5 credits)  2.5\n\nBST 263 Statistical Learning (5 credits) (Machine Learning)  2.5\n\n if it is the one I remember it was VERY hard. High level math\n\nRDS 280 Decision Analysis for Health and Medical Practices (2.5 credits)  2.5\n\nRDS 285 Decision Analysis Methods in Public Health and Medicine (2.5 credits)  5\n\nElectives (10)\n\nEPI 201 Introduction to Epidemiology: Methods I  2.5\n\nEPI 204 Analysis of Case-Control, Cohort and Other Epidemiologic Data  2.5\n\nBMI 704 Data Science for Medical Decision Making  2.5\n\nEPI 288 Introduction to Machine Learning and Risk Prediction  2.5\n\nMIT 6.871 Machine Learning for Healthcare [aka HST.956]  5', 'just do the courses in linear algebra and multivariable calc , is totally worth it and if you can learn that you  will be totally fine in the rest of the program.', 'You might want to take a look over at r/bioinformatics\n\nThey talk a lot about data-sciency careers in the biomedical industry and what types of backgrounds are useful', 'As a to-be MD, I\'m guessing you want to learn stats as a tool for biomedical research. You\'d want to learn enough to understand the models that are used in your research but don\'t necessarily need to know how to develop new models or rigorously compare models. Single courses in linear algebra and multivariate Calc may be enough to develop the former level of understanding but not the latter. One type of program I\'d look into is ""MPH"" programs in biostat. Some biostat programs offer both MS programs that are more theory-focused and also MPH programs that are more geared toward doctors or other specialties who want to get a grounding in stats. At my school there was 80% overlap in the courses between MS vs MPH but the MPH dropped one or two of the harder theory courses and had an applied project instead.', '(1) If the clinical fellowship is at a university, you probably have tuition remission (at least a %) as an employee and you can attend in person. You might be able to enroll as non-degree student as well and just take a couple of courses, instead of a whole degree. It\'s not necessary to collect degrees at top universities.\n\n(2) Even if your fellowship is ""relatively chill"", you are supposed to study medicine when you have your chill moments; not biostats which will not help you when you are with a patient. There\'s so much to learn, that you should be focusing on that. Would you rather be a mediocre doctor that has yet another degree at an Ivy that cost a fortune? Or a good solid doctor?\n\n(3) To communicate studies or results of clinical studies, or whatever you want to do, you do not need a lot of knowledge of biostatistics. You need to be a good communicator. Most of the time you won\'t be in a room full of biostatisticians, because for that you have the PhD student in stats or the postdoc do the presentation. You will be presenting to an audience that\'s full of different people with different backgrounds and for that, your presentation is not going to be 20 minutes of math.', '[deleted]', ""Tbh, the courses can vary so widely on content by university. I'd reach out to the professors who teach the course and ask about prerequisites/expected knowledge. In my experience, admissions people and program heads aren't as useful as profs."", 'Ill be honest, not really interested in bioinformatics. I really am trying to learn biostats for an academic research career. If Im not ready for that now, Ill just have to gain the math background to do it later at some point. This is the thing Im interested in, and my primary job will be physician but I want to ground my research career with strong data science knowledge (of course Ill never touch what a biostatistician can do, but at least a strong working knowledge)', 'The degree terminologies will be different at each school. The main point is to figure out if the programs you apply for are tailored for math undergrads who want to become statisticians or tailored toward other health professionals looking to get training in statistics.', 'Thanks for response \n1-2) my fellowship has time built in and protected for research training. Its also a job requirement to have do a masters at my institution\n3) Im afraid well have to agree to disagree, I do think (based on advice from others doing what I want to do) that this training will be helpful. Everything you are saying also makes sense for someone with different approach than mine', 'Thanks, thats very helpful! I will into those courses. I definitely prefer to complete pre-reqs online! (Although the MSc will be in person)', 'Thank you. I reached out to a lecturer in the program who recommended linear algebra and multivariable calculus before starting the program (in fact entry would be conditional on my completing these)', 'Fair enough. Good luck with your endeavors!', 'This is a great point. I will make sure I clarify this, because Im definitely in the biomedical research not the pure stats category', ""Linear algebra was my favorite math. It's a lot of fun. Especially when applied to computing."", 'Thank you! :)', 'Im super excited to learn it.']"
[Q] Which ML algo. is best for sentiment analysis....SVM or Logistic regression ?,,11zdekb,emo-9,1679559097.0,0,0.33,"['To be honest, the input features are most important. The final classifier can be quite arbitrairy. (logreg over BERT features vs SVM over tfidf)', 'Extremely based reply. For any NLP project, creating a worthwile dataset is like 99.9% of the work.', ""Completely agreed. To add to that, I wouldn't suggest a priori limiting yourself to only those options (logreg vs SVM).""]"
[Q] Transition from Pure Math to Statistics/Machine Learning grad school?,"So I'm a rising senior undergrad in Pure Math who also have taken quite a bit of statistics courses including time series, statistical inference, stochastic processes etc. (with really good grades) and did a lot of personal reading in machine learning but I only have done undergraduate research in pure math topics like number theory or stochastic processes. How would grad schools in statistics/machine learning view these types of candidates? Did anyone make a similar transition as well? I'm afraid I would be at a disadvantage when applying to statistics/machine learning grad schools as a Pure Math student only having done research in pure math before...

Also, I have interned quite a few times as a data scientist and a quantitative analyst before so I guess I have some exposure to working with data, modelling, and coding up machine learning models if that's any relevant to the discussion.",11z4qxz,Bid_Queasy,1679536152.0,19,0.87,"['A lot of grad schools will be happy with any pure-math major who gotten good grades and has taken probability/mathematical statistics. Extra classes like stochastic processes are a bonus. Doing quality undergrad research in any mathematical or scientific area is a bonus. Number theory probably gets more respect that some random applied lab experiment does.\n\nYou will likely find that the machine learning path and the classical-statistics path are about to diverge for you. The former may even live in the CS department or a separate data-science program from the statistics program. You can certainly learn about both, but you may have to choose one or the other when it comes to choosing a thesis topic / major advisor / department.', 'Pure math with good grades should have the door open in a lot of places.  \n\n> Also, I have interned quite a few times as a data scientist and a quantitative analyst before    \n\nThen you\'re not even coming ""just"" from pure maths, you also had exposure to the topics. You\'ll be fine.', 'Stats looks for strong math skills so in my experience that was not a problem. (Pure math BS with no stats courses) machine learning may look more for computer science background (I had a CS minor and a strong rec from a CS professor which Im guessing helped me get into a joint stats and ML PhD)', ""I made a similar leap.  My undergrad degree was in general mathematics and physics, with a concentration in astronomy and astrophysics.  You actually have more experience with statistics than I did.  I feel like I've done pretty well.  It didn't feel like I was hindered by it.\n\nIn fact, some of the pure mathematics is pretty useful. Some people experience difficulties in calculus and linear algebra, but those are pretty common for a pure math student to know. So, that will be helpful in your mathematical statistics classes.  Also, your previous experience will help you in programming. I'm sure at some point, you have also done numerical analysis, so you have experience with programming and will find it useful too.\n\nI think they would look very favorably on your skills."", 'I think you will be welcomed with open arms, at least at theoretically inclined departments. You might also consider econometrics or theoretical economics. Many interesting applications of probability theory there.', 'This post seems like a humble brag', 'You are in the elite regarding math for most programs.\n\nThis means that DS/ML programs will want you (depending on your programming background) and Stats program will want you.\n\nThis also means that **a lot of graduate programs will like you** \\- the ones where math skills are their chokepoint.  Ask around.', 'You would be at an advantage']"
[Question] Don't know when to choose upper vs lower bound of a one-sided confidence interval,"Hey all, 
This may be a dumb question, but I'm in a biostats course right now and we just covered confidence intervals. The one thing that is tripping me up about it is, when performing a one-sided confidence interval, how do I know whether to use the upper or lower bound for that CI? My textbook just assumes we know how to make that distinction and my teacher hasn't explained making that choice very well.

Any help is appreciated.",11z3yj7,Lodicrous,1679534347.0,1,1.0,"['Look for clues in the context such as at most, at least, or the like.', ""It depends on the alternative hypothesis for the study. The textbook assumes you can make the distinction because, as the researcher, you're the one who has to make the distinction.\n\nIt's the researcher who formulates the specific objectives of the study."", 'You shouldnt use one sided tests / intervals unless a trend in only one direction is plausible, or a trend in the other direction is negligible. For this reason I usually use two sided. But if you do use one sided, it entirely depends on your research question.', '> or a trend in the other direction is negligible\n\n... or a trend in the other direction is literally irrelevant, equivalent to no difference for your purposes.', 'And to the purposes of any of your possible imaginable readers, which you cannot foresee, so virtually never', '[deleted]', ""Imagine I am in an organization that uses some procedure right now. We are considering moving to an alternative approach which we hope might be more effective in some way (a way which we're measuring for our test). Given there's at least some cost/disruption associated with changing the way the organization does things, we only want to change if the alternative is better; if it's the same or worse, we will change nothing, but we are prepared to incur some expense for a small improvement (to remain competitive). \n\nIn this case, the new one being significantly worse *is plausible*, but we simply *don't care* whether it is, it's still the same as being no better for our purposes, we won't change our procedures either way. \n\n(Someone might argue we should probably consider a superiority test instead, but it'd still be a one-sided test, just one with a slightly higher cutoff.)""]"
[Q] What analysis should I run for examining school differences in discipline event counts?,"Hi everyone - I really appreciate your help in solving this question. I'm kind of stuck and wanted other opinions.  Keep in mind I'm using SPSS.

My boss asked me to see if there are statistically significant differences across schools in our district in their disciplinary event counts (e.g., aggression, language, disrespect, etc).

She had suggested running t-tests or ANOVAs to answer these questions.  My concern with running an ANOVA is a violation of normality for using a dependent variable that's a count.  What analysis would you run in this scenario (categorical IV, continuous DV that's a count)?  I assumed Kruskal-Wallis would be more appropriate due to the violation of normality, but then there's another issue....

I also have concerns related to differences in group size because some schools are much smaller than others, so I've considered making a disciplinary event to enrolled student ratio to help solve this problem and then use this ratio as the DV instead of the disciplinary event count.  I still have normality concerns using this ratio as the DV, though, but it does solve the issue of group size across the schools.  I'm also unsure as to whether I can run an ANOVA when each group is made up of only 1 member (because the ratio I calculate would be representative of the entire school and would no longer be student level data).

I've also considered running chi-square between the schools (IV) and disciplinary event type (binary code - 0,1 for whether the event was aggressive, language, etc) rather than pursuing the previously mentioned paths.  I'm not sure if group size would be an issue if I pursue this path.

Sorry if it seems like I'm rambling at times, but I'm trying to put my thoughts out there.  I've been really debating about what path to take with this. Thanks again for any and all advice.  Open to ideas and suggestions.",11z2rkp,kentonw223,1679531691.0,1,1.0,"[""Correct me if I'm wrong, but it sounds like you are more or less using census data here, and not trying to generalize to a broader population, right? If so, I don't think statistical tests are appropriate at all. \n\nThe techniques you are mentioning are for if you are trying to estimate something about the population from a sample. So if your district has 50 urban schools and 50 rural schools, and you want to use data from 5 of each to see if there is a difference between urban and rural schools in terms of disciplinary events per student, then your question would be reasonable. \n\nBut it sounds like you have complete data from all the schools of interest. In this case, you are not using a sample to estimate the frequency of disciplinary events in the population. You *know* the frequency of disciplinary events in the population. I would just report things descriptively, and I agree with using the frequency per kid as your outcome. You could make plots of frequency vs size of school (do larger schools have more disciplinary events per kid?), plots of frequency by disciplinary action type, report the average events per year per school, etc. But I can't think of appropriate statistical tests when you have complete data on your population of interest. Someone else might though."", ""Thanks so much for the response. It's great getting some other opinions.  I don't really have someone to bounce ideas off of so it's really helpful.""]"
[Q] How to determine a baseline to compare against future data,"I have data that has average runtimes of ***n*** process over software builds. I am trying to determine how to best determine a baseline given, let's say the runtimes of 20 software builds, and then compare this baseline to future builds' runtimes - but I'm not having the best of time figuring this part out.

My initial idea was to compare the latest build average versus the average of runtimes of ***x*** builds to date (essentially average of the averages), and then see if it is within ***y***% (y = 5 is what I was thinking). If it is, then use the latest builds' average in the calculation of the 'average of averages' number for future builds. The issue with this method is that the 'average of averages' could just keep climbing if the latest build is within 0% to +5% and would hence not be great long term and can hide performance issues.

Any ideas would be greatly appreciated!",11ywbst,patrickgg,1679518495.0,8,0.91,"['What if instead of using the average runtimes of each build, you determined the actual distribution of runtimes for each build?\nThen you can test (possibly with ANOVA or KS) if the distributions are different (specifically greater or less than).', '[deleted]', ""Data comes from normal distribution. \n\nCorrect me if I'm wrong - you're suggesting that I perform an ANOVA on the latest (lets say x) build and compare x ANOVA to x-1 ANOVA and see if there is a significant difference? \n\nThis is possible (if I've understood you correct, of course), but is there a way to determine a baseline given previous build's data? Or is there a way in ANOVA that allows me to do that that I'm not seeing haha"", ""In my case, a build is a version of existing software with fixes, new features, etc. \n\nI want to reject new builds when comparing the runtimes of n processes from this new build to the performance benchmark from past build information, and hence where I thought of the 'average of averages' idea if they are slower than this benchmark."", 'Whoops, I lost sight of your question. I was instructing how to tell if each successive build was quicker or not.']"
[Q] Calculating probability of investment strategy.,"I am an amateur with no background in statistics and wondered simply how to calculate the probability that an investment strategy ""works""

For something simple like a 200-day Simple Moving Average on the SPY... would this be correct?

1. Pick a value (e.g. CAGR, Sharpe, etc.)
2. Create a randomized data set (e.g. 1,000 simulations on randomly sampled SPY data)
3. See how many simulations have a CAGR (or sharpe, etc.) higher than our strategy.
4. Get the p-value by dividing the count by # of samples.

Is that it? Seems too simple, so assuming I'm missing something significant.",11yr7sx,strangeinutah,1679508320.0,2,0.75,"[""Not my field, but I think number 2 is much easier said than done. I have no idea how'd you generate a 'null distribution' of SPY time series from simulations. It's a time series, so you can't just randomly sample SPY data and squash it together. Similarly, if you take 1000 'slices' of the time series, certainly there'd be significant overlap between samples (so they're not independent) which have events that would bias your results.\n\nI think the best you can do is just back test your method and see if it works well."", 'Thank you for the reply.\n\nThe basis for the ""randomized"" data is from two articles I read where they used that method to simulate ""randomized"" data for the S&P500:\n\n[https://www.datacamp.com/tutorial/stocks-significance-testing-p-hacking](https://www.datacamp.com/tutorial/stocks-significance-testing-p-hacking)\n\n[https://www.winton.com/research/seasonal-volatility-and-the-multiplicity-effect](https://www.winton.com/research/seasonal-volatility-and-the-multiplicity-effect)\n\n\\--\n\nThe premise is that historically October has been an especially high volatility month and December low. The conclusion is that, after correcting for multiplicity bias, the volatility effect is not statistically valid.\n\nI had trouble following the argument though, as the first test did seem to show statistical significance, but then they only took the highest volatility values from each backtest to show that it was in fact not statistically valid.\n\nIntuitively the results seem wrong though. December being lower volatility makes logical sense. There tends to be a positive market effect in December, arguably from increased revenue/sales and people\'s generally optimistic mood around the holiday season, which would also show up in lower volatility.\n\nOctober is tougher from a common sense standpoint though. It may be some effect from Q4 beginning and an increase in activity to reach year-end goals, or it may just be a historical anomaly from effects like Black Friday.\n\nI don\'t have enough of a background to make sense of the argument, but was rather interested in validating other models.\n\nYour approach is what most people do though I believe (test something, see if it works, stop using it if it no longer works).\n\n&#x200B;\n\nThank you for the reply.']"
[Q] Which would be the better choice? Majoring in Mathematics and Computer Science or Statistical Science and Data Analytics?,"these are not four majors, they are just two majors available at my school. I am struggling to decide which will be the better option in terms of employment/ being financially stable. Also I dont plan on going to graduate school so which degree do you guys think will hold more weight?",11yq9j0,Local_Slip_1011,1679506440.0,0,0.38,"[""What kind of employment would you rather have? It's probably easier to find a well-paying job with a computer science degree, but if you want to work in the intersection of those fields it's easier for a statistician to teach themselves basic coding than it is for a software engineer to teach themselves statistics."", 'Which do you enjoy more?\nYouve got half a lifetime of work ahead of you- better enjoy it!\nPlus, youll likely produce better results and show more initiative (resulting in more money) if you enjoy your work. \nOut of the gate, CS will likely pay more.', 'Statistical science and data analytics will be more applied and will set you up for a work. Math and CS can also be applied from a software engineering perspective, which will always be useful.']"
[Q] How do you think AI will impact the role of staticians in the near future?,"I ask as an undergraduate in statistics, close to finish it

How do you think AI will impact our role as staticians? Do you think it will lead to new statistical methods that will improve our work? Will AI make it easier? Will it take our jobs (i hope not lol)?",11ynaey,Aston28,1679500552.0,29,0.79,"[""It could automate some simple tasks (e.g., entry level data analyst positions), but I don't see it having much of an impact otherwise. Current AI methods are good at tasks which require reasoning by analogy/superficial similarity, but struggle with deductive reasoning and more sophisticated tasks. This is an inherent limitation of the neural net approach, so it's probably not going to disappear soon. Most of statistics falls in the latter category, so it's unlikely to be affected in the near future."", ""I'm not a professional statistician, but my work involves a lot of stats and ML. I've tried different LLMs and so far non can even begin to help me with my job. I tried getting them to write STAN code, but debugging the result is way harder than if I had just written it myself. If LLMs eventually can translate models in math language into super-optimized STAN (or GRETA or whatever), I don't see them as being much use.\n\nEdit: Moreover, they'd need to be easier and better than things like brms, which is already *very* close to something like this."", ""More companies will need to have human statisticians to come in and actually explain the statistics they've tried to automate because AI is suddenly a buzzword that can attract irrational and stupid investors."", 'I think for code monkeys its kind of a danger but theres more than just being a code monkey and also knowing how to ask it the right questions and checking/debugging the output (it can be wrong totally).', 'I think an emerging approach will be modeling over AI LLM outputs and designing statistical models to account for their specific kinds of variances, biases, and limitations.', 'A proliferation of frauds in the job market.\n\nMore confusion in meetings between business folks and statisticians.\n\nEasier path to proficiency for autodidacts.', ""From my perspective in the Causal Inference world, AI is useful but will probably not take the jobs of folks who study how to find causal effects from observational and experimental data in the near future. For example, there are definitely good methods to learn graphs from just observational data, but it's been well shown that most of the time Causal Discovery algorithms can never learn ground truth graphs. \n\nIt's up to the researcher to use their brain and past literature to make assumptions in how to interpret data. Could AI potentially do that in the future? Perhaps- I certainly didn't think large language models would be this good 2 years ago, but it's hard to imagine systems understanding Causality as well as humans do in the near future."", ""You could probably churn out lots of terrible cookie cutter statistics with AI but miss a ton of value and basically be giving wrong answers. You'll get rid of the people going through week-long bootcamps but not the people who know what they're doing. Many don't appreciate how bespoke statistics can be when you start thinking seriously about the problem you need to model."", ""Not a whole lot. It'll definitely make writing boilerplate much less painful, like for grants or SAPs. Or if I need some code to generate a weird one-off figure, I could see a LLM coming in helpful. But beyond that, especially for research purposes, I don't see LLMs gaining much traction in the field. \n\nRemember, if you're in research, your job is essentially boils down to annotating out-of-distribution examples. You're describing and characterizing phenomena no one's ever seen before. LLMs can't do thatthey have to work off the data they were trained with, which can be months or years out of date. That could change, but the bigger hurdle is trusting the research output of LLMs just as you would another scientist in your field. Even with GPT4, we're still at the point where the best LLMs are hallucinating papers and authors that don't exist. I'm not sure if we'll get to the point where we trust the higher-order reasoning of LLMs anytime soon."", ""In my dissertation research, I am using LLMs to process text data for educational assessment / psychometrics. There's not a lot of researchers working on this. No one has laid out how this connects with classical test theory, item response theory, or other current statistical paradigms in psychometrics. One question, for instance, is how to quantify uncertainty with respect to open ended text responses? There's a lot of work that needs to be done here. At some point, a general AI will be able to do that for us (including designing the research, doing the math theory, etc.), but that probably won't happen in the next 20 years, and even it does, we'll need researchers to at least verify."", 'No', 'Maybe automate supervised/unsupervised learning', 'I see more systems adding on AI and machine learning engines into their products - at least from an HR systems standpoint. Many of these models are some variation of black box machine learning ( or just a stepwise logistic or linear regression). Depending on the quality of data, you can get a lot more false positives than any insight that is truly meaningful.\n\nThere still a need for statisticians to help and advise; provide frameworks and methods to tackle unique issues.  Plus, ad hoc research will still need to be done and there arent really grab and go frameworks that will test exactly what you are looking for.', ""It might be used to verify theories or test new methodologies but it's hard to imagine the black-box ML ai model being something that Walmart or Target uses to pick next season's product lineup. It just seems too risky for those types of decisions right now"", 'Statisticians are VERY safe. It will make the job easier but also will push the field further. It will automate the simple tasks. Imagine if we were still serving by hand? Thank god for the calculator.', 'Complete and utter cope in this comment thread. \n\nNah true obviously because the methodology does not currently automate your job it will never in the future get better and automate your job.', 'Most of the work I do involves telling people that they are incorrectly interpreting p-values, which people have been able to google on their own for 30 years now', 'I will say interpreting the model to someone without stats background would be the trends for stats background employees', 'We use both ML and statistics at my work. Statistics is very useful for building understanding and showing your work, and also for developing labels and features to give some understood relationships to the training data that can make a huge difference in predictive accuracy.', '[removed]', 'GPT3 has been fine at debugging my R code and writing a more efficient function than my original. Or doing the more tedious tasks like writing scrips for multiple ggplots for a set of variables.', 'Yes! A lot of software devs think they can do anything (modern version of the overconfident engineer) and just use these things without understanding any of the limitations, biases, evaluation metrics etc.', 'What is a code monkey?', 'Bayesian hierarchical models for optimal LLM code outputs', ""I couldn't agree more:)"", 'Reply to this email and politely explain to the MD that we cant do this analysis for one of the following reasons: a) we did not collect information on that sub group, b) the trial is still blinded  c) the data are immature or d) I know the PI told you about something but they are still 6 months behind on data entry and I cant work with unmetered data. Would save me hours a week. If only our email retention policy was longer, Id probably have enough training data to get a LLM to do a large part of my work.\n\nIm not so sure it would save much time on SAPs though, Ive got template language for a lot of things I dont need to review. If I let a robot write it all Id need to review everything.', ""I've been thinking about learning ml, where do you think I should start? I already have a profound knowledge of probability theory and a lot of math related to statistics"", '?', ""I read 'code monkey' as someone that doesn't really understand how to program all that well and everything they write is really just a mash of copy-pasted code from Stackoverflow. This is distinct from someone who actually learns how to do things themselves and can solve problems that aren't on Stackoverlow."", 'A person who knows how to code algorithms but lacks the knowledge to fully understand how they work, when they can be correctly applied and how to interpret their results.', 'also data shift and their effects on outputs', 'What kind of programming background do you have?', 'A demonstration of poorly programmed AI I guess XD', 'So, data scientists? \n\nEdit : Im making a lighthearted joke! :)', 'Lot of R and a little bit of C++', '*Bad data scientists', ""Cool, so you should be able to figure it out in R or pick up python and go from there. I started with Coursera myself, this course https://www.coursera.org/learn/practical-machine-learning?specialization=jhu-data-science was my introduction and it's in R. It will show you the basic ideas behind how training models works and what tools to use. It doesn't really go into the math at all though. You can audit the course for free\n\nIf you want something deeper, check out https://www.statlearning.com/ also in R, probably the best way to do it if you are able to teach yourself"", ""Thanks, I'll take a look at it!"", ""Just tried to apply but, is it me or it costs money? I haven't used coursera before"", 'Hm, used to be able to audit it, not sure if you can anymore']"
[E] Recommandations for learning Probability/Statistics,"Hi everyone,

I am in the process of trying to get job interview for quantitative finance. Problem is, I severely lack background in probability and statistics. What is the best resource to learn them? I am looking for books/lecture notes, but I really don't like slides/videos. I don't think I need extensive proofs, but examples would be very nice, as well as exercises.

My background is a PhD in mathematical physics, so you can be as mathsy as you want.

Cheers!",11yn2p5,nomenomen94,1679500110.0,3,0.81,"[""With a background in mathematical physics, you likely will be able to jump right into graduate level material; that is, I'm assuming you have a decent background in linear algebra, calculus, and real analysis.\n\nIf you really don't have solid basics, you may want to breeze through an upper undergrad level treatment of the fundamentals, e.g. something like Hoel, Port, and Stone's Introduction to Probability Theory and Wackerly's Mathematical Statistics with Applications. The math will be simple for you but you'll build a familiarity with the basics.\n\nAt a slightly higher level, Casella and Berger's Statistical Inference is a fairly standard intro graduate non-measure theoretic treatment of statistics that would build on those other texts more.\n\nBeyond that, you get into more serious texts like Durrett's Probability: Theory and Examples, which has a measure theoretic treatment of probability and builds to important concepts that come up in financial math like martingales and stochastic processes. Another alternative would be inlar's Probability and Stochastics, which is another treatment of similar material. A deeper dive into measure theory that still builds to probability applications can be found in Billingsley.\n\nThere are a number of texts to consider if you wanted to explore more traditional statistics, such as laying the foundations of hypothesis testing, constructing confidence regions, etc. Bickel and Doksum is all right and is reasonably rigorous to build up solid foundations on very classical statistical methods. van der Vaart's Asymptotic Statistics does a decent job developing asymptotic theories that underpin many common practices. However, these sorts of classical statistical approaches are perhaps less common in the world of finance.\n\nThe ultimate goal for quant. finance would be to develop a solid background in stochastic calculus/processes. J. Michael Steele's Stochastic Calculus and Financial Applications has a focused approach to these topics, which would probably be a solid primer.\n\nObviously, all the above would require a good deal of time and effort to master. A more streamlined approach would be: build basics of prob/stat -> introduce more rigorous treatments of probability necessary to understand the foundations of stochastic processes -> stochastic calculus and other financial math topics."", 'You could try Elements of Statistical Learning.', ""> My background is a PhD in mathematical physics, so you can be as mathsy as you want.\n\nThe main mathematical tools for general statistics will be calculus and linear algebra (not enough that you won't already have those well in hand) but for quantitative finance you'll definitely need some some work on stochastic processes. If you don't have any measure theory, you'll want to pick it up for that -- and if you get into the statistical theory, again you'll need it for the probability it's based on. You may also want some exposure to time series.\n\nProbability wise I'd start with something simple to get going; the Harvard 110 course is decent (NB no measure theory in this). Go to https://projects.iq.harvard.edu/stat110/home -- the book is downloadable there; there's exercises and such to be found. You don't *have* to watch the lectures, the book is pretty easy to read. You will want something more technical later, especially when it comes to stochastic processes, which tends to come up a lot in finance work\n\nPerhaps Wasserman's *All of Statistics* (the title's a bit overblown) might be a good overview book on some of the statistical ideas? If there's too much of a gap to get to that, you might want to add a more basic mathematical statistics book in between the probability course and this book, but you might be okay.\n\nYou'll probably want some practical regression text. I don't have a specific suggestion for that right now. \n\nIt's possible you may want some exposure to something like *Elements of Statistical Learning* (downloadable for free from the website of the lead author), though you probably only need parts of the book to begin. Some of the earlier ideas would be particularly useful in any case.\n\nI don't have a good suggestion on stochastic processes, nor on stuff like ARCH/GARCH models.\n\nHopefully someone with more finance background that me has some good ideas."", 'Out of curiosity, did you ever take a course on statistical mechanics?', 'Sure, but the only relevant things I did there were ensembles and partition functions']"
"[Q] Multi-input, multi-output regression modeling","Let's say I run a recycling business. Each day, a truck goes out to a variable number of local businesses and picks up aluminum cans and brings them in to our processing facility. In our recycling facility, we melt down each truck load of cans and then output a variable number of batches of aluminum sheets. 

Each batch of aluminum sheets we produce must pass a purity threshold before we can sell it. We would like to use knowledge of the inputs to predict the probability that a batch of aluminum sheets will be pure enough to sell (90% pure or more).

Some complicating details:

* We have knowledge of each supplier's ""purity"" as well as some other characteristics
* We know how much of each supplier's cans go into each truckload, and how much of each truckload goes into each batch
* Most days, we do not utilize a whole truckload, and so we store it and use it later. Thus a produced batch may contain a mix of different truckloads (we know the mix)
* The number of suppliers and number of batches produced each day is variable, between 1 and 30

In other words, the dataset looks like this: [structure](https://imgur.com/a/KjpvLwz)


If it were single input single output, we could use standard regression. If it was single input multi-output, we could perhaps use standard regression with some multi-level structure. But this is multi-input, multi-output and I would like to utilize as much input information as possible. Weighted averages of inputs works OK, but assumes linearity and I would like to see if I could utilize more of the input information for prediction.

It seems like this is a situation where latent variable modeling would work: Combine all of the inputs into a single set of features and use those to predict the output purity. The only example I can find is [this one on stackexchange](https://stats.stackexchange.com/questions/429655/regression-model-with-aggregated-targets), but I feel like I'm completely missing something as this doesn't seem like a novel problem. I would like to use an existing library if at all possible, as it will be more difficult to productionize and/or handoff some bespoke R/py/stan code to someone else.

Does anyone have references, sources, or suggestions?  Thanks!",11ymvv0,rogmexico,1679499723.0,8,0.76,"['If you know the purity, amount and source of input aluminum and also control the mix for batch processing, cant you directly calculate the output purity? Why do you need statistics at all?', 'This looks like multi-input, single-output to me. The outcome variable is \'purity\', right?\n\nTry the following data structure: one line per batch, and for each batch, you have the following variables: S1 . . . S11, which are the indicator variables for each supplier. Example to illustrate ""indicator"" variables: If suppliers 3, 7, and 8 contributed to the batch on this row, then S3 = 1, S7 = 1, and S8 = 1, while all other S# variables = 0.\n\nOnce you put this together, run a regression model where purity is the outcome and S1...S11 are the predictors. The coefficient of S# is the estimated difference in purity for batches that include S# compared to batches that do not contain S#. In other words, if S3 = 0.1 and S4 = -0.2, then we would expect that adding cans from both S3 and S4 would have a net effect of 0.1-0.2 = -0.1 on the purity.', ""You might look into methods from compositional data analysis - it deals with data like proportions (both on the outcome and predictor side). I won't pretend to be familiar with the area, but some of [these](https://rpubs.com/EmilOWK/compositional_data) [links](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwikjtL3ofD9AhU_nGoFHTmGAQkQFnoECA4QAQ&url=https%3A%2F%2Fjds-online.org%2Fjournal%2FJDS%2Farticle%2F136%2Ffile%2Fpdf&usg=AOvVaw0ykEU-2w98G0KZJnMoSTw1) might give you a place to start. There are some R packages for that (including compositional data as predictors), AFAIK, so that might be worth exploring.\n\nIt's not 100% clear from your OP - do you have multiple processors producing multiple batches, or only a single processor? I don't think you need a random effects component here unless you actually have multiple processors, which it doesn't sound like you do from your description. Just having multiple batches per day probably doesn't require some kind of repeated measures-whatever - I'd just include source volume (or whatever other factors lead to multiple batches per day) as another predictor in the model, along with the compositional ones."", ""This sounds like a multivariate classification problem. Couldn't you just use a decision tree? Or decision forest, etc."", 'I would love it if that were true. Unfortunately:\n\n* Both the input and output purity measurements have some noise\n* Parts of the processing can degrade the purity or improve the purity, in ways that are noisy. This is sort of what we are trying to quantify.\n* We do not control the mix of batches, it is arbitrary and we only know after the fact. Although a model like this could help change that process.', 'I think the issue with this is that the number of ""suppliers"" in my actual problem is very large (30k+) and not repeating often, so we might have 30k columns with all entries 0 except for a few which isn\'t practical IMO. I think the input batches need to be modeled as some combination of the features of each batch.\n\nThis could be treated as multi-input/single-output if I make some assumptions on independence (easy), but I think the tricky part is how to deal with the multiple inputs producing an output.', 'Thanks. In the ""real"" version of this problem, there are multiple processing locations but I think I can handle that aspect with a grouping effect if I need to. The main part is how to handle the input data in a coherent way since it\'s overlapping.', 'I think the big question here is ""can I treat the outputted batches as independent?"" \n\nIf you can, just use a standard multivariable regression to predict each batch\'s purity given the X inputs (if you want to predict its over 90% purity or whatever, convert your dependent variable to be binary and use a logistic regression).\n\nIf you can\'t, look into multivariate (note that this word is different than multivariable) regression which allows you to regress on multiple dependent variables (your outputs).\n\nI think how I\'d start with this is see how good your model is assuming batches are independent. If it\'s good enough, great. If it\'s not, compare it to the multivariate model (this model should theoretically be better because it includes more information, but it will be messier to work with and interpret).', 'Thanks for clarifying. I had misunderstood the process.', '>I think the issue with this is that the number of ""suppliers"" in my actual problem is very large (30k+) and not repeating often, so we might have 30k columns with all entries 0 except for a few which isn\'t practical IMO.\n\nI guess the question is what you\'d like to use to predict purity. Are there characteristics of suppliers that you could use instead of individual suppliers? E.g. if some of the suppliers are supermarkets sending you returnables, while others come from home pickup services,\\* would you have this information, and would it be useful if you constructed a model that predicted purity based on supermarket v. home pickup? \n\n&#x200B;\n\n\\*I\'m making things up here - I clearly know absolutely nothing about your business.', ""I think handling the outputs is easy: I can just assume they're independent and repeat each of the inputs for each record.\n\nStandard regression won't work here because there are multiple inputs for each output record (multiple rows in X for each entry in Y). I think the hard part is figuring out how to coherently reduce each set of X records to 1 input vector in a way that loses minimal predictive information.""]"
[E] Online Master's with TA (or something similar) available?," Sorry if this is not the right place to ask for this advice. Please direct me elsewhere if so!

I'm looking to get my Master's in Statistics (or something Statistics-related) and would ideally like to do so online. I'd also like to minimize the cost as much as possible by being a Teacher Assistant or Research Assistant or something along those lines. With my initial search, I've found plenty of online programs without funding and plenty of in-person programs with funding. However, I haven't quite found anything that ticks both boxes. Any advice?

If it's relevant: I have a Bachelor's in Mathematics and Secondary Education, and have 4 years of high school teaching experience (including 2 years online). Would really love to be able to further my learning in Statistics, a true passion of mine, while also doing some form of teaching.

Thanks in advance!",11yixdd,hubagruben,1679491184.0,3,0.8,"['Penn state has a masters online of applied stats. Not sure the cost. Stats adjacent check out Georgia tech online masters in comp sci specializing in ML, its only $10k', 'Thanks for the info! Do you happen to know if either of them offer funding opportunities? I suppose if I just find a relatively cheap program its not as much of an issue, but I kinda like the idea of being a TA or RA to help pay for it.', 'Im not sure I would reach out to the program coordinators.', 'Thanks, I will!']"
[Q] Help with p-values!,"Hello all. I am currently writing up my dissertation and have used SPSS to analyse my data by carrying out a one-sample T Test based on gender groups.

My research is orginal research which aims to explore thoughts and feelings toward AI from a student radiographers perspective. This research topic has never been investigated, so it's hard to have any hypothesis to work from! 

So a long story short, I have some statistically significant results (mostly for how females perceive AI) and wondered two things:

1). As original research, do I need a null hypothesis when using p-values? 

2). If yes, which hypothesis would you work from, bearing in mind there are some concurrent statistically significant results for males too. 

Just worried that right now my statistically significant results have zero context to someone reading my paper!",11yhs5i,AlexS58,1679488557.0,0,0.44,"['> As original research, do I need a null hypothesis when using p-values? \n\nYou can\'t even begin to compute p-values without one. How do you calculate the distribution of the test statistic under the null if you don\'t know what null to compute it under? For that matter, how do you choose a test statistic if you don\'t know your hypotheses?\n\n> This research topic has never been investigated,\n\nAttitudes to AI more generally has certainly been studied, and I expect attitudes to some form of *technological change* among radiographers has probably been looked at before, and likely differences between attitudes of students and working radiographers, if that is needed to bridge a gap to some such studies. \n\nFurther, it\'s often the case that a question of interest can be formulated *even with no prior relevant studies*. I don\'t know that this is the case here. Why do you care about feelings toward AI from a student radiographers perspective? What value would knowing about that have? What sort of differences in attitude would be important to know (between what groups would be important to the goals of the study?)\n\nThe problem is you\'re considering such questions at entirely the wrong end of the process.\n\n> so it\'s hard to have any hypothesis to work from! \n\nIf you were truly not in a position to have any hypothesis, *you should not be testing at all*. You would run an exploratory study designed to help formulate one.\n\n> which hypothesis would you work from,\n\nI would not be in the position of even collecting data without knowing what the purpose was in a very much more precise sense than you have articulated, and what my analysis was going to be to answer that question (or those questions, if there was more than one central issue). How can you design a study properly if you don\'t know what the precise research question is? \n\n> my statistically significant results\n\nIf you have p-values, you had a null hypothesis that the test was based on. The question is, *why would anyone care about that hypothesis*? It appears you didn\'t even know what you were testing, so the question is was testing was clearly not that important to you.\n\nFirst question to address is ""What am I trying to achieve/find out?""\n\nNext question is ""What\'s a good approach to doing that?"" (that\'s something we might be able to help with)\n\nStart with Q1.\n\nDon\'t just throw data at hypothesis tests if you haven\'t thought about what you\'re testing or why. If you\'re looking at p-values of tests you don\'t actually need, you\'ll greatly increase the chances of false discovery.', ""Nothing is really 100% original, we build upon the existing literature. \n\nI won't say you need a null hypothesis. But if you don't have hypotheses, you should have a good reason for that. \n\nIf you are testing differences between females and males, why do you believe there is, or there isn't a difference, based on the existing literature? That will help you to justify your hypotheses. \n\nFor example, is it because females are more risk-averse than males, and, therefore more careful about innovations like AI? In this example, there is a lot of prior research about gender differences in risk aversion. \n\nBut that's just one possible explanation. Exploring possible explanations is usually part of the job. Identifying what explanation is really true can be a huge part of the job. \n\nBy the way, you should have the hypotheses before running the tests and knowing the p-value. Otherwise, you are HARKing (HARK = Hypothesizing After the Results are Known)."", 'What is your actual hypothesis?', 'You must do your analysis based on your conceptual framework. That means:\n\n1. Choose a validated conceptual framework. You can modify it based on your literature review and own hypotheses.\n2. Develop, pre-test, and validate your questionnaire before you collect your data.\n3. Clean your data.\n4. Analyze your data based on your conceptual framework.\n5. Discuss your results according to your conceptual framework.', 'What does your data look like? How did you collect it? If you surveyed a class, you could use it to form a hypothesis. \n\nFor instance, if you asked one question: On a scale from 1:100, how important is AI? You would also need to know if they were male or female. You could use the data to form a hypothesis for a broader study, maybe across multiple clases at different schools.\n\nYour hypothesis could be something like:\n\nH0: On average, there is no difference between genders on the importance of AI for student radiographers.\n\nHa:  On average, there is a difference between genders on the importance of AI for student radiographers.\n\nHa: Genders believ', 'Thus is the right answer', 'I\'ve had to use literature which explores the thoughts and feelings of post-registration radiographers in practice as there is no existing literature focused solely on students. This has allowed for comparisons to be made and some interesting parallels to be drawn.\n\nI believe that the male/female divide is down to cognitive theory which forms preconceptions, i.e., AI is technology, technology is seen as \'uncool\' for girls and a guys domain, so those form barriers. Males spend more time with technology as a whole, so are more confident (which reflects in my results).\n\nI\'m limited to 1,700 words for my discussion but have uncovered so much I could easily do 3,000!\n\nWould having a hypothesis which says ""understanding and perception of AI is gender-identity dependent"" be too generic or non-specific?\n\nFirst time doing this and it shows ', 'I don\'t currently have one as this part of the analysis was suggested by my supervisor to delve more deeply into the male/female divide he thought might (and does exist).\n\nA working hypothesis might be: ""Female understanding and perception toward AI is less than males"", but I worry that\'s not exactly egalitarian!', 'If the male/female divide is related to that cognitive theory, then I believe it doesn\'t matter if they are radiographers in practice or students or something else. The theory should be valid for any of them. \n\nYou may have uncovered so much, but probably not everything is really so interesting and relevant. For example, some of the conclusions may be more obvious, and people would guess them correctly even without theory and data. The surprising results are often more relevant, and require more explanations and discussions. Some of the conclusions may have less managerial implications too. \n\nI think your hypothesis is generic. ""Understanding and perception"" can include so many things that I don\'t really know what you\'re talking about. Perceptions about what, for example?', '> ""Female understanding and perception toward AI is less than males"",\n\n> by carrying out a one-sample T Test based on gender groups.\n\nIf you administered the same measure to both men and women, and you want to determine if the mean values of your measure are different between those two groups, then a one-sample t-test is not appropriate. That would be at least an independent samples t-test, but I would hope you\'d be considering a more robust test that can control for potential confounding factors (like prior experience with computers in general and AI in particular).', 'A couple pieces of advice:\n\nYou almost always have a hypothesis, and you\'re not doing yourself any favours by not making it as explicit as you can as soon as you can. Your hypothesis sounds to me something like ""there exists measurable differences in attitude or understanding (or whatever it is precisely that you believe you\'re measuring) toward AI between genders within this group of people"". It can be as simple as that I think.\n\nYou also seem anxious to actually say that though? If you have genuine trepidation about making claims about gender differences, why on earth would you embark on a project to study them? It\'s very possible that I\'m simply confused, but if you haven\'t thought about the meaning of your study and how well it fits into your and your community\'s web of beliefs, then I recommend hashing that out asap.\n\nGood luck!', 'In your view, would it be better to simply analyse my results by comparing means and measuring SD, as I have also done, and drop the t-test entirely in light of not being able to create a hypothesis in lieu of there being no student specific literature to draw from?', ""I think simply reporting means and SDs on something that's crucial to your dissertation isn't nearly rigorous enough, but this is ultimately a conversation you should be having with your advisor.""]"
[Q] In logistic regression when we calculate probabilities from log(odds) why do we consider maximizing they probability/ likelihood of respective classes...,"Let's say we have to predict whether something will belong to calss 1 or class 2....why while maximizing the likelihood we maximize the probability/likelihood of each data point belonging to class 1 and class 2 and not maximizing the probability/likelihood of every data point belonging to either one of the class...

LEST SAY WE HAVE 4 DATA PONTS.... a,b,c,d.

a and b belong to class 1 while c and d belong to class 2.

Why do we maximize this....

(Here pr means probability )

pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 2) * pr( d belonging to class 2) 

and not this.....

pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 1) * pr( d belonging to class 1) 

Because probabilities of each data points with respect to a single class makes more sense to me.

Please Help ",11yb9xn,emo-9,1679471448.0,2,0.75,"['>pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 1) * pr( d belonging to class 1) \n\nIf you maximise this, then you may as well ignore all the variables and just set p(in class 1) = 1 and p(Y in class2) = 0']"
[Q]Unbiased Estimator and Recall and Accuracy,"I have a doubt about something of the unbiased estimators  


I know I can have an unbiased estimator and it can still have a high variance, but I am not so sure how this would like to practice, and I remember that there is a difference betweeen being accurate and the level of recall you get, are those ideas related? Also, I do not know how values being calculated with a unbiased estimator but with high variance would look? Does anyone have any example of this? Because I know the definitions are not related but i cannot see how something unbiased can high dispersion levels",11y76ez,Unhappy_Passion9866,1679460073.0,1,1.0,['See the image [here](https://www.researchgate.net/figure/The-dart-example-for-a-high-bias-and-low-variance-b-low-bias-and-high-variance-c_fig1_333505702).']
"[Q] Is economics a good major if I plan on working in Statistics or related fields? (risk analysis/management, financial analyst, insurance)",Im a high school senior planning on doing a transfer program at my state university (since its really my only option) and I just figured out it only allows admission into the liberal arts college. So I was wondering if Economics is a good major for me to choose if I plan on working in the field of statistics.,11y5sc4,decoytesting,1679456635.0,38,0.95,"['It can be.  You just need to take as much econometrics as possible.  It would also be good to take some linear algebra, SQL, and python programming, if at all possible.\n\nDepending on the program, an undergrad econ degree can be a lot more theory and a lot less math.', 'Yes, it is a very good major for that purpose. I work as a data scientist at an AI department of a multinational company, and the vast majority of my colleagues have Economics background (and an MSc in Statistics or Econometrics afterward).', 'If you can take econometrics modules yes.', ""Unless you want to do theory, you don't really work in the field of statistics, you work in a specific field of application. The numbers don't mean anything without context so (applied) statisticians need to understand the field they're working in.\n\nHealth research is probably the most advanced in terms of employing applied statisticians, as opposed to relying on researchers who know a bit of statistics. AFAIK economists are still reinventing the various wheels and assuming they got there first. But, if it's a subject that interests you, there are plenty of opportunities for statisticians working in economics and related fields (especially business, finance, and health economics).\n\nThat's not to say that you'd be stuck in an economics-related field if you majored in it, but a major that is related to the applied field you want to work in is useful when you're applying for first jobs. Your first proper job as a statistician will (probably) define the field you work in for the rest of your life. So choose wisely when the time comes, but nothing gets set in stone now.\n\nThe most important thing is to choose something you will enjoy studying. Don't bore yourself to death for years just to polish your imaginary future CV. Finding the right path is a process, not a decision you need to make right now."", 'Ill throw my two cents in. Did undergrad in economics and masters in data analytics. To the point someone else made that you work in a field of application, I do digital strategy and consulting in supply chain. Overall, very fulfilling role with the kinds of projects I get to work on. Its remote, pay is good enough for a wife to work by choice and two kids. \n\nUnless youre in some extra curricular activities and heavily involved with your university, I would implore you to go and work anywhere. Get a feel for what businesses do to get things done. Agriculture, warehouse, manufacturing. Anything where you can intern and get a feel for all of the things that are happening to create data that you will eventually analyze. Hope this helps. Good luck!', 'Depends on the school go to.  Some have a Math Econ concentration that would be good.  If not, you might major in math or statistics.  Now, grad school in Econ would be great preparation!', 'I would suggest you look into Actuarial Science as a major.', 'yes just make sure you take a lot of econometrics. Many of the best data scientists and statisticians ive worked with have come from an economics background.', ""FYI, you're asking this in a stats sub.   \nI've studied econ undergrad and stats for grad and I've found that having studied both fields have helped me with my work. Based on your title, it seems like those are the fields you want to go into so between econ or stats, I would say stats is better suited for you. You've listed a number of specific fields where actuarial science would be better suited for you. If you want more flexibility though I would do stats and then take some actuarial classes if offered to get some exposure."", ""When you're majoring in economics, you'll have the chose of doing at least one course in Econometrics, which I recommend doing if you're looking at working in Statistics. And beyond Econometrics, you can also take other electives that are involved in statistics.\n\nThink about doing a Minor in Finance, A Minor in Statistics, or a Minor in Businesss Analytics along with your Econ major."", 'Totally yes. Learn as much econ basics. Also statistics basic. Then the GPT should be ready enough so you can combine what you learned into practical fields.', 'Is it easier for a statistician to become an economist than vice versa?', ""I am graduated in economics and work in a bank doing DA.\nUNLESS you really delve in econometrics and related topics the course itself don't offer much synergy.\n\nI would opt for  statistics or something akin to computer science"", 'If the Econ program at your state school is like mine, then its definitely not enough on its own. \n\nLook at the course requirements: are there requirements for Calc and stats? If so, can you take them in the math department, or do you have to take something like X for Econ or X for Business? You want strict math requirements, and courses in the math department will likely be more rigorous. \n\nIf I were you, Id check to see if they have a Mathematical Economics program (in my case, it was liberal arts econ plus an extra year of math requirements). Or can you minor in math, or just take a lot of math electives? \n\nIdeally, youd take Calc 1-3, linear algebra, probability, mathematical statistics, and econometrics (and a mathematical economics course if they have it!). That covers your bases really well and opens up a lot of opportunities, plus it gives you the foundation you need to self-teach what you need going forward.', 'If you were a recruiter, would you take you with an economics major when there are graduates with statistics, data analytics, data science majors out there to fill a role?', 'I mean technically economic theory is (or can quickly get) pretty math heavy', 'I took one econometrics class and it was way more theory than I got in my undergrad statistics program, which probably says more about my stats program than econometrics but still.', 'Ill make sure to take these classes I think Ill have room in my schedule for some extra math electives', ""I second this. Take more econometrics courses and less theory. If you can fit an intro to ML course, then you'll be in better shape. Learn some SQL - maybe in an internship? If you learn R or Python in your econometrics courses that would also be beneficial. If you can do an undergrad thesis where you take a dataset, analyze and write it up, that can be really good learning experience."", ""Yea, UT-Austin requires a set of calc classes and I'm sure I can pad out the rest with math and stats electives"", ""No I guess not but you're also forgetting work experience and economics is adjacent to a lot of fields and careers"", 'Really depends, economics gives you good insights on the interpretations and uses of statistical models...  it really depend what the role exactly is, the theory and skills that where developed, what is the most needed for the job, etc.', ""Economics is a math-heavy discipline, but many undergrad programs aren't that rigorous. Just to give a sense of the lower bound, you can get a bachelor's degree in econ from my *alma mater* without taking a single semester of calculus."", ""Even so, you might have difficulty if you don't have experience on the tools and computer languages employers are looking for. Excel, SQL, R, and Python are things that everyone looking to get into a white collar field should work into their curriculum. If you dont you're essentially starting from square zero anyway.\n\nI was 4 months jobless after graduation because of that even though I had econometics and partial differential calc in advanced micro under my belt."", ""Don't go too crazy at first. You have plenty of time and everything doesn't have to be fully optimized. Lots of time to figure out what you want to do and learn."", 'That should help a lot then!', ""Sad agree. One of my undergrad institutions was like this. No req for calc, and econometrics was in the course catalogue but wasn't actually taught. \n\nIf you go to a university like that get at least a minor in math and (if possible) take a few comp sci courses."", ""That's really sad and almost a scam at this point lmao"", 'Idk if my university has cross college majors and minors, i.e. Id have to pick a minor also in the liberal arts schoil']"
"[R] Given that there are 676 computer animated films and the average movie runtime is 130.9 minutes, there is approximately 61.45 days worth of computer animated films. That's only 2 months.","Sources:

[https://en.wikipedia.org/wiki/List\_of\_computer-animated\_films](https://en.wikipedia.org/wiki/List_of_computer-animated_films)

[https://www.statista.com/statistics/1292523/lenght-top-movies-us/#:\~:text=In%202021%2C%20the%20average%20length,one%20hour%20and%2051%20minutes](https://www.statista.com/statistics/1292523/lenght-top-movies-us/#:~:text=In%202021%2C%20the%20average%20length,one%20hour%20and%2051%20minutes)).",11y4q4w,Memorie_BE,1679454115.0,0,0.17,"[""Guideline 4 (at least in old.reddit):\n\n> Just because it has a statistic in it doesn't make it statistics."", ""It's literally only a statistic. What else could this possibly be?"", 'Its more of a curiosity than an application of statistics.', 'Where do I post this revelation them? Not even shower thoughts wants it because they consider it an ""analogy"".', "">Not even shower thoughts \n\nSo we're lower than shower thoughts?"", ""What? I didn't say that."", 'No but you implied it.', ""I don't see how I would have."", 'Statistics is probably not for you then.']"
[Q] Would this be considered a nested design?,"
Here is the scenario:

""An animal scientist conducted an experiment to study the effect of water quality on feedlot performance of steer calves. Four water quality treatments were used for the experiment. The water sources were designated as normal (N) and saline (S). The saline water was formulated to approximate the mineral concentrations in some underground water sources utilized in practice for watering livestock. Four combinations of water used in two consecutive 56-day periods of the experiment were N-N, N-S, S-N, and S-S. The feeding trial consisted of the four water treatments with two replicate pens of animals for each treatment in a completely randomized design. The trial was conducted on two separate occasions (two consecutive summers). ""

For the first part of my assignment question, I conducted a 2 factor analysis of variance with the water quality treatments as fixed effects and the summers as random effects. However, the last part of the question is pasted below, and I feel lost as it doesn't really seem different than what I did in the first part?

e. The water treatments have a 2  2 factorial arrangement. The first factor (*A*) is normal or saline water in the first 56-day period, and the second factor (*B*) is normal or saline water in the second 56-day period. Write the linear model for the experiment with this arrangement, considering summers as random effects and factors *A* and *B* as fixed effects. Repeat parts (a) through (d) of this exercise with the new model.",11y3v4l,Astralnugget,1679452139.0,1,1.0,['Read rule 1.']
[Q] Predicting future price from sales data," I want to be able to predict what a product's price would be based on its current sales listings and its sales history.

The historical sales data would be a list of (date, price, quantity)

The current listings data would be a list of (price, quantity)

A very simple idea would be just to calculate the average quantity sold per day from the sales data(say, for the past week) and assume the same amount will be sold tomorrow. Then, we can remove that quantity from the listings data sorted by price ascending, and predict the new price of this project to be the new lowest listing price.

Obviously this does not account for stuff such as price elasticity, demand trends, etc.

Just wondering if anybody had any ideas/leads on how I can figure out better ways to solve this problem",11y2f1x,1298se,1679448903.0,1,0.67,"['Id recommend looking into the stochastic optimization Newsvendor problem. You can find materials for it on gurobis website as well, and while its different in nature than what youre proposing, I think its quite relevant. Just a note, I hope you arent trying to make any assumptions or prove anything as this type of question is highly speculative and not practical without some hard optimization.', 'Cool thanks! I will take a look - and yeah I am not looking to prove anything']"
[E] Online Resources Like stats110 or cs229,"I'm feeling disappointed about the quality of the corresponding courses offered in my school for stats110 and cs229. I regret not having known about these earlier, and I'm looking for some amazing online resources to learn more.

Specifically, I'm interested in finding resources that cover the following topics:

1. Linear regression
2. Statistical inference
3. Stochastic processes
4. Optimization
5. Data science technical skills

If anyone has any recommendations for online courses, books, or any other resources that cover these topics in depth, I would really appreciate it.",11y1amb,mziycfh,1679446441.0,13,0.81,[]
[S] Stata help?,I have to learn time-series data analysis on Stata in one (and maybe a half) month. I have the software installed in my laptop today. Now zero idea what to do next. Where do I start? Any suggestion would be very welcome.,11y13cp,rlochon,1679445998.0,3,0.8,"[""Do you know any stats programming? UCLA stats has some great step by step programming guides for Stata, though I don't know if they have time series specifically.\n\nIf you can figure out the function names the pdf help files for stata are pretty helpful - the seem too technical at first but there's some decent theory and examples at the bottom"", 'There are walkthrough videos of every type of analysis on every software there is on YouTube. Your best bet is just go there and follow it.', ""Udemy has a few helpful courses on stata, sometimes it's accessible for free through your institution"", "" STATA is a powerful statistical software program frequently used in research, policy analysis, and business. It is also regarded as a key tool for statistics students due to its vast utility. As a result, STATA is used in most reputable institutions' statistics courses, and businesses depend on it to tackle challenging statistical problems. Nearly all STATA-related topics are covered by **STATA help** specialists, guaranteeing that students always get the best assistance possible with their assignments.""]"
[Q] fx= (1/sigma√2π)*e^(-(x-u)^2/2sigma^2),[Q] How is this formula to calculate likelihood used in logistic regression to find the maximum likelihood ? Because I have seen videos where people calculated the likelihood by converting log(odds) to probabilities using the sigmoid function. Is this formula used in anyway to find the likelihood in logistic regression.,11xvjof,emo-9,1679434664.0,0,0.17,"[""> How is this formula to calculate likelihood used in logistic regression to find the maximum likelihood ?\n\nIt isnt\n\n> I have seen videos where people calculated the likelihood by converting log(odds) to probabilities using the sigmoid function.\n\nSure, but what does any of that have to do with the normal density? The sigmoid you're referring to is the cdf of a logistic distribution not a normal. \n\n>  Is this formula used in anyway to find the likelihood in logistic regression.\n\nin *any* way? That might cover a lot of possibilities. \n\nGenerally, a big no (the likelihood itself doesn't involve the normal at all) -- but note that under fairly broad conditions the log of the likelihood function will be *asymptotically* quadratic near the peak, so the normal distribution *could* enter into some computations involving likelihood."", 'This is the pdf for the normal distribution.  Outcomes for use in logistic regression follow a binomial distribution.  This has nothing to do with the likelihood function for logistic regression.', 'This looks like homework? \n\nFrom what I recall with MLE and the normal pdf (this formula), it is rather convenient to work with it as a log.', ""I am just stuck here, i know how MLE works but how is this particular formula used in logistic regression. The thing is that we convert the log odds using the sigmoid function to calculate the likelihood/probability and then we maximize the product to find the maximum likelihood.   \nI don't see how mean ( mu ) and standard deviation ( Sigma ) is used in any way to find maximum likelihood particularly in logistic regression because we r finding it through some other means i.e sigmoid function."", ""> how is this particular formula used in logistic regression\n\nIt isn't. Where have you seen this used in the context of logistic regression?""]"
[Q] How to choose the correct number of items in a batch for testing to deem the batch passable.,"Idk if this is the right place to post. I have a data set of timestamps with data for each timestamp. I need to cross reference each timestamp with another program to see if it matches or not. Problem is there's are 1000s. My task is to test enough to deem the batch passable or not.

I know there is a math problem similar to this where there is a batch and someone can't test each product in the batch but test every nth or a random x% of it to say that it is most likely passable.

I don't need to this, but it got me thinking what would the optimal solution look like.",11xrvrz,cyborg998466,1679427645.0,5,1.0,"[""Optimality requires a more stringent definition unfortunately. Essentially you'll need to determine an acceptable probability that a batch is deemed passable when it actually contains x% of non-passing components. \n\nWithin the world of industrial quality control this concept is generally called Acceptance Quality Limit and there are [reference standards](https://asq.org/quality-resources/z14-z19) available to make sure suppliers and customers can have a default sampling procedure to use when defining contracts. Its been a bit since I worked with these but essentially you'll plug in a few parameters to the model and it will give you back a sampling procedure."", 'What specific condition on the batch as a whole makes a batch passable? What probability do you want to meet the condition with?', 'The hypergeometric series works, you vheck the probability 1-p of zero defects in the sample. That is the probability your sample size will catch at least one defect. but as other comments says for efficiency use the tables. They for whatever reason tend to allow for smaller sample sizes (made by smarter people than me lol). Double sampling plans can get you down significantly which is important for labor costs.\n\nAlso you prob already know there isnt really such thing as a 0 aql. There is no way to use qa stats techniques to guaruntee zero defects so you have to negotiate with customers what they define as an acceptable level of defects.\n\nProcess steps that 100 percent inspect without human error is the only way to get a very low/near zero defect rate. Examples would be ai label verification, metal detection with a size limit, continuous parameter monitoring such as oven temperature, automated dispensing scales or check weighers.\n\nThough calibration is essential to these types of control steps.', 'If the item in the batch, essentially a timestamp with the data match with another program, it meets the criteria. If not it fails.\n\nFor example:\n\nBatch X:\n\n2023-03-21 06:00-08:00 => 5 degC +/- 1 degC\n\nReference Program :\n2023-03-21 06:00-08:00 => 5.4 degC +/- 1.1 degC\n\nThis would be acceptable as the error predicted in Batch X of that timestamp is within 10% of the reference program with the same timestamp.\n\nIdeally, to make sure the batch is passable, >80% must match.']"
[Q] Error of A/B,"Greetings

There are several websites listing the error propagation formula to calculate the error of Z, where Z=A/B. I just want to make sure that I understood them correctly...

1. Divide the standard deviation of A by the value of A and square the result.
2. Divide the standard deviation of B by the value of B and square the result.
3. Sum the results from step 1 and 2.
4. Take the square root of step 3 to get the error of Z.

Does this sound right? Do I need to take anything else into account?

Apologies for any incorrect use of terms.

Thank you

EDIT: I am asking because I want to divide regression coefficients by one another.

EDIT 2: corrected ""of step 4"" to ""of step 3""",11xmoqc,aspiringaresponsible,1679417372.0,1,1.0,"['If by ""the error of Z"" you mean the *relative* error of Z, yes, that\'s the standard formula you\'ll be taught in a physics lab class.\n\nIf you want to estimate the standard deviation of Z, you need to multiply by Z.\n\nThe formula assumes that A and B are independent, and the standard deviations of A and B are small relative to their magnitudes. In particular, B must never be near zero with any significant probability.', ""https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables\n\nNote the E[X/Y] and Var[X/Y] formulas\n\n> I am asking because I want to divide regression coefficients by one another.\n\nBeware; you have to make some assumptions (which mean, among other things, that the the estimates themselves are *not* normally distributed; quite distinct from what you would assume for inference on A or B alone) for the Taylor approximation to work. If the density is not 0 in the neighborhood of 0 or at least if it doesn't decrease to zero sufficiently quickly as you approach 0, the variance of the ratio will not even be finite. (edit: indeed even the expectation will  non-finite; typically it will be undefined)\n\nAdditionally, as a practical matter, the denominator needs to be multiple standard deviations from 0, or the approximation will be poor in any case as the higher order (remainder) terms can't be neglected.""]"
[Q] Multiple comparisons correction when multiple groups and multiple features are being tested,"Hi all,

I think I understand the concept of multiple comparison correction when it is applied to a basic design. If I have 2 groups and I am testing 1000 features (or genes, etc.) I have to correct for multiple comparisons. I generally use Benjamini-Hochberg. However, how do I approach this if I have multiple groups?

These are two scenarios where I don't know how to correct the p-values properly:

1) T-test of 1 control (A) versus 3 treated (B, C, and D) for 1000 features: AB, AC, and AD

2) ANOVA for all possible group comparisons (AB, AC, AD, BC, BD, CD) for 1000 features

&#x200B;

I use R for my analysis, so, for example, should I do a Benjamini-Hochberg using an n = 3000 in case 1) just because I have 1000 features being compared 3 times?

In case 2, I don't even have an idea of what I should do. If I perform an ANOVA followed by Tukey HSD, the p-values should already be corrected for the multiple group comparisons, but how do I further correct for the multiple features comparison?",11xj9fw,gustavofw,1679410547.0,1,1.0,"['When comparing control to three groups you can use Dunnetts test which corrects for the multiple comparisons with control but does not over-adjust by correcting for comparisons among experimental groups. Tukey corrects for all pairwise comparisons. You can use the Benjamin-Hochberg to further adjust for multiple features.', 'Thank you for your reply!\n\nDunnett\'s test already correct for both the group and feature comparisons?\n\nAs for the ANOVA, do I use ""n"" as the number of features since the group-wise comparison was already corrected by Tukey? I am asking about an ""n"" because it is one of the parameters I can adjust using the ""p.adjust"" function in R.', 'Im not sure what p.adjust does but Dunnet and Tukey control for the number of comparisons among means but not the number of features. I think you are right to make n the number of features.']"
[Q] Estimating p in a binomial situation such that the p-value is .05,"\[Q\] I was watching a YouTube video the other day where someone said they had 500 good encounters in a video game without a single bad one. The probability of a good encounter is unknown, but bad encounters are possible. 

I was wondering if there was a way to set this up as a Binomial problem with an unknown p and n=x=500 such that doing a cumulative sum from 1 to 500 gives a p-value of .05. Could this be considered the ""highest plausible value of p"" assuming an alpha of .05? 

With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05. 

Can this be considered the maximized p for this problem? We are 95% confident that p is at most this value?

I am curious if this is something that is regularly done when p cannot be estimated. Is there a formulaic way to solve this rather than trial-and-error?",11xiu3f,syah7991,1679409644.0,7,0.77,"['There\'s a simple approximation known as the [rule of three](https://en.wikipedia.org/wiki/Rule_of_three_(statistics\\)) which gives the approximate 95% CI for p as (0, 3/N) if no events have been observed in N tries (and N is at least 20-30, because it relies on the CLT).\n\nAssuming that they were setting out to measure the probability of having a bad encounter, in 500 tries you can be reasonably sure that bad encounters (for this particular individual, given the way they play and interact) are no more common than 1 in 167.\n\nIf they weren\'t setting out to measure it, just going *""hey, I\'ve had no bad encounters in aaaages, this game must be really cool""*, they\'re likely to be underestimating p. They\'ve generated a hypothesis and need new data to test it.\n\nE2A: plain text link for anyone the original link isn\'t working for: https://en.wikipedia.org/wiki/Rule_of_three_(statistics)', 'Are you using _p_ to mean the probability of a bad encounter in the (unknown) population ?  Or the p-value for some statical test ?', "">With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05.\n\nJust to be clear, isn't it the case that for this P, it's 499 and 1 that lead to a *p*\\-value of 0.05 ? \n\n*binom.test(1, 500, 0.000102579999)*\n\n\\### *number of successes = 1, number of trials = 500, p-value = 0.05*  \n\\###\r  \n*### alternative hypothesis: true probability of success is not equal to 0.00010258*"", '> I was wondering if there was a way to set this up as a Binomial problem with an unknown p and n=x=500 such that doing a cumulative sum from 1 to 500 gives a p-value of .05. Could this be considered the ""highest plausible value of p"" assuming an alpha of .05?\n\n> With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05.\n\n> Can this be considered the maximized p for this problem? We are 95% confident that p is at most this value?\n\nYou are using the same letter p for many different purposes. clarifying/using different letters will make your question much clearer.', ""This is an estimation problem, so there's no need to wrap it up in the context of a hypothesis test. There are loads of [different confidence intervals on a binomial proportion](https://en.m.wikipedia.org/wiki/Binomial_proportion_confidence_interval), though when you have a count of 0 not all are very appropriate. The Clopper-Pearson interval is conservative, but it can certainly handle having a 0 count."", 'You messed up the link.\n\nI assume you mean https://en.wikipedia.org/wiki/Rule_of_three_(statistics) ?', 'I *think* OP is trying to ask about the largest possible proportion which could not be rejected at alpha = 0.05.', 'P is the parameter, not the p-value. Im fixing the p-value to be .05', 'p by itself is the parameter. I only call it a p-value when referring to p-value. Every p by itself is the parameter for the binomial distribution.', '**[Rule of three (statistics)](https://en.wikipedia.org/wiki/Rule_of_three_\\(statistics\\))** \n \n >In statistical analysis, the rule of three states that if a certain event did not occur in a sample with n subjects, the interval from 0 to 3/n is a 95% confidence interval for the rate of occurrences in the population. When n is greater than 30, this is a good approximation of results from more sensitive tests. For example, a pain-relief drug is tested on 1500 human subjects, and no adverse event is recorded. From the rule of three, it can be concluded with 95% confidence that fewer than 1 person in 500 (or 3/1500) will experience an adverse event.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', 'Link works fine for me?', 'Ah.  That makes sense.', 'Your link on my computer is https://en.wikipedia.org/wiki/Rule_of_three_(statistics))%20which%20gives%20the%20approximate%2095%%20CI%20for%20p%20as%20(0,%203/N)%20if%20no%20events%20have%20been%20observed%20in%20N%20tries%20(and%20N%20is%20at%20least%2020-30,%20because%20it%20relies%20on%20the%20CLT\n\nThat article does not exist :P', 'Might be an old Reddit vs new Reddit thing? I use the old skool interface.\n\nJust delete everything after the first )%20 and it is the right link.', ""Did you open the parenthesis?\n\nI think maybe you wrote `[rule of three]link)` instead of `[rule of three](link)`.\n\nIf not, I don't know, but it doesn't matter much anyway. :P"", ""The link works fine on original reddit. I had to add an escape before the final bracket to avoid breaking it, I'm guessing that somehow confused new reddit into adding the rest of the sentence to the link.""]"
[Q] what is the best way to go about this scenario?,"context: I am playing basketball. 
if i had a 57% chance of making a free throw, and the goal is to make 5 free throws in a row. would the probability of me hitting 5 in a row increase with more shots being taken?  or would it be worse since with more shots would lead to me getting closer and closer to my 57% average. i guess what i'm saying is what is the best opportunity to get variance that'd be on my side, shooting a ton of shots and hoping eventually i can get 5 in a row, or shooting a smaller amount? keep in mind, in said competition , the higher amount of shots taken the 5 in a rows is expected at a higher amount. i.e someone who shoots the ball 12 times, they only need to hit 5 in a row once. vs someone who shoots the ball 50 times, they're expected to hit 5 in a row at least 2-3 times now.",11xhw3k,TheForexHokage,1679407558.0,1,1.0,"['If 57% is the true probability, then it doesnt increase or decrease as you take more shots. \nIf the probability of making it is conditional on the number of shots taken, then it is not safe to assume that the probability will stay 57% the whole time - thus, it is not guaranteed that youll have exactly 57% over time. \nTo simplify calculations, assume each free throw is independent of each other. But in reality, fatigue and pressure can have a big effect. \nIf the probability changes depending on situation/number of previous shots, this problem is much harder', 'The usual model for basket ball throws is that each throw is *independent* of the past. That means you chances do not improve regardless of how many throws you have taken before.\n\nIn reality, the probability is not a constant number. Probably it is lower when you are tired/low morale. Probably it is higher when you are succeeding/fresh.', ""Given an assumption of independence (or indeed some roughly plausible assumptions about mild dependence), the probability of *failing* to make 5 in a row decreases monotonically in number of throws (assuming it's already at least 5)""]"
[Q]What is best books for statistics from beginner to advance,"Hey everyone, 
I just start my data science journey and i wanna know about somw good book for statistics and if you know any good youtube channel so its help me lot",11xeig9,rr346,1679399451.0,66,0.91,"[""For the real statistic basics I'd recommend Discovering Statistics Using R by Andy Field. There is also the same textbook but for SPSS, which has more up to date editions.\n\nStatQuest on YouTube is really great in terms of understanding basic concepts. In this regard, it might be the best source. \n\nFor a bayesian perspective (in R) you should definitely have a look at Statistical Rethinking by McElreath. There is a YouTube lecture working through the book chapter by chapter."", 'Once you are advanced youre going to want to crack open casella & burger, but I have an advanced degree and that book still scares the daylights out of me. I would recommend Wackerly Mendelhamm mathematical statistics for now. Really fucking boring and poorly organized imo but it gets the job done for undergrads and some grads.', 'The [American Institute of Mathematics](https://aimath.org/textbooks/approved-textbooks/) has a list of free recommended math books including statistics books.', 'Data science is a broad term, but for a statistical master, my coursebook in machine learning was great: http://smlbook.org/', 'Stats for Business and Econ (non-calc)\nhttps://www.amazon.com/Statistics-Business-Economics-David-Anderson/dp/1337901067\n\nMathematical Statistics - Hogg (Calc)', '*NIST/SEMATECH e-Handbook of Statistical Methods*, http://www.itl.nist.gov/div898/handbook/', 'Under advanced, I\'d suggest ""A probabilistic theory of pattern recognition"" by Devroye, Gyorfi and Lugosi for ML classification', 'Heres a YouTube channel with lectures on probability and statistics: https://www.youtube.com/playlist?list=PLBEf5mJtE6KuZ5NBQMuWIMsiOOrV9ibzm', 'Understanding Basic Statistics - 8e\n\nBy: Brase & Brase\n\nRudimentary stats to introduce you to basic calculations and terminology. Had it awhile back and it felt very grounded. It utilizes real world examples and showcases everyday applications. Had it back in my first statistics class.', ""I'll recommend learning Statistical Theory via MGB (Mood, Graybill, Bose). For basic Statistics, I go for Walpole."", ""Both books are surprisingly fun reads. Field's book is a great suggestion, super approachable yet still takes the time to go into the nitty-gritty; I think it might be my top pick for anyone just getting into statistics. Also, FYI, he's supposed to be releasing a revised edition (of the R book) some time this year. \n\nStatistical Rethinking is also a stellar book. He leads with Jewish folklore about golems, which is the coolest opening a stats book could have imo. The caveat being that Bayesian stats are still considered unconventional (at least in my field, psych science) so it can be a hard sell to get people on board with them."", 'Do you have an opinion on the difference between learning statistics with R by Daniel navaro(?) And discovering statistics?', '> casella & burger\n\nCasella & Berger\n\n> Wackerly & Mendelhamm\n\nWackerly, *Mendenhall* and Scheaffer ... I think since the 4th edition in 1990(ish?).', '[deleted]', 'Nice', ""There's also Richard McElreath's videos: [https://www.youtube.com/@rmcelreath](https://www.youtube.com/@rmcelreath) It is a nice complement to his book (Statistical Rethinking) and may be easier to follow than the book."", ""Yeah, he's working on the R one. It's going to be entirely tidyverse, I believe.\n\nI think it's taking longer 'cos he's doing it himself, that way the other people won't screw it up for him as much."", ""Danielle Navarro's free book is also very good if you want to start there."", ""It has been *Danielle* Navarro for many years now.\n\nThe two books are pitched at different levels. I think Navarro's book is really pretty decent for what it does (I've recommended it to people multiple times), though it's certainly not without a few flaws here and there."", 'Lol burger, and idk I just call it wackerly. Thank you though', ""His videos are a great suggestion. I haven't checked them out in a while, but where he revises them every year I can only imagine they're even better now."", ""Hi Jeremy; I'm not sure I find this to be automatically encouraging news. The most egregious errors in the earlier editions of the SPSS version were all his own work (or at least he seems to be lone author on a number of editions I can recall), though there were certainly improvements over time. \n\nIn particular, I'd have been more encouraged if your name was still going to be on the R version."", 'I read that one its good', ""+1 for this, it was the first stats book that helped me actually understand what's going on. Strongly recommend her work!""]"
[E] Stats major with minor in Business Administration/Economics/Computer Science,"The stats major I'm looking at is actually ""statistics and data science"" at a reputable universtity, so I guess it already has a fair bit of programming, albeit with a quantative focus (primarily using Python and R). Which minor would you pick to maximise income later in your career? I'm thinking business administration could give a fairly broad scope. I'd kind of like to go into finance, but if there is more money (or a more pleasent career, as I would rather earn 20k less than work for 100+ hours a week) elsewhere then I'm also willing to go in that direciton.  


I could also do the major in math but I'm not sure if I'm up to for the rigorous program. Also it would include less programming.",11xcrtw,-Sn3aky-,1679393872.0,3,0.72,"['If you plan on grad school, get some math under your belt.\n\nFormal, professional-level programming classes.\n\nProfessional speaking and writing classes.', 'For maximizing income I guess you will just need to find great jobs in whichever realm you participate in. If you want to be a high tier business support / development Data Scientist then do economics. If you want to be a high tier product developer involving data, computer science. Im not sure which has a higher income ceiling or is more / less stressful but thats my general thought on the matter.', ""I would steer away from business administration. Such programs exist mostly to give grasping, ambitious people the opportunity to network -- if you're not already that kind of person, it will be wasted, and you will be bored.\n\nEconomics could be interesting. However, the number of jobs actually requiring economics is probably pretty small compared to statistics in general. My advice is to look for announcements for jobs you might want, and see what they're stated requirements are.\n\nOverall my advice is stay on your statistics & data science path, and maximize your later employability by actively trying to connect with other people, students, faculty, people inside and outside the school.""]"
[E] Masters program with a below average GPA,"Hi All,

Im currently in my last year as a Statistics Undergraduate at a pretty high ranking public school in the US, and Ive been heavily interested in going to graduate school for a while now. I now have come to the realization that I will most likely end up with a GPA around a 3.2 to a 3.3, which is below the average GPA in my major (3.4). I have internship and research experience under my belt, and would love to continue my education for my masters. Is this possible, or is my GPA too low to go to graduate school?

Thank you",11x5t5w,xnahlahp,1679370141.0,2,1.0,"['I know someone who got in a similarily ranked masters programs to UC Davis with around 3.1 gpa. It was unfunded but he was international. \n\nI think US citizens have a much much greater chance for acceptance and for funding. You should be fine, but try to get around 167 in GRE Quant. GRE just needs a lot of practice, so start early.', 'Have you looked into masters programs at your own institution? You mentioned research; if you have a respected professor in your desired department vouching for you and you meet the base masters entry requirements them I see no reason you wouldnt be accepted.', 'I had a lower GPA and was able to get in to a masters program. Sometimes work experience itself overcomes GPA, so worst case you might just need to take a gap.', 'Thats very good to hear. Ive already been studying for the vocab, so Ill just need to start for the Quant section. Thank you!', 'I have looked into my institution, however its heavily competitive and ranked very highly for a masters program. Im going to apply regardless, but the chances are quite low.', 'Ill be taking a gap regardless to study and take the GRE, so that is duly noted! thank you']"
[Q] Mixed ANOVA question," 

Greetings,

I am conducting an analysis using several mixed 2x2 ANOVAs where the nature of my variables requires that I perform post hoc testing if I have interactions. I know that people will argue that you do not need post hoc testing for less than 3 levels however for these particular variables I do. Does anyone have a source where this rationale is explained or documented? Most sources are just black and white and dont seem to give any leeway.",11x38hd,Ronaldoooope,1679363191.0,3,1.0,"["">for these particular variables I do.\n\nIf you only have two levels, a significant interaction effect defines a significant difference between these levels. Why would you need to conduct a post-hoc test for this? It's already clear and you have your model adjusted coefficient and associated p-value. A post-hoc test is effectively repeating a t-test using the model adjusted terms."", ""Depending what OP means by posthoc, it's a bit worse than doing a t-test, because of adjustments for multiple comparisons that don't exist in this case.\n\nThe main point is with 2x2, the main effects and interaction completely partition the space. If you want to partition it differently, that's fine, too. So instead of 2 main effects, follow up with simple effects of A at each level of B, which I'm guessing is what's desired in this case. I would not expect an adjustment for familywise alpha.""]"
[E] Anyone here with an Operations Research background?,"Operations Research is not a background I see often mentioned, yet it seems so relevant. Curious to hear about anyone with that background: How do you brand yourself? What do you do? Whats your title? Whats the pros and cons of OR?",11x0542,ohcalix,1679355555.0,36,0.97,"[""I'm a data scientist with a master's in OR. No one knows what OR is in any of the industries I've worked in, so I just brand myself as a very good data scientist. I've done some queuing theory and some linear programming for some of my industry projects, but I mostly do machine learning, stats, data engineering, etc."", 'I did one year Masters in OR before switching to Statistics. The field of Operations Research is a Decision Science, while the well-known field of Data Science (clustering, classification, regression) is a Predictive & Prescriptive Science. OR involves simultaneously making a lot of decisions to maximize an objective (like profit) or minimize a loss, while adhering to constraints.\n\nExamples:\n\n* Location Planning: How far apart should franchise stores like McDonalds or Subway so they can maximize revenue, while not cannibalizing each others sales?\n\n* Location Planning: How should a city bus system plan its routes, and bus stop locations. The objective could be to serve the most people, or to maximize profit\n\n* Routing & Scheduling: How should a delivery company like Fedex, Amazon or UPS assign their vans?\n\n* Scheduling: How should an airline schedule their crew so that they are best used, with the constraint that they have to be in their hometowns for days off\n\n* Pricing: How much should a grocery store charge manufacturers to display their products by shelf level and aisle?\n* Dynamic Pricing: How many seats should an airline sell at each price level?\n\n* Demand & Supply: How should an Energy company allocate bids (price & quantity) between its suppliers and consumers of energy?\n\n* Portfolio Management: What stocks should I have in a portfolio to maximize expected profit, with constraints on maximum drawdown permitted, volatility allowed etc\n\n* Sensitivity Analysis: What will be the effect of changing decision variables, like adding a new stock to a portfolio, adding a new train route etc.\n\nThere are many more examples on [Wikipedia](https://en.wikipedia.org/wiki/Operations_research)\n\nThe theory involves advanced math, but applying OR is easier because ""solvers"" are used to do the heavy optimization, and they are available for Python, R, Julia, C++. Heuristics can be used when the optimization is too gnarly even for a solver.', 'I am from OR/ Industrial Engineering background. And it is often tough to convey my background to non-Tech or non DS people.\n\nMostly, I term myself as a Business Analyst nowadays and that is my title too. Or if I had to tell someone what I do generally say: I build mathematical models to solve business problems.\n\n\nThe thing is, what I do lies somewhere between a Business Analyst, A Data Scientist and a Data Analyst. And by my skill level, I have width but lack depth. Jack of all trades and master of none. It is tricky sometimes to find a suitable job.', 'I did IE undergrad with some OR classes, but never quite used it in my analytics career. It is quite relevant in that it is heavily laden with statistics and optimization modeling. I feel confident that if I had the time and need to revisit an OR style problem that I could do so using the things I am learning in my stats MS. \n\nIt has been a long time since then, truly fascinating stuff from what I recall.', ""I have a Masters in Statistics, but a lot of the work I do is OR work (routing problems, and some location planning). I use libraries such as OR-Tools and cvxpy in Python.\n\nI'm a bit bored of it to be honest, I'm looking to go into a more traditional data science role over the next few weeks."", ""My job title is operations research analyst...but I rarely use classical OR methods. Our bread & butter in this org is basic data analytics. Stuff like charts and pivot tables. I brand myself as more of a data analyst or scientist than true operations research. \n\nThe objective of OR sounds different than say a statistician or data scientist, but really it gets blended together once you get down to it. Whatever method your customer is familiar with is what they ask for. They'll ask for flashy things on occasion...and sometimes you get the privilege of actually following up with those asks...but once the reality of deadlines set in they just want whatever gets them value quickly."", ""A quick plug for folks who are interested in OR - INFORMS (www.informs.org) is the international society for OR and management science; I've been a member for a long time and while they lean heavily to academia, they are trying to be more all-encompassing of analytics as a practice versus just OR."", 'Or is not so much a bag of tricks, models or techniques but an approach to problem solving usually with an objective in mind, implementation of the solution and verification and control of the result. It uses the scientific method in an OR project that may cover a larger process. Decision science is on the way to this but has not yet developed a methodology that covers the whole model development and implementation cycle.\nStatistics, on the other hand, provides small steps in the process being mainly interested in proving results through p-values.\nSorry for the rant, I have been doing this stuff for about 50 years and just see the re-invention of the wheel, albeit smoother glossier and more efficiently, but the fundamentals are still the same\nSome questions asked during an OR study- what is the objective function, what are the decision variables, what are the constraints?\nTake care', 'Just adding to my comments- the most difficult part of a study is problem formulation. I have listened to many current projects where this has not been thoroughly done, involving the users ala the agile approach, only to find that you have not quite solved the problem. The output from that step is usually a model, maybe mathematical that is a reflection of the problem and that is validated to ensure that it captures reality. Then you can optimise. Unfortunately, the academic teaching starts at this point and is a just in case you need this maths approach.\nSo, mostly what you learn in an OR course is never used as you really need a course on problem solving using data, stats, maths and software. Then maybe your problem fits into one of the standard models.\nI found the books by Ragsdale and Winston the most useful in the courses that I gave', 'Well thought out and written answer. OR is extremely useful OP, but nobody knows what it is outside of people who have been a part of it.', 'Do you have any favorite textbooks that discuss the applications you listed?', 'How are these applications done? What tools do you use to apply them? I had some OR classes in college but we mostly used pen and paper and Excel Solver(for basic LP) for our exams, so I never appreciated how OR is truly applied in a business setting. Would love to hear examples from anyone who has applied OR in their work (what was the business problem, what OR method did you use, what tools, etc)', 'I see a lot of overlap between Decision Science and Prescriptive Science. OR maybe doesnt have much in terms of unsupervised models, but it has the benefit of being interpretable and transparent in its assumptions. \n\nWhy do you think it hasnt picked up then as a more mainstream toolkit or branding of similar skills and tools?', 'Literally my problem. Took me a year to find the job I wanted, now I want a different one, and boy is it even harder now almost 2 years later to find the kind of role I truly want.', 'Interesting  since you sit in between the definition of all 3 roles, what did you optimize for when deciding to term yourself a Business Analyst? \n\nI know the difference between all 3 is sometimes fuzzy, so do you feel like you are able to just shoot for the position with the best conditions, or do you feel limited in any way?', 'Is the main driving factor for you wanting to change the nature of the work, or do you think theres an inherent/some other benefit to go into a more traditional data science route?', 'Interesting  what industry are you working in if you dont mind?', 'Thanks for the resource!', 'I also had the impression that OR goes deeper into the fundamentals since you seem to learn the mathematical tools instead of an applied version of it. A DOE seems to be ultimately a modeling and optimization problem, and while some statisticians definitely have that depth of background, many people working as analysts will only know the surface level.\n\nSince youve been in it a while then  Im debating with myself what is the best use of my time if I pursue further education: OR seems like it would be excellent content-wise, but an uphill battle for branding, whereas many more accessible data science or applied stats programs would probably be lower quality content but more recognizable by employers. Any thought on that?', 'Thanks so much, thats very useful  I can totally see this happening. I somewhat thought this skill only gets better with practice, so Ill definitely check your book rec.', 'Hamdy Taha is a mainstay: https://www.amazon.com/stores/Hamdy-A.-Taha/author/B001HPGRPO?ref=ap_rdr&store_ref=ap_rdr&isDramIntegrated=true&shoppingPortalEnabled=true', ""Some interesting case studies can be found here: https://www.informs.org/Impact/O.R.-Analytics-Success-Stories\n\nIn terms of tools, there are a lot of specific language tools like LINDO, AIMMS, and Gurobi. There are some open source tools like COIN-OR. I believe there are solvers for many languages like python and Java.\n\nI recently played around with Julia's linear programming extensions and really liked it."", ""Check out OR-Tools - it's a library by Google."", 'Yeah.\n\nAs my current title is Senior BA, whenever I am contacted by any recruiter, they ask me if I am good at Tableau or Dashboarding. Which gets really tiring at times.\n\nI guess for people like us, Analytics consulting might be a good fit.', '> what did you optimize for when deciding to term yourself a Business Analyst? \n\nOne of the reasons I do that is because of[INFORMS](https://meetings.informs.org/wordpress/analytics2023/?utm_campaign=Analytics%202022&utm_source=adwords&utm_medium=ppc&utm_term=business%20data%20analysis&utm_content=register&hsa_tgt=kwd-255714509&hsa_grp=129038753183&hsa_src=g&hsa_net=adwords&hsa_mt=b&hsa_ver=3&hsa_ad=565807077000&hsa_acc=6773547382&hsa_kw=business%20data%20analysis&hsa_cam=15441904480). They\'re the biggest OR related research org and their main yearly seminar is called ""Business Analytics"". I think this term is more pertinent for OR fellows.\n\nAnother reason is: Other two terms seems quite specific. DS mostly translates to ML nowadays. I am okay with ML techniques but I want to solve business problems.\n\n\n> so do you feel like you are able to just shoot for the position with the best conditions, or do you feel limited in any way?\n\nI do feel limited when someone asks for in-depth knowledge in OR/ML techniques in the Job description. The key words I try to filter out are: ""solving Business problems"". I think there is ample scope for someone with breadth rather than depth, but I feel most of these openings are for a bit mid senior level (3-4+ years)', 'Better career path, I think. Working for one of the big tech companies (not necessarily FAANG) is much more profitable.\n\n While it looks good on my CV currently to have multiple projects pointing out ""I saved x%/$ by optimising routes/locations"", the projects are starting to get repetitive. \n\nAdditionally, a lot of the data I work with is an absolute nightmare to clean.', 'Defense', 'Thanks', 'Lots of analytics consultants build dashboards. Ive seen engagements that are nothing but a dashboard building exercise', 'Makes sense, the military is probably the birthplace of OR', ""Yes. The thing is I don't mind building dashboard as a part of sharing my work. But what I would not like is only building dashboards.""]"
[Q] Odds for Magic the Gathering,"In MTG Arena, there's an [event](https://i.imgur.com/pUn1Srp.jpg) when you can play up to 7 games of Magic, and get various rewards. Once you've lost 3 times, or once you've won 7 games, the event ends and you receive rewards based on how many games you've won. So for example, if you win 3 games and lose 3, then you get the rewards for having 3 wins. Alternatively, if you win 7 and lose 2, then you get the rewards for having won 7.

&#x200B;

What I want to know is this: If I have a 50% chance of winning each game, how do I figure out the odds of entering the event and winning 1 game? Or winning 3 games? Or 7 games? Or 0 games? etc

Also, sorry if this is the wrong place to ask, I wasn't really sure where I should go.

&#x200B;

EDIT: I explained wrong, you play until you get 3 losses or 7 wins, so you can play more than 7 games of magic (for example getting 5 wins and 3 losses is 8 games)",11wzl9f,Hamm103,1679354259.0,6,1.0,"[""You could either write out the formulas for the probabilities directly by enumerating the various outcomes, or you could work with the [negative binomial distribution](https://stattrek.com/probability-distributions/negative-binomial), adjusting for impossible combinations of outcomes. Either way I don't think there's a standard distribution that exactly models what you're after."", ""To win exactly 1 game and end with 3 losses, you have to win exactly one of the first three games and then lose the following one. That's a chance of 3/8 * 1/2 = 3/16 because there are (3 choose 1) = 3 options where your win can be. Every other specific outcome can be treated in the same way. To win 7 games (with any number of losses) you can sum the chance to lose 0, 1 or 2 games, or use [this trick](https://www.reddit.com/r/probabilitytheory/comments/10m9s1w/odds_of_striking_out_in_baseball/j6220k8/) to convert it to standard formulas that can sum it directly."", '>  Alternatively, if you win 7 and lose 2, then you get the rewards for having won 7.\n\nHow can you win 7 and lose 2 if you can only play ""up to 7 games""?\n\nYou want the probability of number of wins before the third loss? That\'s negative binomial. If you can\'t play more than 7 games, it gets truncated at 7 games (which makes things a bit more complicated, but still doable).\n\n\n\nhttps://en.wikipedia.org/wiki/Negative_binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Truncated_distribution\n\nIf it wasn\'t for the truncation, I\'d do it straight up as the number of successes version, but I believe the truncation would be easier if you do \nit in terms of the number of trials version and then translate back after.\n\nAlso, if you seek probabilities rather than [odds](https://en.wikipedia.org/wiki/Odds), don\'t call it odds, which is related, but different.', '>How can you win 7 and lose 2 if you can only play ""up to 7 games""?\n\nmy bad, what I meant to say is that you can only get up to 7 wins', 'That ... changes things. Please edit to clarify that part of the original question', ""If you're still looking, the best way to approach this appears to be markov chains. (Insert Sorin joke here.)""]"
"[Q] What's does "" y axis "" mean in a probability distribution or normal distribution ?",,11wuwos,emo-9,1679344392.0,2,0.58,"['For a continuous density, it represents density. For a discrete pmf it represents probability. For a cdf it represents P(Xx)', ""Context matters. Look what the axis labels. Usally you'd either get the probability (or relative appearance) or absolute appearance on y against the charactersitics on x.\n\nExample the common gauian bell for a continous characteristic shows the relative appearance (probability) against the characteristic, e.g. the IQ chart."", 'Are you asking what the values of a probability density mean? [Here](https://stats.stackexchange.com/questions/4220/can-a-probability-distribution-value-exceeding-1-be-ok) is a good introduction with a few examples.']"
[E] Data Science vs PhD in Stats,"I'm a junior now but planning out potential paths of my education and career after my ug in stats. Right now I'm interested in two particular paths. One is becoming a data scientist which is interesting since I enjoy coding and statistical decision making, may or may not need a MS I guess. The other is in research something like a PhD doing research in statistics or machine learning, which I'm not sure on whether or not I'd like it since I haven't had a strong undergrad in upper level stats and ml courses after a late major switch from business. How do I go about deciding which of these paths would be better for me after graduating from undergrad? Any advice would be great because I'm not sure if I should be studying leetcode or similar for data science roles or studying analysis to catch up to phd programs and I can't find enough time to do both.",11wqmna,seriesspirit,1679336244.0,48,0.96,"['Theres a lot of advice here which I think is frankly bunk. A couple of thoughts:\n\n-\tDoing a PhD doesnt preclude your becoming a data scientist, it just delays it by ~4 years. This might seem like a long time when youre young, but its not much in the grand scheme of things.\n-\tIf you go into industry, the likelihood of leaving to do a PhD if you decide itd be fun decreases every year.\n-\tEven on a FIRE path with great success, youre still looking at ~20 years of work, which is an appreciable amount of time.\n-\tBeing a PhD is an instant credibility boost, especially with the C-suite\n\nI went straight into industry after my undergrad, and the all the really clever people appeared to have PhDs. I realised that I enjoyed maths and drinking in the afternoons so much that I left after a year and spent 4 years doing a (pure) maths PhD. I then returned to industry at the tender age of 27 in 2012, and it turned out that the independent learning and problem solving that I developed in my PhD were _incredibly_ useful for proper work too.\n\nPhDs are hard hard work; youve gotta love what youre doing or itll be miserable. But if you do *love* it, its great.', 'How much do you like money? \n\nHow much do you like getting to follow your interests and intellectual curiosity? \n\nIf a > b, choose data science.', 'Im leaning toward the latter at the moment myself  I see a lot of jobs that want Masters with 2X years experience or PhD with X years experience; additionally, Im not sure exactly in WHAT sphere/industry Id want to work, so Im hoping the PhD can spark something for me  also, academia might be a neat career later in life, and so Id like to have that as an option in my adult years (if I could land something  its competitive to say the least).', 'Im not in any of those fieldsis your goal to work in industry? If so, better to get a MA and start working in the field. Phd is mainly for academia. Ill let other industry folks let you know if you even need a PhD for industry', ""I don't personally know how industry works yet, but the feedback I have from family friends is that if you want to climb the corporate ladder, in many industries, at some point your competitors will have a PhD and will therefore have a much easier time than you getting picked for a promotion. Without a PhD, at some point you will likely reach a ceiling for promotions, because the competition will have one.\n\nOf course, if you are good with people and politics, this shouldn't matter."", 'Stat PhD people are math geniuses. Data Science PhD people are not.', ""If you're interested in research at all do a PhD. It'll be fun along the way (also painful, but par for the course). There are decent high paying options in industry for research positions. If you only care about getting out and into a job then do a masters."", ""Think about how much theory you want to know. Stats is much heavier in theory. You learn everything you do in data science plus the proofs. Data Science is much more concerned with applications. Data Science is a term that started in industry and has just recently maneuvered its way into main steam academic spaces. They're very concered with modern applications. That's not to say stats isn't concerned with application, I think it just puts alot more emphasis on learning the theory than Data Science does."", ""Is this US? If its not US I do not recommend PhD if you don't love it and plan to stay in academia.  People my generation who went to work on PhD are far behind on the experience I have as a ML expert now. So basically other than the pretty title in their name they don't come close to me in pay grade and actual industry experience. I'm leading people with PhDs because they lack industry experience and basically start in junior/mid positions.  (biotech specifically) I don't know how it is in US, but preferring someone with PhD than someone with masters and 5 years experience is crazy to me."", 'There are a lot of DS skills that are necessary that you will not get from a stats degree. So you need to augment the degree with experience.', 'Mba Business Analytics', 'The future is more on ML than in Statistics, imo.', 'Strong agree with everything here, much more so than the top comments.\n\nThough there probably needs to be more a realistic consideration of the difficulty of getting into a PHD program on this subreddit in general.', 'Thanks. I\'m planning on taking intro to analysis in the fall and I think that will be my ""do I really want a phd"" class with the more theory based class', 'You can do the second in ML...', 'In most cases, the skills you learn in a phd program will be ultra specialized and not very relevant to the industry.  That being said, currently the academic job market for data scientists is very good and very easy.', 'When i got hired into an industry job, my PhD was able to count as 5 years of work experience', ""That's a good note. I feel like I have more to learn but phd seems like both a big commitment and unsure that I wanna do research. Only problem I see is finding a good cheap option for MS since phd has the perk of being paid"", ""Interesting. I'm also wanting to study more ML and wondering what your path was to becoming an ML expert. Did you pursue a masters in a related field? Also yeah I'm US based"", ""ML is just stats algorithms though. The core of most modern ML algorithms have been around in the stats/math/cs for decades, it's just now with modern computing power that ML is so dominate."", 'I tend to agree but what does that mean in this context? Neither of these are specifically ML but a PhD in Stats could create statistical models and dats scientists use them.', 'Well shit admissions is my next worry', 'Takes much longer and wont see a huge pay bump', 'They may be ultra-specialized (in the sense of your research), but Id imagine the programmatic skills and statistical knowledge are incredibly transitive.', 'Yes, but you spent that time making peanuts compared to the MS who was making a decent living and investing that entire time, who now also has a similar level of experience. You really shouldnt get a PhD unless you want to do research', 'There are many MA programs in stats or quant psychology that will give you a GAship.', 'I got a funded MS Stats offer I can tell you the school to apply too', 'Thanks for your interest. I did my masters in biochemical engineering. After that I started as a junior for a predictive modelling of bio-systems company and now I am working in diagnostics based on genomics data. Overall I have 6 years experience and I am on a lead position since last year. If I would have to do it again I would do my BS in DS and after ML/bioinformatics masters, but nowadays there are a lot of useful courses on the internet that can help you improve on any area you work in :). This is EU, unfortunately I have no experience in US environment.', 'I would bet very good money on a future full of MLs will require more statisticians, not fewer. Increased need to separate between basic analysis an ML might be able to do correctly most of the time and informed analysis that requires understanding of the implied mechanisms in the analysis.', 'There is more funding and research opportunities for ML research in and outside of academia.', ""Sure, but if you want to do interesting (relatively) independent research and also make a ton of money there's few other choices than doing an ML PhD and going to work at google/deepmind/microsoft/meta. I'm a Neuro PhD working and a data scientist in biotech, and that's another decent path for following research interests and also making money, but if I were starting over I'd go the ML path."", 'Say you take 4 years to complete your phd. \n\nYou should compare: 4 years of statistical knowledge + prog skills  learnt in a phd program vs 4 years learning it in any relevant industry. I do not think one option is obviously better than the other.', 'True', ""Thanks for sharing. Unfortunately in the US, most masters programs have a high cost which is why I believe people like me start to like the idea of a phd which is covered by stipend. I still think a masters might be better for me at least to start but I'll have to find some good options as for cost."", 'Completely fair point; I will say that a program I visited suggested Summer Internships that would give some of those practical skills (not trying to argue a point  giving additional context for anyone else reading). \n\nWhen were all old, I dont think one choice will have been better than the other as they simply wont be comparable  who knows whatll happen.', 'Good luck!']"
[Q] Is there any inherent issue with plotting concentration against the factor used to calculate it?,"I'm working on my Master's thesis and I have some data about microplastics in mussels which I standardized in particles/gram of tissue. My results are indicating that weight plays an important role in both abundance (particle/organism) and concentration (particles/gram of tissue). I've plotted both abundance and concentration against weight to show the patterns of distribution between different species/size class, but my PI seemed to think there was maybe an issue with plotting concentration against weight, but couldn't pinpoint why or say for sure. I know this is not strictly a statistical question, but was hoping someone here could answer it. Thanks!",11wnw2n,Duskblade95,1679330532.0,1,1.0,"['To state the obvious, there\'s nothing wrong with just plotting something. The issue is what you say based on the plot, which might be complete nonsense or an actually relevant insights, but you did not indicate that.\n\nI have no expertise on the topic, but my intuition is that plotting abundance vs weight is actually not an interesting relation, since it is kind of expected that bigger organisms have more stuff in them. On the other hand, concentration vs weight would seem to me to be the more interesting relation, as the concentration of a substance is generally what determines its potential impact, so in a null hypothesis-like scenario, if two mussels had different size, you could expect the larger one two have more particles when both have the same concentration but to be equally ""affected"" by the contamination since the ""load"" of particles is equal between both.\n\nOf course, the above argument assumes that all of the mussels tissues and processes scale in proportion to its weight, but the fact that the concentration is not the same could mean there are areas, e.g. nervous system, which may not be readily contaminated but which do not need to grow linearly with weight, therefore representing a larger part of the small mussel and explaining the lower overall concentration.\n\nOn the other hand, the difference in concentration vs weight could be completely unrelated to any specific internal characteristic of the mussel and simply be an association caused by the passage of time: older mussels tend to be larger, therefore they have been accumulating particles for a longer time.\n\nSo as you can see, there are many possible angles to investigate based on that initial finding and you would need a more detailed model of the organism and its interaction with the environment to formulate some meaningful statement from those plots.\n\nBut to reiterate the actual answer to your question, I don\'t see why there is something inherently wrong with plotting concentration vs weight. Perhaps your PI feels that because they are making an intuitive parallel with chemical solutions, where by design the concentration is designed to be constant and so a non-informative flat line against weight would result. But this is different because the weight doesn\'t represent some standardized mixture of mussel mass and microplastics being progressively added, but rather the cumulative result of a series of biological processes in interaction with the environment.\n\n(and just a last minute analogy that occurred to me: if you noticed that the last half of a cup of coffee from a vending machine feels sweeter than first, you would certainly be right to plot the relation of sample weight vs sugar concentration and upon finding a non-flat relationship, would be justified in thinking there is some underlying process of interest that explains the difference e.g. insufficient mixing of the sweetener causing it to stay at the bottom, even if in this case it is not exactly the weight that is causal but rather an incidental association between product weight and its vertical distance from the bottom of the cup where the sweetener lies)', 'Concentration is a function of weight.\n\nIf you plot weight vs. concentration of course there will be a clear correlation. Like the other guy said, nothing with plotting it and visualizing it. But you should interpret it with caution to not mislead', ""I'm not sure what they might be thinking, sorry.\n\nThere's definitely a potential issue with estimating the relationship between two different concentrations when dividing by the same weight value (so a plot of concentration of substance 1 vs concentration of substance 2 could be potentially misleading in such a situation, potentially inducing an 'illusory' relationship that's not representing a real association between the substances), but you're not doing that.  \n\nWhatever else they're worried about, I'm not seeing it right now.""]"
[Q] Monte Carlo Error,I'm struggling to understand how to compute the number of simulations for a Monte Carlo error of 0.1% with 95% confidence interval for a probability estimate (e.g. 0.5811). How does this work?,11wnfm9,acidsyzygy,1679329579.0,6,1.0,"[""This provides a nice overview of what you're looking for: [Monte Carlo simulation ](https://openturns.github.io/openturns/latest/theory/reliability_sensitivity/monte_carlo_simulation.html)"", '> How does this work?\n\nhttps://en.wikipedia.org/wiki/Binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Margin_of_error\n\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval']"
"[Q] How do I estimate a transition probability matrix for each time point (i.e., a time-dependent transition probability matrix)","Hi everyone, I have a Markov chain with twelve states. I want to estimate a transition probability matrix for each time point (except for the last time point).   I found a function in R to do this. It is called seqtrate and it's in the TraMineR package. However, it isn't clear to me how they estimate a time-dependent, 3D  transition probability matrix.

Does anyone know how this is done, or can anyone point me toward any resources where I can learn more about this?

Thank you!",11wndhx,statius9,1679329457.0,4,1.0,"[""For continuous time, discreet state Markov processes, the target of estimation is the transition rate matrix. If you understand survival analysis, it is a generalization of the hazard function for time to a single type of event. That is, \n\n    P( X_{t+h} = j| X_t = i) ~ q_{i,j}(t) h \n\nIt's estimated using the tools of time to event data. For  example, a fully nonparametric estimate is given by the Nelson Aalen estimator, which can be considered a piece of the more familiar Kaplan Meier estimator for survival curve. At each time, t, at which there are transitions from i to j, we count the number of individuals at risk for an i to j transition, e.g., number of individuals, R_i(t),  in state i at time t, and number of transitions, dN_{i,j}(t), from i to j. Then, an increment to the cumulative transition rate estimate is \n\n    dQ_{i,j}(t)  =  dN_{i,j}(t)/R_i(t)\n\n\nOnce we've put these together for all states, i and j, at all transition times, t_k, then we form the cumulant:\n\n    Q_{i,j}(t_k) = sum_{t_l<=t_k} dQ_{i,j}(t_l)\n\nOne more thing. This defines the entries of Q off the diagonal, e.g., for all distinct i and j. The diagonal entries of Q are the negative of the corresponding row-sum\n\n     Q_{i,i}(t_k) = - sum_{j != i } Q_{i,j}(t_k)\n\nNow, the part you've been waiting for ;)   The transition probabilities are expressed in terms of the cumulative transition rates via the matrix exponential:\n\n\n    P( X_t = j  | X_s = i ) =   EXP(Q(t) -Q(s))_{i,j}\n\n\nWhat, you ask,  is the matrix exponential?  It's defined by the power series with terms vbeing matrix powers of an exponent:\n\n\n     EXP(Q)  =  sum_{n=0}^{infty} Q^n / n!\n\n\nUsually, five terms is enough. Hope that helps."", 'You could treat it as multinomial regression with eg previous state and time ( amongst others) as inputs', ':0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', 'THANKYOU!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', 'This made my day', 'Glad to hear it!  Pls reread... when I just came back to and saw your nice comments, I found and corrected a couple of errors....1. the transition probabilities between times s & t are the matrix exponential of the _difference_ in the cumulant transition rate matrix between s and t, and 2. the matrix exponential power series starts at n=0.', 'This may be helpful. This is how the authors of TraMineR say they estimated the transition probability (rate?) matrix for each time point:   \n\n\n""Time-varying transition rates can be obtained with option time.varying=TRUE, in which\r case a 3-dimensional array with a distinct transition rate matrix for each of the positions\r t = 1, 2, . . . , L  1 is returned. The matrix for position t is computed by considering only the\r states at t and t + 1. The third dimension of the array corresponds to the position t index.""', ""Ah, I think there's a caveat, unfortunately. I think my Markov chain is in discrete time rather than continuous time. Specifically, a state may occur at any point of time on an interval made up of integers. So, for instance, a state couldn't occur at time 1.5 or 2.7 or 3.1, but can occur at 1,2,4, or 7. Would that change your answer?"", ""Definitely, my comments only apply to continuous time which means in practice, you know the exact times of transitions and they occur at distinct times (technically, no ties, a small proportion of ties wont kill you). I'm guessing that your data is panel data, which means you don't know the exact times of transition,  you've determined what state everyone's in at fixed times 1,2,4, 7. Correct? You can certainly estimate transition probabilities from this kind of data.  So this is basically a distinct set of multinomial probabilities out of each state between each pair of consecutive times in the sequence of times the counts are recorded. In guessing you can use the package to do that. It's a fairly straightforward task to do by hand."", ""That's right. It is panel data. I'm not sure if this is relevant, but I generated the panel data with a variation of a hidden Markov model (HMM), a hidden semi-Markov model (HSMM). So, the HSMM determined the state of multiple subjects at regular, fixed times (i.e, 1,2,3,4,5,6,7, ..., 600).\n\nI'll look into how to estimate multinomial probabilities.\n\nThank you so much for your help, by the way!"", ""1.Do you just want aggregate estimate or are you interested in covariate effects (age, gender etc)?\n\n2. If the panel data comes from a hidden semi Markov model then the resulting discrete time process isn't markovian because at each fixed time point each individual's transition probability will depend upon the time they've been in that state which you don't observe. If you assume markovian dependence then there is no dependence on the past other than what state you're currently in.  Is that what you're trying to see? The amount of bias incurred by misapplying an an assumption of markovian dependence?"", ""1. I am interested in covariate effects (e.g., age, genotype).\n2. So, are you saying that it is inappropriate to estimate time-varying transition probabilities because the panel data is not Markovian and the validity of an estimate of time-varying transition probability depends on the assumption of Markovian dependence?\n\nIt seems like my understanding of Markov processes is lacking. If you don't mind me asking, do you think it would it simplify analysis if I used an HMM to generate a state sequence?\n\nAlso, if I'm understanding you correctly, you're saying that treating the output of an HSMM as Markovian introduces bias when estimating time-varying transition probabilities/rates? i.e., because estimates of this kind assume Markovian dependence .\n\nEDIT: I've thought a bit about what you wrote. I think I see where you're coming from. Since an HSMM's output is not Markovian, it is inappropriate to estimate time-varying transition probability without accounting for the HSMM's assumptions because we know that the state sequence was generated with those assumptions. \n\nOf course, accounting for an HSMM's assumptions when estimating time-varying transition probability seems complicated, so I may just go with an HMM instead. In that case, its output can be assumed to be Markovian, so it would be appropriate to estimate time-varying transition probabilities from its output if you only use, say, the current time point and subsequent time point (i.e., because this method assumes that the future time point only depends on the current time point, i.e., the Markov property)."", ""     EDIT: I've thought a bit about what you wrote. I think I    \n     see where you're coming from. Since an HSMM's \n     output is not Markovian, it is inappropriate to \n     estimate time-varying transition probability without \n     accounting for the HSMM's assumptions because \n     we know that the state sequence was generated with \n     those assumptions\n\nYou got it! \n\n     Of course, accounting for an HSMM's assumptions \n     when estimating time-varying transition probability \n     seems complicated,\n\nDefinitely. And based upon what's hidden e.g. you only know an individuals current state, and not how long they've occupied that state, there's insufficient data to estimate the state/ time in state specific transition probabilities.  \n\n\n\n     I may just go with an HMM instead. In that case, its \n     output can be assumed to be Markovian, so it would \n     be appropriate to estimate time-varying transition \n     probabilities from its output if you only use, say, the \n     current time point and subsequent time point \n     (i.e., .because this method assumes that the future \n     time point only depends on the current time point, \n     i.e., the Markov property).\n\nYes!\n\n\n     I am interested in covariate effects (e.g., age, \n     genotype).\n\n\nTry the multinomial package.""]"
[Question] Pareto scaling: Can anyone explain why group means are opposite and equal?,"Hi! I'm a bioinformatics generalist (so close enough to good programmer/bad statistician) and am working with a dataset for which I am told the typical normalization steps are:

1. sum-normalization > log10 transformation > Pareto scaling\*.

For each of my response variables (>10,000), I have ten observations, 5/= per group in two groups. I have not come across Pareto scaling before.

After running those three steps on the full dataset, for every single response variable, abs(mean(group1)) = abs(mean(group2)).

I have read briefly about Pareto scaling but still can't get myself to understand why this is the result. Can anyone explain why this is? My values are grouped in my data such that the first five columns are in sequence and then the second five (though I can't see any reason this could possible matter).

I did it by hand for a few response variables and find the same result.

Example:

Input: 7019731.63, 6101363.44, 6639039.79, 6433115.66, 6763978.24, 7267775.37, 6968062.19 7027620.04, 7440666.63, 6161609.30

Sum-normalized (1000\*(x/sum)): 103.501, 89.960, 97.888, 94.852, 99.730, 107.158, 102.739, 103.617

Log10: 2.015, 1.954, 1.991, 1.977, 1.999, 2.030, 2.012, 2.015

Pareto-scaled ((x - mean)/sqrt(sd)): 0.093, -0.265, -0.049, -0.130, -0.002, 0.181, 0.074, 0.096

mean(group1)= -0.0705

mean(group2) = 0.0705

If you try it yourself you'll see that if you randomly group the final ten observations different ways, sometimes abs(mean(group1))==abs(mean(group2)), but not always. However, across my >10K response variables, this is \*always\* the result. What am I missing??

Thanks for any help",11wn6ab,readweed88,1679329017.0,1,0.67,"[""I don't use this technique in my work, so I'm just going off the math. The scaling is accomplished through the division of the sqrt(std).\n\nBut you're also subtracting the mean from each observation in the numerator, thereby centering the results. The mean of ALL the pareto values calculated this way will be zero, and by extension, your subset means will total zero as well.\n\nIn practice, are you applying the pareto to each variable/column?  Hopefully someone more familiar with your field will come along with additional thoughts and we can all learn from them. Good luck."", 'Thanks for taking a look! I think that\'s the whole answer, no field-specific input needed. It was a failure of like sixth grade math that I didn\'t make the step from centering the variables (mean = 0) to the means of any two subsets totaling zero - And I suspect it was because in my ""testing"" I made an error that resulted in the means of two random subsets not always totaling zero - tried it more carefully and of course they do.\n\nThanks again!']"
[Q] significance of cluster,"I have data that is clustered in 2D. I want to use a clustering algorithm like k-means, but I want to determine the significance of the clusters. Is there a way to do that?",11wisx1,SlartibartfastGhola,1679318685.0,12,0.84,"['What does ""significance of clusters"" mean? You might want to take a look at model-based clustering for this (mclust in R). There is a whole world of cluster validation techniques, from profile plots to permutation tests to actual model comparisons.', ""What's your null model? That should give you a direction to go when considering how to build a NHST."", 'Thanks that gives me some key words to try. Hypothesis is the data is clustered more strongly in one direction than in the other, so actually only need 1D clustering.', 'Null that they are uniformly distributed or arent bimodal', ""Might also recommend the silhouette score. It's what I've used in the past."", 'Still a bit unclear what ""clustered more strongly in one direction than the other"" means... Do you know the number of clusters a priori? I have never seen a hypothesis like this.\n\nWith model-based clustering, this sounds like it could be a likelihood ratio test where in one model you force the clusters to have equal variance in each direction, and in the other model you allow a difference in variance. You can do this with the mclust package!', 'Build a null distribution out of clusterings done in synthetic data that are consistent with the null hypothesis. Then do your hypothesis test of the true clustering against that null distribution.', 'Thanks!', 'oh yes, that\'s what it\'s called. I misremembered and called them ""profile plots"" haha', 'Thanks!', 'Oh! That jogged my brain. I should look at the many tests for modality not for clustering.']"
[Q] Probability and likelihood are different but then there are scenarios where they assume probability and likelihood to be same/ equal. So when are these two terms considered equal ?,,11wi0xd,emo-9,1679316594.0,25,0.93,"[""You use them for completely different purposes. Don't worry about when they might happen to be equal. Unlike the probability of an event, a likelihood is almost never useful by itself. It's (nearly) always used in comparison with another likelihood. \n\nThink of likelihood as an optimization metric that somewhat resembles probability, but it really doesn't matter if it's the same. An 'optimization metric' is something that you use to calculate the 'best' estimate. A sum-of-squared error terms is an optimization metric, albeit one that does not resemble probability at all. You can calculate the parameter estimates of a linear regression (slope and intercept of the line) by a procedure that minimizes the sum of the squared error terms.  \n\nIt turns out that these linear regression parameter estimates are the [exact same answer as the parameter estimates you get when you maximize the likelihood](https://en.wikipedia.org/wiki/Least_squares#Least_squares.2C_regression_analysis_and_statistics). In other words, it does not matter which metric you use in your optimization when you calculate the estimates for a linear regression. The regression that has higher likelihood (or lower sum of squared errors) is 'better', so when you've reached the 'peak' likelihood (or nadir of error), you've got the 'best' model. \n\nFor other regression models, the sum of the squared error terms is less useful, so we use the likelihood. The answers based on either approach aren't equal, but that's okay; we know which answer to use."", 'A probability is a general idea that says if you have population, A, then P(A) = 1 (roughly speaking). There are one or two other additivity rules etc.\n\nA likelihood function is often expressed via probability theory as well, but it\'s a specific usage which generally involves evaluating something like P(X = x | theta) = L(theta | X = x). That is, we are interested in a particular function that changes wrt parameter theta, when X=x (X being our random variable) is observed. Now, in order to evaluated the conditional we need information like:\n\nP( X | theta) = P(X, theta) / P(theta), we typically don\'t know the numerator P(X, theta), and we guess a form for P(theta) for the denominator normally. But therefore that means we don\'t really know the true form for P(X | theta), so we consider it as something ""non-normalized"" in practical settings, hence why we tend to work with it as ""the likelihood function"". \n\nAlso it\'s because in conditional probability with something like P(A | B=b), we typically work with B fixed, and A being allowed to vary typically. But we work with P( X | theta) in reverse, X is fixed, and we optimize over theta in some way therefore the form of P(X=x | theta) keeps varying in its functional ""form"" as theta varies also, which is weird in practice (but if we had information about everything, it would be still OK to calculate, and I suppose consider it strongly as a pdf). Based on this I suppose we try to emphasise that we work with P(X=x | theta) in a more ""functional"" way, hence why we like to write it as ""the likelihood function"": P(X=x | theta) = L(theta | X=x), i.e. X is fixed, theta varies.', 'The functional forms are exactly the same but probability is a function of values of one or more random variables, conditioned on values of the parameters, while likelihood is a function of the parameters, conditioned on the observed values of the random variables (the data).', 'What helped me understand likelihood is learning that it is very similar to conditional probability. \n\nPr(X|theta) is a probability when theta is fixed (and X is variable). Imagine a simple coin flip / bernouli distribution scenario. If theta = .5, the probability that X = 0 is .5, and the probability that X = 1 is .5. Note that the distribution adds up to 1, as probabilities should. \n\nPr(X|theta) is a likelihood when X is fixed (and theta is variable). Again imagine a coin flip. If X (i.e. the data) is 0, then the likelihood of theta being 0 is 1 (from p1 * (1-p)0), the likelihood of theta being .1 is .9, the likelihood of theta being .2 is .8, etc. Note that the ""distribution"" of theta is not a probability distribution since it doesn\'t add up to 1. Also note that it\'s highest (i.e. maximim) at 0; this makes sense, right? If you flip a coin once and it lands heads, your best guess would be that the coin always lands heads. (Well, according the logic of ML. ML can be a little dumb sometimes, because it is constrained by the data and the model.)', 'A probability is a function normalized so that the integral over the entire event space is 1. \n\nA likelihood is not normalized, because it is a function used to compare the ""likelihood"" of different outcomes, usually taking their ratio (which would wipe out the normalization anyway).', ""[Likelihood](https://en.wikipedia.org/wiki/Likelihood_function) is defined in terms of density\\*, but the likelihood is not a density (each value is calculated using a density - at each point it is equal to\\*\\* the density - but it's a different density being used at each point). Likelihood is a function of a vector of parameters, not of the random variable; it slices across different densities to get the  (*L*) values\n\nSee this illustration: https://i.stack.imgur.com/jEulu.png  (from [this](https://stats.stackexchange.com/a/284827/805)) which shows the equality of a likelihood and a density at each point, but indicates exactly how the likelihood and density *functions* differ\n\n---\n\n\\* in the general, technical sense that will also include discrete pmfs among other things...\n\n\\*\\* or, more generally, defined up to a common constant of proportionality, just as Fisher defined it."", 'Likelihood IS a probability. Its usually just a joint probability where you consider the data random and the parameters fixed.', '**[Least squares](https://en.wikipedia.org/wiki/Least_squares#Least_squares.2C_regression_analysis_and_statistics)** \n \n >The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation. The most important application is in data fitting.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', 'Can you explain a bit further what you mean in the last paragraph where you know the likelihood of theta being 0 is 1, 0.1 is 0.9 etc? I wasnt sure how you got it from your equation you wrote, but it does seem to make sense intuitively', ""Sure!\n\nThe experiment: We flip a coin once. We equate 1 with heads and 0 with tails.\n\n\n\nThe model: Bernoulli, with the probability of getting a 1 being represented by p. (Note: p doesn't have to be .5. It could be a weighted coin or a trick coin, we dont know.) Mathematically:\n\nPr(X=1|p) = p\n\nPr(X=0|p) = 1-p\n\n\n\nOr, more generally:\n\nPr(X=x|p) = px + (1-p)(1-x)\n\n\n\nLet's say we run the experiment, and our data is X=1. \n\nOur question now is: Given our data, what is the likelihood of p? Likelihood is a function of p, given X. So:\n\nPr(X=1|p=0) = 0\n\nPr(X=1|p=.1) = .1\n\nPr(X=1|p=.2) = .2\n\nEtc.\n\n\n\nOr, using the general equation, comes out to the same:\n\nPr(X=1|p=0) = (0)(1) + (1-0)(1-1) = 0\n\nPr(X=1|p=.1) = (.1)(1) + (1-.1)(1-1) = .1\n\nPr(X=1|p=.2) = (.2)(1) + (1-.2)(1-1) = .2\n\nEtc."", ""Okay! Thanks so much, that did clarify some of my questions. \n\nHowever, I was still wondering if I am misunderstanding something about the general equation that you wrote:\n\nPr(X=x|p) = px \\* (1-p)(1-x)\n\nIs \\* supposed to be multiplication? If I am understanding it right, if X = 0, wouldn't this always evaluate to 0? Should it be a + instead?"", 'Yes, sorry! Just edited it.']"
"[question] Boxes B and C are equally likely (20% each), but B ""looks like"" Box A (60%). Is there a measure under which B is less surprising than C?","You are playing a guessing game where you have to choose one of three boxes, each containing three different marbles. Box A contains red, green, and blue marbles, Box B contains red, green, and yellow marbles, and Box C contains black, white, and purple marbles. You are given the probabilities (60%, 20%, 20%) for each box. You make your choice and open the box. Intuitively, the Box B outcome should be less ""surprising"" than the Box C outcome in ""marble color space"" because it contains the red and green marbles, which are 80% likely to be encountered. Is there a probabilistic measure that captures this intuition?",11wetok,synysterbates,1679307501.0,0,0.4,"['Your intuition is in some sense wrong, because the *combination* B is just as surprising as the *combination* C. However, if you look at how surprising each individual marble is without considering combinations, then it is true that some of the marbles in B are less surprising.', 'I am aware that P(Box = B) = P(Box = C). But suppose you are a machine that represents beliefs about which Box it is. Then you observe the marbles, as a machine. There\'s something about your belief update that is smaller for Box B than Box C. \n\nFor example, if you open Box B, your probabilities for those before and after opening the box are (red: 0.8 -> 1.0; blue: 0.8 -> 1.0; green: 0.8 -> 0.0; yellow: 0.2 -> 1.0; black 0.2 -> 0.0, white 0.2 -> 0.0; purple: 0.2 -> 0.0). The absolute value of the change in probability is 2.6 for Box B.\n\nOn the other hand, if you open Box C, the probability changes are (red: 0.8 -> 0.0; green: 0.8 -> 0.0; blue: 0.8 -> 0.0; yellow: 0.2 -> 0.0; black: 0.2 -> 1.0; white: 0.2 -> 1.0; purple: 0.2 -> 1.0). The absolute value of the change in probability over all marble colors is 5.0 for Box C.\nI am trying to find a meaningful quantity provided by probability/information theory that captures the ""absolute value of the change in probability"", which is larger for observing Box C than Box B.', ""If you reason this way, processing the marbles individually, you need to look at conditional probabilities. For instance, once you've seen black, the probabilities for white and purple become 1. Or in other words, the probabilities for the different marbles are not independent.""]"
[Question] [Research] How do I find the line of best fit (using python) when I'm completely unsure of what sort of equation it is?,"I've been working on some physics research and I'm trying to figure out what the line of best fit for it is. I have tried many types of equations (e.g. logarithmic, exponential, etc.), but none of them seem to fit very well. I was wondering if there is any way I could put the points into some python library and get a line of best fit that looks reasonably plausible. I know after finding this I would need to use some statistics to compare models and see which one is most likely with the Ockam factor included. If anyone could please help me in just finding a type of equation that would work for a line of best fit of my data that would be great. Thank you!",11w9s76,oliv3hue,1679290463.0,0,0.25,"[""The best fit given what assumptions? You could draw a perfectly fitting, likely very squiggly, line, but this isn't likely to be what you want. Understanding of your data, the underlying phenomenon, and the needs of your inference would all be helpful (necessary) to answering your question."", 'Is your data a vector of positive values?', 'Line of best fit as its taught in middle school would likely refer to linear regression: in essence, y = mx + b (although statistics would write this as Y = Beta * X + epsilon). \n\nIt depends on what your goal is  are you predicting the probability of some event (aka is your y a [0,1])? Then logistic regression is what your after.\n\nIn Python Scipy might have something? Tbh Id just use R for this.', 'What are you trying to do with the model?']"
[Research] [Question] data analysis of self collected responses for results section of research paper,"Ive never been so confused

Im just a student researcher and I have a bunch of data from my results. 120 responses to be exact. Ive been trying to teach myself how to analyze data but nothing is helpful. I have no idea what code is or java or python or projects. I have a very small background in statistics but its basic stuffthink high school stats class. Will someone direct me to a website I can use that makes sense? Or even explaining where I should start, what procedures to use for analysis? Id be happy to attach a link to my sheet if anyone wants to look at it themselves or explain my research",11w8nyg,CryptographerRare149,1679287073.0,4,1.0,"[""Hello,maybe the first thing to consider is stating some of the claims/hypotheses you thought of testing before collecting the data. If you had none which will be rare then think of some relevant ones in your research and also familiarise yourself with how the sampling and data collection was done.I would then suggest starting with exploratory data analysis (EDA) using plots, crosstabs and other numerical summaries of your data by groups. I will be assuming you have access to popular software for data analysis like Python or R. If not, a quick google search on installing any would guide you.\n\nHere is a link to a simple example of EDA in R : https://www.statology.org/exploratory-data-analysis-in-r/\n\nHere is a link to a simple example of EDA in Python: https://www.geeksforgeeks.org/exploratory-data-analysis-in-python/\n\n\nAfter EDA you can proceed to test your hypotheses as seen fit..Be careful to check and see if your assumptions necessary before using these tests hold at least approximately or be sure to try robust alternatives or non parametric equivalents of these tests. \n\nHere is a sample guide in R:\nhttps://data-flair.training/blogs/hypothesis-testing-in-r/amp/\n\nHere is one in Python: https://www.visual-design.net/amp/an-interactive-guide-to-hypothesis-testing-in-python.\n\nI would also recommend that in your free time you try to read Sharon Lohr's accessible book on survey techniques titled Sampling and Design and also Jim Albert's/Rizzo Maria's book on  simple data analysis methods titled R by Example -Concepts to Code. Good luck."", ""It looks like you shared some AMP links. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical pages** instead: \n\n- **[https://data-flair.training/blogs/hypothesis-testing-in-r/](https://data-flair.training/blogs/hypothesis-testing-in-r/)**\n\n- **[https://www.visual-design.net/post/an-interactive-guide-to-hypothesis-testing-in-python](https://www.visual-design.net/post/an-interactive-guide-to-hypothesis-testing-in-python)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)""]"
[Q] How do you interpret a negative interaction coefficient in a logistic regression model?,"Say for example, you include an interaction term ageXsmoking in a model predicting heart disease as an outcome, and it turns out to be statistically significant, how would you interpret the coefficient if it is negative? I understand that it would mean that the effect of one of the variables within the interaction term  would decrease as the other variable increasesbut how do you know which one? Does it go both ways? 
Hope my question makes sense.

Edit: thanks so much for your responses everyone, I will take a look through them in more depth soon and see if I understand what everyone is saying",11w68xk,smithimadinosaur,1679280426.0,13,0.93,"[""Trying to interpret coefficients is often hard and confusing. It's best to make conditional effect plots and see what's going on."", 'Dont forget to consider that non interaction terms effect on your variable. But considering the interaction term in isolation, you are correct: if one goes up, the other goes down and vice versa', 'Interaction plots are the best way to interpret interactions in my experience. They might look a bit wonky with logistic regression, but will hopefully give you some insight on whats actually happening.', 'I think the OP\'s question isn\'t so much what does a negative interaction mean in general, but rather in this particular situation. It\'s a bit counterintuitive given that both age and smoking are such strong, known risk factors for heart disease, how could older smokers have lower risk than their younger counterparts. Was this a logistic regression for incident disease over the followup period, or a cox regression for disease free survival?  I think the answer is most likely due to survivor effect. The ones that are going to get the disease have done so by mid 60\'s and the remaining ones are those that are genetically ""immune"" to heart disease.', 'When the coefficient for an interaction is negative, it means the effect of one variable decreases as the other variable increases.', '[deleted]', ""You cannot interpret the coefficient due to the nonlinearity aspect/link function. There are papers on this, like,\n\nAi, Chunrong and Edward C. Norton. 2002.  Interaction terms in logit and probit models. Economic Letters 80: 123-129.\n\nYou need to calculate predicted probabilities and calculate first differences, for instance, and calculate the SE of the difference. If you use MC simulation, it's extremely easy to calculate, basically a subtraction. \n\nLike others said, you can also do marginal probabilities but I find first differences more easy to calculate and interpret and the results end up being equivalent."", ""This is the answer. Plot out the conditional (marginal) probabilities and see how they change for one predictor at different points along the other predictor. I say probabilities specifically because I recommend not even bothering with interpreting terms in odds ratios or log odds. Probabilities are much more directly understandable but you need to plot them since they aren't linear."", 'I think youre onto something and I would add that the younger folks that smoke could also exhibit lifestyle choices or be from families that have much higher risks associated with heart disease.', ""No, it doesn't mean that because this is a logit model and the effect is conditional on the values of the other variables"", 'What is the best method for this when you have many variables?', 'Yup also because log odds is non collapsible while probabilities/risk differences or risk ratios are', 'Isnt that true for many types of models including OLS.', ""The difference is that in nonlinear models like Logit it also depends on the value of the variables that are not a part of the interaction.\n\nY = A + B1 \\* X + B2 \\* Z + B3 \\* X \\* Z\n\nIf I take a derivative w.r.t.  X and then a derivative w.r.t. Z, I get B3 which is the interaction coefficient.\n\nHowever, with this,\n\nY = logit(A + B1 \\* X + B2 \\* Z + B3 \\* X \\* Z) = 1/(1+e\\^-(A + B1 \\* X + B2 \\* Z + B3 \\* X \\* Z))\n\nwhenever you take partial derivatives you end up with EVERYTHING  in there because of the chain rule. Throw some additional variables, they are still in there; whereas if you threw some additional variables in the linear regression, they'd go away because they are constants in a partial derivative."", 'I was referring to a nonlinear OLS with an interaction. I wrote an article explaining this many years ago: Lane, D. M. (1981) Testing main effects of continuous variables in nonadditive models. Multivariate Behavioral Research, 16, 499-510. [This book](https://psycnet.apa.org/record/1991-97932-000) is also informative.']"
[Q] differences in mean or median between two groups that have a positively skewed distribution,"I have two treatment groups (not paired), A and B, and I initially wanted to know if there is a statistically significant difference between the means of the group. However both groups have a positively skewed non normal distribution (A skewness 0.85, B skewness 1.50) (A kurtosis -0.13, B kurtosis 1.63) 

Google tells me comparing means is usually better for symmetric distributions, which neither A or B show. Would a Mann-Whitney U test be more appropriate than a two sample t test? However there is a lot online that says the t test is still valid if sample size (N) is above 30, (N=50 for A and N=53 for B). 

But now Im confused if I should be comparing means or medians! Lets say I am looking at the distance traveled by flies, A in the dark and B in the light, I have removed all 0s and Im only looking at distance travelled if the flies moved at all. I would expect a positivity skewed distribution because not many flies would  be encouraged to move due to the experimental design. So the number of flies that actually moved would be an important part of the analysis, which the median wouldnt consider 

I have no idea what Im doing, does anyone have anything I can read so I can understand things better? Is my reasoning and understanding so far completely wrong and unjustified? The more I read the less I understand statistics.",11w0xg4,ninasayswhat,1679267464.0,1,0.67,"[""> Google tells me comparing means is usually better for symmetric distributions,\n\nGoogle can lead you to poorly informed resources. This is, broadly speaking, wrong. *Population* means are very often perfectly sensible things to estimate, and sample means often perfectly reasonable ways to estimate them, even in the presence of skewness. For example, consider a gamma-distributed variable; it's perfectly reasonable to estimate the population mean, and to estimate it using a sample mean, even though the distribution may be quite skewed.  Similarly consider a Poisson-distributed (i.e. discrete) variable. Again,  it's perfectly reasonable to estimate the population mean, and to estimate it using a sample mean, even though the distribution may be quite skewed.\n\nOn the other hand, if you were dealing with a symmetric variable, there's no guarantee that the sample mean is a good estimator of its center; much better choices may be both indicated and easy to use.\n\n---\n\n> Would a Mann-Whitney U test be more appropriate than a two sample t test?\n\nYou would be changing the hypothesis. With a t-test you were looking at a hypothesis about means. With a Mann Whitney, you *don't* (and neither is it a hypothesis about medians). It's perfectly possible for the direction of of a difference in either means or medians to be in the opposite direction to what the Mann-Whitney looks at.\n\nIf you were interested in a hypothesis about means, you should not change that (though you might change how you test it). If you *weren't* interested in a test of means, why consider a t-test at all?\n\nYou can't just swap hypotheses like this willy-nilly, they're asking very different questions -- and you should definitely not be changing your hypothesis *in response to [what you discover in the data](https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data)*.\n\n---\n\n> However there is a lot online that says the t test is still valid if sample size (N) is above 30, (N=50 for A and N=53 for B). \n\na lot of what you find online is abject nonsense. It's very easy to find distributions which are simultaneously good descriptions of real data, and for which this advice is clearly wrong.\n\n---\n\n> Lets say I am looking at the distance traveled by flies, A in the dark and B in the light,\n\nWhy not talk specifically and directly about what variables you actually have, rather than what you seem to be casting as a hypothetical (if it were the real situation, it would not be necessary to open with 'let's say'), so that the particulars of the problem you invent don't accidentally lead us down a blind alley that turns out to result in poor advice for the actual circumstances.\n\n---\n\n> But now Im confused if I should be comparing means or medians!\n\nWhat was your original hypothesis? If you were talking about distance travelled would you have wanted to look at changing means if you could? Then that's what you look at (not necessarily differences, though; ratios and percentages changes might make more sense for distance travelled than differences, since distances are necessarily positive, and are ratio variables)\n\nYou can't just treat means and medians as direct substitutes; it is quite possible for the differences to point in opposite directions. If the mean increased but the median did not, what would you want to conclude? What if the median increases but the mean did not - what would you want to conclude? What if the chance that a random value from the second group was larger than a random value from the first group was clearly over 1/2, but  medians were equal and (yet) the mean decreased? What would you want to conclude? All of these are possible, and demonstrate, I hope, that *it matters what you are talking about*, they're not just able to be freely swapped one for the other as if they're all telling you a consistent story.\n\nIf you are interested in means, the mean amount moved for the ones that do move will be well right skew; I'd used a continuous generalized linear model for that, such as a gamma GLM. That will address several problems at once.\n\n>  So the number of flies that actually moved would be an important part of the analysis, which the median wouldnt consider \n\nIn your earlier description you conditioned on them having moved. Now you aren't. Puzzling.\n\nIt sounds (perhaps) that what you really should be doing is looking at first *whether* they move (0/1 response) and then at how far they move when they do move, which seems to give a more complete picture. If you are interested in the mean overall (average distance moved irrespective of whether they moved), then you'd have a zero-inflated continuous distribution (for which fairly typical advice would be to fit a zero-inflated model, which corresponds to 'first model whether they move and then model how far they move for the ones that did move')\n\n---\n\n> I have no idea what Im doing\n\nBefore you start to gather data, indeed, before you even design your study -- step 1 is to very clearly identify your specific research question. Among many things, this includes figuring out what population quantity your hypothesis is about.\n\nIt seems like you have not done this, or at least not with anything like the requisite care and rigor. This won't be your fault, it will be what you were no doubt taught. Whole disciplines are very carefully (and seemingly deliberately) teaching their students *very bad* practice under the guise of 'teaching' research/statistics -- including the practice of making hypotheses as vague as possible so you can pivot on a pin after you collect data, including looking at your data to choose your test and hence your hypothesis,  including not thinking about what you can know about your variable before you collect  data (and hence how to formulate assumptions in any reasonable way), and much else besides. In short, teaching you to replace science with some sort of cargo-cult approach to your research question, complete with a whole slew of largely useless activities and concerns that really don't relate to the problem at hand, in order to properly sanctify the study.\n\n(Just in case it is not absolutely clear, my frustration is not aimed at you. You're trying to do what you can with what you've been taught, or been able to figure out, or find. The internet - indeed even published textbooks in many application areas - can be a pretty dangerous resource unless you can already tell truth from half-truth from outright confabulation.)"", 'Anova is generally conservative with skewed distributions so a significant difference is meaningful.  However, you may increase your power with a transformation such as the log transformation to reduce skew.', 'Would an ANOVA still be valid if a levenes test for homogeneity of variance showed a P value of 0.02?', 'Generally speaking heterogeneous variance is not a serious problem when you have equal n. However, to be on the safe side you could use the Welch test that does not assume homogeneous variances.']"
[Q] Random sampling and EDA,"Hello,  I have some questions about random sampling from dataset and sample size from practical point of view. I learn by myself and would like to find an  answer for a few questions. I read many articles but need some practical  point of view from experts.

Let's say I have a dataset consisting of 800 000 rows. This is my entire population (dataset). Questions:

1. I  want to create histograms showing distribution of all features. I  suppose creating histograms for  each column based on 800 k rows  doesn't make sense. So, I will most likely need to take a sample. What  number of (randomly selected) rows would be enough to create reliable  plots? It's more the question about: should plots be created on the  whole datasets (say I would have smaller dataset e.g. 3000 rows) or just  a sample?
2. As  for the sample itself - I read some articles about determining the  sample size. They usually use a formula - example here. It says, that,  given confidence level, sample size is strictly determined (e.g. 385 for  5% CL). Do you use such formulas to determine sample size?
3. On  the other hand I found a source, where there's an info about taking 10%  of the population as a sample (but not more than 1000 elements and not  less than 100). Should I follow this principle? What if I chose 2000 or  3000 elements instead of 1000?
4. What are the goals, other than hypothesis testing, of using random sampling? Is it also appliacable for simple EDA analysis?
5. As  for point 4 - I conduct EDA on abovementioned dataset. So I create  plots, summaries, data cleaning, feature engineering etc. The big  question is: should I either do my EDA on the whole 800 k dataset  (including making plots, feature engineering etc.) or, at the very  beginning, determine a random sample (say 1000 elements) and proceed  with EDA on this sample?

Thank you for your answers",11vslzx,muskagap2,1679249281.0,2,0.75,"[""> I want to create histograms showing distribution of all features. I suppose creating histograms for each column based on 800 k rows doesn't make sense\n\nWhy? That seems like the most reasonable thing to do.\n\nYou haven't explained anything about what these data are, or what you're actually trying to learn from them. What are you trying to do?"", 'I dont understand your concerns? Histogram bins your numbers so you are looking at far lesser than 800k rows. Lets say you have 30 bins. Then thats just 30 rows x 2 cols worth of data?', 'I just wanted to do EDA and was wondering if it makes sense to grab the whole dataset. For example - I need to identify data distribution fo reach feature. Taking 800 k will probably give the same result as if I to the same with 50 k rows. If I had 10 mln rows then I should take all of them to indentify distribution of given feature?', 'The main reason to take a sample is because the cost (in terms of money, time, manpower, etc) of collecting or processing the full data is prohibitive. If you already have the data and the tools to process it, what do you gain from only analysing a fraction of it that offsets the introduction of sampling variance?']"
[Q] Does these data look normally distributed (histogram).,"I want to analyze some data with statistical method that assumes normallity of distribution.

For each analysis (2W-Anova) i have 4 datasets of the meassurment  (4 different timepoints).

Would this data be ""acceptable"" for using in such an analysis that assumes normal distribution?The 4 datasets ""pass"" the Shapiro-Wilks test but to me they dont look so normally distributed.

[https://imgur.com/a/gV6vbaV](https://imgur.com/a/gV6vbaV)",11vs26v,Openpentagon,1679248371.0,1,0.56,"['(Assuming this is a repeated-measures design) Anova is quite robust to non-normality but not at all robust to violations of the sphericity assumption. I would pay more attention to sphericity than to normality.', ""No, the data don't look normally distributed.  Are you sure that Shapiro-Wilk, with a sample size of 2500, doesn't suggest they aren't ?"", 'Check the residual distribution', ""> I want to analyze some data with statistical method that assumes normallity of distribution.\n\nTwo way ANOVA does not assume that *any* of the variables are marginally normal, or even conditionally on only some of the conditions; it's important not to be vague about your variables nor about how the assumptions relate to them. It's the within-cell conditionals that are assumed normal, and even then only really under H0, and *even then*, you don't really need normality (it's not especially sensitive to it -- fortunately because you never actually have it in practice), and ***even then*** you shouldn't be relying on the sample you want to use in the test to tell you about the assumption (even if it were relevant to making a decision about what to assume under H0, which it probably isn't).\n\nOther assumptions will typically matter more -- including independence (which is unlikely to hold - even conditionally - with repeated measures)."", 'Are you absolutely sure the method assumes the data must be normally distributed?', 'q-q plot', 'Ok but there is an option to ""Use Geisser-Greenhouse correction"" for assuming non-sphericity', 'it looks like this (but what can i say based on its looks?):\n\nhttps://imgur.com/a/QYcA9FQ', 'For non-sphericity you should look at covariances and variances of difference scores for all pairwise differences. There are corrections for violating sphericity such as huynh-feltd.', 'If the least sample size is big enough, you can jump straight to homogeneity of variances testing. Or if you like, you can try out log, square-root, or power transformation. But make sure to check influential observations as well.', '>ok. Geisser-greenhouse correction is an option when using gRAPhpad pRISM. so if i use it, check the box, then sphericity is not an issue? and if yes, what do we lose in doing so?', 'and if the sample size is small?\n\nAnd when i test for homo of v. i get a plot that looks like this (what do i make out of it)? can i still do the power transformation if its not homoscedastic?: \n\n  \nhttps://imgur.com/a/DcX3fEd', '>influential observations\n\nWhats the best way to check for these?\n\nAnd if i use transformation - does it matter how large sample size? and should i transform all the datasets in on go or 1 dataset at a time (4 datasets used per anova).', 'Thats a valid solution. You lose a little power if sphericity is met but sphericity is almost never met.', '>u/Openpentagon\n\n[https://data.library.virginia.edu/a-rule-of-thumb-for-unequal-variances/](https://data.library.virginia.edu/a-rule-of-thumb-for-unequal-variances/)\n\nOtherwise, you will need type 3 robust ANOVA with heteroskedasticity-consistent (HC) standard errors.', 'what if i use correction Geisser-Greenhouse ?']"
[Q] What courses are a must in order to be a competitive candidate for statistics masters program?,"Right now I've taken the calc sequence, linear algebra, differential equations, couple of applied stats courses, and a intro to proofs course. Currently enrolled in intro to real analysis and probability theory. Are there any other courses I am missing?",11vok8u,redreaper71_,1679240539.0,10,0.92,"['If youre able to, Id probably take at least one programming course.', 'From someone who has a few acceptances and is in the process of deciding, that + programming experience would be great. Its likely the applied stats gave you that, but having places where youve shown (outside of a course) that you can produce something by way of code is a huge plus.', 'Stats in practice is pretty much all done though code now, so thats something that will either help you get into the program or help you after you get in']"
[D] on randomness and normal distribution.,"I was reading this [post](https://www.reddit.com/r/Economics/comments/11vcwia/eli5_why_does_wealth_distribution_follow_pareto/) and some users are stating thinks like :

"" you wouldn't expect a *normal* distribution unless you believed that wealth was distributed *randomly*""

""Normal distributions tend to arise from randomness""

""random distribution of wealth would give you a flat distribution though...

the normal distribution is non-random, with the central parts being favored...""

""If you take each dollar and randomly assign it to a person until you run out of dollars, it'll be normal ""

So here is this idea that randomness equals normal distributions, if not always, most of the time. This led us naturally to the question of what is randomness, and as far as I remember in probability we studied in depth the random variable concept which in the end led us to the density and distribution function. 

I remember as if it were yesterday a one shocking example that my teacher gave us back in the day:

""a constant is a random variable, it has density function of mass one in the value of the constant."" It has expected value (the constant) and variance as well (0).

Taking all this into account, behind randomness is a density/distribution function always, so things like a uniform distribution, a normal distribution or a pareto distributions, they are all describing different kinds of ""randomness"". I remember that I was talking about this with a partner in the university and in Spanish we have two words that seems to be translated the same way in English: ""azar"" and ""aleatorio"". Some one said that ""azar"" is describing a phenomenon with a uniform distribution whereas ""aleatorio"" is more close the the random variable concept.

by the other hand, this discussion leads to an other interesting question: how normal distributions arise in nature? I was surprised to find this [answer](https://www.quora.com/How-do-Gaussian-distributions-a-k-a-normal-distributions-arise) in quora, in particular:

""The Gaussian distribution is singled out by the [Principle of maximum entropy](http://en.wikipedia.org/wiki/Principle_of_maximum_entropy) as the unique maximum entropy distribution with a fixed mean and variance""

I think this could be a very important fact in explaining why some natural phenomena tends to behave as normal distributions.

Anyway. I appreciate your feedback and thoughts on this.",11vnsss,nmolanog,1679238574.0,1,0.67,"['You have a lot of quotes that don\'t make sense. They\'re misusing the terms ... but  also not properly defining their intended meaning in its place, leading to a lot of miscommunication.\n\nYou have different people using the same word to literally mean different things. \n\n> "" you wouldn\'t expect a normal distribution unless you believed that wealth was distributed randomly""\n\nThis doesn\'t make sense to me; unless they mean something very unusual by the word *random* that doesn\'t work. \n\nI did wonder briefly if they were thinking about a broken stick model of allocation, but that produces a fairly skewed distribution, not a normal distribution.\n\n> ""Normal distributions tend to arise from randomness""\n\nNo, at best only very specific kinds of randomness. You have to define a random process (and a resulting variable to look at) that would lead to a normal distribution. \n\n> ""random distribution of wealth would give you a flat distribution though...\n\nThey mean *uniform*, which is not what *random* means. I don\'t think that\'s what was meant by the first mention of *random* above -- though it\'s not clear what the exact intent was then either.\n\nAgain, they would have to define what scheme this \'random distribution\' should follow, a method for actually distributing this wealth so that we could see. I can think of ways to get an approximately uniform distribution across people.\n\n> the normal distribution is non-random, with the central parts being favored...""\n\nAgain, misusing the term \'random\'\n\n> ""If you take each dollar and randomly assign it to a person until you run out of dollars, it\'ll be normal ""\n\nIt depends on what they mean by \'it\'. If you look at the distribution across people, that won\'t be normal. If you pick a *person* and look at how many dollars that individual got, that would be binomial, which with enough dollars in the original pool (many more dollars than people) would at least be approximately normal.\n\n---\n\n> So here is this idea that randomness equals normal distributions, if not always, most of the time.\n\nSome people expressed that idea, others expressed the idea that it meant uniformity. Either way, they\'re all wrong, that\'s not what *random* is. You could assume some particular scheme of random allocation that yielded different distributions (and even within one scheme, what distribution you\'re talking about depends on what you\'re looking at)\n\nLet me give you an example of that last thing. \n\nA *Poisson process* is a mathematical description of a particular sort of random event that occurs in time or space; depending on what you look at, you may get Poisson (how many events in this amount of time), exponential (how long until the next event), Erlang (how long until the kth event from now), uniform (what\'s the distribution of events across time  during this fixed period of observation), etc etc.\n\nSimilarly with a random Bernoulli process, you could observe the Bernoulli distribution, or the binomial, geometric, negative binomial, etc. depending on what, exactly you\'re looking at.\n\nThey\'re all *random*.\n\nUntil the people conversing can agree to use the terms in the same way (and for that, I very strongly suggest sticking to using the statistical jargon in its conventional way), there\'s no point in them conversing at all, they\'re simply talking past each other, wasting perfectly good oxygen.\n\n\n>  that seems to be translated the same way in English\n\nOnly because you are reading things written by people that *don\'t* understand the distinction. \n\n*random* and *uniform* are very distinct concepts in statistics. English statistics class is not different from Spanish statistics class in that.\n\n It\'s just that the people you\'re quoting are simply misusing ""random"" in a variety of ways.', ""(Approximately) normal distributions tend to arise in nature when you're looking at a quantity that gets contributions from many independent random sources, and none of those sources dominates the others.  A very good description of how this arises is in this playlist:\n\n[https://www.youtube.com/playlist?list=PLzk87GkUC3kbm-MktEVTzZDoPovmKIlpT](https://www.youtube.com/playlist?list=PLzk87GkUC3kbm-MktEVTzZDoPovmKIlpT)\n\n(The first 3 videos are especially relevant to your question.)\n\nThe videos in this playlist are describing why measurement errors tend to be Gaussian.  If you have many independent sources of measurement error, some of those sources of error will push your measured value above the true value of the quantity, and some will push it below.  When you add them all up, the sum tends to look like a Gaussian.  (Although this video series also shows you how that can fail.)\n\nIt helps a lot if you understand a basic random walk first, which is the topic of the first video in the playlist.  The first three videos take you from that to Gaussian errors pretty quickly.""]"
[Question] Data with python. Where do you start if you have experience only with SAS and R?," 

Hello statisticians and python-enjoyers,

I am a statistician in the medical\\pharma field and in my everyday job i only use SAS. In my university's experience when i was studying biostatistics, i also used R and i liked it, although it was profoundly different from SAS.

Days ago i have received a job offer as data analyst that requires python programming. They know that i have never written a single line of code in python, but they are really interested in my experience and they made an offer. Since i don't want to accept with 0 experience with python and python's environments, i write this post. Where do i start? What do i install? Is there some fancy environment for programming like RStudio for R?

Just write me what do you think i need to know to start practicing with python.

Thank you very much for your time and patience.",11vjnas,Haylight96,1679227575.0,57,0.94,"['I was in a similar position a few years back, but had even less modeling experience. I would download Anaconda, open the Spyder IDE, import pandas and  statsmodels, and try and replicate the output and figures that you made in R/SAS for a linear regression. Just use some simple dataset in csv thats commonly used for linear regression examples. \n\nIt helps me to compare my own code side-by-side for simple cleaning tasks and then a simple model fit. And Ill start figuring out how to extract coefficients, plot the coefficients, etc.', ""IDE wise someone already mentioned Spyder Spyder is more or less an Rstudio clone for python data science. \n\nPycharm is also a good IDE more focussed on software development than on data science.\n\nBut if you already use Rstudio, just use Rstudio, it has added lots of Python support.\n\nLibrary/Environment wise: start with anaconda, it the easiest to start and has the most important datascience packages packaged. \n\nStart with some basic data handling and plotting with pandas and matplotlib or seaborn (seaborn is a nice plotting library with a similar approach as ggplot). \n\nAlso try numpy, statmodels and skikit-learn for numerical math, statistics and machine learning.\n\n(That was more or less the syllabus of the introductory python course in my statistics bachelors program. We also started from a similar background, everyone familiar with R and a little bit of stata knowledge. And I think it's a quite useful foundation for further learning.)"", ""I started with *Learning Python the Hard Way* and would highly recommend it. I love Anaconda, but I disagree with it being a good place to start given that it abstracts away a ton of things you're going to need to learn how to do independently."", 'If you know R then learning python should be relatively easy. The languages are very similair imo.', ""I've had really good experiences with dataquest.io. It has a bunch of interactive Python lessons focused on data science. The free tier will get you up to speed with basic Python syntax and some simple analysis tools. If you want to go deeper at that point you can pay for the premium version, but you should know enough by that point that you can start looking for more free advanced resources on your own."", ""Hey, also someone who started with a Stats and R background.\n\nIf I could only recommend two sources:\n\n1) Whirlwind Tour of Python book. Don't get bogged down trying to read every page, but get an idea of the standard collections (lists, tuples, dictionaries)\n\n2) Corey Schafer's videos on youtube. Absolute gold mine. \n\nAs far as libraries, key ones to learn for you likely will be Pandas, Stats, Matplotlib/Seaborn, Numpy.\n\nCongratulations and good luck!"", ""I recommend trying python interactive mode in vscode if you want something similar to rstudio. It offers both the option to script and run code interactively, create charts and inspect variables. \n\nHere's a  video demonstrating it https://youtu.be/lwN4-W1WR84"", 'Found myself in a similar situation recently because Ive had to start using the snake at work. Get the PyCharm IDE (similar to R Studio), pick a project or go to Kaggle/other, and start coding. The syntax takes some getting used to but the fundimental concepts are the same.', 'the jump from r to python is not challenging, dont be too intimidated', 'you write your code in r then ask chatgpt to translate it to python 5head', ""Pycharm is an awesome IDE. \n\nLook, I'm sure it isn't a shock to you, but once you've learned one language, you'll have a really strong foundation for most others. Just gotta master the syntax and learn libraries at that point, which is easy, if time consuming"", 'With solid programming experience, learning to write *working* code in Python is not hard.\n\nIt can take a while to write *good* code that you\'d want to put into a production environment, but that doesn\'t sound like what they\'re asking for.\n\nHere\'s a good resource I link to in one of my classes: https://richardson.byu.edu/624/lpython/PythonforRusers.pdf\n\nThis is also a helpful cross-reference: https://docslib.org/doc/4307113/matlab-commands-in-numerical-python-numpy-1-vidar-bronken-gundersen-mathesaurus-sf-net\n\nWes McKinney\'s book, ""Python for Data Analysis"" is pretty solid too.\n\nI suggest to my students that they use Anaconda and do their development in Spyder.', 'Have ChatGPT teach you. Unironically. I had zero Power BI experience but had a background in R/Python (mostly from school DS classes). Aced the interviews and landed my current dream job using ChatGPT as a tutor for learning DAX and M language.', ""I started with R and later learned and work with Python. Other people are talking about IDEs and have good points I agree with but I haven't heard people talk about the bear in the room that is pandas. From my experience, pandas is an absolute pain to learn despite being by far the most popular option in Python. If you find it difficult learning Pandas, consider some of the lesser known alternatives to it (many of which have an easy way to convert to a pandas DataFrame if you're using a library that requires one) - one that I've been really liking because it has a somewhat similar feel to dplyr is [Polars](https://pola.rs)."", 'Can I just say ugh why are there sooo many languages', ""If you already do R, then I would just get Python, either through Anaconda or get Python standalone and PyCharm IDE and let that handle your environments and packages depending on the analysis you're doing.\n\nBesides Python, if you go the standalone way and PyCharm, you will also need a few packages like numpy, pandas, statmodels, matplotlib, openpyxl/xlswriter, scipy - maybe others depending on field (numpy.org have a decent overview of packages for different fields of usage). Anaconda comes with some of these packages already so that is more plug and play than the standalone route.\n\nFrom there, just start trying out doing known R analysis flows and replicate these in Python. \n\nElse a lot of good info can be found on YouTube - Corey Schafer's channel, as others have noted, is a gold mine for python data processing."", ""I am in a similar situation. I use VSCode and Anaconda. I bought a 12 Udemy course for data science in python. It's kinda boring because programming is kinda similar in the languages I have learned yet, but it's good to get to know the libraries. You can do this, it's not that different in reality."", 'I would definitely avoid anaconda and learn how to use pip and environments without it. Then consider using anaconda if your work uses it.', ""Anaconda for sure. Create the environment for your specific project. It depends a bit of your working routine but if you have to test something local and then full scale on a cluster is a good practice to use anaconda. Pycharm has a direct ssh option to run directly on remote machine so if you have a specific computational allocation for development it's better to directly create the environment there. Spyder for me works fine, it's same as Rstudio. Do not use Rstudio for python. Do not. Strictly coding side learn the most important concepts of python, it is more object oriented than function oriented. Learn the most important libraries like numpy,pandas then the specific libraries you need/use for your tasks. Just my 2 cents."", 'TBH, I think GPT is going to make most python and other coders obsolete. I think the future of data analysts is more about identifying the right problems, find/compile the right data set, ask data questions, and gather insights from the data to assist business decision making. I think instead of diving right into python, you can ask your boss if you can spend some time to understand the business and the role you have first.', ""For starting out or occasional python use, I'd recommend sticking with Rstudio. If you're going to use python a lot, you'll probably want to switch to VSCode eventually."", 'As some said, Anaconda is a really good distribution of some of the more important packages that you would need for data, including a python environment widely used as Jupyter Lab (or Jupyter notebook they are pretty much really similar in a lot of aspects), but using colab I would said is a good option, it has a bunch of libraries already installed and you can install a lot more and it would not use your PC space.\n\nAnd talking about Python itself I think that if you have experience using R you already have some logic programming, so i would learn to apply that on a python way, you can use for on a really similar structure or list comprehension, something unique of python i believe, but in general if you have the logic learning the syntax is the easy part. I also would recommend using pandas, seaborn or matplotlib, numpy, scipy, and statspy, a lot of these were inspired under R concepts, data frame, ggplot, etc, so you are probably going to feel comfortable using them', 'If you are proficient or decent in R you can learn Python pretty quickly for a data analyst job. You likely wouldnt need the complicated stuff. \n\nDownloading python first time can be a pain, use Anaconda and follow a guide. Use Spyder IDE its most like RStudio. \n\nThen start with numpy (for vectors) and pandas libraries for manipulating data. Then learn sklearn + statsmodels for the typical models you will need. And seaborn+plotnine for plotting (the latter is like ggplot2). matplotlib you will have to import but its a pain to use for plotting so I recommend those above wrappers around it. \n\nAlso look up basic OOP (object oriented programming). You probably wont need to write your own classes for a DA job but just to know how the existing ones work in the above libraries.\n\nI agree with trying to replicate analyses you have done before with the above libraries', 'Codecademy is really good for getting up to speed', 'Came here to say this. First thing to learn in Python, after the syntax and basics at least, is what you know how to do using other tools.', 'R studio runs python now', ""> DE wise someone already mentioned Spyder Spyder is more or less an Rstudio clone for python data science.\n\nYou don't need to use an RStudio clone. You can use the real thing. RStudio supports Python now."", 'Thank you, i tried spyder and really seems Rstudio', 'There is a ggplot package in python too if you want to keep doing your viz that way :)', 'In my experience with SAS coding, chatgpt has 50% change to say something that does not exist in SAS', 'Across languages, VSCode is by far the most popular IDE: https://survey.stackoverflow.co/2022/#section-most-popular-technologies-integrated-development-environment', 'It does but I personally feel weird using it for Python. I have an R mind and a Python/Julia mind and the IDE influences that. So I use VSCode though id recommend Spyder for a beginner']"
[Question] What's the Best Approach for Modelling Movie Reviews?,"Basically, it's a y is in [0,1] problem.

I've looked this up on stack and I'm currently reading [this vignette on Beta Regression as result](https://cran.r-project.org/web/packages/betareg/vignettes/betareg-ext.pdf) but I'm wondering what my other options are. This is particularly important because the vignette assumes a greater competence with the mathematical aspects of statistics than I personally possess. If indeed beta regression is my best option, does anyone know a good free resource pitched more towards doing rather than understanding?

Beta regression does seem reasonable-ish. The first sentence notes that we're stuck in (0,1) but it's obviously remarkably unlikely that data which is the mean of discrete votes (typically assumed to be in {0,1,2,...,10} but arguably more likely to be in {1,2,3,...,10}) is ever going to be either 0 or 1. What I believe to be a random sample of movies from IMDB sort of has a IMDB ratings that follow a Beta, though it uses IMDB's weighted average procedure rather than the straight arithmetic mean, so that dataset is much more discrete than the tiny dataset I'm interested in (just 23 values).",11vjklj,FrameworkisDigimon,1679227354.0,1,1.0,"[""Much more important than the functional form or distributional assumptions of the regression model are variables you're considering to predict with. The time spend trying to decipher a note on beta regression is probably better spent on thinking about why movies get better or worse ratings and gathering data this  - unless this is a homework type problem and the real goal is to understand regression models for fractional outcomes"", ""I am interested in trying to imagine what a defunct survey's results would look like if it was continued past its last instance (in late 2019). The predictors are also movie reviews (i.e. IMDB, Letterboxd, Metacritic, Rotten Tomatoes) that don't have this time restriction.""]"
[Q] How to gauge performance for data sets with varying ranges,"So i play this fantasy game called Dream11 with a group of friends and they don't really tally our points gained properly for the leaderboard. So we decided to rank ourselves by tallying the points ourselves.

How this works is everybody makes new team every time there's a new game, over the course of 70 games, and we get points based on the performance after every game. Now the issue with simply tallying up all the points is that some of us forget to make our team before the game, so we miss out on the points for that day. To account for this, we decided to use Average points instead as the metric. The issue now is that the range of points that can be scored in a game always varies. For example, the highest scorer in our group for the 1st game won with 412 points, however the highest scorer for the 2nd game won with 665 points while the lowest scorer for that game got 425 points (which is higher than the best performer in the previous game). What I want to do now is standardize/normalize the performances each game and have a performance index that's better than average so we can properly gauge who's performing best.

a few things I tried are:

1. Used the highest scorer's points as the base/denominator, and divide everyone's points by that base. After doing that for every game, I took an average of the calculated values over each game they played and considered that as the performance determinant.
2. Used the X-Xmin/Xmax-Xmin formula, where Xmin is the lowest in the group, Xmax is the highest, and calculated each persons normalized points, and then averaged all the points they gained for the games they played.
3. Used X-Mean/Standard Deviation, and followed the steps above of averaging all the points for the games they played.

All 3 methods give slightly different results, for some people the ranks remain same across the board but not all. I'm very unsure what the best method is or what the best way is of going about this. I'd really appreciate some expert help on this. Please let me know if i didn't explain the situation well so i can clarify things if required!",11vjic0,MeganeReviews,1679227200.0,1,1.0,[]
[Q] Normal Approximation for Log Normal distribution?,"Let's say, I am trying to extract some inference ( an estimate for a quantile) from a lognormal distribution and for reasons, i am more familiar with normal distribution. Is it possible to transform a log normally distributed random variable 'x' to normally distributed (taking a Log of  'x')? .And would the quantile estimate from the  transformed normal distribution be a good approximation the original log normal distribution if scaled/transformed back?",11vgzam,Aggravating_Data1489,1679219142.0,2,1.0,"['Quantiles are equivariant to monotone transformations. That means, for example, that the exponentiated 10th percentile of log(X) is the 10th percentile of X. So you can estimate the quantiles of the log(X) normal distribution and exponentiate them to get estimates of the quantiles of the corresponding log-normal distribution.\n\nQuick illustration in `R`:\n\n    exp(qnorm(0.1, 1, 2))\n    [1] 0.209485\n    qlnorm(0.1, 1, 2)\n    [1] 0.209485\n    qlnorm(0.1, 1, 2)', ""> Is it possible to transform a log normally distributed random variable 'x' to normally distributed (taking a Log of 'x')?\n\nYes. That's exactly how most people tend to do many calculations involving the lognormal\n\n> And would the quantile estimate from the transformed normal distribution be a good approximation the original log normal distribution if scaled/transformed back?\n\nQuantiles transform exactly, not approximately.\n\nHowever, moments don't; If X=log(Y) is normal, with mean  then the mean of Y is not exp() ... it is the median though, because quantiles transform. See the wikipedia page on the lognormal for details on mean, variance etc."", ""Thank you for the reply. i wish to ask, is it so, with exact accuracy? As upon further investigation, it seems like an approximation which ,to be viable, needs to satisfy certain conditions, unless I'm interpreting it wrongly. Please look here : https://math.stackexchange.com/questions/525179/normal-approximation-to-the-log-normal-distribution"", 'Thank you so much, i understand now.', ""The post describes an approximation of the normal to the log-normal. I was talking about taking the log of the log-normal to make it normal. These are two different things. Reading your question again, you're also talking about log-transforming and back-transforming."", 'Much thanks, i seem to have been confused by the wordings. Would it be much to ask why the reverse is not true? As in what makes it a one-way, lognormal can be mapped to a normal distribution preserving the quantiles, whilst the other way around we would have to account for the transformation and calibrate the quantiles?', 'Not so, the reverse *IS* true. Monotonic transformation works both directions.\n\nAs COOLSerdash was already explaining (but perhaps here in a little more detail), the math.SE  post you linked to has nothing to do with transformation from a lognormal to a normal by taking logs,  but instead with the fact that if  is very small the lognormal itself may be treated as approximately normal *without transformation*. Aside some specific purposes, there\'s no need to approximate though, since you can take logs. (The ""only 0.08"" in the post there is amusing. A 0.08 error in a tail proportion of... oh, say 0.05, for example, would be disastrous. Assuming relative error matters more than absolute error, it\'s a good approximation for much smaller  than that case in the post, as long as you don\'t go too far into the tails)', 'Thank you for the answer. Yes, i seem to have misunderstood the premise of the question in the post.Thank you for the excellent explanation , It makes sense to me now. Yes, taking the log would make much more sense as you explained.']"
[Q] What do you ask interns to do do?,,11veqy7,limedove,1679211405.0,0,0.4,[]
[Q]Weak Law of Large Numbers and Estimators,"I do not know if this is a dumb question but I am going to it anyway, If i understand it correctly the weak law of the large number says, that for big samples the sample mean value converges in probability to miu (population mean value) but then we learned how to get estimators for different things on a distribution, one of them being the mean value, so why do we need to do this if the mean value of the sample is already an estimator that converges in probability to the mean value of a population?

I have two theories:

1. The mean value is not necessarily 1/n of the sum of the x\_i for every distribution and that is why we need to know which one really is the estimator and that one converges in probability
2. There is not always an epsilon for every distribution that fullfils the weak law of great numbers so we need to find one that does it",11vedlk,Unhappy_Passion9866,1679210112.0,1,1.0,"[""> that for big samples the sample mean value converges in probability to miu (population mean value)\n\nClose, but not quite. Under certain conditions, in the limit as n goes to infinity, the sample mean converges to  (mu).\n\n>  then we learned how to get estimators for different things on a distribution, one of them being the mean value, so why do we need to do this if the mean value of the sample is already an estimator that converges in probability to the mean value of a population?\n\nVery often there's a more efficient way to estimate the population mean than the sample mean. Knowing that you'll get there if only you had an infinitely large sample size is no help when you can only afford to take a sample of 8 observations; in that case you want to get as much information from your data as you can.\n\nSo *if* you have a good distributional model, you can often\\* get a more efficient estimator of the population mean than the sample mean. e.g. if you're looking at a uniformly distributed variable between two unknown limits, the average of the largest and smallest observation will be much more efficient at estimating the population mean than the sample mean would be.\n\n\\* Not always, though; if your model is a member of the exponential dispersion family (Gaussian, Poisson, exponential etc etc), the most efficient estimate of the population mean should indeed be the sample mean.\n\n> The mean value is not necessarily 1/n of the sum of the x_i for every distribution\n\nYou seem to be confusing the sample mean with the population mean there.\n\n> There is not always an epsilon for every distribution that fullfils the weak law of great numbers so we need to find one that does it\n\nIf the other conditions hold, all you need is a finite mean.\n\nhttps://en.wikipedia.org/wiki/Law_of_large_numbers\n\nhttps://en.wikipedia.org/wiki/Expected_value\n\nhttps://en.wikipedia.org/wiki/Estimator"", ""1. It's a very simple proof that regardless of the distribution, if a population mean exists the sample mean is an unbiased estimator for it.\n\nE (sum x/n) = sum (E(x)/n) = n mu/n = mu\n\nI do agree when you go through all the laws of large numbers, method of moments, etc it can feel like it really goes on and on."", 'That really makes sense, thank you, I only did not understand one part, why did you say that I am confusing the sample mean with the population mean there? And to check if i understand it correctly, the mean value of a sample always converges to the population mean for big samples(a limit to infinite), but we might not have that size of samples so we need to find an estimator that with smaller samples aproximates to miu, is that right?', 'Thank you, but it is only unbiased under the limit right? for smaller samples that does not necesarily fullfil?', '> why did you say that I am confusing the sample mean with the population mean there?\n\nYou wrote a formula for the mean of a sample rather than either for the mean of a discrete or continuous random variable (or indeed anything more general).', ""Regardless of sample size, sample mean is an unbiased estimator for any distribution's population mean provided a population mean exists."", 'Thank you']"
[E] Science Forum: Ten common statistical mistakes to watch out for when writing or reviewing a manuscript,[https://elifesciences.org/articles/48175](https://elifesciences.org/articles/48175),11v6czs,Stauce52,1679186813.0,48,0.92,"['Why did they write a paper that is essentially just snippets from an introduction to statistics course textbook? All they did was paraphrase all the ""ethical statistical practice"" sidebars.  Some of these are probably not mistakes, if someone is doing them too--such as p-hacking lol  it started with mistakes and ended up just being unethical', 'Taken from the author response:\n\nThis commentary is written by neuroscientists to their neuroscientist peers and their trainees\n\nMany biologists took 1 stats course 15 years ago, and dont have a solid stats background.  With running stats becoming easier and easier for the layman, a lot of these problems are easy to overlook, I have personally seen it myself in genomics/genetics.\n\nThe goal here was just to talk to other neuroscientists about what they found to be important problems with how stats are being ran.', '>Why did they write a paper that is essentially just snippets from an introduction to statistics course textbook?\n\nWithout knowing the reason, now that I am dipping my toes into the LinkedIn world, my answer would be ""because they are padding their portfolio with low-value, on-target content to impress future employers.""', 'if they find it important, they should recognize these things already....  it is not bad to point them out, but some of these things are in the APA guidelines....\n\nLook up Amy Cuddy and how she got in trouble and the awful ways people reacted.  Both sides are bad.  We have to take responsibility whether we are acting without knowledge or trying to stop someone from acting without knowledge.  Both sides are unethical.\n\n--But these things are things you should know before you try... ignorance is not a defense to unethical practices... The latter part of this article talks about things that people do on purpose...  You would be ignorant to assume no one would see you p-hacking...  Especially if we are all statisticians and you are faking it...\n\nIgnorance is not an excuse, because you can ask someone else....  You could even go on here and ask people and they will give you honest answers... or if you have graduated from a university, ask your university...  even if you already graduated, they will still answer you....', 'fake it till you make it...  why ask an expert when you can pretend to do it yourself?', ""i'm guessing OP is possibly the author and isn't getting the response he wants\n\n*Oprah voice*: Someone might be the f'in AUTHOOOOORRRRRR!!!\n\nOP: *shrieks and starts pushing everything off the coffee table and everyone starts getting downvoted for being neutral*"", 'Id say because most of the time the experiment doesnt require robost complex methods that a biologist couldnt learn to, and because there arent biostatisticians hired that have the time to answer these questions for everyone.  Especially in small companies there might not even be a biostatistician.\n\nYes its a problem, and the authors of this paper are trying to address it in the way that they are able, by sharing their knowledge on an issue that is widespread enough that causes them concern.\n\nIm a little confused why you think this paper is a bad thing.', 'Im not the author, I just thought it is a valuable paper that could be worth being a required reading for grad students as they are mistakes I often see students make. Not sure why yall are so hostile about this\n\nI dont know what I did to merit you shit talking me either. Literally just catching up to this now', ""Heh. I'm laughing, now. TBF, I don't know if OP did what I said in my comment above; I'm just saying it's the kind of content cranked out by data scientists, etc. trying to get a fat portfolio for employers who might do a search for their name. There could be other, less depressing reasons OP made the post, as well."", ""And even if you had dedicated experts, you'd still want to know those basic things yourself so that you can ask them relevant questions instead of handing the whole ownership to them and ending up like this\n\nhttps://www.youtube.com/watch?v=BKorP55Aqvg"", ""I don't think it is a bad thing, but I think it is restating what any intro book says.  I don't get the point...  It should be on a pop sci magazine, not in a paper with an abstract.\n\nIf they don't have the statistical methods, they can do without them.  It won't be as valuable, but they can avoid harming themselves by faking statistics.\n\nThey can hire a clinical trial biostatistician, if they can afford to pay a neuroscientist.  I'm pretty sure a neuroscientist costs a lot more than I do lol"", ""I keep saying that is not what I am trying to say.  I said it is taught in intro statistics classes. We even go through cases where it happened and examples.  I know in my classes, I had to write papers discussing different cases where people used unethical practices or buckled down on mistakes they made.\n\nThe only reason I started making jokes is because I was being told I was against it when I am not.  I feel neutral towards it. I was just confused why there was a paper about the sidebars in a textbook lol  my comments aren't meant to be taken too seriously.  I actually did like the parts where they explained how to avoid the mistakes.  Some of those literally aren't mistakes if someone does it though.  They would be someone being sneaky and thinking they won't get caught.  That's why we peer review papers too, to catch that.\n\nIt's like when someone drops parts of their data (I think the paper mentioned this in passing). They should explain in their paper WHY they left some data out...  It may be better to even still do the analysis with the data left in *and* with the data left out, and clearly state their explanation so the reader can make their own judgement and understand why you did it.  --but you might want to shove it in an appendix or something.  That way it doesn't look like you are covering something up or being sneaky.\n\nIf someone did that and tried to hide the data that they didn't use, that is obviously not a mistake and is them being unethical.  So, that was the other part of my argument.  Some of these are unethical practices, depending on context.  Some of them, if the person knows how to do the test, they should already know that affecting the outcome of the test to get the results they want is wrong.\n\nSo, the author was painting with a pretty broad brush.  They aren't wrong at all, but they were lumping a lot of different kinds of things together. Some were mistakes and some could be seen as being done on purpose. The mistakes can be fixed, but things like p-hacking could be career ending.\n\nThese literally are things that you can find in any textbook though.  They usually give examples and show how it happens and why it is bad.  It won't be in a mathematical statistics textbook where you do proofs, but they will be in intro textbooks and ones about practical applications for statistics."", ""The other thing is that, say, in a clinical trial, the study itself is expensive....  their boss would definitely not be happy if they did things on purpose. A typo or a wrong method can be fixed, hopefully...  It can be revised and resubmitted with a note about what was changed... but some of those later ones might cause all the money spent on a study to be wasted....  That would make the people giving grant money a little mad.\n\nSomething happened similar to that in a Pfizer study recently.  They had to toss out half the data, because the people collecting the data did something unethical and the people overlooking the study noticed it and had to get rid of half the participants.  That lowers the power of the statistical tests, because they have a smaller sample size.  Luckily, they can go forward, but it didn't make them happy."", ""Why would literally anyone complain about another discipline working hard to make sure their members are usually statistics correctly? They're not allowed to work to gain knowledge about statistics? Only statisticians are allowed to do stats (the math that is literally the foundation for all other science)?"", ""that's not what I said""]"
[S] Feedback On First Coding Project (Regression),"&#x200B;

I'm an undergrad working in an analytical chemistry lab and I'm trying to teach myself python for statistical analysis for my projects.  Recently I made my first real coding project. I've only been coding for the better part of a week so a lot of it is patched together from a bunch of random sources. I finally got my code to work and output a decent graph, but I feel like I could improve the code a lot. It would be great if someone could look over my code and let me know what improvements I can make!

An area I would like to work on is making a textbox. I can't figure out how to use bbox with strings, I keep getting errors.

Another area would be streamling this code because I feel like there's a lot of clunky junk that just bloats everything up for no reason.

Thanks :)

[Code (pastebin)](https://pastebin.com/kXJzbwi8)

[Graph](https://imgur.com/FAa22QD)",11v2opk,Perfect_Ad_8174,1679177953.0,1,0.6,[]
[Q] How do I set up my linear mixed-effect model correctly in R?,"I have data on days in which the greening of trees happen across America. This includes meteorological and topography data, etc. I want to predict when the day of greening happens through a linear mixed-effect model by meteorological data and topography data being fixed effects while the states of America is the random effect. I have looked into how to conduct the linear mixed-effect model in R, and I tried to perform this, but the output looks strange. I have looked at many examples and [this](https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet) 'cheat sheet' on how to perform linear mixed-effect models in R.

Right now, when I for example plot the relationship between relative humidity and day of greening I get strong positive relationships in many states which makes a lot of sense in this study but in my overall linear mixed-effect model I get a negative variable for relative humidity, while a simple linear regression generates a positive variable for relative humidity as predictor.

This is how my models look (in R):

`lmm.reg <- lme(doy ~ postDC15+postDMC15+postFFMC15+postCAPE15+postPR15+postRH15+postTEMP15+postWS15, data = postSM, random = ~1+postDC15+postDMC15+postFFMC15+postCAPE15+postPR15+postRH15+postTEMP15+postWS15|NA_L2NAME, method = 'ML', control = lmeControl(opt = ""optim"", msMaxIter=1000, maxIter = 1000, msMaxEval = 1000))`

Why does this happen? And, do I need to set-up my linear mixed-effect model differently?",11v2d1f,Sauerkraut33,1679177174.0,0,0.5,"[""I'm sorry, my phone might show it a but funky, but did you put all variables in as fixed AND as random effects?"", 'No worries. I assumed that I, by setting the random effect as: `random = 1+fixed-effects|random-effects` in my `nlme` code, I would basically find a unique slope for each fixed effect variable per state (the random effect). So I would not say I have set the fixed effect as both that and random. Or am I wrong?']"
[Q] What are some must-have books for a data analyst/statistician in industry?," 

I work as a data analyst in a role where I'm routinely tasked with using regression analysis/statistics to assess a variety of business problems (I work in international development, so one example is comparing answers on surveys from people who receive services from our NGO to their broader community members to see whether their are differences in material outcomes for the two groups). Beyond this, I'm also involved in casual inference work with experiments the company is running, and I also will be doing more machine learning oriented work with potentially developing a fraud detection system. As you can tell, that's a pretty wide gamut. What books/resources would you recommend to someone in my position, particularly focusing on the practice of statistics in industry for business problems? I have an undergrad in statistics and I already own/am aware of the books on the list below:

\- Applied Linear Statistical Models (Kutner et al.)

\- Categorical Data Analysis (Agresti)

\- Statistical Inference (C&B)

\- Mostly Harmless Econometrics (Angrist)

\- Introduction to Statistical Learning and Elements of Statistical Learning

\- Time Series Analysis and its Applications (Shumway and Stoffer)

\- Statistical Rethinking

\- Bayesian Data Analysis

\- Regression and Other Stories

Thank you!",11v0up5,Exotic_Bowl,1679173742.0,89,0.97,"[""That's a decent set of books. Where do you feel you need some knowledge or reference material you don't have?\n\nI feel like I could suggest all manner of things you might not need.\n\nI'd probably at least add the book on bootstrapping by Davison and Hinkley though."", ""Great list, only must-haves that I would add are if you're working in their specific areas/applications, which are pretty self-explanatory from their titles:\n\n-\tTrustworthy Online Controlled Experiments by Kohavi, Tang, and Xu\n-\tForecasting: Principles and Practice by Hyndman and Athanasopoulos (free online at https://otexts.com/fpp3/)\n-\tInterpretable Machine Learning by Molnar\n\nThe above I think are useful for anyone in the field if they're working on any of those three problems/applications. \n\n For R users specifically I would add the following three:\n-\tR for Data Science by Wickham and Grolemund\n-\tTidy Modeling with R by Kuhn and Silge\n-\tText Mining with R by Silge and Robinson"", ""This book contains a lot of the kinds of insights that most analysts have to learn through experience and trial and error. It should really be way better known, it's a great resource. \n\n[Edna Ridge - Guerrilla Analytics: A Practical Approach to Working with Data](https://www.amazon.com/gp/product/0128002182/)"", 'If you work with time series data: Im a big fan of Hamiltons book (bible, really) on time series.', ""Some fine books on your list already.\n\nA couple additions that I come back to repeatedly are Milliken and Johnson's 3-volume set on *Analysis of Messy Data* (which volumes you need depends what kind of data you are seeing), and Gelman and Hill's older volume, *Data Analysis Using Regression and Multilevel/Hierarchical Models*: in other words, a whole book on the material of chapters 14-16 of *Bayesian Data Analysis*. As you have that already, you can decide how much you'd like to see more of it."", 'Here is one I love about relaxing some assumptions necessary for inference. Title is  Introduction to Statistics Through Resampling Methods by Phillip Good. I suspect a lot of data you will encounter in your field violate typical assumptions in traditional statistics.', ""great list, but I'd say none, there is always a competing book with similar information. I suppose I only have Categorical by Agresti on that list, but that does not disqualify me"", ""Being aware that a book exist and reading it and understanding is different. \n\nTou would already be an expert if you understood all the books you cited, why do you ask for more if you didn't even take the time to get interested in the ones you cited."", ""Thanks for the suggestion! I think the thing that I could most benefit from is a book that dives into how to best use statistics for glean valuable insights for business applications. I imagine something on the practice of statistical consulting could be up this alley. Like for instance, on a recent project (one mentioned in the post already), we had a survey dataset containing both people whom our company has served and broader community members. I conducted analysis to see whether there were differences in responses on the various questions asked from the survey participants we serve vs. the broader community members by using logistic regression (the dummy variable indicating whether or not a survey participant received services from us being the primary predictor of interest- I also included age group, gender, marital status, and education dummy variables in the regression too). Thinking through whether that was the best approach for the problem is something that I feel the more theoretic books don't provide a lot of help for."", '> bootstrapping by Davison and Hinkley \n\n[This](https://www.semanticscholar.org/paper/Bootstrap-Methods-and-their-Application-Davison-Hinkley/5a343015d755a9001a26287b7a92028b5d026617) one from 1997? I feel like there must be a more up-to-date reference for resampling methods. I mean, I know a good book is a good book and all, but this field has been moving really fast the last few decades. 26 years is a long time in the statistical learning methodology world.', ""That's useful. Unfortunately I don't know that I have anything that's a good fit for that, and certainly not specifically for business applications."", ""Bootstrap methods have been around since the late 70s; a book from about 20 years later is not bleeding edge.\n\nIf you have a more comprehensive reference that covers what that book does, please post about it, because it will help more than the OP. \n\nOtherwise, please do allow me to post what constitutes the best (simultaneously most practical and most comprehensive) reference I have seen.\n\nLook at the age of the OP's reference list. Many of them are far older, or if an older book with newer editions, they have changed little from much older editions. A classic is a classic for a reason."", ""Yeah fair. Closest thing I'm aware of personally might be [this](https://hastie.su.domains/CASI/)? Haven't read Davison and Hinkley myself but now I'm curious.""]"
[Q] Looking for statistics/probability books that follow the scientific literature/divulgation genre.,Hey! I'm just a grad student looking for ways to fall even more in love with my bachelor's and I would really like to know some book recommendations that deviate a bit from technical books.,11uxkh2,camilams,1679167579.0,3,0.72,"['I have read and enjoyed Nate Silver\'s ""The Signal and the Noise"", Judea Pearl\'s ""The Book of Why"" and Persi Diaconis\'s ""Ten Great Ideas about Chance"". The last book is about probability and the first two are more statistics focused.\n\nBooks I want to read but haven\'t yet are Nassim Nicholas Taleb\'s ""Black Swan"" and Stephen Stigler\'s ""The Seven Pillars of Statistical Wisdom""', 'Check Jordan Ellenbergs ""How not to be wrong"", David Spiegelhalters ""The art of statistics"". Both were super interesting and easy to read, with little technicalities. The art of stats has a dedicated part with more maths at the end. Love both of these books! You can also check other books by Spiegelhalter']"
[Q] Stats test for dissertation help,I need to do a stats test for my dissertation. I have done questionnaire data. What stats test shall I use to compare gender (nominal) with multiple dependent variable in the form of ranking data (1 bad - 5 good). Any help would be much appreciated.,11utrmr,lawatkins,1679159027.0,0,0.5,"['Is your ranking variable one item (question) ?  Or multiple items combined into a *scale* per se ?\n\nAre you designating one variable as the independent variable and one as the dependent variable ?', 'are you trying to do something like see if the mean answer value (y var) by each gender (x var) is statistically significant? or are you trying to run some sort of regression?']"
"[Q] Struggling with my dissertation, can anyone help?","Hi everyone, I'm currently very behind on my diss and am in desperate need of help. I want to do a Mann Whitney stats test to see if there is a difference between the amount of socialisation before lockdown and during lockdown. My data is oridinal from 'A lot (5) - Never (1)' and my sample size is 96 people. And I can't find a critical values table that goes up to 96. Any help would be greatly appreciated.",11usurf,lawatkins,1679156883.0,3,0.64,"['Are the data paired, so that the same person answered before and after ?\n\nIn general, these tests are trivially easy to conduct with online calculators, R, or other free software like Jamovi.  This might be a option for you to pursue, especially if you have other analyses to conduct.\n\nConover, Practical Nonparametric Statistics, has an approximation for the Mann-Whitney critical values for sample sizes greater than 20 (Table A7),  Can you just confirm that you are using Mann-Whitney, and not Wilcoxon signed-rank for a paired test ?', 'You should simply use software for like wilcox.test in R for running your test. Common practice is to use a normal approximation of sample sizes are greater than 50, but the software can also compute an exact p-value on the fly. \n\nCritical value tables are mostly an artifact of pre-computer age statistics and really shouldnt be used outside of early statistics education.', 'My suggestion is the last resort which I advise only if you really struggle: www.homeworkhelponline.net/programming/r-programming', 'ChatGPT Could be handy for this scenario.', ""If you can match  peoples during to their before, it's paired so not Mann Whitney\n\nIf you go off the end of tables, either use a program or use the normal approximation (adjusted for ties)"", 'I\'m not a statistician, but my first thought is that you may want to build a simulated set of data that\'s similar to your actual data.  Then use that to figure out how to analyze it and come up with your results.  This would help prevent you from accidentally choosing your analysis methods by seeing the results.  Figure out which one is best, then apply it to your actual data.\n\nI\'d also take a closer look at your literature review.  There should be something in there similar to what you\'re doing and then you can draw from the methods they chose.\n\nLastly, before going much further, consider hiring a statistician to consult with and get this nailed down properly.  As Fisher is said to have said: ""To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.""', ""I'm not sure if it's paired. But the questionnaire was carried out by the same participants. I do think they are independent variables though. So is that not paired?"", 'Ok thank you, appreciate the help.', 'u/JuneauTek , what would ChatGPT be useful for ?  Determining the correct test ?', ""Well, the data aren't really independent, because the answer someone gave on the before survey is likely to influence the answer they gave on the after survey.\n\nThe dispositive question now is, Did you record the identity of the participants such that you can match an individual's before response to their after response ?"", ""One thing I realized might be unclear: Mann Whitney and Wilcoxon tests are very closely related, so R uses the same function to do both. The wilcox.test function uses the argument 'paired' set to either TRUE or FALSE to distinguish between them."", 'Thanks for the clarification', ""If you don't know how to code, give jamovi or jasp a try, they are free to download.""]"
[Q] For those familiar with the Rethinking R package: Looking for help with specifying a Bayesian logistic regression model,"Hi all,

I'm attempting to specify a Bayesian logistic regression model using the rethinking package in R. This is for a research project I am working on. The posterior estimates from the model I fit seem very off from anything that would make sense and I'm having trouble understanding what I am doing wrong. I didn't post the results or my code here because I'm not sure if this would violate the rules. If this would be okay to post please let me know. If not, are there any forums focused on the rethinking package where I can get help? From someone new to Bayesian statistics, thank you!",11ursc6,zwildin,1679154280.0,6,0.8,"[""BRMS is really easy to use if you're looking to try an alternative package."", 'Hi, I have some familiarity with that package. What are you using to fit the model, `quap()` or `ulam()` ? Feel free to message me', '[removed]', ""Thanks for letting me know! I'll give it a try with BRMS too."", ""Thanks. This was using quap(). There weren't any errors. I think I probably just did something in the model definition that I'm misunderstanding how to use properly. I'll message you with more details in a moment.""]"
[Q] Finding raw data for data analysis course,"Hey, 
So i got asked by my prof to prepare some example data analysis (from like whiskers plots to anova) for one of his courses. 

The challenge im facing rn is finding suitable raw data that is free to use and in the domain of the automotive industry. Can be anything from questionnaires about autonomous driving to idk, just has to be vehicle related

Anyone an idea on how to find something like that? I looked at statista but they are alle paywalled. acea only publishes the results of their analysis and i couldnt find anything suitable on data.world

I would need 3-4 data sets",11ume24,Such-Coast-4900,1679140403.0,5,1.0,"['There are a couple of small datasets in the base R software that you can access.\n\nIf you go to [https://rdrr.io/snippets/](https://rdrr.io/snippets/) , you can run each of these lines separately.\n\nE.g *cars* will give you the data.  E.g. *?cars* will give you the description.  The output is a little funky. You can ignore the *Examples*.\n\n*cars*\n\n*?cars*\n\n*mtcars*\n\n*?mtcars*', ""Try to look for datasets published in journals, be sure to check the SI. You can also browse journals like Data in Brief. It's open access."", '[This dataset](https://dasl.datadescription.com/datafile/cars/) may be suitable.', 'Openml.org', 'The world values survey is very good for this kind of thing. \n\nhttps://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp', ""P.S. There's another one you can access with:\n\n*library(ISLR)*  \n*Auto*\n\nlibrary(ISLR)  \n*?Auto*""]"
"[Q] i feel like a failure, I really need some help.","Im writing this as i have gotten rejected from all the MS stats programs that i applied to, and I suddenly feel like i dont know what to do. Im really passionate about Math and it has been my dream for the past couple of years to pursue a formal education in this field.

I am a computer science major with a 3.25 gpa, and have 3 years of experience working as a Quant Researcher in a hedge fund. I have some academic projects in deep learning as well. 

I applied to schools ranging from ~30th to ~170 ranked universities on QS. And have gotten rejected from all of them(around 6). 

I feel directionless suddenly and im not able to focus on anything at all.

I dont want to give up on my dream but i feel like I already gave my best shot and even if i try next year Im not sure what Im going to change about my application.

I am desperate for some help, i am not even sure if this is a right sub for this but i feel like crying and hoping to find something of value. If this is not the right sub, please direct me there. Thank you in advance.",11uj1j9,ineed_somelove,1679130499.0,68,0.95,"['While I dont have advice for your next step, I still want to send good vibes. Remember, this, too, shall pass.', 'Youre not a failure, but if you intend to re-apply next year you can ask the graduate admissions people from those courses for feedback. I think your work experience is impressive. Also worth mentioning that Masters programmes will differ in countries and institutions about their aims / goals - some are intended as pathways to academia and further learning as a continuation of your bachelors, while others are more explicitly designed for upskilling people with work experience, and a way to transition into a new field / career. Youre looking for the latter. Either way your best bet is to find people to speak with at the places you want to apply to, be upfront about your qualifications and why you want to do a masters, and theyll be able to tell you whether the program is right for you or not. They wont tell you directly dont bother applying, you wont get in, but if your explain your background and ask something like are there current students and recent graduates on the course who had a background similar to mine? youll be able to read between the lines. Its not about being a failure or success, its about being resilient and having the ability to deal with setbacks. All the best to you.', 'The GPA is too low to be competitive. Retake the math courses and get A grades in them. That will likely improve your chances. Source: Im a math/stats professor.', ""Odds are the programs that you applied to require a bachelor's degree in mathematics/statistics. \n\nI understand that you are passionate about mathematics, but if these programs require formal qualification in it, it is for good reason. \n\nFor example, I have a difficult time imagining someone doing a measure theoretic probability course in a master's degree without the appropriate background knowledge (hopefully) obtained in a bachelor's degree."", 'Being passionate about something means you already know who you are. Trust yourself.', 'You work as a quant researcher for a hedge fund - they only hire super smart people. You are not a failure!!', '[deleted]', 'I had a 2.9 in undergrad. My first two years I almost failed out, then did a lot better my second two years. Ill attribute it to working 24/7 and barely surviving the juggling act of school and work. Anyway, I graduated and got a job as a data analyst in a research lab and worked my ass off. As a result I was able to get some pretty good letters of recommendation. Then I retook some classes I did poorly in. Its called a post bacc program basically where unis let people who have already graduated enroll. I then was able to get into a grad program with these LORs, professional experience, and post bacc classes.', 'I have an engineering background too and experience in software engineering . Bein in a similar boat I can understand, I also want to study maths but I understand my background doesnt not provide much incentive for admissions committee to consider me . Dm me if you wanna talk z', ""Hey OP. I've been in a similar place and I remember the panic that set in, as well as the existential dread that came with such rejection. Just remember that life is long and that your self worth is not determined by your job, GPA, or salary. In 50 years, what you'll look back on won't be that you got rejected, but how you persevered."", 'Try another city, and even maybe.. another country where someone who could do this kind of field of expertise is rare??', 'Did you have any adversity that led to your GPA? If so, did you write about it? What about your work makes you want to work in statistics?\n\nYou cant change your GPA, but you can tell a fantastic story.', 'Move on from this failure and try again but differently. Maybe open yourself to lower rank programs. Also if possible take some upper division math and stat courses, many colleges allow this. Try real analysis, abstract algebra and a calculus based statistics course; do well in those and Im sure youd be in a much much better place when applying again.', ""Please provide an example of the syllabus of the courses you applied for, together with the maths courses you have done up to now.\nIt's hard to know whether you are unlucky or applying for the wrong courses.\n\nMaybe you should be applying for courses aimed at engineers"", ""Rejection sucks and throws everything haywire. We've all been there even though we always like to pretend we're the shiz and put our best food forward. Take it easy on yourself these next few days."", '> if you intend to re-apply next year you can ask the graduate admissions people from those courses for feedback.\n\nI agree totally.  My story is similar, but different. I applied to several Ph.D. programs and got accepted to them all.  Most of them also offered me scholarships.  I went and sat down with the program director of one that didn\'t offer me a scholarship, and asked what I was missing.  He said that with my poor math background, I would probably fail out, but they let me in anyway (""We are happy to take your money"").  He gave me a list of 4 courses I should take. I then talked to some of the other programs, and they said that they hadn\'t caught the fact that I wasn\'t mathematically prepared for their programs, because my GRE Math score was almost perfect. I went back to school for the next summer, fall and spring and took 6 more math courses, got a scholarship to my preferred program the next year, and later found out that I was admired by the faculty there because I was ""the only person who had ever actually listened to their advice."" Believe it or not-- if I had had any less preparation, there is no way I would have made it through the program. Half of the people I started with never finished.\n\nIn courses like the Math Stat sequence, you discover the holes in your mathematical education very quickly. It is very hard to learn the stats when you are scrambling to also learn or re-learn the math. To test yourself, a book like Hogg/Tanis/Zimmerman should be ""light reading"" and a good review of things you are already comfortable with, and a book like DeGroot/Schervish should contain new ideas, but the first 3 chapters should be accessible to you. Look at the notation and mathematics in Chapter 3 in DeGroot/Schervish-- think about how comfortable you are parsing through the definitions and theorems and making sense of them. Personally, I struggle with this kind of thing, but can do it given enough time.\n\ntl,dr: Not getting what you want can be a blessing. Being OVER prepared is your ticket to success. The least prepared/qualified students in good programs wash out after a demoralizing struggle-- MUCH more demoralizing than not getting in.  If they do graduate, they do not get recommended for the best jobs or Ph.D. programs.', 'I would try that.', 'Im already out of my college, how can i retake them?', 'I did check that, the programs did have some prerequisites in Linear algebra and calculus, which i had taken in undergrad. \n\nThey didnt have a requirement for bachelors in math, but maybe they had such filters internally?', 'MS programs generally dont have measure theoretic probability, thats PhD.', 'I mean this. One of my dreams is to be a quant researcher.', 'My whole reason to apply for stats programs was to get into a very rigorous curriculum', 'I applied to applied statistics, U Mich for example, which didnt have any prerequisites. I applied to similar courses', 'I have actually done degroot/schervish, probability and stats. Ive made extensive notes out of that book and solved almost all the exercises. But i dont have a proof of that on paper', 'Many courses could be done at a community college. If theyre upper division, Id do a state university. Most stats programs require Calc I-III, Linear Algebra, a calc-based probability course, and at least one applied statistics course. Good programs usually want one semester of real analysis to ensure you have the mathematical maturity to learn probability theory. If you can get an A in all the courses, it will show an admissions committee that you are serious about graduate studies and you will likely get SOMEONE to admit you.', 'Enroll as a ""non-degree seeking student"".', 'Check some of the universities you want to go to.\n\nUIUC offers courses through NetMath, they can be done online and asynchronously', 'What grades did you have in those courses?', 'UMich is EXTREMELY competitive. You need the background I mentioned Id you want to go there. DM if you have questions.', 'This one?\nhttps://lsa.umich.edu/stats/masters_students/mastersprograms/applied-stats-masters-program.html\n\nPrerequisites\nIt is strongly recommended that prospective students have a good background in calculus and linear algebra and have taken one course in probability, one course in theoretical statistics and at least one in applied statistics. Students who have not taken these prerequisite courses are generally required to take them in the first year of graduate study, with no credit toward the requirements for the degree.\n\n?', 'Thats good advice thank you', 'Of course community college/any public university as well, like the other commenter said', 'I had both C+(not so great)', 'Yes', 'I mean its one of the places i applied to, it was the top choice i applied to several other schools which are supposedly much less competitive', 'Thats most likely why you werent accepted. You need to retake those courses somehow (like a community college) and show you can master the material.', 'I was in a similar situation. I talked with the admission advisor for the school I wanted to go to and cut  a deal that if I retook calc and linear algebra and aced both I would be admitted.\n\nTry and get in contact with a person to find out why you were rejected and what it would take to get in. It will depend on the school and program size but you might get some leeway.', 'Alternative is, dont retake them, and apply to lower ranked schools that will accept you. That way, you can start a Masters NOW.\n\nJust depends what you want.']"
[S] Need help with figuring this out,"I am trying to create a data simulator for a entity ( stock trades ) where each entity has a attribute called valueDate , I am expecting two input parameters

Total trades : example - 1 million
Date range : example - 02/Jan/2023 to 09/Jan/2023

I want to know how to calculate the number of trades that belong to a particular valueDate such that it roughly follows a normal distribution. 


Example : 

Total trades for 02/Jan/2023 : 10k
Total trades for 03/Jan/2023 : 20k
Total trades for 04/Jan/2023 : 30k
.
.
.
Total trades for 09/Jan/2023 : 10k

These numbers should add up to the input :
 1 million",11uggxm,tchikennMayn,1679120893.0,5,1.0,['Very unclear to me what should follow a normal end even less clear why.']
Can you get jobs with an MS in statistics nowadays? [Q],"Im in an MS program in statistics, and have the choice of going into a phd program. My goals are to be within the quantitative research space in finance post phd (if I go for phd). Many of these roles in banks also are allowing MS candidates (they say at minimum MS). However, outside of quant finance, and in general for statisticians going to industry, are the MS level statisticians still able to get high paying jobs? Should I consider a PhD if I want to get into industry in finance? Truthfully I havent really thought about doing research, but Im wondering how much the market is saturated with MS and if a phd is needed in statistics to secure a good role these days in the industry.",11ud3ar,Direct-Touch469,1679109918.0,12,0.8,"['You can easily get a Data Science role if you know some programming. The DS on my team in a large tech company dont even do machine learning, they just run regression.', 'I just hired a BS with 2 YoE for a $110k position.\n\nId imagine MS holders do okay.', 'Yes, at insurance companies, in the actuarial sciences group.', 'Yes absolutely lol', 'Please dont do a PhD just for the job prospects afterwards! If you are not invested in the process of research and struggling with ideas it is a miserable process.', 'Phd is not required, but it helps quite a bit. I can mostly speak to Pharma. Generally entry level statistician roles require either a PhD or MS + 5 years experience. What they count as experience is going to depend on how desperate they are for statisticians, which, right now, is moderately desperate.\n\nMost MS in pharma tend to be funneled into stats programming. That pays moderately well, but is not a very scientific or independent research type role: statistician tells you what tables to make, and how to make them, then you program it in SAS.', 'Yes, though PHD will open more doors with the trade-off of a several years at a lower income so thats worth bearing in mind. For reference Im a Applied Stats MS with a data science job mostly building machine learning models. You definitely need to be good at programming as well though, its becoming more and more like an engineering job if the ML route is where you want to go.', 'At mine, we just do plots', 'lol', 'Which country?', 'If you are okay with telling, what kind of things are looking for in candidates?\n\nProgramming, machine learning, orrr', 'Its funny because I have a M.S in statistics and I work this exact role for one of the largest insurance companies in the US', 'But I want to get paid and have a job man.', '\\> entry level  \n\\> 5 years of experience  \n\n\nExcuse me?', 'What kind of doors open with a phd vs an ms?', 'US', 'You can definitely do that without a PhD. The question is, do you want to primarily engage with poorly scoped projects and have to identify how you can add value in general? (Because that is sort of what a PhD gets you)', 'Phd OR (MS + experience). Its entry level jobs for PhDs. The experience does not need to be the job, just the industry. I got it with an MS by working in biomarkers.', 'Im most knowledgeable in the ML space, and in there its mostly the ML research focused roles that often want the phd even in industry, which is more a niche career but one that seems pretty desirable to a lot of folks. To a lessor degree some of the jobs doing what I do as an ML practitioner will ask for it (annoyingly) so youll have some more postings to choose from. Outside of ML specifically as some other folks mentioned it seems some of the more interesting/higher level roles in pharma/government are gated by the PhD. Also in industry many casual inference jobs seem to want the PhD. If you want to make a career out of academia naturally that will require it as well.', 'I just want to do data analysis and statistical modeling, and not sql and dash boarding all day', 'Gotcha. Yeah see the thing is idk if I even like the ML research roles enough to justify doing a phd. Who knows, I may do a phd, realize I hate research, and then go into those roles and still hate my job. I think I just want money, truthfully, and am willing to just make money in those 6 years than do research for 6 years. Some other reasons to, I just dont want to look back on my twenties saying it was taken up sitting in academia.', 'There are very few jobs where the majority of your day is just statistical modeling/data analysis (you are often engaging, in addition, with the upstream and downstream pieces that add actual value). That said, there are many many jobs where you get to spend a bunch of time analyzing data without a PhD I think its possibly easier to spend your time analyzing data without a PhD rather than with\n\nAlso, just to be clear, data manipulation and cleaning is always a huge part of the analysis pipeline', 'If this is your true interest, look for bio stats jobs at hospitals or universities']"
[Q] How to compare two means from non-normal distributions?,"First time posting here, so please let me know if I break any rules / question is dumb lol

As a part of my research, I'm analyzing two sets of ambulance response times in a state: one rural, one urban. I want to test if the urban times are longer (that is, higher). I have a list of mean rural response times (from about 63 towns) and a list of urban response times (from about 97 areas). I have found the average of both lists.

Now, I can't compare the two using the students T-test, because I can't assume both distributions are normal. Indeed, when I did a Shapiro-Wilk test on both, the rural dataset was normally distributed, but the urban one was not (it was skewed right).

How do I proceed from here? I thought Whitney test could only be used if both distributions were not normal and also skewed the same way?

Much thanks!

Edit: Thank you everyone for the responses, it helps a lot!",11ubbyj,ImperialCobalt,1679104832.0,3,1.0,"['The most practical options are to stick with the T-test, which is robust to normality, or use a bootstrap approach to estimate the sampling distribution. Non-parametric tests are an option as well but they dont test precisely for the mean.', ""> How to compare two means from non-normal distributions?\n\nFinally! Someone who doesn't change their hypothesis to no longer being about means the second they hit a non-normal distribution. Congratulations. There is quite a lot that can be done (assuming it's needed at all).\n\nOne question though -- are there any times where the waiting time grew so long that the attempt was essentially abandoned, so that we don't know how long the waiting time *would* have been, only how long it was before the waiting ceased? (or indeed if the actual waiting time was only known to be longer than some specific time for any other reason). This situation -- where a known waiting time is less than the actual (but unknown) time it would have taken -- is called [*censoring*](https://en.wikipedia.org/wiki/Censoring_%28statistics%29#Types), specifically, such data are *right censored*. You can neither omit the censored times, nor include them as if they were the actual waiting time - such a situation requires special techniques (which are nonetheless easily dealt with in a good statistics package).\n\nIssues resulting from that question aside, there's two cases to consider, and a third resulting from those:\n\n(i) you have a plausible model, whether from theory, previous studies, pilot data, expert knowledge, understanding of similar variables, or whatever else. Response times are a decent candidate for this sort of treatment, because many similar response time data sets exist and we know things about response times -- they're necessarily positive for example, they're almost certainly going to be right skew, heteroskedastic, and so on.\n\nIn this case you can do a suitable parametric test, such as via a likelihood ratio test, Wald test, score test for example. In some cases the model might be in the exponential family (e.g. exponential or gamma wait times), in which case a generalized linear model will be straightforward. Another possibility is survival regression models which include many right-skewed distributions explicitly designed for duration data, including the Weibull, for example.\n\n But also see (iii)\n\n(ii) you don't want to assume any particular distributional model\n\n In this case you can do a form of permutation test. I'd tend to suggest basing it off the ordinary two-sample t-statistic.\n\n You could instead do a bootstrap test; this has an advantage and a disadvantage. The disadvantage is that it no longer gives you the significance level guarantee (the test is not exact, though it gets better as the sample size increases); the advantage is that you can make one that relies on slightly weaker assumptions than the permutation test.\n\n They are both similar in that they rely on a form of resampling of the data, but under different basis.\n\n(iii) the test statistic you get from an approach like (i) might also be used as the basis for a permutation test like in (ii), if you want a little extra exactness guarantee for the case where  the sample size is too small for the asymptotic distribution to kick in, or in case you want a little coverage for the possibility that the model might just be a tad off.\n\n----\n\n> I can't compare the two using the students T-test, because I can't assume both distributions are normal.\n\nThis is not necessarily so; your sample sizes are reasonably large, so a certain amount of skewness is not necessarily all that much of an issue; your significance level in particular should tend to be reasonably close (and if it is off, it's likely slightly conservative rather than anti-conservative).\n\n> the rural dataset was normally distributed\n\nNo it certainly was not. It is literally impossible for it to be normally distributed, because all normal distributions take values less than 0 with non-zero probability but waiting times can never be negative! Testing a hypothesis that is *certain* to be false is pointless, you already know H0 is false, but it's also useless, because reject or not, it doesn't tell you how much the test would be affected by the non-normality you have (probably not too badly level wise, perhaps a little more power wise).\n\nIt would be better to spend the time to investigate the behavior of the t-test under similar conditions than waste time doing a pointless test of normality.\n\n> I thought Whitney test could only be used if both distributions were not normal and also skewed the same way?\n\nNeither of those things are true. Having a normal distribution is not a problem (albeit you don't), and the *data* don't have to have the same exact shape. What you need is that the distributions would be the same *when H0 is true*. The shape could be different when H0 is false, without any necessary harm to the test whatever.\n\nThe *problem* with the Mann-Whitney test is that *it does not test your hypothesis* which was about means. You should test the stated hypothesis, not a different one. (You might be disturbed by the fact that it's quite possible for the Mann-Whitney to reject in the opposite direction to the direction the means differ in; I've seen more than one researcher in confusion over this, when they could have just tested the original hypothesis and remained unconfused.)\n\n(You mean Mann-Whitney by the way, since it comes form Mann & Whitney 1947, though Wilcoxon 1945 - and arguably some other people as well - should get precedence.)\n\n---\n\nIf it were me, I'd be inclined to use a gamma glm (barring that censoring possibility), but a permutation test should also work just fine."", 'There are a lot of good alternatives already describes by other people. The simplest one, in my opinion, is to use a permutation t-test. That way, you can obtain p-values robust to non-normality, while being able to measure the effect size.\n\nIn R, you can use the `perm.t.test()` function from RVAideMemoire package.', 'T-tests are still quite well behaved even when data are not normally distributed. If you really are specifically interested in means, I would suggest just going with the t-test.', ""Thank you for all the information! I'm not a statistics person by any means (as is evident I assume) but you actually explained everything quite neatly. Your assumptions about response time is certainly true (positive, right-skewed), but let's say I were to want to go with a permutation test. As someone who's not super experienced with R, would u/civisromanvs approach below work?\n\nEdit: Realized that I didn't answer your question. The real answer is that I can't be certain, but I'm fairly sure that censoring isn't an issue here. Even if it was, the problem is that each town's mean response time is in itself a mean from a dataset, and I don't have access to those base datasets."", ""u/Kroutoner , in general, this is bad advice.  It's true that *t*\\-tests can perform well with other distributions.  But it's also true that there are many many situations in which the *t*\\-test will not perform well.  There's a reason why every textbook in the history of world addresses the assumptions of a given model.  It's madness to use a hypothesis test while simply ignoring the assumptions of that test."", ""> As someone who's not super experienced with R, would u/civisromanvs approach below work?\n\nI'm not familiar with the function as implemented in that package -- I always write mine from scratch, it's literally a few lines in R, takes about as long to write as to run it. However, I'd bet that calling that perm.t.test function works exactly as one would expect that it should (and so, should suit your needs). There's also a function with the same name in the package MKinfer and another in Deducer (and no doubt other packages with similar functions)\n\nOne thing that concerns me is that the documentation doesn't make clear which two-sample t-test it's using (the equal variance one or the Welch one). It's possible to double check by trying some examples which don't have the same n's in both samples (edit: which I have just done -- it's using the ordinary two sample test, which makes sense given the exchangeability assumption that's already required).\n\nI just tried the function on the example code (the unpaired one, naturally) at rdrr.io/snippets. It seems to work okay. However, I'd implore you to use more permutations than the ludicrously small number in the example code. I'd suggest at least 10000, but I normally do at least ten times that myself.\n\n>  the problem is that each town's mean response time is in itself a mean from a dataset, and I don't have access to those base datasets.\n\nOkay, that reveals a different issue (though I should have guessed this would be the case already): if your values are themselves means, be aware they your observations (those mean response times) are all differently precise, since they're based on different sample sizes. *If you know those sample sizes, that should be relevant to the test but it would make the permutation test impossible, I think.)"", 'In this case I recommend the t-test, though I should have specified the Welch t-test should be used, which is the default at least in R, because t-tests specifically are remarkably robust to violations of the assumptions. \n\nThe textbook assumptions for the t-test are how it is derived, but it does not rely on the those assumptions for its validity. The t-test can be easily shown to be at least asymptotically valid under extremely general conditions. In practice t-tests are very often far more robust to assumption violations than alternatives.']"
[Q] Can I treat my ordinal dependent variables as continuous for the sake of a regression analysis?,"Hi all! I've consulted a research methods book by McNeil and checked on stats stackoverflow but I'm still struggling with this concept. Apologies for the rookie question. If it's too basic, please just direct me to a great resource!

I have medical data with multiple independent variables (all continuouspercentages or ratios) that I am trying to map to multiple ordinal dependent variables (questionnaire responses). 

1. Is it wrong to just run a multiple regression (unsure of linearity) and the dependent could have a number that's not possible (like 1.2 but it has to be 1 or 2)? 
2. If I suspect that some of the independent variables may overlap or impact each other, is that confounding and how do I check this between pairs of independent variables?

Thanks!",11ua1uo,Dark_Selah,1679101348.0,2,0.67,"['You need ordinal regression, likely with item-varying cut points and respondent-varying propensity. See [here](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html).', '1. Yes.  Just use ordinal regression, like MASS::polr.\n2. Calculate VIF on the independent variables.  VIF >10 indicates significant multicollinearity.', ""I'd be inclined to use an ordinal regression model like an ordinal logit or an ordinal probit.\n\n(It does seem ... unusual to try to predict questionnaire responses, though)"", 'Yes, you can treat likert scale questions as interval when they have more than 5 response values.', ""Doesn't seem unusual to me."", 'ah maybe ive got this wrong in my head my idea is how can certain variables indicate to us what someones well-being is like (in very loose terms). should my research question be flipped? im pretty much looking for correlation, hoping to do a mediation analysis later.', ""Probably not, no, but there is a concern there I'd like to raise (which relates to my earlier concern)\n\nSome issues aside\\* if you're just looking for correlation it doesn't matter what direction you look in anyway. For the mediation you might need to think carefully about the distinction between what you're measuring and what you want to model. Those IVs you're thinking about might make perfect sense as drivers of well-being, but that's not what you're actually doing here.\n\n\n\\*  The problem is you're not measuring the same thing as you are interested in talking about (this seems t be a very common problem). You don't know what someone's well being is from that instrument, you oniy know what they *reported* to be their well being (and that rather different variable measured by a fairly crude instrument). If they're anything like me, that report of well-being might be very different to their actual well being. I notice a lot of people seem to be pretty poor at figuring out what condition they're in; a few years down the track they might finally start to recognize they had a problem.\n\n If you don't conflate *reported well being* with *well being*, there's no problem, but if you're going to claim they're the same thing you better have a good argument handy for that claim.\n\nDepending on your instrument, it might *perhaps* make sense instead to take your mediation arrow-diagram\n\n [IV] ---> [mediator] ----> [Well being]    (leaving out the direct arrow that you hope is null or weak for now)\n\nand add a box that allows for some distinction:   \n\n [IV] ---> [mediator] ----> [Well being] ----> [self-report of well being]\n\nThe problem is, of course, that Well-being is unobserved in that diagram, which may require some additional techniques (or additional argument, such as why the other arrow might be drawn to that last box instead)"", ""That makes sense. Thanks heaps for this! The issue is often that there is no validated measure for things like wellbeing, so you're right, I'd need to be careful with how it's worded.\n\nAnd yes, we're not looking at what drives well-being. Rather, we're first looking for surrogate measures. I don't understand mediation analysis yet :( My supervisor mentioned it to be able to distinguish the actual variables of clinical interest.\n\nDo you have a favorite stats resource for grad students that will mean I ask less of these questions in future? Hahaha"", ""Sorry, not really. If you have some mathematics, I can suggest some resources to learn enough stats to start to answer such questions for yourself, but it's not quick.""]"
[Q] GLMM and correcting for type 1 error?,"Hi all! Would you recommend doing Bonferroni correction, or holm-Bonferroni (sequential Bonferroni) correction when multiple GLMM were run for the same experiment?

I have two experiments, A and B.  
*A:* Ran **two** GLMMs, one for each response variable. Both results show significance in my interactions, so I did a-priori custom bonnferonni post-hoc to determine where my differences were.

*B:* Ran **three** GLMMs, one for each response variable. All three results show significance in my interactions, so I did a-priori custom bonnferonni post-hoc to determine where my differences were.

the question is, how should I determine significance? alpha = 0.05 across the board for both A and B or for A: alpha =0.05/2 = 0.025 and for B: alpha = 0.05/3=0.016",11u6zml,sunshinesugar754,1679094160.0,6,1.0,[]
[Q] How should I address these outliers?,"I recently finished collecting data for an experience sampling study about emotions and social interactions. In the study, 70 participants answered 4 surveys per day for 10 days. During each survey they were asked questions about specific social interactions they had in the time since the last assessment One question, *which I included purely for descriptive purposes and is not a part of any of my analyses,* asked participants to report how long the interaction lasted in minutes. Several of my datapoints include absolutely wild values that make no sense (interactions lasting several hours or even days). Despite this, there is no indication that my participants gave false or invalid responses. I also strongly prefer to alter data to the next highest non-outlying value rather than deleting things, but I'm not sure what to do in this case.

I tried to examine histograms and boxplots, but since there are so many observations and the spread of data is so wide I can't distinguish where I should draw the line on data that is acceptable vs. needing to be altered (see here: [https://imgur.com/a/b81On8j](https://imgur.com/a/b81On8j)). And given the uniqueness of the study design there isn't literature to clarify that for me. I did calculate z scores but there are only 5 cases out of over 2000 that are >3.29, and based on that the next non-outlying value should be 600 minutes (aka 10 hours) which still does not make sense. Is there anything I can do to get a general idea of the average length of my participants interactions? I think if I could figure out how to get SPSS to list the points it identified on the boxplot in a table that would be useful, but I can't seem to find a way to do that.",11u4e8h,dammit_sammy,1679088531.0,1,0.67,"['Outlier management depends on the purpose you are trying to achieve. Are you simply exploring this data or are you trying to test something?', ""Is there any way you could reach out to the participants and ask them to clarify? Maybe they accidentally entered the wrong value or they misinterpreted the question. \n\nBeyond that, the obvious thing to do would be to look at the median since it should be more resistant to the outliers.\n\nBut more importantly, if you aren't using it for any of your analyses, it's possible that it's not that important. It might not be worth your time to solve this problem."", 'Some ideas:\n\n\\- Bin the data into intervals, where the last interval I\\_c contains all observations above some threshold c. \n\nThen report the average as weighted average of  each bin weighted by what proportion of the data falls in that bin.  \n\n\\- You could report a weighted average. Where the weights come from a distribution that you consider to be reasonable. For example, let us say the median of your data is 1 hour. So take the weight of observation xi as f((xi - 60)/60) where f is the standard normal distribution.  You could vary the parameters to pick a robust weight. \n\n\\- Assume that the interactions follow some distribution. Then vary the parameters of the distribution and report the Bayesian expectation \n\n\\\\int E(x|P) f(P) dP \n\nwhere P is the parameters.', 'This particular variable is just for descriptive purposes', 'Take a CDF, and say 90% of people take less than 5 min to complete the survey']"
[E] [C] Has anyone had any luck at the Joint Statistical Meetings?,"I am a master's student who will be graduating soon with my degree and would like to know if there were any statisticians of any field out there that had any luck getting a job after going to JSM. I've been applying to jobs and internships and they reject me left and right. I have been out of work for close to a year now with no good luck. I need some advice on what I should be doing to improve my profile as a statistician or data analyst.

I have a github with all my projects, I have writing samples, I have samples of statement of qualifications, cover letter samples, resumes I've constantly received feedback on and modify for every jobs, every few months I'm changing the by-line in my LinkedIn and improving the descriptions on the jobs I've done. The few times I haven't been automatically rejected by ATS was because there was an actual human reading my resume, but when I get to the zoom conference call stage I usually strike out. I'm told that I'm highly qualified but at this point I feel like I'm being lied to.",11u0m8t,sherbertstats101,1679080249.0,29,0.94,"['What kinds of jobs have you been applying for? Which industries? Post an anonymized version of your resume or PM me and we/I can help you out.', ""The industry job I'm currently having is what I got from the JSM Job Fair back in 2019. I was a postdoc at that time. Got screening by phone then face-to-face interview during the conference. Another colleague of mine was also hired at the same occasion."", ""Have you looked at Handshake? That's a great site for people looking for internships and jobs who are about to graduate or have recently graduated.\n\nAlso, check with your college at your university.  The program coordinators and advisors in your department often have leads on jobs."", 'I will dm you. Thank you', 'Thanks for responding. This gives me hope ']"
2 period dynamic panel correct standard errors [Q],"Hi there, I have 2 periods of panel data. I want to estimate an AR(1) regression on some of the variables. I know that in general, this is dodgy in panel data since errors won't be exogenous anymore. However, all the solutions I know require more periods of data. How would I correctly estimate the coefficients + SEs in this case?",11tnkr6,onemandave1,1679049974.0,3,1.0,[]
"[Q] Can I use weighted regression to analyze the relationship between log-transformed GDP per capita and life expectancy, accounting for country population size?","Hi everyone!

I am working with country-level data on average life expectancy, population sizes, and log-transformed GDP per capita. I am interested in using weighted regression to analyze the relationship between GDP per capita and life expectancy, while accounting for the potential impact of country population size.

Specifically, I plan to weight the data by population size to account for the fact that larger countries may have more impact on the overall relationship. I am wondering if it is appropriate to use weighted regression in this case, and if there are any particular considerations or caveats that I should be aware of when doing so.

Any advice, suggestions, or resources on this topic would be greatly appreciated. Thank you!

Edit: deleted logistic, I meant log regression, yet log regression is basically linear regression with log transformed variables, which is why I deleted that. Please excuse the confusion.",11tn6sb,dududu87,1679048738.0,1,0.67,"[""Are you simply just interested in controlling for population size?\nEvery observation is a country right?\n\nMaybe I'm missing something but why could you not simply include the pop size variable in your model?\n\nI've heard mention of random effects but my worry is that if observations and countries map 1-1, then I don't believe that you'll be able to use them as a random effect, because each group would simply be comprised of a single observation. Maybe I'm not grasping something that others are.\n\nCheers"", 'Mixed effects model?', 'Not a statistician and I may be misinterpreting your study, but here are my thoughts. \n\nIt seems like you would want to run a linear regression with life expectancy as your response variable and population size, log-GDP, and pop*log-GDP (the interaction between the two) as your predictor variables. \n\nAre you interested in whether life expectancy differs across countries? Its possible the model may be overfit if you include county as a fixed effect. It seems like country is only relevant for understanding how your data were collected. You may want to consider a mixed effects model and include country as a random effect. This should hopefully incorporate any variation unexplained by your predictors.', 'Hey! I want to weight the countries according to its population size. A large country should have more influence on the model. I regress Life expectancy on log gdp per capita. But hence china has 1 billion inhabitants and Germany 80 Million, I want that China has more say in the model. I thought weighted least squares regression could be a tool, but I only found that this is to be used for heteroskedacity problems.', 'Could you elaborate?', 'Hi, thanks for your response! do you have some good resources for learning mixed effect models?\n\n\nEdit: \n\nMy idea is just to see how much life expectancy increase if gdp per capita increases. You are right, that data was collected country wise.', 'Ok. The simple answer here is ""oh yeah it sounds like you are just trying to control for population size. Include it as a term in your model and then your life expectancy estimate will be appropriatey conditioned.""\n\nThe tricky part is that the causal flow between life expectancy, pop size and GDP may play a really important role in how you approach this.\n\nIf both GDP per capita and life expectancy have a justifiable causal impact on population size, then conditioning on pop size may actually introduce some perceived association between the other two variables that shouldn\'t be there otherwise. It really comes down to how it has been established that these things affect one another, and that knowledge needs to come from outside your model.\n\nSo maybe you should control for it, and maybe you actually shouldn\'t, depending on that causal knowledge.', 'Using a linear mixed effects model you can incorporate the random effects caused by the variation in countries \n\nA quick google should find you everything you need', 'Check out Ben Bolker. He has a few papers on this and has resources for implementing the models. If you use R, they are very helpful.']"
[Q] Small sample size,"Can I run a linear regression with only 16 cases and 4 predictors? If not, what is the best way to establish correlations in such a small sample?",11tn6de,F_M_G_W_A_C,1679048704.0,21,0.87,"[""Is it possible to observe more than the 16 data points currently available? \n\nThere isn't enough information and context to provide a suitable answer to your question. A number of scenarios and use cases exist where statistical inference is made on with minimal specific data. But in each of those domains, appropriate methodologies are used in light of the constraints, and it's never a one size fits all exercise.\n\nWith that being said, 16 observations and 4 covariates for an OLS regression? Don't"", ""Yes, you can. For small sample sizes an alternative is to have strong priors. This will depend on your domain knowledge and problem you're trying to solve though."", 'Of course you can. People do this all the time with designed experiments. Just keep in mind the assumptions of the linear model and be sure to check whether they reasonably hold.', 'A bit of a far fetch but if you eliminated data points due to missing information in the columns you can use imputation. I.e. to fill in those missing values with an average for example.', 'You can. But you have to consider several things:\n\n1. Is it worth it to run a model? If the sample has 16 observations only, how large is the population?\n2. Will descriptive analysis be sufficient to derive any insights?\n3. With 16 observations and 4 predictors, can you analyse using summary statistics?\n4. Is this school homework or a 1 million worth project?', 'Others have mentioned Bayesian modeling which is a great option.\n\nId also bring up bootstrapping. Both are computer intensive and both are more than just an lm command in R, but they will be your best options in making up for the low n-size.', ""This is possible, but check the assumptions!\n\nFurthermore, it might be the case that you know what the error is for your individual observations. If you know what the error for a particular observation is, you can generate X rows of data with that value and error, as to perform weighted regression using 1/X as weight (I'd use 100 rows per observation)."", ""There are alternatives but collecting more data will make your analysis easier and won't need fancy alternatives if you're new to statistics, otherwise this is a good time to learn bayesian"", '[deleted]', 'A classical linear regression cannot cope with less than 15-20 cases per predictor. You can use a Bayesian model that is less sensitive to small sample size.', ""Maybe a non parametric test could give you better results. When your sample is that small, is hard to comply with normality (gaussian distribution) assumption.\n\nYou can:\n\n\\- Use Spearman's Rho, and consider your result with that correlational test. I consider this the best option.\n\n\\- Use a normalization algorythm like StandardScaler of SciKit Learn package (This is Python), and then re-run your linear regression\n\n\\- Settle with a descriptive analysis and get away with it"", 'Are you looking at happenstance data or are you planning an experiment? With 16 runs, we design experiments all the time in industry and we can get fractional factorials with good resolution with up to 8 regressors.\n\nOf course we are trusting in the sparsity of the effects to be able to give a bit of degrees of freedom to error, but that gives a good estimate on what factors are significant and allied with sequential experiments we can predict the behaviors of several phenomena.', 'New to statistics. What do you exactly mean by ""strong prior"" here? I understand Bayes, but not sure what exact is the prior in a linear regression equation', 'Bootstrap is not a fix for small sample size', 'The bootstrap has only large sample validity', 'Introduce some assumptions to compensate for the small sample, and take your result to be (effectively) weighted average of the assumption and the data. For this sample I would stick to descriptive statistics, or some of the methods for small samples other posters have outlined. But the assumption could be anything - maybe you assume the beta is 3 lol.', ""A prior is a probabilistically sound way to regularize the coefficients in your linear regressions. In other words, you're telling your model which ranges of values are more plausible, based on your domain knowledge. For example, say you're interested in the effect of height on weight; then you know the effect is very unlikely to be negative. For an introduction, have a look at [chapter 9 of Bayes Rules](https://www.bayesrulesbook.com/chapter-9.html)."", 'Eh. Not strictly a FIX true but one of the few frequentist tools (aside from perhaps rote simulation) that addresses the situation even a little but.\n\nCombine it with jack knifing (reasonable with 16 observations) and youll be squeezing just about as much info out of the situation as you can this side of gibs sampling.', 'There is no make up for under power, as nothing really changes the fundamental issue  large standard errors. Meaning whatever resampling method OP uses doesnt improve the validity of established correlations. Even Bayesian priors are technically from previous data.\n\nUnless those effects are very strong, e.g. temperature of oven ~ time to cook pizza. What OP can do here is to perform descriptive analyses, and possibly based on domain knowledge as well as correlation assessment between variables, to reduce the number of variables in the model.', 'Great comment']"
[E] Minoring in statistics,"Im thinking of getting a minor in statistics to build a better profile for when I apply to graduate school (undecided on which career path). 

Is there anyone who is currently a math/stats major/minor that can tell me about your experience?",11tiav1,andi_dandi,1679031490.0,12,1.0,"['I did math and stats. It is always beneficial to have some statistical fundamentals under your belt. In retrospect, a lot of math & stats majors suffered from theoretical probability & statistical inference courses. If your math maturity is not yet there,  take more applied methodology courses without harming your GPA.\n\nGreat reference: [https://www.eecs70.org/](https://www.eecs70.org/) (Fundamentals)\n\nCheck out the lecture notes and ask yourself whether you are comfortable with them or not. \n\nCheers!', 'Public health with a focus in epidemiology or biostatistics are both great fields for applied stats and interdisciplinary thinking.', 'I think that entirely depends on your program and breadth of class choice variability, as well as what your primary degree is in. Speaking generally, I can share about my personal experience. Having my undergrad degrees in physics and CS, every additional statistics class was monumental in building my tool belt for research analysis, experimental design, and modeling. That\'s why I ended up going to grad school for statistics. I looked back at my publications pre-grad school and realized how much easier certain problems could be solved if I had known more about stats and data modeling. \n\nThe saying about being a statistician goes, ""You get to play in everyone\'s backyard."" Whether you\'re in strict STEM, social/humanities, or anything that involves experimental design, I recommend as much statistics as you can take. It would have made a world of difference in my physics and CS publications and it is all I do now, as a global health and health economics researcher at an R1 medical school.', ""I have an undergrad and most of a graduate degree in applied math/math stats and like make no mistake you're going to be taking heavy math oriented courses especially at upper undergraduate/graduate levels. You don't need to have a special natural talent to do well (I sure don't lol), but if you just don't really like math at all it's going to suck to get through/be hard to stay motivated to finish. It might be worthwhile to instead consider a data science or social science (economics, maybe) minor where you will get some more stats classes but they won't expect you to have taken multivariate calculus and linear algebra."", ""I did a study it for a while the exact combination and i found it to be much too theoretical to my liking. The whole thing seemed to be a funnel into academia later on which was not my end goal. I wanted to solve problems in industry and the major focus of the math/stats programme was to turn stats interested students into great problem solvers for specifically the esoteric flavor of problems that occur most often to PhD / postdoc students. YMMW but i'm enrolling in an applied math course instead and hopefully that will be a better fit."", 'Much appreciated. \n\nMy biggest fear is that I might not do so well in upper division stats classes. Despite that, I took a look at the site you provided. For the most part, Im sure with continuous practice and actual study time on my end, it would not be so bad. It seems like reasonable material one can retain. \n\nI just always had this assumption that one either has to like numbers or be naturally gifted in math to even consider majoring in mathematics or statistics.', 'not OP but this is precisely what im planning to do (if  accepted into the public health program!)', 'Yeah I didn\'t want my inquiry to sound like an ""all about me"" post, so I didn\'t include specifics. Nonetheless, I\'m a political science and psychology major. The only stats class that applies to my current major is the Elementary stats class I took before transferring. Though, here at UCD, we are required to take a Poli-Sci stats class, which feels like another intro to stats class (more practice for me!).\n\nI\'m debating between med school or law school. I would hope a minor in stats would prepare me med school if I go such route.\n\nMoreover, I was thinking a BS would make me standout, but I guess we\'ll see. I appreciate the response.', ""Applied math sounds like a nightmare. I've always felt weak in the subject of math, but for some reason, stats intrigues me. \n\nWhat's your career goal (or what career are you currently in)?"", "">For the most part, Im sure with continuous practice and actual study time on my end, it would not be so bad. It seems like reasonable material one can retain.\n\nHaha, I am glad you found it useful, but don't post this in the Cal sub, each year students are screaming about how cruel this course is :). I think the best shot would be to talk with the program coordinator. The requirement for minoring would not be as strict as double majoring in stats. \n\nFrom my experience, one must have the passion to develop intuition over time in statistics. Honesty, statistics is much harder than one might think of it. The deeper you dig, the more theoretically and philosophically challenging it will get, and you need to be persistent enough to power through them. I wouldn't say everyone can do well / have fun in statistics, but I would like to encourage you to explore the possibilities. Cheers!"", 'You got this!! Im in an MPH epi program currently.']"
[S] Why does alpha_results$std.alpha not work in R programming?,"Hello r/statistics community, posting here for the first time!

I just need some help, I've already successfully performed cronbach's alpha, and ran a bunch of them. In an effort to see only std.alpha values, I decided to use the operator ""$"" pulling just that in the output. However, all it returns with is NULL.

*Call: alpha(x = alpha\_results)*

&#x200B;

*raw\_alpha std.alpha G6(smc) average\_r S/N   ase mean   sd median\_r*

*0.87      0.87    0.87      0.46 6.8 0.018 0.66 0.33     0.48*

&#x200B;

*95% confidence boundaries*

*lower alpha upper*

*Feldt     0.83  0.87   0.9*

*Duhachek  0.83  0.87   0.9*

*> alpha\_results$std.alpha*

*NULL*

Does anyone have any idea how to do this?? Thank you!",11tbj70,Follhim,1679012712.0,0,0.33,"['The $ operator is how you pull things out of named lists (or derivatives of them like data frames). Check the object type of alpha_results using class() and then try to figure out how to index that. (You might be able to coerce it to a data frame but I have no clue)', 'The psych package `alpha` function (is that what you used?) stores those results in the total component. You can often see learn the output structure quickly if you call `str` on the output. \n\n    results <- psych::alpha(dataset)\n    results$total$std.alpha', ""Thank you! That makes total sense, I knew something wasn't right. With a little bit of help from ChatGPT I was able to coerce it into a data frame and then was able to pull from it! \n\n`alpha_results_df <- as.data.frame.matrix(alpha_results$total)`\n\nIn case anyone else is interested in how to problem solve this!""]"
[Q] What test of statistical significance and post hoc test are best for determining if one UX design is preferred over another design. Design A vs Design B vs Design C.,"Im used to working with more complicated statistical methods, ratings, etc.and simple categorical data is breaking my brain. Please help.

Edit: the order in which the various designs are presented have been counterbalanced to minimize order effects, but the final question asks which design do you prefer A, B or C (while all 3 are visible on screen)? They choose one option. 

What is the best way to show customers 3 potential versions of a UX, and then gauge preference quantitatively for statistical significance?",11t8hgd,RobKAdventureDad,1679005889.0,1,1.0,"['What data do you have?', 'If you have numerical ratings of each design by participants, you can do ANOVA with pairwise t tests for means. If the same participants rated multiple designs, you can use a linear mixed model with fixed effects for design and random effects for participants.\n\nIf each participant viewed all three designs and picked a favorite, you can use a chi squared goodness of fit test against the null hypothesis that all three designs are equally preferred (or whatever null hypothesis you want).', ' Is it randomised and controlled?', 'I only captured their favorite choice for each person. So in excel, if they selected C as their favorite, I coded it as:\nUsername (Tom653), A (0), B (0), C (1)\n\nAnd I have that for 200 participants.', 'I only captured their favorite choice for each person. So in excel, if they selected C as their favorite, I coded it as:\nUsername (Tom653), A (0), B (0), C (1)\n\nAnd I have that for 200 participants.', 'There was only 1 group of participants, but the order in which they saw the various UX designs was randomized. The final question asked them which was their favorite, and all 3 designs were visible beside each other.', 'There are a few things you can do depending on what you are trying to accomplish.\n\nIf all you want to do is ask ""is some design preferred more or less than the others"", you can do a chi square goodness of fit test with a null hypothesis that all three are preferred equally.\n\nIf you want to estimate the proportion of people in the population who prefer each design. You can make confidence intervals for each proportion, ""prefers A"", ""prefers B"", and ""prefers C"".', ""Did the group of participants talk to each other to come up with the best design? i.e. could a participant's response have affected another one? i.e. peer pressure etc... if not then use chi square goodness of fit as u/shagthedance said. [link](https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/goodness-of-fit-example)""]"
"[Q] Curve fitting at the ""edges""","I am fitting a Gaussian curve to a series of values (using Excel).

Let's say this is my data:  { {1,0}, {2, 20}, {3, 74}, {4,96}, {5,65}, {6,14} }

A value of X=0 can never happen, and a value of X>=7 can never happen.  That's the root of my question.

I calculate the Gaussian parameters \`A\`, \`mu\`, and \`sigma\` using Excel trying to minimize the sum of squares of the difference between the value the formula yields and the value in my data.

For example, Excel figures out that the A=99.78, mu=3.92, and sigma=1.10 for the data above.

&#x200B;

The issue is that that curve will have a non-zero value for X=0 (0.18), for X=7 (2.0), X=8 (0.11), etc...

&#x200B;

In general, how do you handle data at the ""edges"" of what you measure?  How is this sort of thing done properly?

&#x200B;

I'm thinking about the Chernobyl thing where they didn't think the radiation was too bad because the meters they were using didn't go very high...",11t84mb,Lance_E_T_Compte,1679005091.0,0,0.5,"['Constrained and penalized optimization is the general set of keywords relevant for this kind of thing, **but** what you are asking for is inherently impossible. The gaussian curve is strictly positive across the entire real line. It will never exactly reach zero at any point.', ""> A value of X=0 can never happen, and a value of X>=7 can never happen\n\n> The issue is that that curve will have a non-zero value for X=0 (0.18), for X=7 (2.0), X=8 (0.11), etc...\n\nOf course. If you want a smooth curve that goes to zero outside a finite interval fitting a Gaussian is definitely wrong, because that never goes to 0\n\n> How is this sort of thing done properly?\n\nBy not fitting a curve that doesn't match your constraints in the first place?\n\nOn what basis did you pick a Gaussian? Presumably not theory if you know things that rule it out altogether.\n\nbesides the curve going to zero outside that interval, what are the actual requirements on your function?\n\nWhy does ordinary least squares make sense here? I doubt it makes sense for the noise to be the same for y=0 as it does for y near 100.\n\nWhat are the response values? Are they counts, for example?"", 'However, you can approximate the Dirac delta function a infinitely narrow infinitely tall impulse with a Gaussian curve as the std approaches 0.', ""Thanks.  I understand it can never be zero, but because it HAS to be zero, something must be done in the real world where it matters.\n\n I'll google those words and see if I can learn something!\n\nThanks!"", ""The values are, indeed, counts.  It is impossible to count something for bin zero or bin 7 or more, since there's no way to get a count there.\n\nI picked a Gaussian because my initial data looks like a bell curve.  As you and the other person have noted, this is not correct.\n\nMy question comes from just wondering how, for something that matters (not my application), this sort of thing would be handled."", ""(1) This isn't relevant to this context at all.\n\n&#x200B;\n\n(2) While it's true that the gaussian distribution converges to a dirac distribution as standard deviation goes to zero, it's much less meaningful to talk about the gaussian *curve* approximating a dirac delta *function*. Dirac *functions* aren't particularly well defined on their own, and only make sense when used in some context where you're integrating them, e.g. distributions."", ""If you are insistent on using a gaussian fit, you can just clip the predictions to zero beyond those points. If they're impossible you shouldn't ever be generating predictions outside of those points in the first place though. If you are doing this you should go back and re-evaluate if you are doing something incorrectly, framed the entire question incorrectly, or otherwise need to adjust something else. \n\n&#x200B;\n\nThe other better alternative is to abandon the gaussian curve, since it inherently violates the basic constraints of your problem."", '> The values are, indeed, counts.\n\nThe variance of counts is larger when the mean is larger, so fitting by unweighted least squares doesn\'t make sense in that case. I\'d probably select a suitable count model for the response (e.g. Poisson, binomial, etc), or perhaps just play with the weighting function, depending on circumstances.\n\n> It is impossible to count something for bin zero or bin 7 or more, since there\'s no way to get a count there.\n\nSo far, that\'s just an argument for truncation of the curve where it has to be zero, but not clearly one for modifying it in between -- it\'s not clear whether/why it should go smoothly toward 0 or not, *because there are no details of the particular circumstances*.\n\n> My question comes from just wondering how, for something that matters (not my application), this sort of thing would be handled.\n\nExactly what could be done depends on the specifics of the situation -- a bunch of stuff you\'re leaving out. Change the situation each time (what you\'re looking at, what properties you require of the fit) and I might end up giving you 5 or 6 quite different responses about what to do with it.\n\nThere\'s no single ""this is the right method in every situation"". The details of what you\'re looking at and what you need to get out of it typically matter.', 'Ehh, I think youre too hyperbolic in your dismissal. Everything is connected. \n\nIm curious, what do you mean when you say delta functions are not well defined. that is not very constructive? \n\nAre you saying that impulses and singularities are hard to conceptualize?', 'Yes. I understand...  I set values to zero where they cannot be zero, then I complain that they are not zero.\n\nI chose Gaussian fit because, in general, except for these ""edges"", it seems to be a very good fit for the values that I have.  I\'m not super invested in it and will choose something else...\n\nThanks again for your help.', ""I am saying they are simply not well defined. Dirac functions are not actual functions that exist. When we use the Dirac function in calculations we're using it somewhat as an abuse of notation. We use it as a syntactic construction and algebraically manipulate it as if it's a function, and the calculations usually work out. However, the actual meaning of what we're doing has to be kept in mind to make sure everything is actually correct. E.g. we might be using dirac functions as a means of compactly working with mixtures of different measures and referencing the densities of these measures. The Dirac density is a fictitious density associated with a Dirac measure."", 'Fair enough. Its a well reasoned construction. But so is a Feynman diagram both are valid objects. I wouldnt characterize it as fictitious.']"
Seeking Advice for data analysis with my quality project [R],I am a medical laboratory science student doing a quality project. In my project my question is: Is it valuable to monitor patient data over time for critical care patients? I have data from a patient that is now deceased and have it organized in an excel data sheet with established normal ranges. I have also logged the dates that all testing took place. My question is what would be the best way to prove or disprove the significance of fluctuations of patient data over time? I am familiar with basic statistics and am pretty decent at excel. Would just like to know how other people would approach this.,11t6stj,gracewren-,1679001983.0,1,1.0,"['You only have one patient, and you have no patients who did not die.', ""It's hard to give a blanket answer to this other than to say that there are what is referred to as medical decision levels (MDLs) where, once you pass that, some intervention is recommended. (To be clear, I'm not a medical professional, my only experience in this is from the R&D side having developed diagnostic assays.) Other than that, it's really hard to generalize. Some things vary normally. Think about sodium levels in the blood before and after a very salty meal. Obviously they are going to change, but probably not so much that it warrants concern. On the other hand, something like cardiac troponin (an indicator of MI) is something that (last I knew) you would monitor if, for example, someone showed up at the ED with symptoms consistent with an MI. What used to be the practice is in a situation like that (again, to the best of my knowledge) you would check the cardiac troponin levels, if they're elevated above the MDL, send the patient to the cath lab. Otherwise they would be monitored over some number of hours to see if troponin levels were increasing. If the levels are increasing over time the patient probably gets admitted, if not, sent home or they look for other causes for the patients symptoms.""]"
[question] Expressing intuition probabilistically: comparing surprise factor between three boxes in a guessing game,"Suppose you are playing a guessing game where there is a box with three marbles. There are three kinds of boxes. Box A contains red, green and blue marbles; Box B contains red, green, and yellow marbles. Box C contains black, white and purple marbles. You are told that there is a 60% chance that this is Box A, 20% that it is Box B, and 20% that it is Box C. 

Suppose you make a choice, and open the box. Although Box B and C are both equally unlikely, there is an intuitive sense in which Box B would be a less ""surprising"" choice than Box C, because it contains the red and green marbles (which are 80% likely to be encountered). How do I express this intuition probabilistically? Is the KL divergence between my marble color beliefs before and after opening the box smaller for Box B than Box C? How do I express this?

I know that P(red AND green AND yellow) = P(black AND white AND purple) = 20%.",11sxmue,synysterbates,1678981329.0,3,1.0,[]
"[Q] Linear Algebra, What is it for?","For some background, I started taking ap stats my junior year in high school about 4/5 years ago and now im almost a junior in college with it being my major. I took linear algebra last semester and went through it with flying colors due to an amazing professor. 

All that being said, why exactly is linear algebra used for? Right now Im in a stat class where were using R for ANOVA, simple, and multiple regressions. What exactly am I able to do with linear algebra in statistics to draw parallels in my learning?",11sx1w5,Miller25,1678979991.0,7,0.74,"['Well, almost everything related to regression and machine learning is basically some flavor of linear algebra under the hood, and thats two reasonably broad categories. One example is the Moore-Penrose pseudo-inverse, what is it and why is it useful? What even is an inverse? Linear algebra is quite useful to answer questions like these.', 'How do you express your least squares estimates for multi-variable regressions? \n\nHow do you express the joint distribution of two or more random variables? \n\nHow would you minimize the likelihood function of any data that has more than one data point and more than one parameter?', '[removed]', ""You're doing linear models (ANOVA and regression) right now... the mathematics of linear models is linear algebra all over the place. \n\nIt's a lot to cover though, not really feasible in a couple of paragraphs. You might need to look at a textbook that covers multiple regression / linear models"", 'In general, linear algebra shows up whenever you take a one variable thing and try to turn it into a multivariable thing. In basic statistics this happens in a number of places, but perhaps multivariate linear / logistic regression and principal component analysis are the most prominent. If you want to pick one thing to study that emphasizes linear algebra, have a look at PCA.', 'Algebra in general is what you end up with when you notice you\'re doing the same thing over and over in different contexts. You see what the various problems have in common and pull that out, then you can talk about stuff in these common terms -- you can work at a level with broader chunks, so to speak.\n\nLinear algebra is what you get when you apply that process to stuff you want to do in statistics. You multiply one thing by another and add them up ... divide by this other thing ... factor out these terms ... hmm, didn\'t I do that last week when I was working on a different problem? After you factor out the common operations, you can move a lot faster, and, maybe more important, you can think about stuff at a ""higher"" (more chunked) level.', '<insert Gary Oldman gif here>', 'what would be of modern statistics without linear/matrix algebra?', 'I would highly recommend using what others have given you here and probing chatGPT.\n\nI just did that tonight to understand more about various time series models (ARDL, ARIMAX, VAR, and VARMA) which allow for multiple explanatory time series variables in addition to the series of interest.\n\nThese are all expressed in terms of linear algebra (typically regression, so matrices and vectors)', ""yeah I've spoken to a few other students a litter further on than me in the data science program and they all mostly said machine learning is heavy in statistics so I shouldn't have any issues. definitely helped ease my worries when it comes to taking those classes lmao"", 'me not knowing how to exactly answer these questions lets me know I have a lot of reviewing to do...', ""Yes this is right. I teach MV stats. I'd recommend checking out Johnson and Wichern as the best middle ground for the linear algebra behind MV stats without being overly theoretical. That being said, linear models are the basis underlying nearly all of the analyses you're likely to have learned as an undergrad. They're encompassed under the General Linear Model. Even linear algebra basics help to understand things like why multicollinearity is a concern. It's all linear algebra really."", ""do you have any readings you'd recommend on it?"", 'Please advice the letters PCA. Thanks', 'Can you elaborate more on the use of ChatGPT, and Linear Algebra. Im a rookie so speak to me as if I were a small child or a Labrador. \nThanks', ""The common theme here is that linear algebra provides a systematic language that allows you to express questions in multivariable statistics. Irrespective of your statistical prowess, without linear algebra you wouldn't be able to express yourself and you will find it very difficult to understand others."", 'Principal component analysis, useful for dimensionality reduction via reducing potentially correlated variables into a fewer number of features (attributes/columns), aiming to maximize retained information in the data (as far as I know)', 'I understood that reference.\n\nChatgpt has access to almost all human knowledge up to like 2021. You can ask it about linear algebra and it will provide detailed answers']"
[Q] I need help! Possibility of an draw in a game of 2 players Yatzy?,"We had a 264p draw with my friend, it has been bothering me to know the possibilities of it happening. Thanks Reddit!",11suxlv,irannoK,1678975092.0,1,0.67,[]
[Q] Standardization vs normalization and other techniques,"Standardization vs normalization

Good evening, I would like to have some insights or resources about which is better and in which context. Also I would like to know about more techniques and compare them, like logarithmic transformation, for instance.

Thank you in advance.",11stjoe,Cerricola,1678971737.0,2,1.0,"['My understanding of standardization is with location-scale family distributions, you can find probabilities of any combination of location scale using the standardized combination of location and scale. The same logic for a Z~normal(0,1) distribution being equal to (Y-Theta1)/Theta2 where Y~normal(theta1, theta2^2 )\n\nThere are other location scale families besides normal. Not all distributions are location scale distributions. \n\nI bet there are others that can comment more confidently on this topic. \n\nFor normalization, Ill let someone a bit deeper into their education give a response.', '""normalization"" is an overloaded term (it has at least three distinct meanings in common use). Which sense of the word to you mean? \n\nAre you talking about scaling values to be on the unit interval?', 'Thanks :)', ""Yes I'm referring to that: (X-min)/(max-min)"", 'No problem. Location scale families have a bit of depth to them beyond what I have stated. Good luck!', 'Please edit this clarification into the question']"
Accuracy of 3Blue1Brown’s video on Central Limit Theorem? [Q],"For those who havent seen it yet: [Link](https://youtu.be/zeJD6dqJ5lo)

Since the CLT comes up often in this subparticularly regarding how its taughtIm wondering your impressions on the videos accuracy.",11ssxzz,SometimesZero,1678970218.0,108,0.97,"[""There wasn't anything wrong with what was said.  But none of the questions it asked were answered.  Apparently there is another video (or perhaps more) that will try to answer.\n\nI don't think he actually can answer those questions at the youtube level, but we'll see.  Somehow he has to prove the characteristic function uniqueness theorem, and I don't see how he is going to youtube that."", 'I watched this. I have zero bones to pick. Some of it is slightly imprecise, but he explicitly _tells you_ that it is imprecise, and to a significant degree, improves the precision later on.\n\nHe did a good job.', ""Watched it. I've watched all his videos and interviews and he seems to be very passionate about statistical integrity. Because of how deep the topic was he seemed to be focusing a lot on describing the behavior of CLT, saying he'd dive into the explanation in a follow up vid. I have no clue how he'll communicate it to his wider YouTube audience"", 'I saw it. It seems pretty accurate to me.', 'If youre looking for other stats YouTubers, Ive always found statquest to be very good', 'I thought it was a decent video but so far we only have part 1; he\'s only done the first half -- basically he\'s covered  ""give a motivation for the theorem and state what the CLT is"". He has really only just started to discuss the CLT itself.\n\nI\'d have liked to have seen some additional motivation beyond the dice example but doing that in detail might cause problems (e.g. change the level of the exposition he seems to be going for and make it longer than would be practical), and I\'d have liked to have seen him move to also discussing what happens with the distribution function in a little more detail, rather than just illustrating with histograms; I expect that will be what happens in the next one (albeit probably covering the issue with why the jump between them only very briefly).\n\nI do have several quibbles with the motivation part, but since he indicates he\'s going to pass to outlining the mathematics in the next video rather than just leave it at the motivation and claim that the motivation *is* the CLT (as many bad textbooks quite literally do, and then typically add general claims on top that are not true in general), those quibbles don\'t grow to the point of insisting there\'s a problem with what he did. A few places I would take some issue with specific wording but it\'s not that big a deal because he revisits the statements and usually fixes them up as he gets more specific. *Some* liberties are reasonable in motivating a result. \n\nI wouldn\'t *just* point people to that particular video, because that one is almost entirely motivation for the CLT plus a somewhat informal statement of the CLT (up to some minor handwaving of details) at the end. In particular I\'d hate to point someone to that video and risk them thinking they\'d covered the CLT. I\'ll wait for the next one where he actually *covers* the CLT, see what it is like and then (I expect) recommend them together.\n\nSo on *accuracy* -- what is there so far is largely accurate in two senses:  \n\n(a) what you see happening with sums of (fair and unfair) dice *does*  really happen, but is not *quite* the CLT (he does sort of briefly indicate that there is a gap between them more toward the end when he sets up the problem properly in order to be able to take limits next time). \n\n(b) His outline of the CLT at the end does look like an accurate description of the \'classical\' i.i.d. CLT (at least the sum-version of it)\n\nAll in all, I thought it was decent given some fairly necessary compromises, but ... it\'s 3blue1brown, so I would have been surprised if it wasn\'t.', 'Lol I coincidentally watched this last night. I was trying to predict what he was about to say in the video every time he brought up something. \n\nI also accidentally learned to derive the formula for the CDF of the average of multiple iid variables. I stared at it and put the pieces together and realized where it actually comes from. That was nice.\n\nBut it did seem pretty accurate and I think he implies more will be expanded on later. He is working on a playlist for probability.', ""It's OK, I think.  I find his videos confusing, but the visualizations do sometimes help with the intuition behind things (especially the stuff about Fourier).  I'm just an armchair statistician, though.  If anyone's looking for a good discussion of the Central Limit Theorem, I would highly recommend the paper linked below (used to be on NIST, but on MAA now).  It's how I learned the intuition behind it years ago + it goes into the history behind it which is helpful in my opinion.\n\nhttps://www.maa.org/sites/default/files/pdf/upload\\_library/22/Allendoerfer/stahl96.pdf"", ""I don't watch too much 3Blue1Brown--I get most of my continuing ed and current resource through data science/stats Twitter--but my brother is an engineer and loves him."", 'The only sort-of-error I noticed is that at 7:52 he says that a distribution is ""skewed towards the left"" when technically it is right-skewed.\n\nBut I think he was using ""skew"" in its natural language sense, which is contrary to the technical definition.\n\nThat\'s what prompted me to ask about it here\n\nhttps://www.reddit.com/r/statistics/comments/11rk81a/q\\_what\\_does\\_it\\_mean\\_to\\_say\\_that\\_a\\_distribution\\_is/\n\nAnd to write this blog post about it\n\nhttps://www.allendowney.com/blog/2023/03/16/what-does-skew-mean/', ""Can't speak on the accuracy as this is the video that allowed me to understand the CLT, but I trust 3B1B to be accurate."", ""I've never found a 3blue1brown video to be helpful in actual understanding a topic, despite how pretty the graphics might be."", 'My comment was about his animation representing ""random""...', ""Is there a sentiment held by some people that 3Blue1Brown's explanations are sometimes not to be trusted?"", 'Theres also a nice proof by Steins method but that too will be too much for a YT video.', 'He has other wonderfully simple explanations for things', ""He probably won't go that far (he might, but I don't expect it because of the level he covered the first video at; his motivating pitch looks too low a level for that jump). \n\nThere will still have to be substantial gaps unless he's going to do at least three videos."", 'Bam!', 'Thanks for the link.  Very interesting!', 'Where can I find more papers like this?', 'Stupid engineer here, I really like them as well. Really fits my type of ""view"" of problems.', 'Well, to each their own I guess. You clearly seem to be in the minority though, the vast majority of people who are exposed to his content clearly find it to be very helpful. I have a masters degree in math and stats and find myself learning new stuff from his videos periodically.', ""Some time back when he did his Fourier series, I actually implemented a crude version in R or Python (can't remember now) using my own SVG files as a starting point."", 'Thats because his videos are mostly just presenting mathematical ideas in a more intuitive way making them accessible to people without them needing to have prior knowledge.\n\nIt introduces university level concepts to non-university students who dont need to understand the broader context of what hes presenting. Even for students who are studying what he talks about in his videos, it can provide a different perspective that helps get over the hurdles of a dodgy lecturer or confusing notation. It wont necessarily teach you anything new, but it can help you to understand certain core concepts more intuitively if you didnt already, which in turn helps you understand things beyond the scope of his videos.', ""same here.  I generally don't watch his videos. I find them to be a bit pointless, but the visualizations can be pretty."", ""Yes.  And Lindeberg's proof doesn't need characteristic function inversion either.  But that too is, as you say, too much."", 'I dont know, 3B1B does a good job at reexplaining things once youre somewhat familiar, but he does often start at a simple level and goes more complex. In saying that, I dont know how simple hes started and how much of a jump itll be.', ""Hard to say.  I imagine the MAA has more like this and you might find them on ScienceDirect, the NIH, Arxiv, or some other academic paper repository.  It's worth noting that I found this one in a happy accident some years ago.  At the time, I think I was researching the **history** of math/statistics.  I think one of the reasons this paper is so intuitive, for me at least, is that it walks you through the history of how each piece sort of came together, meaning you can track the thought process behind the people developing the math itself.  Good luck!"", 'I can assure you that if you are anything like my brother, you are not stupid!', ""I thought his video on Eigenvalues/ vectors as well as determinants to be wonderful for providing a visual example to what's happening."", ""But Fourier series aren't enough.  You need Fourier transforms (characteristic functions)."", 'I think interpolation/swapping would actually be pefect for a video!', '> I think one of the reasons this paper is so intuitive, for me at least, is that it walks you through the history of how each piece sort of came together, meaning you can track the thought process behind the people developing the math itself.\n\nThat\'s what I like so much about it as well. Seeing the evolution of the ideas ""lifts away the veil"" from the abstracted/generalized/refined definitions you find in a textbook.\n\nIt also helps to realize that people didn\'t just come up with this stuff out of nowhere. There were very clear motivations and hundreds of years of discourse behind the ideas. One shouldn\'t be ashamed of not fully understanding something, even as ""elementary"" or ""trivial"" as the arithmetic mean as a representative value of a sample.\n\nI wish school involved more learning in this way.']"
[Q] Could you interpret these two graphs in terms of distribution?,"Hi,

There are two boxplot graphs in the google document i attached below. Could you interpret these graphs in terms of distribution? Is their distribution normal, right skewed, or left skewed?

[https://docs.google.com/document/d/1OGU2WYj8QggNE2-bTu7ZBWVZcaNWB\_DlAPrfnHWP2vA/edit?usp=sharing](https://docs.google.com/document/d/1OGU2WYj8QggNE2-bTu7ZBWVZcaNWB_DlAPrfnHWP2vA/edit?usp=sharing)

Thanks in advance.",11so9s4,Stillness__________,1678955782.0,0,0.33,"["">  Is their distribution normal, right skewed, or left skewed?\n\nThis is homework?\n\na) there's an infinite number of distributions that are not any of these three options (distributions may be asymmetric but not so obviously skewed one way or another and typically symmetric distributions aren't actually normal)\n\nb) there's not really any good basis to assert that a population distribution *is* normal from just looking at a boxplot (very non-normal distributions can have exactly the same boxplot as a normal distribution), and little basis to talk about skewness (inconsistent or even quite misleading impressions of skewness can happen with boxplots). \n\nThe main value with boxplots is comparing location and spread across many groups, not in assessing distributional shape."", 'Do you have the data, so that you can plot histograms and compare to the boxplots ?', 'I see. This is not a homework, i have never use boxplot to interpret a data set and i wanted to understand it. I understood the basic shapes but i couldnt understand the ones i attached to google docs link.\n\nThank you.', 'I asked because it would affect the answer I would give (if it was homework there would likely be an ""expected"" answer that would not match reasonable statistical advice, which is what I was attempting to give; I could easily lead someone to give an answer that would be marked wrong).\n\n> i have never use boxplot to interpret a data set\n\nAs I said above, it\'s very difficult to infer shape from a boxplot.\n\nYou can tell some things -- there appear to be a couple of relatively extreme observations (many IQRs above the upper quartile) in the second sample that mean that it\'s not consistent with having come from a normal distribution. The first one *could* arise from a (not very large) sample from a normal population but it could easily arise from a sample from a non-normal distribution.']"
[D] How much data is needed to successfully train a Machine Learning model?,"From a statisticians point of view, how much data would be needed to (successfully, meaning higher testing accuracy than training accuracy) train an ML model? Is there a fixed yardstick? Is there a formula for calculating this number. I being a beginner in stats myself do not know of such formulae, but I have done a lot of Machine Learning and from my perspective, to just train the model the input size would have to be greater than 1, but to successfully train it, the input would have to be great in size and diverse in variety, but is there a fixed number and a formula for calcifying such number. Suppose, for example, a neural network to recognise emotions from faces, and a total of 7 emotions.",11smv1t,azzamnow,1678950401.0,0,0.5,"[""> successfully\n\ndepends what you mean by 'successfully'\n\n> an ML model\n\ndifferent models require different amounts of data to achieve good performance\n\n> Is there a fixed yardstick?\n\nNo"", 'There definitely is an equation which quantifies this. If you look at ESLR, you will see that epe (expected prediction error) is a function of N sample size and p your dimension. In the formula you can explicitly see how the curse of dimensionality takes play. Taking the limit as you approach N>infinity, N being your sample size, reduces the epe.', '>successfully, meaning higher testing accuracy than training accuracy\n\nWhy would you ever expect to get higher testing accurcy than training accuracy?', 'Machine learning is not all that different from more traditional statistics for the purposes of this sort of question. Let\'s say someone asked you: ""How much data do I need for a significance test to pass at the p=.05 level?"" What answer would you give?', ""Traditional formulas for sample size calculations are based on two general criteria: 1) precision, i.e confidence interval amplitud and 2) power: the ability to detect a given effect size as significant (reject H0) in a given hypothesis testing setting. \n\nAFAIK I haven't seen a sample size for the criteria you are seeking. Not saying it cannot be done, just I haven't seen that yet.\n\nNow the easiest way in which sample size is calculated (in the case of the aforementioned 2 criteria) for complex models is by means of simulation, and I think you could start there. You should see how is this done in the traditional criteria and then try to do it for your case.\n\nI am writing this from my phone and I haven't searched if there is something related in the literature. Most often than not someone has thinking this before us and there could be a paper addressing this."", 'Successfully means it has a higher testing accuracy than training accuracy. Updated my post.', 'Sorry, not higher, but closer', 'then it will depend on the model and problem, there is no fixed answer.', 'That almost never happens unless the test data is much easier than the training data and that would obviously be problematic.', 'If you get a higher test score than train score your test dataset distribution is not representative.', 'Why is that successful? If you score higher on your test set, it is usually something ""wrong"" about it.', 'Higher test score than training score is definitely not a success haha', 'Okay then what is the minimum amount to start training?', ""I think you're missing the point. It really depends. As a general rule of thumb, you need at least 10 observations for each estimate for linear regression. If it's a simple linear regression, you're estimating the slope and intercept. Thats 20. \n\nDifferent methods are more sensitive to sample sizes. Different datasets can represent the relationships with less error. Asking general questions like this is not helpful, because there's not a general answer for all datasets and all methods."", 'Minimum = 1 data point /s']"
[Q] significant between subject factor in mixed ANOVA,"Hi! Sorry for potentially rookie question, but I just wanted to do a little sanity check.

Lets say I am running mixed ANOVA with within subjects factor (repeated measures, intervention 1 and intervention 2) and a between subject factor (Gender). My results show no main effect of within subject factor (Intervention ), nor an interaction with gender. However, gender by itself is significant. My question is, how should I interpret this significant effect of gender? My understanding is that averaging for each subject across levels of within subjects variable (I.e., (Intervention 1 + intervention 2)/2), there are differences between levels of between subjects variable, between men and women in our case. Is my understanding correct? Thank you!",11siuc2,hal_leuco,1678937211.0,2,1.0,"['Yes - the model indicates there are significant differences between genders. Since there is no significant main or interaction effect for intervention this difference can be attributed to gender alone (with respect to the model).', ""That's right, though the main effect in a RM design is often not very interesting. The interaction is usually what we care about. Do the interventions affect change over time? For example, do those who consume protein powder after workouts gain strength faster than those who don't?"", ""Of course. But it's not relevant here as it's not significant."", 'I guess what I\'m saying is yeah the main effect might be significant, but it\'s probably a ""so what.""', ""That's up to OP to determine, not us. We don't know any context here. It might very well be interesting to their work."", 'Exactly, the lack of interaction could be extremely important in some contexts.']"
[Q] Should I take Linear Algebra or Sets and Logic (or both?),"Hi! I have to sign up for my Spring courses in a couple of weeks and I was wondering which Math elective I should sign up for? My stats majour requires at least one math elective, but the way I've stacked my courses (making room for volunteering + double majouring) I think I can only take one along with the already required courses. I heard that Linear Algebra is pretty useful for advanced stats, but i generally suck at Algebra and I'm a bit hesitant of taking it since I want to maintain a good GPA for grad school. Thank you!",11sg533,EVENTHORlZON,1678930328.0,31,0.87,"['I would take linear algebra. Frankly, not knowing the subject is a bigger impediment to pursuing graduate work than a lower undergrad GPA.', 'Linear algebra. Its very different from high school algebra.', 'I generally suck at Algebra\n\nBetter fix that before grad school son!', 'Linear algebra and its not close.', ""My advice would be linear algebra, and you don't need to get to advanced stats for it to matter -- it's fairly important for  a good understanding of regression, but also in many other things -- e.g. glms, time series analysis, survival analysis, etc. \n\nDepending on what they cover the other course might be useful, but linear algebra will be helpful in stats.\n\nIf you're hoping to go to grad school in statistics and you don't have linear algebra, they're probably going to wonder why."", 'Are you sure than linear algebra counts as an elective? For a stat major I thought it was usually required.', 'You really should take linear algebra', 'Linear algebra, linear algebra and linear algebra.', ""Linear algebra is fundamental in statistics, especially multivariate analysis. You'll generalize your models into matrices.\n\nPartitioned matrices  came up a lot in multivariate analysis.. know matrix properties is useful. I don't remember a ton, but positive definiteness, quadratic forms, etc. \n\nLinear algebra's a pretty powerful topic in mathematics/statistics."", ""take linear algebra, because a lot of stats courses and advanced math courses will devote some time to sets and logic (even if you take physics, you might have some of it).\n\nlinear algebra can also be kinda fun, if you treat them like puzzles (it's matrices).  later stats books will devote chapters in the beginning to sets and logic, but for some reason they always assume you know linear algebra at some high level that some students don't have."", 'Linear algebra.', 'I noticed you go to UF. From what I remember, Computational Linear Algebra is a degree requirement for the BA and the Proof-based Linear Algebra and Sets and Logic are requirements for the BS, so they wont count as electives. They recommend Advanced Calculus or Numerical Analysis as electives.', 'Linear algebra.', 'Linear algebra first. Once you understand eigen values and SVD youre chilling. SVD is just a generalization of Fourier decomp - you can decompose matrices into their component singular values and vectors that can provide details about the connected clusters and cluster importance similarly you can decompose a wave form into its component frequencies. This is also used for dimensionality reduction and compression. You can make a cut at a singular values and vectors (since theyre ordered by importance) and take an outer product of the component matrices to recompose into a lower dimensional representation of the original matrix. Often called eigen faces, used into facial recognition tech. \n\nIf you understand SVD youll have a firm grasp of inner product spaces as well as clustering algorithms like kmeans.', ""The fact that your program doesn't make linear algebra mandatory is insane."", 'I would take all of them.  Depending on the graduate program they can all be quite important. My masters was very math heavy and I massively regretted not having taken real and complex analysis before doing probability theory.', ""I think you should take linear algebra but I'm going to give a different reason to others here.\n\nLinear Algebra isn't that useful at grad school from my experience - but many people think it is. I say this because while, yes, linear algebra stuff is in stats, you (probably) won't learn the relevant pieces in the intro course you're going to take.\n\nNever the less, people expect you to have it and to have a good grade in it. So I would really advise studying hard and studying early for it. But if you find it difficult, don't stress too much as you'll be doing different things in grad school with linear algebra to your undergrad.\n\nBest of luck :)"", 'Dont skip set theory and logic in my opinion, fun and interesting', ""Both. You'll need the logic to properly understand linear algebra"", ""You should really have both, but if you can only have one, Sets and Logic.\n\nLinear algebra is hugely important in statistics and data science, so much so that there exists software libraries to handle all of the calculations. You wouldn't want to do them by hand, even if you did have Linear Algebra under your belt. Knowing how the calculations are done has only minimal practical value.\n\nOn the other hand, the better you understand set theory, the better you'll be able to efficiently subset data, something you'll do over and over in stats/data science."", 'Markov chains are often described by transition matrices. Matrices are a core topic of linear algebra.', 'Both', 'Let me put it very delicately. The answer is Yes', 'Linear Algebra 100%', 'I will suggest both if you also want to be proficient in coding since most CS courses require Discrete Math. Otherwise, take Linear Algebra first.', 'Linear algebra. Honestly, I think this class is underrated.', 'You will find liberal algebra to be extremely different from algebra. It is useful, and very tedious if you get the wrong teacher. Sets and logic is more for a professional mathematician track, that would be your foundation for doing proofs.', 'Linear Algebra is very important when you get to Multivariate Statistics', 'Linear algebra will be far more useful in stats. And in all honesty, the bulk of what you need to know from sets and logic you can review in a couple days by yourself, a lot are common sense or you will learn in other courses. \n\nDepending on your program its often touched upon in a proofs class which is highly recommended.', 'you wont survive grad school (be it maths, physics, engineering or computer science) without a very good foundation in linear algebra and calculus/analysis (calculus for more applied minded stuff, analysis for more abstract). Everything builds on top of these really. sets and logic really depends on what you plan to do...', 'Im in a similar position to you, I suck at algebra but Im only minoring in statistics. Ive settled on doing a couple of algebra electives even if it means slightly lower grades because I know I dont have the knowledge base and skills that Ill need, especially as my goal is to do a master of public health or straight epidemiology.\n\nYou cant be worse at algebra than me. I struggled with even the most basic concepts at school and pretty much clocked out before dropping it as soon as I could. I was convinced I didnt have a maths brain but so far Im enjoying it at a uni level. None of it comes easy to me and a lot of it is completely new to me but with time and effort its still possible to do reasonably well.  I feel like I have to do so much more revision than other people but at the end of the day, whether it takes one go to learn something or 100 goes you still end up with the knowledge.', 'Definitely linear algebra', 'Both would be good but LA definitely', 'As someone currently in the trenches on figuring out appropriate regressions for a project, god I wish i had formal training in linear algebra. While my undergraduate was very good in terms broad introductions to methods, there is simply no replacement for good understanding of what is being asked of the model once you need to select an appropriate one for a given dataset.', 'im considering grad school in epidemiology, but based on everything ive seen in this thread is seems its about time i lay my woes towards algebra to rest ', 'its not at my school stars majour is just 40 credits stats with the highest math required being calc 3', 'thank you. i might have to go over the degree audit again ', 'This isnt accurate.', 'thank you. im signing up for computational linear algebra next semester ', ""You can only get so far in math before you need to learn the structure of formality and proof. I would argue that linear algebra is the first place in an ordinary curriculum where logic and proof becomes necessary. Sure there are lower order linear algebra courses on the same level as the calculus sequence but nobody really gets a deep understanding of linear algebra at this level. Would you care to explain why this isn't accurate? Or are you just here to disagree and add nothing to the conversation?"", 'I seem to remember my linear algebra class being fairly heavy on proofs.', '> You can only get so far in math before you need to learn the structure of formality and proof.\n\nI assume this happened for all of us some time before university.\n\n> I would argue that linear algebra is the first place in an ordinary curriculum where logic and proof becomes necessary.\n\nI remember it happening far earlier in grade school.', 'Wrong and wrong', 'Im sorry it took you so long to learn how to do proofs. They taught me in grade school.']"
[S] I'm not able to install packages in R/RStudio.,"I am currently using macos Catalina. It's abundantly clear that there are issues with the the installation. For example, I had ran with:

`install.packages(""tidyverse"", dependencies=TRUE, type=""source"")`

After I attempted to install the package, I got errors such as:

**ERROR:** **configuration failed for package ragg \* removing** /Library/Frameworks/R.framework/Versions/4.0/Resources/library/ragg Warning in install.packages : **installation of package ragg had non-zero exit status** \* installing \*source\* package rlang ... \*\* package rlang successfully unpacked and MD5 sums checked \*\* using staged installation \*\* libs xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun ERROR: compilation failed for package rlang \* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rlang Warning in install.packages : **installation of package rlang had non-zero exit status ERROR: dependencies rlang, fastmap are not available for package cachem \* removing** /Library/Frameworks/R.framework/Versions/4.0/Resources/library/cachem Warning in **install.packages : installation of package cachem had non-zero exi**t **status ERROR: dependencies cli, rlang are not available for package lifecycle \* removing** /Library/Frameworks/R.framework/Versions/4.0/Resources/library/lifecycle Warning in install.packages : **installation of package lifecycle had non-zero exit status ERROR: dependency lazyeval is not available for package rex \* removing** /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rex

Afterwards,  I tried to library the package but the error message like the one in the photo above:

**Error in library(tidyverse) : there is no package called tidyverse**

I  tried the same process with other packages like olsrr but I got the same outcome.

I would like to know how to rectify this problem.",11sey6t,Xemptor80,1678927473.0,1,1.0,"['Blow it away and reinstall?', 'With r install issues ... you might find one of the r-specific groups (e.g. /r/rstats) helpful', 'You might want to install additional packages on your system in order to make tidyverse intallation work.\n\nFor example on my fedora machine, i need to \nsudo dnf install python3-devel openssl-devel cmake curl-devel etc...\n\nThe error message from R is quite long but it tells you what to install. You have to go up the error message. It should solve your problem', 'In RStudio, in the menu bar: Tools > Install Packages\n\nA small window should open. Use this window to install your package. Maybe you can avoid the error.\n\nFYI, If you\'re on MacOS, BBEdit + R.gui offers a better UI than RStudio. RStudio is great on Windows, where the stock R console is horrible. On Mac, the stock R ""console"" gets a lot of love from R developer team.  It\'s actually a well-featured IDE masquerading as a console window.', 'Quote the name of the package:\n\ninstall.packages(""tidyverse"", dependencies = T) \n\nShould do the trick', 'tidyverse has dependencies which are not being installed correctly. Start with lazyeval and see what error message you get.', 'Do you have the command line tools installed on your Mac?', ""Have you tried updating R itself? I had this exact same problem recently, turns out my R on my desktop was on version 4.0 something instead of the current 4.2.\n\nEdit: Just saw your post on Rstudio that you're on 4.0.0. Soo... yeah. This is almost definitely the issue."", ""I use R in Linux and find that some packages won't install correctly because some of the build tooling is not installed on the system, independent of R.  To fix this, I need to install `r-base-dev`, and often the make-tools (which brings in compilers, etc. for other languages).  This is because often packages need to built from source on your system where say with Windows, there's a binary package that's simply downloaded and installed.  The errors when these things are missing cryptic and it's not clear what's missing sometimes.  \n\nIt can also happen if an underlying library is not installed on your system, since the R package is just a set of hooks for that library.  GLPK is an example of this.\n\nWhen I google for your error, I found this on stackexchange: https://stackoverflow.com/questions/68824450/error-configuration-failed-for-package-ragg\n\nOne solution (for Linux) is to install some libraries:\n\n    sudo apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev \n\nSo it's possible these are missing from your system too.\n\nI'm not sure if that's relevant to MacOS but it might be another place to start."", ""This didn't work for me."", 'I did use quotes when I ran the code....I forgot to add them in the description box though which I just edited.', 'I tried to install lazyeval and I got a similar error message as described in the description box.', 'I do not have xcode command line tools installled and in order for me to be able to install the latest version, I would have to upgrade my macOS which I do not plan on doing anytime soon.  Do you know where I can find older versions of command line tools?', ""What didn't work, exactly? Did you get the same errors when trying to install the package?\n\nThe next thing I'd try is installing each package mentioned in your error message manually, starting with the first one, and retrying between package installations. There's at least one package that isn't installing, and other packages depend on it."", 'What is the message exactly?', 'Yes. Sign up for an Apple developer account. They have an archive of previous versions of command line tools.', 'The part in your response where you mentioned ""Tools > Install Packages"". I got the same error messages as described in the description box.', 'Here\'s what I did:\n\n`install.packages(""lazyeval"", dependencies=TRUE)`\n\nHere\'s are the errors I got:\n\nError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI\\[\\[i\\]\\]) : \r  namespace htmltools 0.5.2 is being loaded, but >= 0.5.4 is required\rCalls: <Anonymous> ... withCallingHandlers -> loadNamespace -> namespaceImport -> loadNamespace\rExecution halted\rERROR: lazy loading failed for package bslib\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/bslib\rWarning in install.packages :\r  installation of package bslib had non-zero exit status\r\\* installing \\*source\\* package callr ...\r\\*\\* package callr successfully unpacked and MD5 sums checked\r\\*\\* using staged installation\r\\*\\* R\r\\*\\* inst\r\\*\\* byte-compile and prepare package for lazy loading\rError in loadNamespace(j <- i\\[\\[1L\\]\\], c(lib.loc, .libPaths()), versionCheck = vI\\[\\[j\\]\\]) : \r  namespace processx 3.5.3 is being loaded, but >= 3.6.1 is required\rCalls: <Anonymous> ... namespaceImportFrom -> asNamespace -> loadNamespace\rExecution halted\rERROR: lazy loading failed for package callr\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/callr\rWarning in install.packages :\r  installation of package callr had non-zero exit status\r\\* installing \\*source\\* package lifecycle ...\r\\*\\* package lifecycle successfully unpacked and MD5 sums checked\r\\*\\* using staged installation\r\\*\\* R\r\\*\\* inst\r\\*\\* byte-compile and prepare package for lazy loading\rError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI\\[\\[i\\]\\]) : \r  namespace rlang 1.0.2 is being loaded, but >= 1.0.6 is required\rCalls: <Anonymous> ... withCallingHandlers -> loadNamespace -> namespaceImport -> loadNamespace\rExecution halted\rERROR: lazy loading failed for package lifecycle\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/lifecycle\rWarning in install.packages :\r  installation of package lifecycle had non-zero exit status\r\\* installing \\*source\\* package pkgload ...\r\\*\\* package pkgload successfully unpacked and MD5 sums checked\r\\*\\* using staged installation\r\\*\\* R\r\\*\\* inst\r\\*\\* byte-compile and prepare package for lazy loading\rError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI\\[\\[i\\]\\]) : \r  namespace rlang 1.0.2 is being loaded, but >= 1.0.3 is required\rCalls: <Anonymous> ... withCallingHandlers -> loadNamespace -> namespaceImport -> loadNamespace\rExecution halted\rERROR: lazy loading failed for package pkgload\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/pkgload\rWarning in install.packages :\r  installation of package pkgload had non-zero exit status\r\\* installing \\*source\\* package knitr ...\r\\*\\* package knitr successfully unpacked and MD5 sums checked\r\\*\\* using staged installation\r\\*\\* R\r\\*\\* demo\r\\*\\* inst\r\\*\\* byte-compile and prepare package for lazy loading\rError in loadNamespace(j <- i\\[\\[1L\\]\\], c(lib.loc, .libPaths()), versionCheck = vI\\[\\[j\\]\\]) : \r  namespace xfun 0.30 is being loaded, but >= 0.34 is required\rCalls: <Anonymous> ... namespaceImportFrom -> asNamespace -> loadNamespace\rExecution halted\rERROR: lazy loading failed for package knitr\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/knitr\rWarning in install.packages :\r  installation of package knitr had non-zero exit status\rERROR: dependency lifecycle is not available for package stringr\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/stringr\rWarning in install.packages :\r  installation of package stringr had non-zero exit status\rERROR: dependencies bslib, knitr, stringr are not available for package rmarkdown\r\\* removing /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rmarkdown\rWarning in install.packages :\r  installation of package rmarkdown had non-zero exit status', 'Hi, I got help from someone else on how to install Xcode with R but thank you for your input.', ""So install rmarkdown. Keep going through the lowest dependency until you get a specific error. Most likely, you're missing a system library required by one of the dependencies."", 'Great glad you got help.  On the CRAN site when you download the r package for Mac OS they list all the other installations needed for R to work on Mac for example xQuartz. It is a nice guide.', 'I tried to install rmarkdown and I got the same type of errors.']"
"[Q] Does a random intercept with 0 variance ""do"" anything?","Imagine an analysis that collects reaction time from participants, ten times each. The predictor is something like colour of the shape being responded to: red or blue. If there is 0 variance in the random subject intercept, is there any difference between a linear model with a random effect, and a linear model without one?",11s93vt,UnderwaterDialect,1678914015.0,7,0.9,"['A random variable with zero variance is constant with probability one. So a random effect model where the random effects have zero variance is the same as a linear model.', 'I think you basically have it right. Look here for a more in depth perspective.  Mostly focused on fitting your models in R but also full of good general information: \n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1', "" A random intercept model under the assumption that the variance is zero corresponds to each random intercept being the same constant almost surely.\n\nIn practice, an estimate of a random intercept model's variance being zero can arise if the conditional sample means of each individual's data points are identical. In the trivial random intercept model, where the only covariate is each individual's random intercept, then the maximum likelihood estimate of the random intercept is the sample mean of each individual's corresponding data points. If these are all identical, then the variance of the random intercepts is zero. Making inferences on such a model is identical to a model without random effects."", 'I think others have answered your question well, but just wanted to say that if I got that result the first thing I would assume is that something went wrong somewhere, because it seems very unlikely to get a variance of 0 with this type of data.', 'The degrees of freedom will be different than in the fixed of effect model. This often leads to different residual errors and associated measures of significance, eg p value estimations', 'True in the predictions, however, couldnt the within subject variance be greater than 0 while the between subject variance equals 0, which would at least provide some information regarding the subjects, and ultimately result in differing predictions if the error structure, and not just the mean point estimate, is used?', 'How would the within subject variance be greater than 0 if the random effect variance is 0? Arent within subject variance and random effect different names for the same thing?']"
[Q] low ppk when I transform due to non normality.,"I have a data set n=50 and need to meet a ppk above 1.33. I get a ppk over 1.59 but was told I need to do normality for Ppk. I ran AD, RJ, checked skewness/ kurtosis, and transformed to distributions with p value above .05 also was told try chevrons theory, but that doesn't work either. I can't get the normality to pass or a transformation that will give me a ppk above 1.33  with a p value above 0.05. Best I achieve is 1.15 on transformed data(largest extreme value)

Do I have any other options to get the data normal in order to accept the Ppk thats above what i want or do I need to measure more samples, and would this even work if I did or would I be wasting time.",11s7agu,Cold-Positive-818,1678910053.0,0,0.5,"['Note to readers of this post. If, like me, you went ""huh? what\'s this even asking about?"" I can save you some time.\n\nP a term from Statistical process (/quality) control (SPC/SQC).\n\nspecifically see:\n\nhttps://en.wikipedia.org/wiki/Process_performance_index', ""Its really hard to make judgments without seeing graphs of the data or understanding the process. What industry are you in?\n\nEdit: It sounds like the data are skewed towards the nearest spec limit. If there is a technical reason why the data is skewed, you might be able to make process/measurement adjustments. Just re-running the process more times is unlikely to help, since the acceptance criteria is not based on a confidence interval (its based on a point estimate). The only confidence intervals I know for ppk assume a normal distribution, so a confidence interval wouldn't help to create a rationale either."", ""Thanks. Plastic injection moulding. Data appears slightly skewed, but this is due to one or two samples out on their own. Of course, the samples were scrapped after measurement, so can't check for anomolies.Take them and lovely bell-shape.\n\nYes, confidence interval same issue. It's a five up tool and no issues with the other cavities. \n\nThanks anyway.""]"
[Q] Can you recommend me a good introductory course for various distribution/regression?,"First, I have mechanical engineering background but I did not take any statistic courses during college. Thus, I vaguely understand what is standard deviation but I was fine for 99% of my career. However, recently I am responsible for working on scripting Weibull Distribution and regression.

It has been very difficult to follow the concept. With lots of mentoring from my lead, I was able to somewhat understand the concept. However, I feel very lost when talking about Hessian matrix, covariance, gamma function, location, scale factor etc. There is also talk about maximum likelihood for optimization, mean versus basis, chi squared, 90% confidence etc It is all overwhelming. These terms are all foreign to me and I am chugging along but wondering if there is a some awesome introductory class that can light the bulb in my head.

Right now, I am okay with math so I just think as the equation to solve for and know the I need to use maximum likelihood optimization because there are multiple variable to solve with not enough equations, so I have to numerically solve and MLE is the way. But I dont know what all this means.

On top of it, I am trying to practice using Python and it seems like there is scipy package and people who built custom libraries on top of scipy that already can do what I am trying to do with Weibull distribution/regression. However, since I dont understand all the things, I struggle to use these functions for my specific case.

Plus, is this type of work related to data science?",11s6mdg,ag_silver,1678908620.0,1,0.6,"[""You are ranging over a fairly wide variety of topics here. You basically want a probability intro course, a statistics intro course (think Master level for both) and then Generalized Linear Models. \n\n>because there are multiple variable to solve with not enough equations, so I have to numerically solve and MLE is the way\n\nThis is not generally why you use MLE or why you solve numerically. \n\nYou solve numerically because you don't have a closed form solution.\n\nYou use MLE \n\n1) because it makes some sense, find the model that maximizes the likelihood that the model would generate the data you observed\n\n2) because it is efficient and has nice asymptotic properties\n\n3) because someone already wrote a library to solve for the parameters using MLE and not something else obscure you could dream up as a criteria"", ""You need some material in probability and mathematical statistics, plus some applied material related to what you're doing, by the look.\n\nIt may require more than one course or book to get what you need for all of those things - maybe 3 books or so.\n\nFor a mostly reasonable basic reference on distributions specifically, Wikipedia is often adequate.\n\nWhat sort of thing will you need to do with the Weibull distribution and regression? Do you mean you want to specifically fit Weibull regression? (Or something else?). \n\nWhat sort of phenomena are you modelling with the Weibull (e.g. failure times, wind speeds, rainfall etc)?\n\nAre any of your data [censored](https://en.wikipedia.org/wiki/Censoring_%28statistics%29)?\n\n> there are multiple variable to solve with not enough equation\n\nI'm not sure MLE will help there; it's useful for the opposite situation (more data than variables). If you have more variables than data to estimate them all you will either need some kind of *model* (to reduce the number of individual parameters by relating observations via the model) or perhaps  some form of smoothing or  *regularization* (to constrain the parameters somewhat)."", 'Given your background, you can probably draw a lot on linear regression and introductory statistics from  \n\n\nGreene, W. H. (2011). Econometric Analysis (7th ed). Upper Saddle River, NJ: Prentice Hall.   \n\n\nFor more extended [statistics study program](http://stanfordphd.com/StatisticsTutor.html), you would need to take a serious step inside probability theory and stochastic processes. Many modern problems in modeling and estimation involve [stochastic processes](http://stanfordphd.com/Stochastic_Process.html) / time series one way or another.', ""Thank you for detailed response. This also humbles me that I don't know what I am talking about regarding MLE haha. Looks like I gotta start slow and look into those intro classes."", ""Well, the gist is I have fatigue failed coupons and need to define a curve that fits the trend. So I can use the curve to predict when things fail when applying some certain fatigue load. So  industry standard is using Weibull distribution for a group of population. Then since I have multiple groups, there are different load levels, so I have multiple characteristic life. In order to fit the curve for those multiple lives, I use weibull regression. I think this isn't anything new, it was already researched and established like 50 years ago. But I am basically writing a script to automate the process and also be able to easily tweak things to explore different scenarios. In order do so, I would like to understand the concept under the hood.""]"
[Question] Expectation of Sample Standard Deviation,"Is there a clever way to find the expected value of the sample standard deviation (S)? In particular for Xi distributed iid as N(mu, sigma^(2)), where S^(2) = \[(X1 - Xbar)^(2) \+ ... + (Xn - Xbar)^(2)\]/(n-1).

I thought there was but I'm having a hard time finding it online.

edit: Changed mu in S^(2) formula to Xbar, where Xbar is the sample mean.

**Follow Up:**

I think we have confirmed that the best way to do this with the minimum amount of knowing things beyond what most graduate level mathematical statistics courses require is to directly compute this:

Define a random variable *T = (n-1)S**^(2)**/sigma**^(2)*. It is known that this follows a *Chi**^(2)* distribution with *n-1* degrees of freedom. Then define a function *g(t) = (sigma**^(2)**/(n-1))**^(1/2)**t**^(1/2)*. Then *g(T) = S*.

Use the law of the unconscious statistician to find the expectation. From here, you can pull out the constants and rescale whats left in the integrand to match the pdf of a *Chi**^(2)* distribution with *n* degrees of freedom. The value of the integral is then 1. 

Multiplied by the constants you pulled out and the constants multiplied in order to rescale, the expectation turns out to be *sigma* x *\[ ( 2/(n-1))**^(1/2)* *G(n/2) ) / G((n-1)/2) \]*, where *G* is the gamma function. 

thanks everyone for the help and suggestions. ",11s2543,zebrapaad,1678899235.0,0,0.4,"['The sample *variance* is well-studied and well-documented, [Wikipedia in particular has explanations of the entire sampling distribution thereof](https://en.m.wikipedia.org/wiki/Variance) (look at the section Population and Sample Variance). From there you can get whatever you want with the usual rules for functions of random variables.', 'Not sure what you mean by ""clever"". But one way is to approximate it via simulation:\n\n1. generate Xi ~ N(mu, sigma^(2)) for i = 1, ..., N\n1. compute SD, \n1. repeat step 1-2, for large number of times, say 1000 times for example\n1. compute the mean of the 1000 SDs you obtained from step 3.', '[This simulation of mine](https://onlinestatbook.com/stat_sim/sampling_dist/index.html) could be helpful. It estimates  the sampling distribution of several statistics including the sd.', 'E(Xi-mu)\\^2 = sigma\\^2\n\nBecause of iid,  E(S\\^2) = n sigma\\^2/n-1\n\n&#x200B;\n\nUsually sample standard deviation is calculated by\n\n\\\\sum (X\\_i - X\\_bar)\\^2 / (n-1)  we use the sample mean.', 'If you can follow Cochran\'s theorem to show that the variance follows a (rescaled) chi-squared distribution, then the standard deviation follows a similarly rescaled [Chi distribution](https://en.wikipedia.org/wiki/Chi_distribution).\n\nIt isn\'t a distribution that\'s much fun to work with, but it has a mean that can be written in terms of the gamma function. I am not sure there is any ""clever trick"" involved beyond integration by parts and recognizing the integral that defines the gamma function.', 's\\^2 is an unbiased estimator of the population variance without an assumption about the pdf.\n\nSquare root is a concave function, so the sample standard deviation is not unbiased, depending on the distribution of the data. But when do we ever need to estimate std dev?', ""No, there isn't an easy way to find that expectation."", 'I commented snarkily too soon. The distribution of the sample variance I think might be the most reasonable way to find the expectation of the sample standard deviation. \n\nSorry for being an ass.', ""I'm looking for a clever way to find the expectation of the sample standard *deviation*"", ""whoops sorry. You're right. should be Xbar in place of the mu's in the original post."", ""Thank you, I'll look into this. \n\nI also completely forgot that computing expectation manually is something I can do. Using the fact that the scaled sample variance follows a chi-square (n-1 df) distribution, you can simply express the sample standard deviation as function of that and then use some algebra to create a chi-square pdf with n degrees of freedom in the integrand that will integrate to 1 times a constant."", 'Integration by parts is unnecessary.  You only need to rescale as suggested above', ""I'm so glad you asked!\n\na past qualifying exam question asks for the UMVUE for the (1-p)th quantile with an iid sample from a N(mu, sigma^(2)), where both parameters are unknown. \n\nUsing Lehman-scheffe, I believe that you can show that a function of the sample standard deviation and sample mean is the UMVUE, but like you said, sample sd is biased, so you need to find the appropriate scaling constant."", 'https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation', 'Then the answer would be sigma\\^2\n\nS\\^2 is an unbiased estimator of sigma\\^2', ""I was looking for the expectation of the sample standard deviation. I've found it using direct computation and the fact that (n-1)S^(2)/sigma^(2) \\~ X^(2)(n-1)."", 'That is a distribution not an expectation.  You question did not mention distribution.', 'The distribution comes from the fact that the sample is iid normal. Since the data is iid normal, T = (n-1)S^(2)/sigma^(2) \\~ Chi^(2)(n-1).\n\nWith this, we can use the law of the unconcious statistician and compute the expectation of the sample s.d. as Eg(T), where g(t) = (t^(1/2) x sigma) / (n-1)^(1/2). By rescaling, the integrand becomes a Chi^(2)(n) pdf and that will integrate to 1.\n\nedit. So to compute the expectation here, you need the distribution.', ""And for non normal distribution this gives an answer that is a very good approximation for n > 50 say.\n\nBTW the answer is \n\n(2/(n-1))^0.5 G(n/2) / G((n-1)/2)\n\nwhere G is the gamma function.  That's a clever as it gets."", ""You don't need the distribution or normality assumption to directly calculate the expectation of S^2.\n\nYou asked for a clever way. Writing down the distribution is definitely not the clever way, arguably it is the most convoluted way."", ""Thanks!! I'm not sure if me realizing I can directly compute it counts as me being clever, but I'm going to pretend that you just called me clever and pat myself on the back b/c i need a win here. \n\nNow I'm interested in how quickly it becomes a good enough approximation for different levels of non-normality. I'm sure there's a theorem or something, but it seems like a fun thing to simulate."", 'Do enlighten me then please.', 'E(Xbar) = mu \n\nE((Xi-Xbar)\\^2 ) = E((Xi - mu)\\^2)  + E((Xbar-mu)\\^2)  - 2 E((Xi - mu)(Xbar - mu))\n\n=> sigma\\^2 + sigma\\^2/n - 2 E((Xi - mu) (Xbar - mu))\n\n&#x200B;\n\nAdding it up \n\n(n+1) sigma\\^2  - 2 sigma\\^2 = (n-1) sigma\\^2', 'okay, but I was talking about finding the expectation of the sample *standard deviation*. Is there a non-convoluted way to do that, without any distributional assumptions?']"
[Q] Mplus - Can we use multiple imputation with latent class growth analysis?,,11rwp3r,majorcatlover,1678887552.0,4,1.0,"['Nothing pops up on the mplus forums?', 'Most of it is super old and showing preference for FIML, but I have a longitudinal dataset with biased attrition rates, so I think multiple imputation would be a better approach.', 'Have you looked up other studies? Are they using FIML instead of MI? Like Patrick Curran or Greg Hancock, what are they doing with their longitudinal data? I dont have an answer for you, though. Sorry!', 'I have done a SEM course with curran and he seems to use FIML.', 'Spoke to Linda Muthen who recommended FIML to avoid class switching.', ""You can fix the classes with Svalues then use\n\nStarts = 0;\n\nPersonally I don't see why you couldn't use imputation but I guess it depends on the condition of the data? \n\nWould be an interesting trial."", 'What we wanted to do was to run the model on all imputed datasets and then pool the findings, if we fixed the classes that would defeat the purpose.']"
G*Power [Q],"So I am doing a g power analysis for sample size. I have 2 groups that I am comparing for 2 dependent variables. Do I do a separate analysis for both groups and each dependent variable, or somehow do it all in one calculation?

Thanks so much in advance",11ruykb,celadonium,1678883405.0,5,0.86,"['You can perform a single analysis that takes into account both dependent variables and both groups simultaneously.\n\nTo do this, you would need to use a multivariate approach. In G*Power, you can use the ""Multivariate: Repeated measures, within factors"" option under the ""Test family"" drop-down menu. This option allows you to input the effect size, alpha level, power, number of groups, number of dependent variables, and the correlation between the two dependent variables.\n\nOnce you have entered these parameters, you can click on the ""Calculate"" button to calculate the required sample size for your study. It is important to note that the required sample size obtained from the power analysis assumes that the assumptions of the statistical test are met, and the effect size and correlation estimates used in the analysis are based on prior research or theoretical considerations.', ""Yes, do the dependent variables separately if that's how you intend to analyze the data.  And then of course, you would have to use the higher sample size, unless you want to abandon one of the dependent variables."", 'Under the test family drop down I only have exact, f, t, x2 and z tests options', 'Okay thank you so much', 'For one variable the g power said sample size should be about 180 for one variable and the other was like 11000 so I think I did something wrong', 'f you do not see the ""Multivariate: Repeated measures, within factors"" option under the ""Test family"" drop-down menu in G*Power, it is likely because you have selected an inappropriate test family.\n\nFor a power analysis involving two groups and two dependent variables, you should select the ""t test"" option under the ""Test family"" drop-down menu. Once you have selected this option, you can choose the ""Multivariate tests"" option under the ""Type of power analysis"" drop-down menu. This will enable you to perform a multivariate power analysis that takes into account both dependent variables.\n\nIn the input fields, you can specify the effect size, alpha level, power, number of groups, number of measurements, and correlation between the dependent variables. Once you have entered these parameters, you can click on the ""Calculate"" button to obtain the required sample size for your study.', ""Possibly.  I assume it has to do with the effect size you're using.  But that may be the reality with one of the dependent variables."", 'I got the effect size with cohens d and it was like -.040', ""That \\_is\\_ a pretty small effect.  I'm not surprised the sample size comes out huge."", ""Definitely. With an observed effect that tiny there's virtually no evidence of a difference, so looking for the minimum sample to be significant at some arbitrary significance level is sort of ignoring what the analysis is telling you (not you you, just some you). Significance is kind of a fools errand anyway, which isn't to say I don't teach about it. Identifying such a sample size here is sort of zeroing in on the problem with significance testing.""]"
[Q] What does it mean to say that a distribution is skewed to the left?,"I have a poll running on Twitter ([https://twitter.com/AllenDowney/status/1635796990842875904](https://twitter.com/AllenDowney/status/1635796990842875904)) where I asked:

>When people say a distribution is ""skewed to the left"", some mean  
>  
>A: the central tendency is in the left side of the range  
>  
>and some mean  
>  
>B: the tail extends farther to the left side  
>  
>Which do you consider right?

The results so far are

1. A is right and B is wrong -- 46%
2. B is right and A is wrong -- 30%
3. It's ambiguous, so you have to specify what you mean -- 24%

What do the good people of /r/statistics think?

That is, what do you think the answer is, and what do you think of these results?",11rk81a,AllenDowney,1678849394.0,0,0.31,"[""If positive = right and negative = left then the answer is B. But I'd probably describe it in more detail or even show a picture if I really wanted to make someone else knew what I meant."", 'Technical definitions are not a matter of general public vote\n\nRead stats books written by statisticians (which won\'t be very likely to contain the phrase ""central tendency"" at all)', '2 and 3. And Im not surprised by the results. Prior to learning it in school I would have said 1.']"
Test for categorical data to determine which level is different [Q],Suppose I have the count of males and females in a dataset over 4 years. I know I can use chi squared to tell me if overall any years had a different proportion of males than other years. I also know that I can use a regression to tell me if any particular year is different from a reference year.  Is there any test that will tell me if a given year is different from all the other years?,11rgta7,OhSirrah,1678841580.0,3,0.8,"['Look at the Distributions, and compare if the mean/media are statically different from each other... t-test or ANOVAs should do the trick.', ""I asked chat gpt and its response suggested using a post-hoc test. Tukey's Honestly Significant Difference (HSD) and Scheffe's test both seemed appropriate based on the description. Its been a long time since I took a basic statistics course, but I vaguely recall learning these, at least Tukeys."", ""ChatGPT lied to you. Tuskey's post-hoc procedure is used after ANOVA, which, in turn, assumes the response variable is continuous. That's not the case with your data - you don't have a continuous variable, you have counts, i.e., the number of males or females by year.  \n\n\nThe easiest option is to run a chi-square test for each pair of years, and then adjust the p-values for multiple comparison. The pairs for which p < 0.05 are considered significantly different"", 'Hmm. It did say these are post hoc tests. But either I missed the part about being used for continuous dependent variables, or it didnt mention it. Thanks for the explanation.']"
[Q] Stats knowledge,"How can one get a deep understanding of multilevel models work? I keep being shocked by the amount of information I don't know about them and I feel like having more mathematical knowledge maybe would help?!? Like today I just realised I didn't know what the fixed effects correlations represent and have spent a bit on it and whilst it is not crucial for my work, I feel like I am always playing catch up. Does doing an undergrad or master's in stats help? I am a PhD in STEM, but not in stats. Or should I just keep adding and adding until at some point I'll know enough? I like stats btw, just have other obligations alongside learning about it.",11rgj1f,majorcatlover,1678841002.0,33,0.91,"['This is the curse of knowledge. The more you know, the more you realize what you know pales in comparison to what you don\'t know. That\'s why the old phrase ""ignorance is bliss"" is such a mainstay. If it\'s not crucial, don\'t put much thought into it. Play catch up when you need to. That\'s life - constantly learning and adapting. Nobody knows all, and that\'s the beauty of it.', '> I feel like I am always playing catch up.\n\nMe too, all the time and my PhD *is* in stats.', ""[http://www.stat.columbia.edu/\\~gelman/arm/](http://www.stat.columbia.edu/~gelman/arm/)\n\n&#x200B;\n\nI don't think knowing the detailed math helps if your goal is to work on real world models. In most cases you can look up the math and if you can understand the assumptions and the conclusion then you should be good. \n\nBut modelling isn't a science it is an art and in general you cannot find one true answer. However with practice you get better at it."", ""Track down the textbooks for undergrad (and grad) stats courses, and work your way through them as time permits. A PhD should have a pretty decent handle on independent study!\n\nI'd recommend using R or Python for the exercises, whenever appropriate, particularly if you are not familiar with either language. (Math folks tend to prefer R; computer science folks tend to prefer Python, with good reason in both cases.)"", 'I worked with multilevel models while working on my PhD and somehow they always were some kind of mindfuck to me. I built models which made sense to me in one moment and the next day I could not understand why I went that route with a model. For some reason non of the examples online were a fit to my problem so I am still not sure what I did, but I thinkt it was kinda the best I could do. Multilevel models are an interesting area...but they fucked up a few of my colleagues.', ""Here's a learning path for probability and statistics that might help  \n[https://www.skytowner.com/explore/population\\_samples\\_and\\_sampling\\_techniques](https://www.skytowner.com/explore/population_samples_and_sampling_techniques)"", ""The Raudenbush and Bryk book is in one of the best stats books ever. If you're interested in longitudinal analysis Singer and Willett is also quite excellent and a bit more accessible place to start. Two great reads. Page turners."", ""That's very reasonable but I still want to know everything xD"", 'This', ""Wow, that's interesting. I guess there's always more to learn even when your topic of expertise is stats. Do you feel that as a stats PhD you are able to represent things very well in your mind as oppose to mostly memorizing facts about how things work?"", 'Thank you for these resources and advice!', "">Track down the textbooks for undergrad (and grad) stats courses, and work your way through them as time permits.\n\nRegarding this, I recommend [Congdon's Bayesian Hierarchical Models](https://www.routledge.com/Bayesian-Hierarchical-Models-With-Applications-Using-R-Second-Edition/Congdon/p/book/9781032177151)."", ""I use R and Julia, but also know python even though I don't use it often so if you have any book recommendations let me know. I have gone through some, e.g., An Introduction to Statistical Learning. However, I am a bit afraid of not getting the maths of more complicated ones and also want to know the models more deeply. I am prone to incoherence as you can tell. Thank you for your suggestion!"", ""Yes, I have a lot of colleagues in the same situation. We all just assume we need to learn them alone and pretend we get it all. That's why I feel like I need to understand the underlying mechanisms involved in these models so can better move between models. I have been engaging more in Bayesian analyses and I think they care more about actually understanding things than just doing it, at least those doing it at a higher level. The knowledge of probability, caring about uncertainty and distributions more than just the results is very useful. But again, I am always playing catch-up."", 'Thank you so much! I will definitely check them out!', "">Do you feel that as a stats PhD you are able to represent things very well in your mind as oppose to mostly memorizing facts\n\nI'm not a PhD, but I've got some grad school under my belt.\n\nThings in stats didn't really *click* for me until I completed 2-3 undergrad courses. Until then, it was just applying formulae from note cards. Fortunately, in applied stats, the math itself is pretty basic, so I brute forced my way to good scores. Once it clicked, the material was very straight-forward in my mind. I was trying to make it more complicated than it really is."", ""I really don't know how to respond to that --- I wasn't doing 'mostly memorizing facts' since way before the PhD. \n\n(Though there's still some of that, of course.)"", ""In grad school stats, we're not memorizing things. Even in upper-division undergrad, if you are in STEM, you probably aren't memorizing things anymore after your sophomore year. It didn't matter if it was physics, mathematics, or stats. by that point in time, you are working on things that probably don't have neat formulas."", ""If you haven't read it try \nhttps://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659\n\nwhile it is not a very advanced book, it does give you an insider's view."", 'Thank you!', ""An Introduction to Statistical Learning is a dumbed-down (in a good way) version of Elements of Statistical Learning. The *Intro* version offers an excellent overview of many ML algorithms, without getting bogged down in the math.\n\nIf you are *using* (as opposed to *writing*) ML functions in r/Python/etc., *Intro* is probably plenty.\n\nAll that being said, those books are about machine learning algorithms. They won't provide much in the way of elementary/applied statistics. I had stats professors at two different universities use *Statistics* by McClave & Sinich, so I'd assume it's a reasonably good text (for applied stats)."", 'Yes! I felt stupid when I didn\'t know something, but at some point we found out, that we all had our problems with it. But at some point I got why the answer to statistical questions is so often ""it depends"".', 'Do you recommend any of the courses? I might check if I can attend some at my university and check the books people have been recommending in the comments.', ""I see, that's what I want, to be able to represent what is happening in the background more than just being able to interpret the models."", 'Thank you!', ""Thank you, I'll check it! I am also a dumbed-down version a statistician xD"", ""most of my colleagues' slogan is: it's all about convergence. If it converges, it fits. Like cats: If it fits, I sits"", ""I wouldn't recommend sitting through a brick & mortar undergrad applied stats class. I think you'd find the pace painfully slow."", ""Thank you, I guess I can just work through the books at my own pace and come here when I am being dumb for clarification -.-'"", ""Consider contacting a professor in your university's stats department about borrowing/buying an applied stats text, along with an instructor solutions manual."", 'Any particular text that you found helpful?', ""I used what my professors chose for their classes. They were helpful, but I don't know how they compared with texts not chosen by my professors.""]"
[Q] Is a LASSO an appropriate way to deal with predictors that sum to 1?,I have five predictors that are proportions and sum to 1. Would a LASSO be an appropriate way to approach the analysis?,11r8fdh,UnderwaterDialect,1678805998.0,10,0.92,"['Proportions, percentages, probabilities etc are called ""[compositional data](https://en.wikipedia.org/wiki/Compositional_data)"".  The two problems is they\'re inherently multicollinear, and that all points are constrained to [0,1]. \n\nPerhaps the simplest tool is to use some sort of logratio transformation (see link above) so you can typical linear models. There\'s a lot of literature on compositional data analysis and it\'s easy to get in pretty deep. IMO doing an additive logratio transform is a great first step and will give some familiarity with CDA.', 'Probably not. \nIf they all sum to 1 you more than likely have compositional covariates and should consider methods from compositional statistics.', ""As noted by the other comments, you want to use techniques from compositional analytics. \n\nHeres a rough outline of using CLR and LASSO in Python, noting the `centralize()` being necessary to give predictors mean 0 for LASSO:\n\n```python\n\n# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LassoCV\nfrom skbio.stats.composition import clr, centralize\n\n# Load compositional data into a pandas DataFrame\ndata = pd.read_csv('compositional_data.csv')\n\n# Separate predictor variables from response variable\nX = data.drop(['response_variable'], axis=1)\ny = data['response_variable']\n\n# Perform CLR transformation on predictor variables\nX_clr = centralize(clr(X))\n\n# Define LASSO model with cross-validation\nlasso = LassoCV(cv=5)\n\n# Fit LASSO model to data\nlasso.fit(X_clr, y)\n\n# Print results\nprint('LASSO coefficients:', lasso.coef_)\nprint('Intercept:', lasso.intercept_)\n\n```"", 'they key book on this was experiments with mixtures from Cornell https://onlinelibrary.wiley.com/doi/book/10.1002/9781118204221\n\nin simple regression, you drop the intercept, for interactions you use things like x1\\*x2 not like x1^2', ""Thank you! I was able to do this in R. But I don't find the results all that easy to interpret because I lose one predictor. \n\nIt's the reason I was drawn to the LASSO because I imagined I'd be able to include all predictors.\n\nThere is a reply below saying that I could run a simple regression and remove the intercept. Would that work just as well as a transform?"", 'I dont fully understand this:\n\n- the predictors being constrained to 0-1 doesnt directly violate any assumptions of linear regression. It might lead to heteroskedastic residuals, but then Id think a logit transform would be better than a log transform\n- the logratio transform doesnt help with the inherent multi-collinearity you reference, since its monotonic (right?)', 'Thank you! I suppose I could adopt this into my current LASSO in R by adding the CLR transform step?', 'Oh interesting! Am I right that it would be as simple as doing the following in R:\n\n    DV ~ IV1 + ... + IVn -1', "">the predictors being constrained to 0-1 doesnt directly violate any assumptions of linear regression. It might lead to heteroskedastic residuals, but then Id think a logit transform would be better than a log transform\n\nThe log-ratio transform isn't just a log. The logit transform is actually a special case of a log-ratio transformation when you have a simple proportion. \n\nFurther, while it's definitely not mathematically impossible that the linear regressions might be met on the untransformed data (other than dropping one possibly to remove collinearity), it does make the interpretation a mess. Because of the joint constraint across the covariates it's impossible to vary a single value of the covariate while holding the other covariates constant. This makes the coefficients basically totally un-intrepretable. \n\n>the logratio transform doesnt help with the inherent multi-collinearity you reference, since its monotonic (right?)\n\nIt's not the constraint to 0-1, but the constraint that they sum to 1 across the covariates. This guarantees that the covariates are perfectly multi-collinear. The additive and isometric log-ratio transformations prevent this from being a problem by embedding the covariates in a p-1 dimensional space."", ""It's the unit sum constraint that makes the variables perfectly colinear. `x_i = 1 -  (x_1 + ... + x_p)`"", 'I think so, when I did that SAS was still king. the key is that now its all estimable and interpretable.', ""> The log-ratio transform isn't just a log. The logit transform is actually a special case of a log-ratio transformation when you have a simple proportion.\n\nOh very cool! I misunderstood these transforms, thinking the denominator was a constant. Thank you.\n\n> It's not the constraint to 0-1, but the constraint that they sum to 1 across the covariates. This guarantees that the covariates are perfectly multi-collinear. The additive and isometric log-ratio transformations prevent this from being a problem by embedding the covariates in a p-1 dimensional space.\n\nThat makes sense  though I guess I am a bit confused still, because the CLR doesnt actually drop a component, so would that one still have issues with collinearity?"", 'Can I ask what the coefficients are comparing to, if there is no intercept?', "">That makes sense  though I guess I am a bit confused still, because the CLR doesnt actually drop a component, so would that one still have issues with collinearity?\n\nThat's correct, which is why the CLR is not suitable for covariate transformations while ALR and ILR are. CLR is the inverse of the softmax function, so it can be relevant when you have compositional response data."", 'mostly each other. If a coefficient is larger adding this will increase response.', 'Ah ok, comment below suggested using CLR which threw me off.', ""Okay! This seems like a much simpler solution than others I've seen (transforms), and one that allows all predictors to be included. Is there any downside?"", ""when I was doing experiments with mixtures (as in powders where only the relative amounts were of importance of the property) no. But I don't know your application. \n\nCornell is dead it seems, but most of his papers/books would help. https://www.researchgate.net/publication/321977797_Remembering_John_Cornell_Everything_Mixtures_from_Gasoline_to_Fish_Patties""]"
[Q] [R] - How to analyse event frequency?,"I urgently need help analysing event frequency. I recorded how many times a fly turned in two different conditions and am trying to figure out how to determine if there is a significant difference. Because I simply have two values, how many times they turned in the two conditions, I cannot produce any standard deviation. Am I missing something obvious ?",11r6enn,harrymayes,1678801250.0,3,1.0,"[""Spend a bit more time explaining your experiment and then statisticians can help you.( Add to main \nQuestion)\n\nAre you counting number of events in a fixed time?\nThen maybe a Poisson distribution test will help you\n\nhttps://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/hypothesis-testing/hypothesis-testing-with-the-poisson-distribution.html\n\nI guess you need a 2 sample Poisson test.\n\nThe standard deviation of a Poisson variable is function  of the mean\n( Similar to binomial) so you don't need to explicitly estimate it...(you have to convince yourself that your data is approximately Poisson distributed)\n\nSee a worked example with minitab\nhttps://support.minitab.com/en-us/minitab/20/help-and-how-to/statistics/basic-statistics/how-to/2-sample-poisson-rate/before-you-start/example/"", ""This is count data. You could be clearer but it sounds like you want to compare the two proportions; since they're perfectly dependent (there's only two options; the counts add to the number of trials), this is equivalent to testing whether the proportion of condition 1 differs from 0.5 (straight binomial proportions test), or it could be done as a chi-squared goodness of fit if the expected counts aren't too small.\n\nDo you have a basic stats textbook you can refer to? A decent one should cover both tests of proportions\\* and goodness of fit tests.\n\n\\* though a very basic book probably won't do the binomial test itself, but just the normal approximation"", 'It is not possible to draw meaningful statistical inference based on a sample size of one, at best you can report the frequencies themselves.', ""Umm... this isn't a sample size of 1. Every run can be treated as a Bernoulli trial.""]"
Mediation Analysis (Baron & Kenny) [Q],"Hi all,

I am currently doing my MSc dissertation and have just finished a series of mediation analyses on my data. I used the 4 step Baron and Kenny method (without step 1 as path c is arguably not relevant). I understand that there are alternatives to this method, such as bootstrapping. Is the Baron and Kenny method that bad? Is there argument to use it over newer methods? I kind of don't want to have to start all over again but will if I have to. [Q]",11r64wz,diarydiario,1678800562.0,3,1.0,[]
[Q] prediction with grouped data (scale data by group?),"This is a repost from r/askStatistics but I havent gotten traction there yet. Thanks for the help!


I have an assignment at work that is driving me a bit crazy that I need some help with. I can't divulge too much about the details of the project, given that the data and work is proprietary, so I will do my best to describe the data as best as I can given my limitations. 

I have a regression prediction problem that has data from the last 3 years, approximately 300 observations for each year. The distribution for the outcome for each year is clearly different, in that the range of values vary significantly, but all are roughly Gaussian and overlap. Furthermore, I believe that all of the features I've developed are likely to have a similar but not exactly the same relationship with the outcome across years (this pans out in the training data). I'm not satisfied with my out-of-sample MAE though, and I'm pretty sure that the differences in ranges are causing problems in my predictions.

What I""m wondering is if it is reasonable to standardize the values by group to get all of the values on the same scale and then reverse that transform by year when calculating MAE? 

For further context, this model will be deployed for future prediction. If i am to do this scaling I would wait for about 20 observations, retrain the model with the new year's scaled data and transform parameters (e.g. the new years mean and sd) and then deploy it. 

Under normal circumstances I would also try a multilevel model with a random effect for year, but this model is deployed in excel by my coworker, where he takes the model coefficients and plugs values in. It's truly not ideal but he ""has a system"" and spreadsheet made for this work and I'm relatively new so don't want to blow his work up. 

Thanks for the help!",11r54gn,ParlyWhites,1678797851.0,1,1.0,[]
[R] Calculating the distance of two variables to their correlation value.,"Hello everyone,

I  calculated for every age/year a correlation-matrix for different  variables. Then I vectorized the lower triangular matrix and ordered the  correlation values in a new matrix. This results in a matrix, where the  rows represent the age/year and columns represent the different unique  correlation values.

Now I have the  following problem, I would like to know distance of one subject to the  correlation values in their specific age group (to the vector of  correlation values). However, I only have the raw values of the  variables for one person and not the correlation values, and I can't  calculate a correlation with only one subject. Is there way to calculate  the distance from 2 variables to a correlation value?

I hope this brief explanation is clear.

Thanks in advance for any tricks!",11r43q8,bamnotadoctoryet,1678794847.0,1,1.0,"[""I think you're describing the residual, y - yhat, in the regression that the correlation coefficient came from.""]"
[Q] Partial Autocovariance,"**W**e **def**ine **part**ial **autocovariance o**f **l**ag **h** **t**o **""isol**ate"" **t**he **eff**ect **o**f **oth**er **lag**ged **varia**bles **wh**en **calcul**ating **autocovariance**. **Part**ial **autocovariance** **i**s **giv**en **b**y **t**he **coeffi**cient **o**f **t**he **vari**able **X** **s**ub **h** **fr**om **t**he **lin**ear **regre**ssion **o**f **t**he **vari**able **a**t **ti**me **t** **o**n **i**ts **lag**ged **val**ues (t-1, **.**.. **,** t-h) **.**

**M**y **ques**tion **i**s, **w**hy **isn**'t **part**ial **autocovariance o**f **la**gs h-1, **h-2.**... **equ**al **t**o **t**he **coeffi**cient **o**f **t**he **varia**bles **X\_t-1** **,** **X\_t-2** **.**.. **i**n **t**he **sa**me **lin**ear **regre**ssion **mod**el **?** (on **t**he **pa**st **lag**ged **val**ues **u**p **unt**il **X**\_t-h **)** **?**",11r3rkx,Icy_Put_659,1678793759.0,9,0.91,"['Why are some letters bolded in your question?', 'Here is an explicit description of the regression coefficients in general.\n\n[https://stats.stackexchange.com/questions/196807/explicit-solution-for-linear-regression-with-two-predictors](https://stats.stackexchange.com/questions/196807/explicit-solution-for-linear-regression-with-two-predictors)\n\nConsider a model x_t ~ x_t-1 + x_t-2.  It shows that the regression coefficient of x_{t-1} would be related to lag one auto correlation coeff with some  additional stuff. \n\nIn fact the algebra gives you a recursive formula\nhttps://en.wikipedia.org/wiki/Partial_autocorrelation_function\n\n(See the recursion formula).', ""I use a chrome extension that bolden the first letter of every word on a page, it helps with attention while reading (helpful if you have sever attention issues and can't read a few sentences without skipping words/zoning out)\n\nI thought it only showed up in my own browser though"", ""It's a technique called bionic reading that's meant to increase the speed at which you can read materials.\n\nFor materials written at a 6th-grade level, it works well in my personal experience. For materials written at the level of a master's degree holder, it's not so great."", ""Yeah and it doesn't work with latex""]"
[C] SAS Clinical Statistical Programmer vs Data Scientist vs Data Engineer: which of these jobs is more demanded and pays the most?,,11r3hhd,Born-Comment3359,1678792856.0,0,0.2,"[""Titles mean very little, so you could tell me any of these paid the most a d I'd believe you.\n\nBut I still wanna play the game\n\n1. Data engineer\n\n2. Data scientist\n\n3. SAS Clinical..."", 'DE >>>>>>>>>>>>>>>>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS', 'Really?! DE more than DS?', 'YES. If you get a degree in data science you probably wont even work in ds starting out. Its data analysis/data engineering to start typically. Data engineering is arguably more important since you cant draw any insights if you arent storing and moving data efficiently. \n\nMaybe DS is more in demand, but theres just so much supply of fresh grads since a lot of schools are making programs for the field. Less people want to work on storing data in a cloud compared to fitting fancy models.', ""Sure? It's all just made up, but in general I thought an engineer would make more than a scientist.""]"
[Q] [D] MSc in Statistics or Computational Statistics & Machine Learning,"Hi all, Im looking to do postgrad after my Bachelors in Data Science. Which of these MS would be more employable and able to get a higher salary?

Location in UK",11qyt7w,No_Canary_5299,1678775660.0,0,0.5,['What do you want to do?']
[Q] Term for a statistical method similar to a weighted median,"Hi r/statistics,

There is a fairly simple statistical method I have come across with work, which appears similar to a weighted median. I am searching for the proper term for it. Is someone here perhaps be able to identify it?
The situation is that we have a large dataset containing the numbers of items per group. Were interested in finding the balance point where the sum of items is equal above and below.
For example, with the following data, the sum is 30, the mean is 3.33 and the median is 3. The statistic were interested in is at what point does the sum of the bottom portion equals the sum of the top portion. In this case, its 4.

1
1
1
2
3
3
4
6
9

What is this called? It feels like it should be simple but it is eluding me.",11qyqfa,Nightlight10,1678775406.0,1,1.0,"[""I've no idea if this measure exists and has a name. If you had the set of \n\n1 2 3 4 10\n\nwhat would you expect your metric to be? 7?"", 'This is minimizing l1 loss subject to an exact constraint. Because of this its an alternative estimator of the median, but one that is not at all robust and also not guaranteed to exist.', ""Interesting. I would say my metric would be 10, as that's the point where the sum of the bottom portion equals the top. It really falls down with a small dataset like that, but with the large dataset I'm dealing with at work, it shows a valuable 'balance point'."", 'Yeah, okay. Thanks for the comment.']"
JS A/B testing [Question],"I'm writing a small analysis of data retrieved from testing variations of a website. The goal is to see which version cause the most events (clicking a sign up button, for example) to occur.

This is not a homework problem. I'm working on an open source project on github. I volunteered to attempt this portion since I have some knowledge of stats.

Each experiment has a control and up to 4 variants. I believe using a two sample binomial test to compare the number of distinct user clicks to the number of users for each group is appropriate. Since I have multiple variants, I also wanted to add a correction for that. I think the bonferroni correction is appropriate. For website designs, we prefer to error on the side of having false negatives instead of false positives. My understanding is that this is a conservative correction.

I appreciate any advice on the analysis design.

I'm also looking for advice on javascript libraries to implement this analysis. Some options I have found are: 
1. Stats.js << possibly discontinued? Erased?
2. jStat
3. Simple statistics
4. Math.js

Last time I did statistics I used python and excel so I have no experience with these libraries. Do you have one you prefer and why?

Thank you.",11qtu28,Any_Examination2709,1678760377.0,3,1.0,"['Why does the JavaScript even need to perform the hypothesis test? Just do it yourself when you have enough data. Repeatedly testing the same data over and over again, as would be the case if you dynamically updated the test statistic for every new data point, is not sound statistical practice.', ""Javascript will be responsible for doing the statistical analysis in the backend of the website. This is an application that will be self hosted, so I won't have access to their data for more privacy. JavaScript will automatically perform the analysis once the experiment is complete, not an ongoing analysis.""]"
[Q] Is the effect sig higher than zero?," I want to check if the response time difference between the two types of trials (e.g., response trials of incongruent trials - response times of congruent trials) is significantly higher than zero in order to classify each individual as a learner/non-learner. Is there any good way of doing it? I have a decent number of trials so I was thinking about running a model per participant and checking whether there is a sig effect of trial type. Would this work or is there a better way?",11qsnwz,majorcatlover,1678757399.0,1,1.0,"[""How do you know that your classification rule is yielding *correct* labels? (That is, the ones you're calling learners are really learners and the ones you're calling non-learners are really non-learners?)\n\nPresumably (if you can be confident that you really can tell a learner from a non-learner) it would be better to try to minimize some sort of misclassification cost rather than to fix the rate of one and try to minimize the rate of the other.\n\n\nWill it work anyway? well, yes, in that you can define a classification rule based on it. Whether it's especially useful is not as clear, but you could perhaps make some assumptions and see whether it is satisfactorily dealing with simulated cases."", 'That\'s a great point especially when considering how difficult it is to determine what ""learning"" consists of. Do you have any suggestions for a paper which has looked at this issue?', 'Not my area, sorry.']"
[Q] How would you approach this problem? Predicting sales for a month.,"So I have a dataset from 2019 Dec - 2020 Nov (plus 10 days of data in 2020 Dec)
So 12 months worth of data at a date level. Some dates are missing. And I have 10 dates for December. 

I have to predict the total monthly sales in 2020 Dec overall. 

- Is 12 months of data enough to predict December's sales, specially a month with a spike in shopping behaviour? 
- Would you group total sales by month, or create the forecast at a date level and sum the dates in December? 
- What model would you use?
- What would you do the 10 dates of data available in December? If calculating at a month level, do I just filter those out?",11qr2hr,johnnypanics,1678753432.0,2,1.0,"[""* There is a strong seasonal effect that you cannot ignore. With just a year of data, it means almost all months are useless for your analysis.\n* There is an obvious difference between 2019 and 2020 that you cannot ignore.\n\nIf you can, try to get data on seasonal variations (or even comparable Nov 2020 to Dec 2020 ratios) from other sources. If not, you could try to scale the 10 days of 2020 to the full month using the ratio observed in 2019, but it's likely the pandemic shifted the shopping behavior within December as well."", 'COVID broke the ""the future looks similar to the past"" assumption of most forecasting models, particularly those trained with supply-side data. COVID notwithstanding, you\'ll need at least two years of data to model your seasonality. Three years or more is better.', ""Let's say we don't want to factor in COVID, I just want to understand conceptually - would it be better to run the model at a day level dataset (ie. Model predicts the remainder of the days in December and I add them up), or run the model at an aggregated month level. \n\n\nI'm fantastically new to this, as you can imagine."", ""You have an annual cycle, so you'll need at least two years of historical data to train your model for seasonality. Generally speaking, at least one additional year is desirable for validation.\n\nIn terms of grain, there's the grain of your data. You usually have no control over this. There's the grain of your delivered forecast. This is a business decision, not a data science decision. Finally, there's the modeling grain. This is a data science decision.\n\nSuppose you have daily data, and the business desires a monthly forecast. You might get the best results by modeling days and aggregating to months, or maybe by modeling quarters and disaggregating to months. Or maybe modeling months to start with is actually best. You might even find that one method works better in the short term, while longer-range forecasting is improved with a different method. These are the sort of things you'll need to determine for each project."", ""Got it. Thank you very much, this is a really helpful response. I'll keep this in mind moving forward with the project. Probably try a bunch of different approaches. If nothing else, I'll at least learn something.""]"
Findings jobs with undergraduate degree [Q],"Im taking a year off after my undergrad which Im finishing this spring. My concern is that Im having a hard time finding stats-related jobs that Im qualified for, and Im not sure where to set the bar if I want my job to help me get into a good masters program.

Most of the stats-data related jobs Im finding that dont require a masters of PhD, either require data science experience or its an internship for current students. Ive taken data science classes but my only hands-on experience is in a statistical genetics lab on my campus, which is a field that generally requires at least a masters (Im an unpaid undergrad researcher). That experience is more helpful for getting into a grad school than finding a job with an undergraduate degree. I found a few biostats internships for recent graduates but theyre extremely competitive with hundreds of applicants. I almost wish that i got a data science internship instead of joining the lab. Ive only found a few data science jobs in all my searching that did not require extensive experience and wanted a recent undergraduate. 

Ive also found some more general jobs in labs, like psychology or bacteriology. They dont require any specific major so it seems like a general research role, and the descriptions mention that you would handle data but probably not in the way that would impress a stats ms program. 

Does anyone have advice for finding jobs? And what kind of job should I aim for if I want it to look good to a stats ms program?",11qon85,rabidsaskwatch,1678747801.0,1,1.0,"[""Depends what you want\n\nYou can probably learn SQL online in under a month and apply for data analyst roles. They will likely be reporting-heavy and light on actual statistics but you can use one to then pivot into data science more easily\n\nOr you can go for an internship if you think it's the stepping stone towards better roles""]"
[Question] Best book for dynamic linear models,"I am looking for a good resource on dynamic linear models. I generally prefer books because of the strong editorial voicing and educational framing. I have only been able to find two books on the topic: [West and Harrison's Bayesian Forecasting and Dynamic Models](https://link.springer.com/book/10.1007/b98971) and [Campagonli, et al Dynamic Linear Models with R](https://link.springer.com/book/10.1007/b135794). Can anyone recommend either book, or a third option?",11qmhk0,Secret_Identity_,1678743147.0,8,1.0,[]
[Q] What does it mean to choose a distribution vs have the qualities of a distribution?,"This might be a broad question but I'll help narrow down the scope as best I can.

I'm trying to reconcile ""distribution"" as the shape/nature of a sample/population versus the idea that you *choose* a ""distribution"" to calculate a probability.

Some statistical tests assume a population has a Normal distribution to work properly. This gives me the impression that a population has a type of distribution by its nature.

However, when reading about discrete probability distributions, the writers tend to use language like ""select the right distribution"" to calculate probabilities (e.g., Geometric vs Hypergeometric).

But in both continuous and discrete distributions, authors often use a more active language when describing distributions. Student's T has its own distribution but it can ***become*** more Normal as DF/n increases. 

So what does it really mean for a population to *have* a type of distribution versus telling the student to *choose* the correct distribution?",11qkv7e,werdunloaded,1678739663.0,1,1.0,"['> Some statistical tests assume a population has a Normal distribution to work properly. \n\n\'Work properly\' suggests that we\'re discussing  the assumptions under which the null distribution of a particular test statistic might be derived (which gives alpha to be the value youselected from the available significance levels if the assumptions are true)\n\nSomeone might make some assumptions under H1 in order to calculate power functions under some sequence of alternatives but that\'s usually not what people mean when they say \'work properly\'\n\nOf course (a) we don\'t have to assume normality, any suitable distributional assumption under H0 might be used in its place, or indeed in some cases just a few mild conditions might be imposed on the distribution rather than some fixed family; (b) distributional assumptions are almost never going to be true - though in some cases that won\'t make a substantive  difference to (say) significance level. Models are always going to be abstractions/ approximations; what\'s important is how much it matters to the inferential properties you care about. (Inexact probability models - like the proverbial spherical cow in physics - can still give very useful answers. What matters, then, is being able to tell *when* they\'re going to be useful and when they aren\'t.)\n\n> This gives me the impression that a population has a type of distribution by its nature.\n\nI\'d perhaps put it this way: \n\nDefine the population distribution function F(x) as the proportion of the population values less than or equal to x. \n\nSo far this is just description,  but here\'s how we get probability (and so tests, CIs etc) out of that. A random variable has a distribution. The random variable you obtain by randomly selecting a single member of some population  is a random draw from the population distribution.\n\n[When you\'re dealing with some data-generating process rather than some ennumerable population, replace the phrases referring to population with ""by observing some  random process"" and ""process distribution"" respectively.]\n\nThe problem is you simply don\'t know the population distribution (at least not exactly, though you may be aware of its general characteristics), and hence you don\'t know what the probability distribution of a randomly selected observation will be. It still comes down to choosing a model - an abstraction / approximation. It\'s really about models all the way down. \n\n> However, when reading about discrete probability distributions, the writers tend to use language like ""select the right distribution"" to calculate probabilities (e.g., Geometric vs Hypergeometric).\n\nI dont know that I\'ve seen this particular phrasing but presumably this is mostly about recognising characteristics of the situation/problem that would suggest a particular model as a suitable approximation (and other possible distributions as relatively unsuitable because they dont arise from those abstracted characteristics). \n\nE.g (a) recognising a situation might be modelled as a set of  Bernoulli trials and knowing which of the associated distributions fit the random variable of interest, (b) noticing whether you\'re dealing with sampling with replacement  in a finite population (or not),  and so forth.', '>I dont know that I\'ve seen this particular phrasing but presumably this is mostly about recognising characteristics of the situation/problem that would suggest a particular model as a suitable approximation (and other possible distributions as relatively unsuitable because they dont arise from those abstracted characteristics).\n\nAfter coming back to it today, this is exactly what it is.\n\nMy new understanding: Sometimes you don\'t know the distribution of the population you drew your sample from, so trying to make an inference or prediction about the population requires matching your sample as best as possible to a distribution (i.e., ""choosing"" the distribution). Matching the data to an incorrect distribution can cause you to select a poorly fit model (because the model follows certain assumptions that aren\'t being met).\n\nIf my understanding is off, feel free to let me know. Thank you for your help.']"
[Q] Multivariable regression: how to account for uncertainty in estimated variables,"Hey everybody!

I'm looking for guidance on how to account for the uncertainty in the point estimates that I'm using as variables to build a multivariable regression model. 

I have the 90% confidence intervals for each estimate, but I don't know how to ""plug in"" that uncertainty to be appropriately reflected in my analysis. 

In short, I'm trying estimate the correlation between two variables using publicly-available U.S. CDC/Census Bureau data, with several other estimates used as controls:

>Dependent variable = county-level Labor Force Participation rates

>Independent variable = county-level Diabetes prevalence


>Control variables: county-level Poverty Rates, Median Educational Attainment, Disability Rates, etc etc...

My dataset basically looks like:

>County 1: Diabetes prevalence = 10.0% (+/- 2.0%); Labor Force Participation = 60% (+/- 5%); Poverty rate = 20% (+/- 3%) ... ...

and so on for 100+ counties.

What method(s) could I use to account for the uncertainty in the ~1000 county-level estimates I'm using to estimate the correlation between my two main variables of interest? 

If anyone could point me in the right general direction, I would be very grateful. I have Stata but I'm willing to explore other tools if needed.

Thank you!",11qi83g,jesushitlerchrist,1678734003.0,8,1.0,"['Interesting question! You could use an errors-in-variables model like [Deming regression](https://en.m.wikipedia.org/wiki/Deming_regression) or you could go the Bayesian route where you include a measurement error parameter in your final model [like this](https://mc-stan.org/docs/2_21/stan-users-guide/bayesian-measurement-error-model.html).', ""Apart from the methods suggested by one of the commenters here, I think you can try a simulation based method.\n\nLets say your final estimate of the quantity of interest (e.g. correlation) can be written as\n\nY = f(X1, X2, ...Xn)\n\nIn your case, (X1,...Xn) is not fixed and has uncertainty associated with them.\n\nEach Xi has a distribution with (mean =Mi, sd = Si).\n\nCurrently your final estimate is\n\nY\\_hat = f(M1, M2, ...,Mn)\n\nWhat you can try is to generate a realization from those distributions.\n\nInstead of (M1, M2, ...,Mn),  \\*generate\\* data (z1,z2,..zn) where say\n\nzi \\~ Normal(mean =Mi, sd = Si)\n\nThen you new estimate will beY\\_hat = f(z1, z2, ...,zn)\n\nDo it again and again.Each time values of (z1, z2, ...,zn) change and hence the value of Y\\_hat changes. Thus if you repeat these 1000 times, you have 1000 Y\\_hat values. That is the distribution of Y\\_hat.You can then report mean, median, other percentiles of that distribution.For example, you can report mean of these 1000 Y\\_hat numbers as your point estimate and sd of those numbers to report CI.\n\nThere are some of the catches though.\n\n* Here  we assume zi \\~ Normal(mean =Mi, sd = Si). that may not be true. Some Xi may not be symmetric, unimodal. Some Xi's distribution may be highly skewed with long right tail say. Some Xi may take only positive values, but you simulation occasioanlly generates negative values.\n* We are also assuming that the Xi's are independent. Hence we can simulate them independently. That may not be true.\n\nHence if you have some idea on how the individual distributions look like, you may want to simulate from them. The distributions are additional assumptions and your final point estimate and the CI is going to be affected by them."", 'If I understand what you\'re saying correctly, when you say ""account for the uncertainty in the ~1000 county-level estimates"" I assume you\'re talking about the (+/- n%) parts of your data. So concisely, you have a measure of uncertainty which is different for every variable of every observation which you want your model coefficient estimates to reflect. Two possible ways forward to include this in your regression: 1. Average the uncertainty over all observations of a variable, or 2. average the uncertainty over all variables of an observation. \n\n1: Bayesian regression allows you to specify prior uncertainty about an entire variable (the average (+/- n%) value of a variable across all counties). You plug this in as the prior distribution for that coefficient estimate. \n\n2: Weighted least squares allows you to specify the uncertainty of each specific observation (the average (+/- n%) value of a county across all its variables) and use that as a weight for how much each one affects the coefficient estimates\n\nIn both of these cases you are losing some information. The best choice depends on if there is a higher difference of variance between variables or observations. For example, if counties who have large CI for one variable typically have large CI for all of their variables, then approach 2 makes more sense. It\'s possible to combine these methods to do a ""Bayesian weighted least squares"" (google that and you will see some discussion), although you will likely have to write the code yourself and do some research because it\'s not very common. Your scenario of having uncertainty estimates for every variable of every observation is pretty uncommon, so that\'s why it might be hard to find information about how to model it. \n\nHope that helps', 'Thank you so much for the helpful response! It feels good to at least be diving down the right rabbit hole, haha', ""Thank you for the help!\n\nThis is kind of the direction I was trying to go down before I confused myself really badly haha\n\nI don't want to lose all the detail and nuance in your post, but quick question:\n\nI have the mean (point-estimate) and 90% CI for each estimated variable, which means I can calculate the Standard Error (I think).\n\nBut I do *not* know what the sample size was (i.e how many survey responses the original researchers used to create their estimates). I know how many people in each county are *estimated* to fall under each variable, but that's a population, not a sample, right?\n\nTherefore I don't think I'll be able to calculate Standard Deviations - is it possible to do this kind of simulation without knowing each variable's SD?"", 'You can compute the SD from the confidence interval assuming it is a normal distribution. https://en.m.wikipedia.org/wiki/Normal_distribution#Confidence_intervals']"
[Q] studies quantifying amount of bias when log transforming dependent variable vs GLM,"I am interested in whether we know how much of an issue it is to log transforming the dependent variable when doing linear regression instead of doing log-linear regression. I keep seeing articles doing this, and I am aware of some [issues](https://stats.stackexchange.com/questions/47840/linear-model-with-log-transformed-response-vs-generalized-linear-model-with-log), but I haven't seen studies clearly showing how much of a problem this is irl (I'm not a statistician, and realize non-statisticians sometimes [reinvent the wheel](https://www.reddit.com/r/statistics/comments/11nwhbz/r_statistical_control_requires_causal/)). Has this been systematically studied?",11qfld7,cat-head,1678728005.0,21,1.0,"[""It depends on the IVs in your model.\n\nThere's been a few papers on this in psychology because a lot of people liked log transforming reaction time data.\n\nhttps://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\n\nTo transform or not to transform: using generalized linear mixed models to analyse reaction time data\nSteson Lo* and Sally Andrews\n\nSchool of Psychology, University of Sydney, Sydney, NSW, Australia\n\nLinear mixed-effect models (LMMs) are being increasingly widely used in psychology to analyse multi-level research designs. This feature allows LMMs to address some of the problems identified by Speelman and McGann (2013) about the use of mean data, because they do not average across individual responses. However, recent guidelines for using LMM to analyse skewed reaction time (RT) data collected in many cognitive psychological studies recommend the application of non-linear transformations to satisfy assumptions of normality. Uncritical adoption of this recommendation has important theoretical implications which can yield misleading conclusions. For example, Balota et al. (2013) showed that analyses of raw RT produced additive effects of word frequency and stimulus quality on word identification, which conflicted with the interactive effects observed in analyses of transformed RT. Generalized linear mixed-effect models (GLMM) provide a solution to this problem by satisfying normality assumptions without the need for transformation. This allows differences between individuals to be properly assessed, using the metric most appropriate to the researcher's theoretical context. We outline the major theoretical decisions involved in specifying a GLMM, and illustrate them by reanalysing Balota et al.'s datasets. We then consider the broader benefits of using GLMM to investigate individual differences."", 'Can use the delta method taylor expansions to determine it. The bias is greater when the conditional variance var Y|X in the data is higher.']"
[D] Help finding study with SPSS,Hello! Would anyone know about any studies that used the ANOVA test and have an spss data set? A link would be great! Thank you!,11qec6w,Back-Opposite,1678725012.0,0,0.25,"['Sounds like homework. Read the sub rules.', 'I mean asking for places to look for resources isnt exactly the same as doing someones homework.']"
[Q] Why my PCA looks like this?,"Hi all,

I am dealing with omics data of blood samples. Briefly, I have dataset A with 10 samples per group, 2 groups in total, and a quantification of 7500 features per sample. We ran a t-test for each one of the features and found no proteins statistically changing after correction for multiple comparisons using Benjamini-Hochberg. The collaborators still wanted to analyze the genes with unadjusted p-values < 0.01, despite the high chance of false positives, leading to 17 genes. Afterward, they ran a PCA on those genes which gave a perfect separation of the two groups. However, that sounded suspicious to me.

In R, I wrote a code to sample the same number of subjects and features, but everything from a single population of mean = 15 and sd = 2. I ran a t-test for the 7500 features and, to no surprise, I consistently got around 75 features with unadjusted p-value < 0.01. A PCA for the corresponding features resulted in the perfect separation of the mock data, even though all data points came from the same population. So far so good, I would be able to prove my point that the PCA the collaborators got at first was no better than a mock population.

However, I have another dataset, also blood samples but a different disease, let's call it dataset B. For this one, I have 50 samples per group and the same number of features. For this one, we ran a t-test, corrected multiple comparisons using Benjamini-Hochberg, and got 18 features with an adjusted p-value < 0.1. In theory, these are ""truly"" significant hits. If I plot a PCA of these features, the PCA has almost no separation!!! I was not expecting this! A mock data gives me perfect separation in the PCA while a dataset with features likely changing does not. What am I missing here???

I already tried to replicate the PCA for dataset B, where features are statistically changing but no separation is visible, but I failed every time. I am using 50 patients per group, the mean and sd of dataset B (11 and 0.25, respectively), and selecting unadjusted p-values < 0.01, which is around 75 features.

  
All PCAs can be found in this imgur link: [https://imgur.com/a/dpavXq6](https://imgur.com/a/dpavXq6)",11qe75p,gustavofw,1678724681.0,11,0.8,"['First off, it\'s very weird to subset only significant data to make the PCA. I don\'t see why you\'d ever do that. Usually PCA is done as an exploratory step to see what Metadata corresponds to different principle components of variation. \n\nTo your question, probably your blood data separate by treatment on some other PC than 1 it 2. If this is human data from multiple donors, then my hypothesis is that ""donor"" is associated with most of the variance.\n\nThe mock data you have ""forced"" into having the most variance by treatment, so it splits on PC 1.\n\nYour collaborators are polishing a turd, don\'t put your name on that without being very careful.', ""Separability of groups in PCA does not mean very much, since PCA does not consider the groups. It would just mean that the dimension of highest variance happens to correspond to the groups you've already identified. \n\nAs mentioned by forever-erratic, there is no reason to only use significant data to make the PCA. If you're interested in genes that show the greatest variability between your groups, you should use PLS regression. It is similar to PCA in that it can be used for feature selection, but it also considers the labels of the data. Therefore, the first dimension will be a combination of genes that show the greatest variance between your groups. \n\nAs with anything in statistics though, be careful how you use it. From your description it's unclear how you're using these methods for your research. Good luck !"", 'What you are doing is ""differential expression analysis"". It\'s a solved problem and you do not use t-test. Go read up on limma, edgeR or deseq. Also, doing a PCA using ""significant"" genes only is meaningless.', ""Thank you for your help!\n\nThe idea of the project is to find features to be used in diagnostic settings that can distinguish both diseases. From my understanding, they used PCA to test if the supposedly changing features were able to distinguish healthy from diseased. Since the PCA did separate both conditions, they are planning to follow up on those features.\n\nIn my own dataset, dataset B, I plotted the PCA just to show that I would get the same appearance as the mock data, and the one they generated, but was surprised to see no separation. I plotted other PCs and still see no separation. The thing is, I don't necessarily want to see separation in my dataset, I just wanted to understand how this result is possible. In my mind, selecting features that are statistically changing would inevitably generate a PCA with good separation between the groups.\n\nOh, and all the mock data I generated came from the same distribution. I never simulated a healthy vs treatment. It is almost like I am comparing healthy vs healthy, where no difference should be seen."", ""Thank you!\n\nI am sorry, but I am having some trouble understanding the first part of your response:\n\n>Separability of groups in PCA does not mean very much, since PCA does not consider the groups. It would just mean that the dimension of highest variance happens to correspond to the groups you've already identified.\n\nIn PCA, if subjects are close to one another, is it wrong to assume they have similar quantification values for the features used when plotting the PCA?"", ""Thanks. I am familiar with those packages. I mentioned the t-test to simplify the question. My main point here is not the differential expression analysis. Their dataset has nothing statistically changing, that is life, and we have to deal with it.\n\nThe problem is that they used an approach that is invalid and, in the search of proving them wrong, I found data that I don't understand; hence why I'm here looking so lost hahaha"", 'Picking a bunch of nominally significant genes plagued by false positives from multiple comparisons, then running a PCA on those and trying to use it for classification is a really dumb approach. Whatever their application is meant to be is going to be heavily biased and unlikely to replicate in any other datasets. Add to that that the biological meaning of individual genes is now lost (due to PCA), and you have a steaming pile of garbage par excellence.', ""It looks like there is separation though, just incomplete, which isn't surprising given there was no differential expression."", ""I should have said non-separability does not mean much in a PCA plot. This is because you're only plotting two dimensions, so there may be other dimensions for which they're separable. If you're specifically trying to separate/classify high dimensional data, then PLS regression will create the dimensions that have the greatest separability between the classes. In turn, this will tell you which genes show the most significant differences between your groups."", ""Yeah, I agree that it is not the best approach. They are not used to data analysis, so I don't think they know the problem with this approach. I am trying to learn data analysis and stats and can't really understand why I got those PCAs, so I can't condemn them."", 'That is the thing! There were things statistically changing! I was probably not clear enough, so sorry about that. This is what I got in a simpler way:\n\nDataset A  \n\\- 17 features with ""raw"" p-value < 0.01  \n\\- 0 features with adjusted p-value < 0.05  \n\\- clear separation in the PCA when using those 17 features (likely false positives)\n\nDataset B  \n\\- 168 features with ""raw"" p-value < 0.01  \n\\- 18 features with adjusted p-value < 0.05  \n\\- PCA with a large overlap between the two groups when using the 18 features\n\nThis is why it twisted my mind, I\'m getting the opposite of what I expected', 'In general, principal components are used to collapse a high-dimensional problem, especially if you dont care much about how a system works and just want to control for a bunch of things you dont understand at once. Applying PCA to significant associations is sort of like double dipping, with the disadvantage that the second dip just obscures the relationships theyre probably most interested in.']"
[Q] which sampling approach to take when measuring data of traffic speeds.,"I am recording traffic speeds on a road for 4 weeks in order to determine the best time of week for trucks to use the road to make deliveries. I have collected data at the same time each day for 4 weeks but I'm unsure what sampling approach to take. If I'm separating traffic speeds by day of the week, would this be stratified sampling or is this more like simple random sampling?",11qbr92,Vdorei,1678718967.0,1,1.0,"[""You have data for 28 days, four samples of each day of week.  Is this 28 data points? Do you have data for multiple samples at the same time of day (multiple vehicles)? For multiple times of day within day of week?\n\n\nIf it's one vehicle a day for 28 days, make a graph with day of week on the horizontal axis and speed on the vertical.  Each day will have a group of four data points. See if there is a clear separation of the groups.  If there is no overlap bewteen any of the groups, then the answer is clear: the day having the fastest speeds is the highest group.\n\n\nIf there are multiple samples within multiple times of day, then your sub-grouping will be nested.  If there is overlap and you need more analysis to establish confidence in a decisions, then the analysis becomes statistical.  Consider components of variation analysis, using control charts to look for instabilities within groups and differences between groups, and nested ANOVA to assess confidence in the results.""]"
Regression with bounds on certain coefficients [Q],"Hi all ,

 My problem is of the form sales= intercept+ beta*var1 + beta2*var2+ betan*varn ... 

For var 1&2 i  have a prior understanding of what range they can lie in between max and min limits the other variables are unbounded.

How would I proceed to perform a constrained regression on the same ?

I tried Bayesian regression, but it doesn't really bound the coefficients to it's upper or lower bound, just provides me with a posterior range.

Can you folks point me in the right direction, any ideas will be appreciated.

Thanks",11qaelq,ranjithjames1994,1678715748.0,2,0.67,"['> I tried Bayesian regression, but it doesn\'t really bound the coefficients to it\'s upper or lower bound, just provides me with a posterior range.\n\nWhat are you using? this should be easy to do in Stan. I think even [brms](https://paul-buerkner.github.io/brms/reference/set_prior.html) allows for this:\n\n>If desired, population-level effects can be restricted to fall only within a certain interval using the lb and ub arguments of set_prior. This is often required when defining priors that are not defined everywhere on the real line, such as uniform or gamma priors. When defining a uniform(2,4) prior, you should write set_prior(""uniform(2,4)"", lb = 2, ub = 4). When using a prior that is defined on the positive reals only (such as a gamma prior) set lb = 0. In most situations, it is not useful to restrict population-level parameters through bounded priors (non-linear models are an important exception), but if you really want to this is the way to go.', ""Sounds like you want least squares with box constraints on some variables\n\nhttps://en.wikipedia.org/wiki/Constrained_least_squares\n\nThere's a variety of ways of imposing such bounds; one approach is discussed at the link.\n\nThere's a number of approaches discussed here:\n\nhttps://stats.stackexchange.com/questions/61733/linear-regression-with-slope-constraint/\n\nThough if you're using the code there, I suggest carefully checking that the functions still work as they did.\n\nThere's some potentially useful discussion here as well (note, for example, one approach to introducing a positivity constraint as a nonlinear term in an equation in the question)\n\nhttps://stats.stackexchange.com/questions/295951/re-parameterization-of-a-nonlinear-model-as-a-linear-model/"", 'To stop reddit markdown from interpreting pairs of \\* characters as instruction to display the characters between them in italics, put a \\\\ in front of \\*, i.e. \\\\\\*. It will then appear as \\*, as you want it to in your regression equation', 'Well i was trying pymc with the nuts sampler, but i see what I did wrong , i was using a normal distribution with small std to limit the values, but essentially not having strict upper and lower bounds , will try with a bounded distribution function as you have suggested.\n\nThis was exactly what I needed thanks , sometimes the most simplest things slip your mind.\n\n Will post an progress update.', 'Thx will keep that in mind.']"
[Q] How to use regression in the right way to check for a suppressor variable.,"Hello dear community, I am currently writing my bachelor thesis and am at a loss. I hope I can explain my problem in a comprehensible way. English is not my mother tongue.

&#x200B;

I would like to investigate the correlation between positive perfectionism and procrastination. These variables should correlate negatively. Positive perfectionism also correlates (positively) with negative perfectionism. Therefore, negative perfectionism 'contaminates' the positive perfectionism and I want to adjust this relationship for the influence of negative perfectionism. The relationship between positive perfectionism and procrastination should then **strengthen**. I wanted to calculate a simple linear regression (with positive perfectionism as IVs and procrastination as DV) and then a multiple regression (with positive perfectionism and negative perfectionism as IVs and procrastination as DV). 

&#x200B;

**My hypotheses (right now) are as follows:**

1a: There is a significant negative relationship between positive perfectionism and procrastination.  
1b: The strength of this relationship increases when negative perfectionism is taken into account.

&#x200B;

To answer 1a, I want to check the standardised regression weight of the simple linear regression  including the respective p-value. I can still keep up with that.

  
Can I then answer 1b by simply looking at the corresponding standardised multiple regression weight (the one of positive perfectionism) and comparing it with that from 1a? Is my hypothesis confirmed if this regression weight is larger than the one from 1a? As far as I know, standardised regression weights can be interpreted like a correlation according to Cohen's conventions. I really hope that I'm not completely wrong here.   


I'm thankful for every advice!",11q90br,simplySchorsch,1678712327.0,7,0.89,"[""> Is my hypothesis confirmed if this regression weight is larger than the one from 1a?\n\nI don't think that this is sufficient, because the difference might just be by chance. I'd say you need to do a t-test that checks if the difference between the two coefficients could be zero. Maybe [this](https://andrewpwheeler.com/2016/10/19/testing-the-equality-of-two-regression-coefficients/) helps."", 'What does taken into account mean?', ""Thanks so much, I'll check that out!"", 'control for the shared variance of positive and negative perfectionism. My first idea was to calculate a (semi)partial correlation, but my supervisor insisted on solving it with regression.', 'Tbh I (almost) never see analyses of data that are like: var1 is positively correlated with my DV, but when we control for var2, the coefficient for var1 ceases to be significant. Therefore I conclude XYZ. There are so many reasons why that could happen, its just not a procedure that has any probative  value.', "">but when we control for var2, the coefficient for var1 ceases to be significant\n\nMy hypothesis claims that var1 (positive perfectionism) or rather its relationship with my DV (procrastination) will be *even* ***more*** *significant* (and negative) when we control for var2 (negative perfectionism).   \n\n\nThe theory behind suggests that negative perfectionism is negatively connoted, as is procrastination. In contrast, positive perfectionism is rather positively connoted but is being 'contaminated' due to its correlation with negative perfectionism. Therefore I want to calculate the 'true' relationship between pure positive perfectionism (without its shared variance) and procrastination - which should then be even more negative.   \n\n\nI hope one can understand what I'm trying to say :/"", 'Theres no such thing as more significant which is why Im having trouble understanding what youre trying to do.', ""What I'm trying to say is that I expect the correlation/relationship of positive perfectionism and procrastination to strengthen. So if I calculate a pearson-correlation it might show r = -.15 and I expect it to be stronger (such as r = -.30) when controlling for the shared variance of positive and negative perfectionism."", 'So you dont actually care about statistical significance but rather about the strength of the effect? If Im understanding you correctly you probably want an interaction term there but you have to be very careful interpreting them', 'Yeah exactly. The correlation of positive perfectionism and procrastination should be significant anyway and I don\'t expect it to ""lose"" significance (I doubt you say it like that) when taking negative perfectionism into account.  \n\n\nI thought I could simply compare the standardized regession weights of positive perfectionism in a simple regression (which should represent the correlation) and a multiple regression. Could I not interpret these according to Cohen as they are standardized?   \n\n\nThanks so much for taking your time with me!', 'I think you really need to consult an expert. Long story short comparing two models, one with an extra control, and then seeing if the coefficients in both models are different, tells you nothing only that there are correlations between the variables.', ""I'll consult some kind of tutor as soon as I find one :) Still, thank you very much for taking your time to discuss my question!""]"
"[Q] Regression model where observations are nested into groups, and the value of your independent variable for each observation is only comparable to other observations within the group?","I am curious about whether you can make statistical inferences when you have nested data, and the way your independent variable is measured makes it only comparable within its own group.

For example, you want to know the relationship between a town's investment in recreation and the level of physical fitness of the people in the town. Your towns are nested within states, and each state has a different way of allocating public funds for things like recreation to towns and each state classifies recreation a bit differently. So the level of recreation funding in Town A in State 1 includes Town 1's spending on, say the local public track, but the level of funding in Town B in State 2 does not include spending on running tracks. If we simply regress fitness on investment in recreation, then we could just be capturing the effect of different expenditure classification schemes. Spending on recreation is only comparable within states.

So instead of measuring gross per capita recreation spending in each town, let's say we measure the amount of revenue that each town spends on recreation per capita as a percentage the entire state's spending on recreation. For simplicity's sake, let's say every town is the same size and has the same characteristics (although you could obviously use population weights if this weren't the case). Let's also say that you have reasonable evidence that cross-state variation in recreation spending [according to a common definition of recreation] is not very large; that is, most states spend a similar amount and that the variation is mostly at the sub-state level. So a town in State 1 may have higher gross spending but a lower percentage than a town in State 2, since State A has a more expansive definition of recreation.

Could you run a random effects, fixed effects, or mixed model estimating the relationship between recreation investment as a percentage of total state rec. investment and overall fitness? I understand that you lose some insights, since you're no longer looking at gross spending. But my question stands: Is it possible to make inferences about the relationship between fitness and recreation spending with independent variables that are only comparable within the group they're nested in, or is it kind of impossible to draw any conclusions about this relationship?",11px0gi,spicysnake333,1678673064.0,10,1.0,"['It sounds to me like a hierarchical model would be the perfect tool to use. You model one general relationship but assume that the data in different groups can have underlying details of that relationship that can vary. See these examples\n\nhttps://num.pyro.ai/en/stable/tutorials/bayesian_hierarchical_linear_regression.html\n\nhttps://jramkiss.github.io/2021/01/29/hierarchical-models/', ""Thank you, i had suspected a hierarchical model would work, but I wanted to ask because of the specificity of my problem.\n\nFor the analysis I am doing, I am regressing a measure of fiscal performance (specifically, percentage of a fiscal transfer that is absorbed by a region) at the regional level on a number of predictors. I have 27 countries in the sample and 1500 observations in total. My observations are regions nested within countries. I have a measure of subnational public procurement autonomy, which is the percentage of total public contracts in a region that are awarded by a local government (as opposed to a central government). I have some concerns about whether this variable sufficiently captures how much autonomy a region actually has. So I want to include a new independent variable that represents a region's *share of total subnational procurement in the entire country*, adjusted for its fraction of the country's population. So for each observation, the variable would tell us how decentralized public procurement is in a given region, compared to other regions in the country. \n\nI know I can't compare this measure for regions in different countries (region 1 in country A may be overall more decentralized than region 2 in country B, but it could still have a lower value of this variable since it's less decentralized *compared to other regions in country A*). In this case, if I do a hierarchical model where I regress absorption on this decentralization measure, could I make valid inferences about the relationship between absorption and decentralization of public procurement?""]"
"MDS plots interpretation, [Q]","Hello, Im just trying to implement a random forest model for my project, and I came across a multidimensional scaling concept and its 2D plots. What exactly is it telling me? For example, if I my model has some sort of mediocre predictive power with a OOB error rate being 0.4, would I be able to see that in an MDS plot? Can I use the plot to back up my findings, in my case poorly clustered samples on the plot would suggest more missclassifications? Thanks!",11pwyfl,henrybios,1678672908.0,1,1.0,[]
Which School Would you Choose? [Q],"I applied to Statistics PhD programs. I am currently trying to decide between Rice and UT Austin. Which school would you choose and why? I do not yet know exactly what I want to research, but I know that I enjoyed classes in programming, statistical learning, and numerical analysis, so I am most interested in computational stats. The programs are similarly funded. Any help or advice would be much appreciated.",11pu0sy,1point5guy,1678665128.0,2,0.75,"['UTA Stats is definitely known for computational statistics.', 'Both are good schools. Your choice should be guided by two questions-\n\n1. Which school is giving me the best package?\n2. Which schools graduate students are going on to be what I want to be.', 'Okay, thanks.', 'Related to #2.... alumni network. My grad schools alumni network and networking events were the non academic biggest pros of my ms program', ""Thanks for the advice. It seems like #1 is pretty much the same, so it's going to likely come down to #2. I'll definitely look into that some more."", 'Then you really need to feel out the department culture. Not everyone works everywhere. Where I did my PhD, it was very hierarchical. Faculty were Dr. So and So, graduate students were expected to be obedient and subservient, any deviation was punished harshly and sometimes publicly (this is at a top US university). At the university where I am faculty, there is a more egalitarian outlook. Faculty are on first name basis, doctoral students are treated as future colleagues, and bullying/abuses of power are not tolerated at all at the dean level. \n\nBoth R1s that place many of their grads in faculty positions. But vastly different cultures. \n\nSome people thrive in highly structure environments and others dont. Try to find out about the departmental culture.\n\nEdit- also, would you rather live in Austin or Houston?', ""Still trying to get a feel for the culture at UT Austin. I wouldn't mind living in either city. Do you know which program is better for computational stats?"", 'Take a look at what the faculty are publishing. Neither of them have a reputation as one of the Hubs of a discipline (eg Duke and bayesian stats). They are both just solid top 50 programs. Though, I tend to think of bio stats when I think of Rice. Not sure about UT. \n\nHonestly, when I think about statistics in Texas, TAMU comes to mind more so than either of those schools.']"
[Q] Is there a statistical test/model that can tell whether there are long-range temporal dependencies in a Markov chain?," Hi all, I have a Markov chain. It consists of twelve states. I would like to determine whether there are temporal dependencies in this Markov chain, e.g., whether knowing a state at t may help you to predict a state at time t+10. I have tried fitting a higher-order Markov models to this Markov chain, but it occurred to me that to validate whether there are indeed temporal dependencies in this Markov chain, I would need some way to validate that higher-order Markov model.

So, is there an out-of-the-box method out there that could be used to determine long-range temporal dependencies in a Markov chain? Or, if there isn't, would a kind of cross-validation procedure be helpful to validate a higher-order Markov model (e.g., second order or third order), i.e., where that validation would validate whether there are long-range temporal dependencies in my Markov chain.",11ppyt9,statius9,1678655437.0,1,1.0,"['> whether knowing a state at t may help you to predict a state at time t+10.\n\ncompute the tenth power of the transition matrix, to get the effect of moving ten steps in time.', 'You could look at the tenth power of transition matrix to get a rough idea.\n\nYou could also look at the autocorrelation of your Markov chain to check temporal dependencies.', 'So I suppose you could look at the transition matrix at the tenth power. If in this matrix the probability of transition to any other state seems random then you can deduce that knowing the state at t has no affect on knowing the state of the system at t+10', 'Intuitively, i think the probabilities of transition matrix at tenth power would have to all be roughly equal in each row in order to say that knowing the state at t has no influence on the state at t+10', 'So, in this case, a statistical test could evaluate the null hypothesis that the row transition probabilities are uniformity distributed. If they are uniformly distributed, then knowing the state at t is not informative for knowing the state at t+10']"
Most useful chapters in Elements of Statistical Learning [Q],"Im a senior stats major and after going through roughly 90% of introduction to statistical learning and then taking an elective course on it for my major, its safe to say that I would like to get a more theoretical understanding of some of the concepts in statistical learning. My goals are to get a better understanding of data mining tools in practice for analysis of financial data. I am mainly wanting to gain a theoretical understanding of some of these concepts for the purposes of model development in practice for applications, ie, be equipped with the theoretical knowledge to build empirical models. 

I wanted to know what the most useful chapters of this book are to give me a solid understanding of these methods. On a first pass, I figured the chapters 1,2,3, 4,7,13, 14, 18 are some of the most useful. 

1 - introduction 
2 - overview of supervised learning 
3 - linear methods for regression
4 - linear methods for classification 
7 - model assessment and selection
13 - prototype methods and nearest neighbor methods
14 - unsupervised learning 
18 - high dimensional problems 

I didnt include chapters on  ensemble learning, random forests, undirected graphical models,neural networks, support vector machines, model inference and averaging, additive models, kernel smoothing methods, basis expansions and regularization.


my question to you all is if this is a   good list of topics to read for my goals, and if i should still read about any of the chapters not listed. My reasoning for excluding these chapters are because i feel the ISLR book has a fairly practical treatment of these topics, and reading about them in ESLR would be for if i was interested in research.",11pp8d5,AdFew4357,1678653678.0,25,0.94,"['""Additive Models, Trees, and Related Methods"" since I think this is the chapter that they they talk about GAMs which are awesome.', '>  I would like to get a more theoretical understanding of some of the concepts in statistical learning\n\nThen ESL is a better place to start than ISL, since the goal of ISL  is to skip most of the theory. The couple of decades of papers on which it relies may be better still, if theoretical understanding is the goal, but you need to start somewhere.', 'Just skim, and slow down when you get to parts you dont know. Skimming the things you do know will be good reinforcement, and you have the context for the parts that are new.', 'ISL is baby ESL. Just go read ESL and do the exercises if you want a deeper understanding of the material.', ""If your goal is to work in finance, most of the chapters aren't useful. You can probably start with the first five chapters you selected (1-4, 7), which are solid. I think you can hold off on the last 3.\n\nEnsemble methods and model inference can be pretty important in practice as well. ESL provides a decent foundation, but other resources should be used to compliment it."", ""*Disclaimer*: I come form CS background.\n\nI like Kevin Murphy's book for its modern perspectives. You may use the following two resources as useful references (or main text if you wish):\n\n**Theory**: [https://probml.github.io/pml-book](https://probml.github.io/pml-book)\n\n**Practice:** *Python for Finance* by *Yves Hilpisch (O'reilly)*"", 'How useful are GAMs in practice? Ill definitely check it out.', 'But is it okay to skip the chapters Ive listed? Does it meet my goals? Are the chapters I am skipping still important?', 'But all of the chapters? Like each and every one?', 'They are very useful when you want a nonlinear method but dont have enough data for ML methods. And also are interpretable\n\nTho interestingly Microsoft has done some research for GAMs that fits them using trees, so there is a connection', ""Once you start reading it you will get a better idea of what you want to know. You're kind of like a chef asking which part of the cook book he should read. The answer is all of it until you know more"", 'I just feel like its so rare for data for actually take on structure of a GAM', 'The nice part of gams is that you can specify the type of relationship with each dependent var. if a lot of the relationships appear to be noise besides one weird spline, a gam may succeed over methods that do variable selection for you.']"
[Question] Is ARIMA good for forecasting time series forecasting with only 12 observations?,"How would you go about forecasting time series data with a small number of observations?

**Example:**  
Let's say we have 12 months of revenue data from an ecommerce business. How would you go about forecasting what the next 6 months look like?

My initial thoughts would be to use something like ARIMA but with only 12 observations, I feel like the accuracy wouldn't be great.

LSTMs, Prophet? Something else?",11pkfop,ShredMontana,1678642524.0,39,0.87,"['Find a single covariate that correlates well with your variable over the  time span that you have data for, but has longer history. Fit a linear model with that covariate. Predict your covariate based on its longer history, then use the linear model to predict your time series.\n\nIf this is revenue data, the covariate could be some industry-wide index, maybe from the supply or demand side. If so, there are probably also industry-wide predictions made for that index already, which will likely be better than run-of-the-mill ETS or ARIMA.\n\nMore flexible models like the ones you mentioned will likely overfit.', '12 observations is not enough for a quantitative approach. I can almost guarantee you that simply asking a relevant expert to provide a best guess forecast would be better than a model in this case. You cant even model any seasonality with 12 months.', 'It depends on the profile of the revenue over time.  Is it growing exponentially?  Is it growing in a more linear fashion?\n\nYou might be better off trying to get some kind of seasonality benchmark from another set of publicly available data -- like the US Retail Sales data provided by the Census Bureau.  Then, if you have some kind of growth estimate for the following year, you can make some back of the envelope calculation in Excel.\n\nYou could try to fit some kind of Gompertz or Pearl/S-curve forecast for the long-term trend component.  It would be tricky though in such a young business.  Then you overlay that with monthly seasonality.', 'Either be ok with lots of uncertainty or supplement it with additional assumptions/data, maybe from a parametric model or some related info about the process.', ""Don't. Literally nothing is going to be much good at forecasting a (probably noisy) monthly series with 12 observations. And if you're worried about sample size for ARIMA, why would you even think about Prophet or LSTM? No method will tell you anything useful. Almost any amount of complexity will be unjustified.   \n\nIf you must try, you don't really even need a model. Just use math. Back out the trend, convert what's left to month over month changes, project the trend forward and apply the month over month changes from your original data. Ad-hoc decomposition forecaster, basically, but you can't get a good decomposition out of 12 months of data either.     \n\nBetter, find a proxy series of sales (competitors?) that's longer, or use more granular data, or find covariates and leverage those. If you have good proxies or covariates, you can start to play with back-casting synthetic revenue series, which might help in building out forecasts. But again, that's all going to be quite difficult if you're trying to be purely empirical with 12 data-points."", 'I would use the \\`ets\\` function in the R \\`forecast\\` package.  It will select between simple exponential smoothing and exponential smoothing with trend by AIC criteria.  \n\nI think it will be impossible to estimate seasonality or fit a complex model.  \\`ets\\` will produce a model with one or two smoothing terms.', 'Start with the seasonal naive: January 2023 forecast = January 2022 actual -(collect more data first and try ETS and auto ARIMA first after another few months.', ""I wouldn't..."", 'I wish this was a youtube or article because this commenter has seen battles I want to learn from.', 'With only 12 data points, how do you find out whether the covariate correlates ""well""? If it isn\'t redundant data?', '>Find a single covariate that correlates well\n\nWhat\'s the definition of ""correlate well""?', ""Yeah this use case would really need at least 24 months for a seasonal ARIMA to make sense. In this case a useful forecast could probably just consist of an educated guess at a constant percentage year-over-year growth rate, then tack that on to last year's observation for the month. You could build prediction intervals using the standard deviation of the 12 observations or something but I'd guess that they'd be too wide to be all that useful."", 'Makes sense. For this example I saw that ad spend had a .65 postive pearson correlation with revenue...which makes sense for eCommerce...ad spend typically is positively correlated with future revenues. So incorporating that data somehow might also be helpful.', 'I\'ve seen these battles too and my take-away is ""keep it simple"" and also ""incorporate domain information"".', ""Usually with low data cardinality, expert knowledge comes into factor. \n\nFor example, I don't need 100 observations to know that pressure correlates with altitude."", ""12 data points is enough to test one, maybe two candidates using classical methods without risking false positives too much, so you need to carefully choose them with some prior knowledge, like domain knowledge. This is the moment to creatively reach for data. For example, if there is an index that correlates with revenue streams of 10 other e-commerce businesses in similar domains, then that would be a premise to consider it for yours as well.\n\nIf there is no such domain knowledge to draw from, you can't use this method. Sometimes the answer a statistician needs to communicate is *I don't have enough data to answer this specific question* and work with your customer towards a question that is possible to answer and still useful. For example, maybe it would be already useful to the customer to know whether their revenue stream is similar or dissimilar to their competitors?"", 'Yes, but this about fuzzy business numbers of a startup and not about Physics where you can rely on battle-tested formulas, aka natural laws.', 'It can still apply, we just don\'t know complete context. For example, the price (stability) of GPUs are very strongly correlated with AI technology. \n\nIf they were a ""GPU"" business for example with not a lot of revenue history they can still forecast revenue (growth) based on the market growth of AI tech, and or observe the growth of the PC gaming market, as well as how NVIDIA etc performed in relation to all this.']"
[Q] One way Anova vs general linear model,"Hello everyone! Im conducting my first research project so apologies if Im being stupid.

I initially ran a general linear model of neuter status (in dogs), orthopaedic disease and age of onset (response factor) and got significant results (P<0.05) of both orthopaedic disease and neuter status. 

I then proceeded to run 2 separate one way Anovas of neuter status (entire vs neuter) and orthopaedic disease and got a P value >0.1 in both. Why all of a sudden are the results insignificant? Any help is much appreciated!",11pd94t,glitter-gorilla,1678623625.0,14,0.94,"['Did you include an interaction term in both models? Which software did you use?', 'These questions pop up all the time. When you fit a one way ANOVA you are ignoring an important source of variation. That variation has to go somewhere! It goes into the error sum of squares which is what you divide your model sum of squares by, and so your test statistics become deflated. Boom, lower p-values. Basically, if the model with all effects has each effect statistically significant, you shouldnt fit a model with fewer effects.', 'ANOVA is not an alternative to GLM but rather a special case of it. Your difference apparently is because you had two factors in your GLM and one factor at a time in your one-way ANOVAs.', 'Make sure to adjust your p-values when preforming pair-wise tests', ""> Why all of a sudden are the results insignificant?\n\nTwo main potential reasons.\n\n1: The denominator of the F is larger. 2: The numerator of the F is smaller. \n\nLet's look at these one by one:\n\n1. When you leave out a potentially important variable, the MSE will inflate. Consequently F goes down, typically considerably more than the small gain from extra d.f.\n\n2. when you leave out a potentially important variable that's not uncorrelated with other variables, you change its estimate (indeed, sometimes you even flip its sign). This can often make the numerator of the F statistic smaller.\n\n  See the diagrams here: https://en.wikipedia.org/wiki/Simpson's_paradox\n\n  and also this article: https://en.wikipedia.org/wiki/Omitted_variable_bias\n\nIn many cases, both these effects operate, sometimes in the same direction (i.e. both acting to decrease F)"", 'I also ran a chi-squared test on neuter status and orthopaedic disease which was significant. What happens to the data when I spread out the two factors (neuter and entire) and analyse them independently that changes the significance?', 'Im super new to this so thank you so much for the explanation and links!', 'I would advise sticking to one planned approach to data analysis and not trying multiple alternatives. No method is valid when it is chosen over other methods that have already been tried.', 'I ran the chi squared before the one way Anova to check whether there is grounds to perform a one way Anova of the two factors in the first place. So do I conclude then after running both that data is insignificant? Despite what the GLM and Chi squared says', 'I don know why or how you did a Chi Square test with a quantitative dependent variable. No prior testing is required as a basis for an ANOVA. I would stick with the two-way ANOVA because neither the Chi Square nor the one-ways is justified.', 'Im not sure what grounds your referring to. ANOVA only needs its assumptions of indecent variables, equality variance, and and normality verified to perform. Its rather robust as well - but if youre concerned about these assumptions thats another question entirely', 'Perfect thank you! Much appreciated for your help']"
Random Forest paper recommendation. [Q],Can someone recommend an academic paper on theory/implementation of random forest?,11p6nyz,henrybios,1678599949.0,8,0.91,"['Here is the first paper \n\nhttps://www.stat.berkeley.edu/~breiman/randomforest2001.pdf', 'Breiman paper is the best. Ground-up proof for why randomization can improve accuracy, and it applies to far more than just ensembles of trees.', 'Perhaps look at Susan Atheys Generalized Random Forest paper', 'Strobl et al., various.', 'just watch indians on youtube', 'I wrote my own implementation of Random Forest and I was most satisfy with this book - [https://www.amazon.com/Machine-Learning-Random-Forests-Decision-ebook/dp/B01JBL8YVK](https://www.amazon.com/Machine-Learning-Random-Forests-Decision-ebook/dp/B01JBL8YVK)', 'Can anything be truly random?']"
[Q] Unsure of which statistical method to use for testing my research.," I  am wrapping up my dissertation proposal and have hit a snag on choosing  the correct analysis method for my research questions. Hoping this  group can give me some direction.

The first question/data set is from the following categories, and I will have five sets (five different companies)

 Utility generation capacity from all sources

 Excess generation produced from the utility source

 Forecasted growth of generation capacity from company assets

 Yearly electric demand on the utility from customer base

 Yearly received distributed generation from customer assets

 Forecasted growth of distributed generation capacity from customer assets

 Distribution system delivery efficiency

The  goal is to determine if any of these variables have an impact (and at  what level) on supporting electric vehicle growth. Linear Regression  seems to fit, but open to ideas.

In  the second data set to be analyzed, I have a fixed value and I want to  test different variables against it to determine saturation (0-100%+).  For example, if a generation source had a fixed capacity of 1MW, and I  wanted to test how many and what combination of level 1 chargers (5kW)  and level 2 (20kW) could be used. For this one, I am thinking a t-test,  but again, open to suggestions. Thanks!",11p4o9u,ashenfang7404,1678593740.0,10,0.86,[]
Sports Models Education [E] [D],"Hey guys, 

I am new to the sub, I started looking into making sports model to predict games. I have been able to create a basic models, but would like to really get into making a great in-depth model. I am having a hard time finding education videos. Does anyone know where I should start looking? Or any good Youtubers that walk through a very in-depth model?

&#x200B;

This is not homework. This is something I have started to do as a hobby.",11p3dnm,Cryptomillionss,1678589986.0,6,1.0,"[""Not sure of any YouTube channels, but if you're looking for datasets you might try out Stathead or Sports-Reference. \n\nAlso R is pretty good for scraping data from online sources that host sports data. I think the name of the package is called rvest, but there are definitely others. \n\nCould also look into Bayesian Hierarchical modeling if you want to do more advanced sports modeling."", 'Thanks, I appreciate it!']"
What are these plots called? [Question],"I have seen more than one of these recently, I know there is an r package to make them, but I cant search for it because I don't know the name:

https://imgur.com/a/v9oPP3F",11p1tmq,python_noob_001,1678585620.0,4,0.83,"['you have 3/3 different answers already. clearly a good question lol', 'Dumbbell Plot', 'Maybe a [lollipop plot](https://stats.stackexchange.com/questions/423735/what-is-the-name-of-this-plot-that-has-rows-with-two-connected-dots)?', 'Feel free to throw a name in the hat', 'For me, a lollipop plot only has a dot at one end (double ended lollipops are not that common!). Id go with dumbbell plot for this.']"
[Q] [R] Logistic Regression help,"I need advice on the logistic regression formula I'm working on.

&#x200B;

The dependent variable is a binary variable (1 for the good financial performance of the firm and 0 for bad performance). Because the purpose of my paper is to research the effect of managerial experience on firm performance, I gathered financial data on managers' current firm performance (the binary dependent variable) and on the same managers' previous firm's performance (the proposed independent variable - similarly a binary variable 1/0 for firm performance). The timeline of the measured performance in 4 years for both the new and the old firm. The only difference is that I'm using the average performance values for the previous firm and the maximum performance values at the end of year 4 for the new firm.

&#x200B;

So basically I have the data for managers' old and new firm performance and want to see if the experience in the previous firm affects the performance in the new firm.

&#x200B;

The proposed logistic regression formula is: Newperformance = a + b1 \* oldperformance + b2\* controls

&#x200B;

Now my supervisor has said that there is an issue with such an approach and I should choose a different method, while I cannot see any issue with it.

&#x200B;

Thanks",11oyed3,midagi79,1678576708.0,1,0.66,"['Hello!  \nI believe it makes sense to use logistic regression in your case. I don\'t really understand why your supervisor doesn\'t find this method convenient.   \n\n\nAs far as I understand you are using previous financial performances to predict current financial performance. So the example would be like:   \n*previous performances = (0,0,0,1,1,1). Mean previous performance = 0.5 and you use this value to predict current financial performance which is either 1 or 0.*   \n\n\nEven though I don\'t see any problem with the methodology, I think by using this approach, you are losing time-order information. You are saying that (0,0,0,1,1,1) is equal to (1,1,1,0,0,0) because you are taking the mean. Maybe that\'s why your supervisor didn\'t like the method?  \n\n\nAlso, I don\'t quite get what you mean by ""*controls*"" in the formula.']"
"[Q] ""Self learning"" statistics a viable career path.","Hello, long story short I am looking to change careers possible into statsics. I say self learn but I do have a bachelors in mathematics but no classes in statistics and had a mediocre GPA so I am doubtful I could get into graduate school . So I have a few questions

1. How viable is it to get a career in statistics without a proper degree in stats and/or a master?
2. How much is the minimum to learn to start applying for jobs? Looking through some online job applications the least usually say like ""24 degrees hours in mathematics including 6 in stats"". Would learning at the level of say *Statistical Inference* by Casella and Berger be adquate or are more advance topics needed (data mining, Bayesian stats, time series, etc)
3. What software/programming is worth learning R,SPSS,SAS, SQL, python?
4. If anyone was in a similar situation any advice to ""get my foot in the door"" so to say?",11os4ns,Geometryisfun,1678561454.0,4,0.67,"[""With this background, it would make more sense to go for data analyst/data scientist jobs. There you don't need such a precise understanding of the math behind the tools you use. \n\nI don't want to discourage you if you really prefer stats though, nothing is impossible with enough time/work/motivation, and I personally find stats a bit more interesting than a lot of data science work (or more precisely: I find data science interesting when it gets closer to actual stats). Just consider these options too, as they may also check what is drawing you to stats in the first place."", 'I honestly don\'t think even fully memorizing C&B would contribute towards employment in any meaningful way. That\'s all theory, employers care about what you can actually do.\n\nI would also  make a distinction between ""data"" jobs and statistics proper. Stats positions are almost always understood to be academic or research-adjacent e.g. clinical trials and the such. And all of those probably require at least a master\'s level education.\n\nMy personal path was self-learning R and Python, getting a job as a research assistant at my uni, then leveraging that into a job as a data analyst in the public sector and then using my salary to pay for my stats masters. I kept working in the public sector, doing mostly repetitive Excel/R based tasks but kept in touch with my professors and did some research projects on my free time. Then I found a PhD position that was related to my thesis topic and got in.\n\nBut whatever you pick, being comfortable with at least one programming language is really crucial as that\'s the way in which you actually implement the methods you learn about. SQL is a must but not too hard to achieve minimal competence; whether R or Python is the better choice depends on opportunities available to you, so you\'ll need to check with potential employers or job offers in your area. SPSS and SAS I would absolutely not pick due to their lack of versatility (although, again, you might actually find employers in you area in need of such knowledge, but I don\'t think it\'ll be too common and would probably mean it\'s a company with some fairly dated practices).', 'Im not in industry, so keep that in mind. I would learn SQL and python. There are sites like DataCamp that will teach you. Then lean on your math degree in applications and interviews.', ""People with a real MS in Stat or Biostat still don't get a phone screen with a recruiter without real stat or biostat experience."", 'One thing to look into is getting a ""certificate"" in something like R programming or applied statistical analysis.  I haven\'t looked into this in a long time, but e.g. Coursera used to offer certificates in these kinds of fields for taking like four courses in the subject area.  It was reasonably inexpensive and gives you a document to show your aptitude, achievement, and willingness to learn.', 'I was recommended DataCamp, and honestly it\'s not great. Most of the exercises are just ""fill in the blank"", and barely require more than knowledge of the syntax. Udemy is better.']"
[Q] Mplus 8.9 PROCESSORS=,"I'm using Mplus 8.9 on an M1 MacBook Air, while I know it isn't built for M1 architecture, I  am wondering if there is a PROCESSORS= (or something else) parameter I can use to get Mplus to use the entire processor rather than a specific number of cores/threads?",11oq56d,jrdubbleu,1678556655.0,1,1.0,"[""You can tell mplus to use what processors you want using that command. \n\nYou'll see a corresponding numbers of Windows pop open as it starts the analysis (at least it does in Windows).\n\nCheck the mplus docs for the correct command per above and let me know if it uses all procs, as I'm interested in Mac's for mplus based on this very reason.""]"
[Q] Linear regression or logistic regression?,"I'm looking for data analyst internships and I got a task from a company. They gave me a dataset with 240 different factors (& 9k+ observations). My response is either -1, 0, or 1. The point is to predict the -1s with accuracy and avoid predicting -1 when it is in fact 1. The 0s seem to be kind of irrelevant (i have no idea what this data is, it has no labels, except for Value1 Value2, etc.)

Note: I started learning R specifically for this task, I've only done stats in JMP and Excel in school

So I was thinking, should I do a regression in R and round the predictions (if negative =-1, if positive =1) or do some kind of logistical regression? Thank you!

Note2: This is the actual description of the task (quite short): *""Predict the ""Regulating"" value in the data set. The value can be 0, -1 and 1. If easier, ignore all 1, we are interested in finding -1 values and to avoid 1 values. If we predict -1 and it becomes a 0 value, it doesn't matter. We just don't like to predict -1 and get 1 in regulationg value.""*",11oq4gd,leocorleo,1678556603.0,50,0.9,"[""I'd consider two options:\n\n1) Based on the encoding of the response variable, you could run an ordinal logistic regression.\n\n2) But it sounds like you have a specific loss function you want to minimize, so you might set up the problem as a custom regression that minimizes that loss.\n\nThe first is easy with off the shelf software; the second would require some programming and/or math."", 'He\'s got 9K+ observations so even a plain old logistic regression with glm should work. That said, it can\'t hurt to use  regularized regression even when the ""width"" of the data is only 240/9000 ~ 1/36.\n\nIt sounds like you want to model -1 (success) versus 0 or 1. \n\nLogistic regression in the glmnet package works with only binary variables or sums of them. The 0/0.5/1 comment below is leading you astray.  A response with more than two values requires the family=""multinomial"" option, but you should use family=""binomial""\n\n1st recode \n\n     Y1 <- 1*(YourData$Y==-1)\n\nAlso, as these fitters have no formula/data interface you\'ll need to create a predictor matrix\n\n\n     ## create a vector corresponding to all columns \n     ## containg the response variable or stratum \n     ## e.g. everything that\'s not the predictors\n\n     dr.cols <- which(names(YourData) in c(""Y"", ...))\n\n     X <- as.matrix(YourData[, -dr.cols])\n\n   \n\nThen proceed as follows\n\n     fit.glmnet <- cv.glmnet(y=Y1, x=X,  \n                 family=""binomial"",\n                  type.measure=""auc"")\n\n\nNext, keep in mind that when you use this fitted model downstream, you\'ll need to specify the regulation parameter e.g.\n\n     ## regression coefficents interpreted \n     ## as the log odds ratio for risk of 1 \n     ## vs 0. positive increases the\n     ## probability, negative decreases it\n     B <- coefficients(fit.glmnet, s=fit.glmnet$lambda.min)\n\n     ## AUC from 5 fold cross validation of best fit\n     with(fit.glmnet, cvm[index[1]])', 'Might be a good dataset for classification and regression trees (CARTs) - maybe consider random forest and xgboost models', 'I recommend lasso with ""binomial"" family.  Research the glmnet package in R.  Your function call will be something like `cv.glmnet(y ~ ., data = df, family = ""binomial"")`.  You can\'t do GLM with 240 predictor variables - the amount of multicollinearity will be staggering.', ""I'd estimate a conditional probability model (not a classification model), and then use that to make a prediction function that has a low chance of making the bad error you want to avoid.\n\nEg, the conditional probability model might yield -1 40%, 1 30%, 0 30% for some predictor values. But then you don't want to predict -1 even though it is the best guess. Because there is a decent chance the value is actually 1. You could set a threshold like, never predict -1 if P(Y=1|x)>= eg 5%. The threshold would depend on what your model output is - you are asked to find -1 after all, so you don't want to set it too low either."", ""Recode and do a logistics regression? You are only interested in a binary outcome from what I understand.\nIs this post under the rules though? I know it's not school, but I don't know.\n\nEdit: nevermind. I don't understand the data from the question."", 'One Idea:\n\n\\- You can try two stages.  First a logistic regression to classify if something is -1 or not -1.  (Or binary trees)\n\n\\- Then given that something is predicted to be NOT -1 calculate the probability that it is not +1.  (or binary trees). \n\nYour probability of misclassification will be roughly product of the two.', 'You might consider a support vector machine. \n\n(Donning my trusty flame suit...)', 'Isnt is more of cluster? Why regressionb', '[deleted]', 'They gave you this exercise to asses if you have the necessary skills for the job. If you cannot solve this exercise without the help of reddit, then you are clearly not suited for the job. I also think that posting this here is just generally really unethical', 'Everyone has talked about the method, but i would like to talk about two different approaches for those methods.\n\n1. Takeout all 0 response from that dataset, do all those methods others suggested, check the confusion Matrix accuracy \nThis way, pure -1 or 1 response will be checked \n\n\n2. Recode the 0 variable to -1, and run the above analysis on the full dataset. Check the accuracy.\n\nWhichever model gives better accuracy, use that.', 'Can u explain why u use logistic regression', 'Use logistic regression to model the relationship between features and targets (potentially regularized as others have said), but then minimize the *expected cost*, because a false negative has a higher cost than a false positive based on your description.', 'looking into it. Thank you! <3', 'Thank you for pointing a path to take. Bless you!', '>\tYou cant do GLM with 240 predictor variables - the amount of multicollinearity will be staggering.\n\nWith 9k+ plus observations? I cant possibly imagine multi collinearity is going to be a staggering problem.\n\nThe bigger issue is overfitting if its a prediction task.', 'I thought of this too. One simple tool applied twice :)', ""Huh, I didnt think that 0.5 would work if it's replacement for a failure (0). I'm intrigued, got an example use case?"", ""Do you never Google anything at work? Part of what can make someone a great hire is their resourcefulness. Who cares if they knew what to do when you asked - if they find a way to deliver what you want, that's value"", ""I dunno man, it is not a job, it's an unpaid internship. The whole point is to learn. And I'm confused about the Data. I thought someone  can give me clue. Sorry if I broke any rules."", 'I just want an internship. It is completely different then what i done in uni. Maybe give a tip instead of shaming me for trying to learn. Whoopty Doo helpful community.', 'Maybe I am missing the point, but a lot of answers only focus on which model to use.  \n\nIt is unclear to me what happens if we predict a 0 value to be +1. I assumed it is also something we would like to minimize. \n\nA simpler (and more robust) approach might  just apply binary classification (+1 or not) and then minimize misclassification error.', ""You broke no rules dude. Never be ashamed to ask for help unless you're explicitly told not to"", 'Unpaid? Dude, have some respect for yourself and walk away. Companies get away with this type of shit because of people like you. I report every single unpaid internship I see on LinkedIn. Im not supporting that kind of free labor.', ""Nah man it's complicated. It's a long story. I'm not I'm the US . I'm forced to take an internship due to some bureaucracy. And I don't mind working for free for a bit. Getting some experience is useful, especially since stuff u do in school is so different from real life needs."", 'I respect the grind. Keep your head up.', 'Yeah, it is a shame that unpaid interships exist. On the other hand you gain knowledge. At least it was the case where I did an unpaid internship for three months. It also made it possible to finally have a Bachelor degree. (:']"
[Q] How to deal with unequal distribution of target variable in dataset?,"I'm currently working on ANN that should predict the probabilty of the certain event. I have a dataset (~100k observations) with 1 binary, target variable (0 if event occured and 0 otherwise) and 20 continuous variables, describing the environment. But the problem is with the unequal distribution of the target variable (95% - 0 and only 5% - 1), which after training, leads to understimation of the probability - the max y_pred in the test set equals only 0.2.

What should I do to get better estimations? Am I suppose to limit the dataset to equal amount of 0 and 1?

Any advice would be appreciated. Have a nice day!",11onif1,Rand0w0,1678550041.0,15,0.89,"[""Why do you conclude that the probability is underestimated? In the dataset you have the baseline probability that an observation is positive is 5%. A 20% estimated conditional probability that an observation is positive is a factor of four change from that, which indicates some strongly informative features in your model.\n\nThis is generally just how models work, it's a manifestation of regression to the mean. You shouldn't expect models to provide you with anything close to certainty in most situations. This often is not an issue that needs correction.\n\nWhat are you using the model for? Is this a classification task where you are deciding to take some action or interventions in the positively classified observations? If so the correct procedure is to tune the classification thresholds with the costs of false and correct classifications in mind.\n\nTo make it explicit, I disagree with the other two answers here. Oversampling and undersampling, while popular in blogs and beginner literature, are often a solution in need of a problem. Quoting Murphy's *Machine Learning: A Probabilistic Perspective*:\n\n> It is worth remembering that all these difficulties, and the plethora of heuristics that have been proposed to fix them, fundamentally arise because SVM's do not model uncertainty using probabilities...\n\nThese techniques were developed for situations where models, like SVMs, combine the tasks of probability estimation and classification into one, and do not model the world in probabilities.\n\nSee [this CV thread](https://stats.stackexchange.com/a/405437/74500) for a more detailed summary of this area."", 'This is a classic problem.\n\nLook up the following:\n\nRandom oversampling\nRandom undersampling\nSMOTE (Synthetic Minority Oversampling Technique)\nROSE (Random Oversampling Examples)\n\nThese are different techniques that balance your dataset, either by oversampling the minority category or undersampling the majority. Try all, see what produces the smallest error in the validation set', ""I'm wrapping up a similar project now. What worked best was a combination of oversampling and undersampling (only the training dataset). But worth trying different sampling thresholds.\n\nBtw, chatGPT / BingAI has been really helpful at answering such questions (and follow up questions) on the fly"", 'This is a good answer.', ""These methods don't work and may actually harm model performance. For example, check out this paper: [The harm of class imbalance corrections for risk prediction models](https://academic.oup.com/jamia/article/29/9/1525/6605096)\n\n> Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed."", 'Its a classic problem with a classic solution  weighting or considering new methods. Generally the class imbalance is important and reflects the actual situation (conditional/marginal probability estimation) so your low probabilities arent anything to worry about statistically.\n\nPlease do not oversample, and definitely dont under sample as thats just throwing away information\n\nWeighting or simply changing the probability threshold is generally seen as preferable to oversampling via SMOTE or another method. A nice discussion in this sub from last year  can be found [here](https://www.reddit.com/r/datascience/comments/sxtppd/stop_resampling_data_in_classification_problems/)\n\nYou can think of over sampling as just making stuff up that seems reasonable, while this might make sense as the only thing to do about missing values other than dropping them, you really shouldnt be making up entirely new observations based on your current, thats a great way to overfit- put this way it should be fairly obvious to avoid these techniques', ""I believe there are also methods in which one can add a weight to the classes to compensate (for example. They don't give exactly the same results as oversampling."", 'Of course if one\'s goal is to accurately estimate probability, then anything that changes the balance of classes in the dataset will yield a biased estimate - as suggested by the paper.\n\nThese techniques are useful though when the goal is to build a model that detects the rare class: detecting fraud where most cases are not fraudulent, detecting disease where majority of patients are healthy etc. \n\nSince OP mentions the word ""training"" I assumed, perhaps mistakenly, that the latter was the goal', 'Why do people double down when presented with a simpler, viable alternative? You should want to remove complication from your models as much as possible\n\nThe point of the above paper and much of the discussion against sampling methods here is that simply adjusting the probability threshold yields similar payoffs\n\nWhy introduce fake data with oversampling or throw out data with under sampling? Its kind of nonsensical imo to alter your data in this way by just making stuff up or throwing it out. and the papers and discussions posted here corroborate that\n\nIf you really want to favor the minority, use weighting and/or adjust the probability threshold, dont use sampling techniques', 'I am only ""doubling down"" bc in my experience random undersampling (but not oversampling and not SMOTE) yielded better AUROC, although that was with XGBoost, and not logistic regression or ridge.\n\nBut the discussion re improving sensitivity in the supplementary material seems convincing enough, so perhaps under/oversampling is indeed mostly useless, even for detecting minority class.', 'Im unfamiliar with how xgboost assigns probabilities and youre right the above discussions are about logistic regression\n\nId be interested to see a simulation study on xgboost and perhaps RF performance here as well, Im not necessarily doubting your results but was the improved auc-roc you saw on an original unaltered testing set or was it on the altered training?\n\nIm just a bit hard pressed to understand how throwing out data would improve auc-roc from a conceptual standpoint but again maybe I dont understand how boosting assigns probabilities?\n\nDefinitely willing to consider these techniques if shown something about their helpfulness on unaltered test samples', ""It was on the unaltered testing set, but I can't publish the data, unfortunately. The model concerned identifying donors to a charity appeal (only about 3% donate)\n\nBut again, it was just one dataset and could have had something to do with model hyperparameters. Hence my suggestion to the OP to try different techniques and see if they help minimise the error.\n\nMy guess would be that hyperparameters that impose a constraint on node sizes in tree-based models (to prevent overfitting) might also prevent the model from learning meaningful relationships if the target group condtitutes a small proportion of the training set."", 'Did you try weighting as well? Were there similar improvements? Honestly Im so unfamiliar with xgboost that I dont know if it supports weighting']"
[Q] How to induce non-proportionality of the risks in a Cox model?,"For a thesis in survival analysis, I'm studying what happens when the assumption of proportional risks in a cox model is not respected and, consequently, which models to use in this case. The problem is that in my dataset out of twenty variables none violates this assumption.  I've been working for over a month on data cleansing and various exploratory and non-parametric analyses, so I don't want to have to start over on a new dataset.  I need at least one variable, even if it were only one, to violate the proportionality of risks assumption.  Is there a way to induce this?",11ojon8,hawkeyeninefive,1678539786.0,7,0.77,"[""If you don't understand the underlying truth of the data, how can you possibly know if your attempts to rectify non-proportionality are successful? In other words, simulated data makes the most sense here.\n\nFurthermore, you don't induce non-proportionality in the model (if you somehow did it would no longer be a Cox PROPORTIONAL hazards model!), that is a feature of the data itself. And while I would be quite surprised if your data always met the proportional hazards model for 20 different groupings (which is what I assume you mean by variable), then that is that. The model doesn't change the data. \n\nYou might be able to combine groups, I suppose. But at the end of the day you are far better off biting the bullet, simulating data under known and controlled assumptions, and then, if you really want, apply your results to this real dataset of yours."", 'Just transform the independent. Do something like sqrt(X), X\\^2, or log(X).\n\nIf X truly satisfy the proportional hazard assumption, then those non-linear transformation are sure to violate the assumption.']"
"[Q] Flawed sample, should I still use Anova?","Hi,

I am new in statistics and I have trouble understanding the different tests to check for statistical significance. I plan on taking an actual class to compensate for this lack of knowledge.

I have 153 elements (tweets) gathered in 5 groups (topics). For each group then, I collected values (if a specific person was mentionned, number of likes, number of retweets and number of replies).

I tried use one-way Anova to check for statistical significance, although for each values the *p-*value was way above 0.05. While I do not absolutely seek statistically significance if that would mean twisting results, I have a few theories at what could have biaised my results.

&#x200B;

1) My 153 elements were made of all the tweets that used a certain hashtag and had a minimum of 100 likes OR 100 retweets. Therefore, given they are in a way only the ""top"" tweets, the distribution is flawed as tweets that received low likes and retweets are underrepresented.

2) One category was made of only 2 tweets so I excluded it completely from the test as it seemed too low.

So my question is: should I use a different test? Or is my sample too flawed to even be tested?

Thank you very much",11oiqo3,,1678536930.0,4,1.0,"['Before we go any further, what test(s) did you actually run? It sounds like you intended to look for differences in the mean number of likes and retweets across topics, but that would require two one-way ANOVAs, and its not clear how that person being mentioned or not fits into the picture. If youre interested in the likelihood of that person being mentioned across different topics, an ANOVA isnt the most appropriate way to go about studying that.', 'Thank you for replying, I used a one-way Anova test per value (so I run a test for likes, one for retweets etc.). For whether the person was mentionned, I replaced ""yes"" by 1 and ""no"" by 0', '>\tFor whether the person was mentionned, I replaced yes by 1 and no by 0\n\nThats a binary response, not a continuous one. ANOVA is a linear model under the hood, and while you can choose to model probability linearly, there are some potential issues with doing so. For example, the model may predict probabilities less than 0 or greater than 1, which are meaningless. Instead of limiting yourself to the significance test, you can fit an appropriate model directly (e.g., via logistic regression) and look at estimates for the probability across groups.', ""Interesting, I didn't think about that, thank you, I'll give it a try""]"
[Q] Guidance on probability and generalisability,"Apologies, as I know this is a bit basic, but I have read books, watched videos, asked colleagues, and paid people on upwork, yet still, I'm non the wiser... I would be very grateful for some help, please.

I carried out a survey with 88 Japanese university students studying English. I have one question from the survey that I'd like perform inferential statistical checks on, but I'm really not sure what tests to apply. Currently using JASP.

The survey question:

I asked 'What medium of goal setting do you prefer?' I offered them three options to choose from

a) online goal setting

b) paper-based goal setting

c) either is fine

The responses were

a) 58

b) 9

c) 21

What tests should I apply?

Thanks in advance for your assistance!",11ohsi7,nipponcouture,1678533913.0,4,1.0,"['I was told by a statistician on upwork that ANOVA is the best way to get a p-value for this survey response. I\'m questioning this, as scaling categorical data like this seems wrong. Chat GPT suggests using a ""chi-square test to determine if the observed frequencies of each answer are significantly different from what you would expect by chance."" When I\'ve questioned the statistician on upwork he says:  \n""Anova is the best to check the significancy between three categories,,p-value tell us the results are not coming by a chance""  \nIt\'s fair to say I\'m still very much confused...', "">What tests should I apply?\n\nDepends on what you'd like to make an inference about. No testing necessary if you want to simply describe the responses of your sample. If you feel like your sample should tell you something about a larger group of people, you need to consider how your sample was selected from this group"", ""Thanks for your reply. I selected the sample based on convenience sampling.   \nRegarding population, I have a few options: EFL students at the institution (around 10,000); EFL university students in private universities within the prefecture (around 200,000); or within Japan (perhaps 5,000,000). Let's say within the institution. How would I test for significance in that circumstance?""]"
[Q] Compare multiple groups proportions (for binary question),"Hi I was googling this but unsure if Chi2 or confidence intervals for the group are more suitable.

I have 4 groups with sizes between 100-1000. In ecevvery group I have numbers for outcomes A vs B (binary variable).

I can calculate the point estimate for A_share per grousroup
1: p_A = 0.8 (n=990)
2: p_A = ...
3: ...
4: pA = 0.3 (n=100)

How would you quantify if the groups are different?

Naively, I would have constructed CIs using binomial/normal approximation of it.
For groups with non overlapping CIs I would conclude that they do not habe the same p_A with a certain confidence.

However, online I mostly find that Chi2 tests are suggested.
To my understanding, a significant result would only mean tha not all groups come from the same population/same underlying p_A. This is a weaker claim than I would do with comparing CIs per group...

Can I do the CI approach? What to take care of when doing so?

(part of a private, non-academic and certainly non-representative survey I am doing)",11ohj86,fendrix888,1678533044.0,7,0.82,"[""Non-overlapping CIs will definitely tell you which groups are different. The trouble is that two groups can still be statistically different at some p-value level and have CIs that still overlap to some extent.\n\nAnd you are right, an omnibus test of a model with categorical (group-wise) predictors being better than a model without such predictors won't tell you \\*which\\* groups differ from each other - just that groups differ.\n\nI suspect a typical approach would be to treat it as a post-hoc comparisons problem. E.g., do a bunch of independent binomial tests, with some sort of p-value adjustment for multiple comparisons, like a Benjamini-Hochberg (aka FDR) correction. This is probably fastest.\n\nAnother method might be to take a Bayesian approach, treat the group effects as random effects, and directly compute the posterior distributions of any comparisons of interest."", 'Whats the survey ', ""Looking at non-overlapping confidence intervals is sometimes a controversial approach, but I think in this situation it's the approach I would recommend.  It's simple to calculate, interpret, and present graphically.  \n\nI would recommend using simultaneous multinomial confidence intervals.  (E.g. Sison-Glaz, and others)."", 'Question of the form ""Do you prefer A or B"" with no other options. Posed to different subredits which I want to compare.']"
[Q] How to determine what stands out in treatment levels after post anova?,"Hello an engineering student here. After using post anova whether it is confidence interval method, Fishers LSD, and/ or Tukeys studentized range statistics, how do you determine what treatment level stand out the most ?",11oewjv,theunknowxn,1678523571.0,1,0.67,"['Have you made and look at the boxplot of the data (response by treatment)? I think is a good start', 'What does ""stand out the most"" mean? Maximising the response variable?', 'I think the question was poorly worded because it appears it is asking for significance tests which are not a good way to determine what stands out. For significance testing, the LSD is considered too liberal (too high a Type I error rate) whereas the Tukey hsd controls the type I error rate. Some dont realize it is fine to start with the hsd and skip the ANOVA entirely.', 'I suspect the question on the test is intentionally vague in order to get students to look at and think about the results.  Which group(s) have the statistically the highest mean(s) ?  The lowest ?  Are there two groups with a large difference ?  Do the differences have any practical importance ?  Is there one group that stands out compared to the others ?', 'Well in our test that was asked. There are 5 treatments and using anova, the alternative hypothesis was rejected thus post anova was used. Each treatment in post anova was tested (treatment 1&2, 1&3, 1&4 . Etc). Then it was asked what stand out the most between each treatment']"
"[Q] Why is there ""total sum of squares"" and ""total degrees of freedom"", but no ""total mean square"" in ANOVA?","In SPSS's ANOVA output in the ""Total"" row, the ""Sum of Squared"" cell is populated, as well as the ""df"" cell.

However, the ""Mean Squares"" cell is empty. Why?",11o4xeo,TaylorBrow,1678492323.0,0,0.5,"[""You can find it if you want; it'll be equal to the variance of your variable of interest. (Total sum of squares divided by n-1 should look like a very familiar formula to you.) \n\nBut it's not one of the numbers that is needed to calculate the F statistic; for that you need MSR, MSE, and their respective degrees of freedom."", ""Such a quantity exists (naturally) but it's not particularly useful/interesting typically""]"
[D] Job more challenging than university,"Hi all! I work as a statistician in an factory. I would like to share my experience with you to know if it is common or not. For many reasons I find my current job more challenging than (or as challenging as) university. I had no difficulties during the first 3 years of university, while the fourth and the fifth year where tough but I finished with high final grades. Before getting a job, I did not expect to encounter so many difficulties at work. There are many things that troubles me:

* I realise I don't have much experience. I focused most of my time as a student to study statistics rather than to analyse many datasets. I still see myself as a beginner. I learn from every analysis. I always feel like I am not good enough and that data can be analysed in a better way.
* Datasets are more messy than university. It is very common to deal with outliers, short and/or intermittent time series, biases, etc.... Moreover data wrangling can take a considerable amount of time. I struggle a lot to get exactly the chart I want to report (maybe I need more time to get handy at using ggplot2)
* It is ridiculously easy to spend too much time doing a project
* I don't remember all the details of the methods I studied at university. Sometimes I feel the need to  revise some topics but there is not much time to do that. Sometimes I need to make decisions which I don't know fully how they would affect further analyses.
* At university it is obvious which methods are more appropriate to use for a specific dataset. Except for prediction problems, sometimes it is not easy to choose which method to use
* Sometimes it is not easy to think statistically
* I have poor social skills and talking is very important
* I tend to overthink about work a lot, even when I am not in the office. Having no teammates does not help either. I often feel the need to discuss with other statisticians but I don't have anyone to talk to except for online communities
* I often feel that the amount of effort I put in an analysis is not rewarded enough. I always compare my analyses with what I learnt at university. My analyses still look quite rough
* I feel a lot of pressure to solve tasks in a short time and get easily exhausted

Is it common ? Will it get better? Should I quit my job?

Thank you in advance.",11o1cmi,Neverstop50,1678483569.0,144,0.95,"['All of that sounds incredibly normal.', 'Wow. \nI thought I was the only one. \nYou just recited my exact experience!!!', 'It might get better. Youll certainly get better and faster at the technical skills of analysis. \n\nCompany culture will have a big impact on whether the pressure and stress decrease or you just burn out.\n\nThe biggest thing you can do to up your game is to learn to translate your analyses into results that are meaningful to your customer. Talking to the production manager? Theyll probably be interested in takt time, defect rate, first time quality. Talking to business managers generally? Its all financial: cost and profit. Engineers? Probably they care about performance against requirements. None of them understand statistics, so presenting outputs like p-values, odds ratios, or regression parameters is the least impact (good to have to show youve done your work, but theyre unlikely to understand).\n\nReal-world datasets are messy. In my experience, careful, by-the-book analyses was rarely possible. You do the best you can with what youve got; dont let the perfect be the enemy of the good. That said, you can probably post small projects mid- to long-term to improve data quality. Look for opportunities to improve data collection methods, especially those opportunities that are easy and quick to implement and will allow you demonstrate a real improvement in the quality of your work or turnaround time. Think low-hanging fruit.', 'It will get better. Work hard, and you will alleviate yourself', 'you need to have courage to talk with your peers and ask them for help or guidance.', ""What's your title? That's the type of job I want, but I'm a biostatistician and all I do is word processing."", ""> I tend to overthink about work a lot, even when I am not in the office. \n\nComing to people who donate their free time to continue talking about stats *for free* to ask how to put stats issues aside outside work hours probably isn't a great idea."", 'That is great that your job is more challenging than university. Always try and have the big picture in mind, think about what processes you can batch and set boundaries in the amount and scope of work you think you are able to achieve right now, give your employer YOUR time frames as it appears you are the one and only expert in the department. Take time outside of work to chill out and congratulate yourself on doing the best possible job you can.', 'It gets better. You figure out where its important to spend your time. Its enlightening becoming a manager and seeing how minimally some other people think. Try not to put undue stress on yourself.', 'It definitely gets better, it just comes from time and experience. Dont be afraid to make mistakes. We all do and its the best way to learn. College is great but the thing about it (I have a physics and engineering degree) is that you pay for a lot of stuff to be handed to you on a silver platter (or at least that is what it felt like for me.) so I had to learn how to learn again which was very weird. The people skills will come with time. Just dont be afraid to ask stupid questions because really there arent any for people that are new.', 'This could also caused by the fact that workers are exploited (in for profit companies), while students are not', 'It is normal. But try not to overthink, it will not help your mental health (been there). Just make sure the problem statement you are working on is very clear and the goal or output of your analyses are clear. Make your solution as simple as possible. And convey your conclusion as you are speaking to a 6 year old kid.', 'At university you learn how to solve exercises. \n\nAt work you are paid to solve problems: not enough info,  not enough time,  not enough feedback. \n\nGood luck', 'Try using chat gpt to speed up your data processing steps for working messy data.', 'It is the real world. At the uni everything is clean and pure in the real world dirty and confused and you are the pro in charge of dominate the chaos. Anyway 3 years of uni experience it\'s not generally sufficient to gain the ""pro attitude"" but take it easy, will become way easier over time.', 'Bro you look at the number how hard is it', ""My experience is employers are asking way too much from juniors these days. You'll probably not improve without massively sacrificing your free time outside of life. If you have any other commitments, I'd probably give them up now"", 'Tl:dr', ""I feel the same way, and I've learnt to be more forgiving of myself on many of this hardships."", 'When you do something do you document it?\nAlso do you set up macros to automate a certain task once you see it is being repeated?', '\npretty normal experience. \n\nMy suggestions:\nWhen discussing the deliverables for a project, ask the level of uncertainty that will not affect its use.  10%, 5% or 2% . That will determine the level of sophistication and complexity your models or work will require. For time series forecasting, I use stl a lot because the data was seasonal and a value with a 10% uncertainty was good enough.\nBy doing this I saved effort and feeling disappointed when people treated the results without the respect I thought they warranted.\n\nMost probably, through the data, you will start to learn a lot about the business. Improve your social skills by talking to people through informal chats or joining task forces/committees to learn how the data and your insights can have impact so you can suggests projects. Also you may find ways you can influence how the data collection can be improved.  Sometime little changes can make a big change to quality of the data that was not implemented it never occurred to people that do not use the data.\n\nWrite code so that you can reuse it.\nThis important for data wrangling and visualisation.\n\nThink of what forecasting, prediction, classification, etc tools you need that you may be missing.\nAs you gain experience, you should aim to have a toolbox of techniques ready to go.\n\nAt the moment, Im testing different ways I can forecast when the inputs are time series. Im sure someone will ask me to do that at some point  \n\nGood luck', 'Did you never have a job before getting out of uni??', ""Agree that most of this is nearly at the level of cultural universals for statisticians. However, many statisticians at big companies work on teams with other statisticians and statistical programmers and don't feel alone in the stats work. Good teammates to chat/commiserate with can change the whole dynamic from misery to a great job. Bad teammates can make it even worse than being alone."", 'However I am enjoying learning how to make the skills translate. \nI spend an epic amount of time on google refreshing all the statistical methods.', 'same here lol', 'You are right. It is all about courage. Thank you', 'I don\'t have a precise title at work, I was hired because I had a degree in statistics. The office is called ""Statistics and data analysis"". My employers do not know the difference between data analyst, data scientist, statistician, etc...', 'OP is asking for life advice not stats advice', 'I see the inconsistency now', 'As a student being paid $15/hr for research work that commands at least 3x that in the private sector, Ill have some of what youre smoking.', 'I had multiple jobs before getting out of university. I worked both as a worker and a office worker', ""... And I'm pointing out why the people who would be likely to answer it here would not be likely to have good life advice on that subject."", 'Fair enough!']"
How to plot freq of zero in a scatterplot where taking the axes are log transformed [Question],"I want to make scatterplot comparing frequency of different diseases between two different populations

The issue I am having is where one population have a freq of zero but the other one does not.

How to deal with this? 

One thought is just to make it smaller than my smallest observed frequency, like .0001 or something. Thoughts?",11nx6qo,python_noob_001,1678473818.0,2,1.0,"['It is common to use Log(X+1) in this type of situation.', 'Don\'t just take logs of values that include 0? Why must it be logs, exactly?\n\nIf you must do it (take plain logs), insert a full scale break across the entire plot and mark it on that way (but make sure the axis-mark labels are in original-scale for that)\n\n(See the third plot [here](https://stats.stackexchange.com/a/63542/805) for an illustration of what a ""full scale break"" entails. That\'s for a boxplot, but it can be done for scatterplots just as readily. ... there\'s some other alternative options discussed there.)', 'that makes a ton of sense']"
[R] Statistical Control Requires Causal Justification,[https://journals.sagepub.com/doi/full/10.1177/25152459221095823](https://journals.sagepub.com/doi/full/10.1177/25152459221095823),11nwhbz,Stauce52,1678472191.0,10,0.75,"['I think it\'s funny how in a different context, the variables that are being ""controlled for"" are considered the ""features"" the model is extracting signal from. Who would\'ve guessed that the model would be biased towards the signals you use to fit it?', '> In this article, we illustrate that controlling for an inappropriate variable can result in biased causal estimates. \n\nHow does this stuff get published...\n\nOhh right, psychology', 'What is it that is wrong with psychology?', 'How is this inaccurate? Controlling for a collider absolutely biases your estimates. You disagree with that? Below are a bunch of links describing collider bias in fields besides psychology. Are you disputing the premise that collider bias introduces artificial/biased associations and false positives?\n\nMaybe I\'m misunderstanding you but collider bias is the main thing they are referring to and can definitely bias estimates. What\'s your qualm or disagreement?\n\nYou said, ""How does this stuff get published..."" to that quote. Are you instead arguing that we *do* control for inappropriate variables, and throw the whole kitchen sink into the model? I am baffled by that if that\'s what you\'re suggesting.\n\n[https://www.medrxiv.org/content/10.1101/2020.05.04.20090506v3.full](https://www.medrxiv.org/content/10.1101/2020.05.04.20090506v3.full)\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9131185/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9131185/)\n\n[https://www.healthcare-economist.com/2022/03/30/what-is-collider-bias/](https://www.healthcare-economist.com/2022/03/30/what-is-collider-bias/)\n\n[https://lovkush-a.github.io/blog/data%20science/causality/tutorial/2021/02/21/collider.html](https://lovkush-a.github.io/blog/data%20science/causality/tutorial/2021/02/21/collider.html)\n\n[https://blogs.cdc.gov/genomics/2022/05/09/colliding-with-collider/](https://blogs.cdc.gov/genomics/2022/05/09/colliding-with-collider/)', 'File drawer effect at large. Nobody wants to publish things that arent recognized as significant. Its a major problem.', 'I would guess they are complaining that this has been known for a while (although maybe not widely in certain fields).', ""I'm not sure this has to do with the file drawer problem. Can you elaborate on how unprincipled modeling of statistical controls has to do with file drawer problem? Maybe I misunderstand"", 'Has nothing to do with file drawer effect, just a poor journal in an empirically poor field.', ""Ah, if that's the case then we're on same page. I agree, it is baffling and the reason I shared it is because I encounter reviewers who recommend throwing in a bunch of unjustified covariates or consult with students who have models with a million unjustified covariates and I'm really shocked by it sometimes."", 'The unbearable thing is researchers passing off stuff econometrics people were writing about in the 80s as new research.', ""They clearly are not passing it off as theoretically original. Many psychologists do not know about bad controls and colliders. Publishing what is basically a summary of prior theoretical results plus some applications to/implications for the field seems like a valuable addition. Psychologists probably won't seek out econometrics or causal inference literature from other fields, but might be more receptive to CI literature with some vague psych flavor."", 'Thats how I feel about this as well. It seems kind of bad faith to hate on the authors for writing a paper on an important statistical which many psychologists are clearly naive about. Even if economists have written about this in the 80s, academic research is very siloed and people often fail to visit journals or research from other disciplines so this seems plenty valid to me', ""> Psychologists probably won't seek out econometrics or causal inference literature from other fields, but might be more receptive to CI literature with some vague psych flavor.\n\nYes, that would be the problem, not the defense.\n\nAnd they are very much passing it off as new and original. No open admittance that they are simply illustrating a very known and researched issue in a psychology context. Bad controls biasing results is not new nor interesting on it's own and i will be incredibly surprised if this paper gains any traction whatsoever even inside psychology."", ""I think it's both the problem and the defense. One or a few authors can't change the fact that most people in most fields don't read papers outside their field, but they can do their best to import ideas into their field.\n\nDid you read it? I admit, I only skimmed, but they do mention bad controls and cite someone else. I agree, it probably won't gain much traction, as it's a methods paper in what I think is a b journal (but I'm not super familiar with the field)."", 'A few mentions of previous work on related issues is not being upfront about your paper presenting no new theoretical knowledge whatsoever. These issues have been written to death about in the last 30-40 years inside the field of causal inference.\n\nMethods papers can get plenty of traction. Just requires that they are actually interesting.']"
[Q] Model prediction -- Can I report the model trained on full dataset first?,"Hi all, I am writing an essay on predicting a variable. I will be running multiple models like lasso, ridge, random forest, neural network etc. I would also like to explain on how to interpret the various model and their parameters. Can I first apply it on the full dataset and report their outputs for statistical inference and explanation, then perform prediction with cross-validation?",11nwead,No_Canary_5299,1678472003.0,2,1.0,"[""Usually it's done in the opposite order.  You select a model, using the performance with cross-validation or a test set, then you train the model on the full data for production.  You can do inference on the final model, however there will be a bias introduced from model selection that is difficult to quantify."", 'What bias do you mean? The generalization error estimated through cross-validation is an estimate whatever you do with that model afterwards, whether you use the full dataset or not.', 'Its a noisy estimate, and by choosing a model youre introducing a selective pressure for both high signal and high noise at the same time.  After you choose the model, the cv estimate is no longer valid because youve already used it to select a model (no peeking!).  In cases where theres a lot of models to choose from, eg Kaggle competitions, the highest performing models likely have high variance.  For example, see https://laurenoakdenrayner.com/2019/09/19/ai-competitions-dont-produce-useful-models/ for an explanation of this idea.']"
[Q] Directional Statistics: Cramer-Rao and Rao-Blackwell,"Hi all,

I've been reading more into directional statistics and came across a question that I couldn't find an answer to. So I figured I would ask you all to see if there was a good reference for or a proof that the Cramer-Rao lower bound in directional statistics is truly the best variance and if there is a Rao-Blackwell theorem for the circle?

More specifically about the first part of my question. It's fairly easy to show that the unwrapped distribution (typically) fails the usual Cramer-Rao bound since it does not satisfy the regularity condition with interchanging differentiation and integration. In part, this is due to the fact that the estimator need not be cyclic with some frequency. It seems like, to me anyway, directional statistics only looks at the class of estimators that are cyclic and derives a bound on the variance. Does this mean that there could be an estimator that does not fall into this class of cyclic estimators that has a better bound on the variance? 

As a bonus question, suppose there are two parameters of interest: x and y. For illustrative purposes, I shall choose the distribution to be the generic cardioid (1+x cos(t-y))/(2pi). In this case, it's clear to see that y requires the directional version of the cramer-rao bound, but x does not and can be solved via the usual Fisher information. Can such a hybrid Fisher information matrix make sense and how would one define it? My best guess is that one could define the diagonal entries the same usual way, but then one has a weird off diagonal term. I'm almost certain that one could define it using some hybrid approach, but a proof illudes me at this time.

Best,
QoO",11nu3nk,QuantumOfOptics,1678466662.0,5,0.86,"['Theres a YouTube channel called statisticsmatt (also a Reddit user with the same handle) with a video on Cramer-Rao that may answer your question. I was going through his playlists myself and the info seems high quality but I dont have a sufficient background to guarantee the video answers your question as Im not studying that part yet.\n\nLink: https://m.youtube.com/watch?v=KDsd1F6oy78', ""I suspect you've had issues looking for your first request because the proof you are looking for probably doesn't exist. The CRLB is a rather general lower bound, and it's not always the case for a given problem that there are any estimators that can in fact achieve the CRLB.\n\nThat is, the CRLB is not the best possible lower bound generally, there are cases where tighter bounds may exist. There are of course scenarios where estimators do in fact achieve the CRLB, which makes it a tight bound in those scenarios.\n\n&#x200B;\n\nFor the Rao-Blackwell theorem, I would suspect the standard proof of Rao-Blackwell can be pretty directly transferred, e.g:\n\n[https://gregorygundersen.com/blog/2019/11/15/proof-rao-blackwell/](https://gregorygundersen.com/blog/2019/11/15/proof-rao-blackwell/)\n\nThat said I wouldn't spend too much time focusing on the Rao-Blackwell theorem for directional data. Non-maximal sufficient statistics are actually pretty specialized to exponential families and often don't readily generalize. I don't have much intuition of how they will generalize to directional data. That said I haven't really worked with directional data and am just barely aware of the topic. That said, I probably will be deep-diving into it in about three to six months for a portion of own research I've been pushing off, so if you want to discuss with someone you're welcome to reach back out later.\n\n&#x200B;\n\nRegarding the bonus question, this can easily get messy. Your joint distribution is now a distribution on a new manifold that is a product manifold of Euclidean space and the circle. The covariance structure on this new manifold is going to be naturally defined in terms of the metric of this manifold. A lot of results you might expect to carry over into statistics on manifolds, e.g. Cramer-Rao: but you need to develop a lot of machinery to deal with it properly.\n\n[https://doi.org/10.1016/0047-259X(91)90044-3](https://doi.org/10.1016/0047-259X(91)90044-3)\n\n&#x200B;\n\nIf you need practical results soon, I do have an alternative suggestion that's not totally correct, but not totally incorrect either. Rather than treating the parameter as lying on a circular manifold, you can consider a rotation of the data so that you genuinely do not expect the parameter to lie anywhere near 0, and consider the parameter to be constrained to the interval (0, 2pi). You can then apply either constrained optimization techniques, or alternatively transform this interval to (-inf, inf), and use unconstrained estimation. Provided your parameter does not lie particularly near the boundary you should get roughly correct results doing this."", ""This is a good video. I've actually watched it before when I was learning about Cramer-Rao bounds. Unfortunately, this is for distributions described on the real line. It's quite interesting how the geometry of the underlying space changes how one should think of and utilize the statistics. This is effectively the heart of the questions I've posed. How is it different and how can you combine them (if that's even possible)?"", ""Hey u/Kroutoner! Thanks for your reply. Sorry that I missed it. I agree about the CRB is merely a tool and that generally a minimal estimator doesn't exist. In general, I am setting up a design of experiment and wanted a way to compare two similar methods of getting and analyzing the data. I guess I was a bit spooked that there might be some new way of looking at the data that I was missing. After thinking for some while, I don't think such an estimator exists and hence one should only be looking at the cyclic estimators. \n\nAfter thinking about it. I also agree with your assessment of the Rao-Blackwell theorem. In fact, I suspect your reasoning of the exponential group to be on the mark. Hence, why everything plays so well with the von Mises distribution. \n\nI think I've read this paper a while back and taken some time for it to sink in. In this case, I'm almost certain the covariance structure can be obtained in this case quite easily. I've been sitting down again and think I'm pretty close to a proof. The hardest part is that Mardias circular information inequality has a different cost function, but I think that can be overcome. I'll update if I get anywhere.\n\nI agree. I used that stance a while back and got some good results. In this case, we want to at least be a bit more fundamental with it and want to do a bit more careful analysis. Thankfully, I think I'm close just worried about some of the finer details.\n\nThanks again for your insight! As always, it was very helpful and informative!"", 'My apologies the question is a bit out my expertise and Id rather not mislead but I hope you find someone more familiar', ""It's alright! I've found that this subject is quite new, there isn't a lot of literature on it, and niche. I mean, how often does one want to analyze a distribution which is defined on a circle? Anyway, it's a great video and thanks for reminding me about it. I'm currently writing a small tutorial about Cramer-Rao for others in my group and I'm sure that this will help write it. \n\nAnyway, there was no misleading here. If you are interested in learning more, I highly recommend Robby McKilliam's thesis and the book(s) by Mardia and Jupp. Unfortunately, they didn't quite get into the questions that I'm interested in, but they are incredibly readable and instructive."", 'Sounds good and thanks! Im currently self studying core stats after a related stem masters and pure math undergrad and was planning to work through measure theory, multivariate stats, tsa and the like with online resources such as statisticsmatt. The combination of geometry and  machine learning struck me as interesting a while back so learning about the intersections such as distributions defined on circles seems similar so Ill likely check it out in the future. \n\nAs a suggestion every academic I can think of has some level of admission that maybe 5 people will read their thesis. Id suspect that theres a non trivial chance youd get a response if you were to email the authors with your question, being able to talk about their thesis might be interesting for them.']"
"[Q] Is this a ""Robust"" Mahalanobis distance (outlier detection)","Hello! 
I've found this webpage (https://github.com/friendly/SAS-macros/blob/master/outlier.sas ) where some SAS code is used to use Mahalanobis distance for outlier detection. Actually, since the first step consist in standardizing the data via PCA  ( proc princomp) the  distance used is the sum of squares.
So, can this be considered a robust version of the Mahalanobis distance to get outliers?",11nl9ao,_throw_hawaii,1678443438.0,0,0.5,"['Anything that uses a ""traditional"" variance computation (squared distances of all points) won\'t be ""outlier-robust"" in the usual meaning of that phrase.', 'Thank you for your answer! Then what do you think the author meant by using the adjective ""robust""? ', ""I hadn't clicked through fully. The description says:  \n> The macro makes one or more passes through the data. Each pass assigns 0 weight to observations whose DSQ value has Prob ( chi ) < PVALUE.\n  \nSo it's not just normalization by PCs, it's iterative reweighting and excluding data points. That's where the robustness comes in."", 'Ohhh got it, thank you very much!']"
[Q] What advantages does a bachelor's in statistics have over a bachelor's in computer science?,"Nowadays everyone and their grandmas are majoring in CS, and it seems like CS is a degree that can get you almost every job in tech or data analytics like software engineering, product management, data science, data analytics, data engineering, etc. That being said, are there any benefits to doing a bachelor's in statistics instead of CS for getting a tech or analyst job?",11nikj4,,1678434345.0,4,0.75,"['Actually knowing statistics and making better analyses of data?', 'CS major here. ""Advantage"" depends on the kind of job you are looking for. For most jobs it doesn\'t really matter what you have studied. \n\nThere are however, a few jobs where you want a more mathematically minded person. For example I work in the medical device industry. And we definitely want people with a strong background in mathematics and especially statistics. \n\nIn the end we need to convince the regulatory agencies that our products are safe and have clinical value. And a proper statistical analysis goes a long way in doing that.', 'If youre stopping with a BS, its hard to beat CS. If your career path necessitates grad school, things get murky.', 'Are you talking about statistically significant advantages?', ""Statistics is a large part of data analysis and data science for most business, engineering, science, and social science problems.  Many of the methods developed for these applications (even many machine learning methods) came out of statistics departments.  \n\nThe big exception is deep learning, which is relevant for image recognition, natural language processing, and artificial intelligence.  Deep learning has little to do with statistics.  \n\nI think most future data analysts or data scientists would be better suited with a statistics degree.  It's more widely applicable and business ready.  Doing original work in deep learning requires that you're at the cutting edge.  Most of the rest of us will simply use existing chat bots etc. as a tool.  However, if you have an elite intellect and a passion for DL, CS is the choice.  \n\nIf this is not about math or data at all, and you want to code applications, then CS is also the way to go."", 'It is easier to learn programming independently. There are tons of resources to learn online.\n\nLearning statistics without proper instruction is much harder, in my opinion. If you have the opportunity to study with faculty that know what they are doing, you will be much better off in the long run.', 'You bring the perspective of a statistician to whatever data team, its valuable on any of them. You can understand a lot about computers/networking/programming and still do a piss poor analysis.', ""Why not both? Only somewhat kidding, but I ended up doing undergraduate degrees in math and CS because I found passion in data analysis and machine learning (plus math is just neat). If you're just trying to get a job out undergrad then CS is probably an easier path. If you still want to lean into stats you can probably throw in a math minor or do the stats degree and take computational math/stats courses to get some experience programming as well."", 'I asked many people regarding this question and most of them told me that programming skills are valued more than statistical/mathematical skills in tech but I can definitely see how being good at math/stat will be more advantageous for analyst jobs in finance or business.', 'Booooooo', 'Deep learning has lots to do with statistics. A statistician immediately has the benefit of understanding activation functions, since a sigmoid is just a logistic curve. \n\nDeep learning is just a non-linear model. I think the hard part is the computer vision and NLP applications that are catered to CS. You can still learn those as an elective or in free time though.', ""Don't get me wrong. Programming skills are important. But as an analyst you don't have to be a pro. A little bit of SQL, Python/R/Matlab or whatever your favorite poison is, is enough. Nobody is going to let you touch the production code base anyways. You are mostly building and evaluating models on your own. And nobody cares how bad the code you are running on your machine looks like.\n\nOnce you hit upon something that might be useful in a production environment, you team up with the engineering colleagues who have the expertise to properly implement it."", 'This, the programming and CS skills required by the average statistician or data analyst is quite overstated. Can you query a database effectively, do some data frame manipulation and light feature engineering and run the functions to fit your models? Then youll probably be fine, other people can handle putting it into production and lots of models and analyses arent even put into production in the first place. But theres of course always exceptions to the rules.']"
"[S] [E] Tool to ""play around with"" statistical analysis","Hi!

Is anyone aware of any tool where you get some set of data and can test different statistical methods for practice and fun (just to get a lot of repetition and practice an intuitive response to data analysis)? I have studied applied mathematics but that was many years ago so it would be nice to refresh my memory.",11ni0c8,WhackedUniform,1678432533.0,1,1.0,"['R', 'Jamovi if youre not ready for R just yet.', ""Orange.\n\nR and Python have a steep learning curve if you are not into programming.\n\nOrange is all about drag'n drop & configure widget.\n\nThat being said you will need to learn Python or R if you want to get anything serious done."", '*jamovi* has an amazing user-friendly interface, and it also has a relatively large number of extra modules for stuff like SEM, Bayesian Inference, etc.', 'Yes, but then you need to find you own data to load and you need to know the methods that you are supposed to use for the analysis (or is there a R-package that will give you exercises in this?). I am looking for something similar to a tutorial on statistical methods with preloaded sets of data.', ""I'll second Jamovi.  R is too much fiddley monkey business just to play around for refreshing memory.\n\nHowever, let's establish the games of the playground.  Here are things that Jamovi would be good for by working in a tabular format:\n\n1) Variable A has two levels, variable B is the response.  Let's have the A column contain {1,1,1,2,2,2} and the B column {10,10.1,10.2,20, 20.1, 20.2}.  How should I analyze and what will it say?  Press T-tests and see a dialog with all the input and output options you might consider.  This is way easier than looking up the coding in R for the same result as R will give.\n\n\n2) Let's add another level to A, so add {3,3,3} to A and {30,30.1,30.2} to B.  Now what analysis? ANOVA, with even more options.\n\n\n3) With those three levels, what would it say if the means were closer?  Add a new column C {10,10.1, 10.2, 11, 11.2, 11.2, 12, 12.1, 12.2}.  Try another column D {10,10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8}\n\n\nAt any time you could make scatter plots.  This avoids the torture that is learning plotting in R.\n\n\nIf you want to refresh your memory on, say, modelling sampling distributions and confidence intervals, then use R."", 'R has loads of built in datasets (possibly by installing a package, I cant remember which datasets are in base R and which arent).\n\nFor the methods, yes, you need to know what you want to do. Just find a list of statistical tests online, I guess? This isnt a problem a software can solve, they are tools, you need to tell them what to do.', 'You can try Swirl: [https://swirlstats.com](https://swirlstats.com/)\n\nIt has interactive lessons in R that teach basics of data science, stats and R itself with real datasets.\n\nAlternatively you could try generating your own data from probability distributions with certain parameters (population mean for example). See how much you can infer about the population parameters from the data using statistical methods - e.g. how close is the sample mean to the ""true"" population mean that was used to generate the data? How much closer does it get if you increase the number of datapoints? Etc.', 'Thanks!']"
[Q]Principal Component Analysis vs Multiple Regression,"I'm curious about PCA and what does it actually do to the data. And what does it explain compared to multiple regression? What are the differences between the two. Say I have 5factors for IV, and 1 DV. What gives me merit to use the PCA instead of Multiple Regression",11nd922,Desperate_Second,1678418673.0,44,0.95,"[""PCA doesn't have a response. It's unsupervised and is aiming to explain the variance in the data itself.\n\nRegression is supervised and aims to explain the response."", 'It is perhaps worth mentioning that PCA can be used for regression. This is called Principal Component Regression (PCR). Although, since PCA attempts to describe the variance in the independent variables rather than the dependent variable, one will more often see a similar concept called Partial Least Squares (PLS). This regression approach also uses principal components but the PCs are estimated with covariance constraints.\n\nIn fact, PLS is a very popular tool in my field (chemometrics) as it can be used to predict outcomes from thousands of highy correlated independent variables. It is often applied to infrared spectroscopy.', 'PCA basically helps you find the combination / set of of variables that describe your data the best. You can then use these factors or a subset of them as IVs in your regression. For example, if you have a large number of variables (some of which may be correlated), PCA helps to reduce this down to those which capture the most variance in the data.', 'PCA and Regressions are used for different purposes. \n\nRegressions are supervised as people already mentioned, it means you have historical data about your output you also have historical input data, so you can understand the relationship between input and output.\n\nPCA is a dimensional reduction technique. You only have inputs. Using algebra you can create a small number of new variables that have zero correlation between them while maintaining a big chunk of the variability.\n\nThe two methods can also be used together because the input variables are quite often not independent as they should.', 'SVD is one of the most fundamental decompositions in all of linear algebra. Listen to Strang at MIT. It an extremely powerful tool\n\nIt can be used for interpretability and dimensionality reduction when preforming graph analysis. For example When computing kernel ridge regression you can run SVD on the the adj matrix resulting from the radial basis kernel function to better understand the structure of the communities see how many components, clusters there are in the map by looking at the drop down of the plotted singular values. This allows you to look at your heads in  multiple kernel ridge regression', 'Both use the correlation matrix, if you include the DV and so you should be able to pick out the  multiple regression coefficients from the loadings as the pc s are linear combinations of the independent variables. The benefit of pcs is that they highlight when there is multicollinearity or near multicollinearity', 'PCA can be useful in some cases where you have a lots of information to summarise and you risk to overparametrise your model. One example is to run FAVARs (factor augmented vars) instead of VARs', ""Pca it's just the linear version of a VAE, multiple regression is just a DNN with one linear layer"", 'What do you mean by supervised in this context?', 'How does PLS assign independent variation when variables are coordinated? Does it credit one variable over another or divide equally among them?', 'Can you recommend a resource for using PLS with time series data?', 'It means there is a ""response variable"" or ""dependent variable"", and an algorithm is trying to make a map from a dataset to predict/recreate that variable.  \nThe specific terminology - guessing a bit - refers to the optimization, where the error/loss is ""supervised"" by comparing the ability to recreate the response variable, so there is an objective way of saying if a model is good. Unsupervised algorithms have no ""guide"" for what a good model is.', 'https://en.m.wikipedia.org/wiki/Supervised_learning', ""It does the regression in a new latent space that all predictors and response variables are projected into. It doesn't assign variation to any of the original independent variables"", ""Adding a quick example to illustrate 'supervised'. \n\nIn supervised regression, you might control for confounding factors, but the decision as to whether to control for a mediator is something not at all based on the statistical strength of the relationship. \n\nExample: Assume the following causal chain: Smoking -> lung disease -> death\n\nIf you're trying to determine whether smokers have a higher risk of all-cause death, you shouldn't adjust for lung disease. It's a mediator; the primary (albeit not the only) causal path by which smoking causes death. Adjusting for this will bias the regression model's estimate of the association between smoking and all-cause death. A supervised model will leave lung disease out of the model on purpose.\n\nIn an unsupervised approaches, the fact that lung disease has a strong statistical association with death would mean it would probably be included in the model."", 'Interesting, that sounds like interpretability might be affected but good to know', 'Of course but what does interpretability with respect to the original independent variables mean anyways in a world where they are highly correlated and frequently many more of them than you have observations? The point is that you suppose the correlation is due to some underlying factor drives the values of multiple IVs in a coordinated fashion. Dimensionality reduction techniques help you to potentially discover these factors based on some criterion like variance minimization, etc.', 'Yeah, thats when you have to turn to theory and basic research to seek out stronger interpretability; for example, identifying what the underlying factor(s) might be']"
[Q] Cannot remember the name of this specific sampling error,"For example, if you want to collect data on job prospects of young adults in a certain town, but use a list of high school graduates as a way to collect your sample, you're missing the young adults who never graduated, thus not sampling the entire population. I swear there was a name for this specific kind of sampling error, but I cannot remember it. Any help would be appreciated, thanks!",11nckyp,communistagitator,1678416922.0,1,1.0,"['Its called under coverage.\n\nHeres the first Google result so youll believe me:\n\nhttps://ec.europa.eu/eurostat/cros/content/under-coverage_en', '[Selection bias?](https://en.m.wikipedia.org/wiki/Selection_bias)', 'Awesome, thank you very much']"
Standard deviation of a frequency [Question],"

500 of 10000 of my samples have a disease, which gives me 5% frequency.

I want to get a standard deviation for the 5%, because my thinking is that the error from 500 of 10000 samples would be smaller than say 5 of 100 samples and I want to communicate that.

Any help is greatly appreciated.",11nb7aw,python_noob_001,1678413382.0,3,1.0,"[""1. https://en.wikipedia.org/wiki/Binomial_distribution\n\n  Variance of the *count* (X) of people with the disease, out of n people is  np(1-p)  (where p is the population proportion)\n\n   The proportion with the disease is X/n. By [basic properties of variance](https://en.m.wikipedia.org/wiki/Variance#Addition_and_multiplication_by_a_constant), the variance of the proportion with the disease is \n1/n^(2) x np(1-p) = p(1-p)/n\n\n  The standard error will be the square root of that\n\n2. Also see https://en.wikipedia.org/wiki/Margin_of_error#Standard_deviation_and_standard_error\n\n Standard error of the sample proportion is [p(1-p)/n]\n\n  Of course you don't know p but you can estimate it by X/n\n\n3. Some information here as well: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"", 'Your frequency is the sample average.  So your st. dev is the sample standard deviation. It is proportional to 1/sqrt(sample size). \n\nSo increasing sample size by a factor of 100 will decrease your st dev by a factor of 10.', 'So if I follow\n\np = .05\n\nn = 10000\n\nso stderr is\n\nsqrt((.05 * (1 - .05)) / 10000) = 0.002\n\nIs it right?', ""Well, p is the population proportion, not the sample proportion, but yes, if you estimate p by X/n then yes, that's the estimate of the standard error. \n\n[When p is small, a quick approximation is sqrt(X)/n  (about 224/10000 in this example; i.e. 0.00224); the 'correct' answer will be a little smaller than that approximation]""]"
[E]Triangle Test to see if two samples are the same (null hypothesis) at 95% confidence shows near half of respondents choosing correctly would result in failing to reject the null hypothesis. Am I missing something?,"I'm trying to wrap my mind around [triangle tests](https://www.sensorysociety.org/knowledge/sspwiki/Pages/Triangle%20Test.aspx). It might just be statistics in general as it's been a few years since I took a statistics class...

If the null hypothesis is the two samples taste the same, why can upwards of half the respondents (26/60) taste the difference and still be at a 95% confidence interval that the samples taste the same? It looks to me like half the population just said that they could taste the difference. I get that the chance of guessing it is 1/3 or 20 respondents guessing it correctly, but if comments are all in line with ""this was obviously different"" it doesn't sound like guessing. They could be lying, but still... close to half?

Unless triangle tests are meant to be taken by thousands (1000 people looks like roughly 358 correct responses) of people. Most of what I can find seems to suggest in the 30-50 range though. At 40 people that is 18 correct responses would yield a ""these effectively taste the same"" response. So even though half of the people chose the sample as being different it would be 95% confidence that their is no difference between the samples.

[Here](https://www.awri.com.au/industry_support/winemaking_resources/calculators/sensory-difference-test/triangle/) are a [couple](https://onbrewing.com/triangle-test/) of the online calculators I used after I doubted my math in google sheets. Seems to me if half of the people tasting something say it tastes different... saying they taste the same wouldn't be a great choice.

What would be a better confidence interval to choose? 99.9% shows me at 50 respondents would be 5 people getting it right (probably needs double checking by someone smarter than me).

Am I misunderstanding what a 95% confidence interval means? I read that as I have a 5% chance of being wrong. And when the null is the samples are the same... but half of people can taste the difference... that seems like more than 5% chance of being wrong.

Thanks for any help in getting this through my skull.",11nb4hu,jespucci,1678413181.0,5,1.0,"['Thinking about it in terms of what percentage of people get it right is misleading since the critical value (over what percentage you would reject the null) decreases when the sample size n increases.\n\nInstead, think about the probability of getting results at least as extreme under the null hypothesis (i.e. the p-value).  In this case with a probability of getting it right by chance of 1/3 the exact probability of getting at least 26 right answers out of 60 is 0.068 > 0.05. This means that your data are ""compatible"" with the samples tasting the same and that you did not gathered enough evidence to reject this hypothesis.\n\nIt\'s important not to change the significance level after observing the results. That\'s a definite no-no!', ""Is a triangle test's purpose to show that in a larger population you would expect to see similar results as what you tested? For example, that we are 95% confident that if you tested 600 people, you would see roughly 260 respondents choosing the correct sample where we expect 200 of those to be by chance, and that the more people you test the lower those numbers would be? Or is this also incorrect?\n\nI am currently under the impression that a triangle test shows that samples are indistinguishable from each other. Failing to reject the null hypothesis should mean that the samples are effectively the same. If half the respondents can pick it out though, you should reject the null, as to half the respondents it was different. Do we subtract the amount we think would be randomly guessing? 6/60 seems more reasonable to say the samples are the same than 26/60."", ""You are testing whether the probability of getting it right is 1/3, which means that the participants are unable to detect a difference and are choosing the right option purely by chance. An event with a 1/3 probability occurs, on **average**, one-third of the time. However, in a single experiment, this proportion might change.  \nAn extreme example would be testing whether a coin is fair, with each side having a 1/2 probability. If you toss the coin four times and get heads all four times (i.e., 100% of the time), under the null hypothesis of a 50-50 probability of results, the probability of getting results at least as extreme as this (the p-value) would be 0.0625. Since the p-value is greater than 0.05, you fail to reject the null hypothesis, even though you observed a 100% frequency of heads. Observing the same outcome of all heads with a larger number of throws would allow you to reject the null, but with small sample sizes, you can obtain these kinds of results.  \nFinally, failing to reject the null hypothesis does not mean proving the null hypothesis. You may still have an effect, but if it's not that big your experiment may not be able to detect it since you don't have enough participants. \n\nI hope this is somewhat useful to you. I really cannot tell where you are at as far as statistics is concerned. I apologize for any confusion."", 'Thanks! That clears up what I thought the test was for.\n\nMy statistics comprehension is: I took the couple of classes I needed at university for my degree and then promptly forgot it for 5 years. So... I know enough to get in trouble, but not enough to be useful. ']"
[Q] Forecast Accuracy,"Question:

My company is looking at a different way to calculate forecast accuracy, which is great, because it will start incorporating WAPE instead of MAPE. This will solve for our low selling, high volatility items.

As one of the main users of this data, I can say that I HATE seeing accuracy that is outside the range of 0 - 100%. HATE IT. As a fix, I'm thinking about suggesting that we can make an adjustment to how the WAPE is calculated in the form of an IF statement.

IF the SUM of all forecasts are less that the SUM of all the sales, then SUM(forecasts)/SUM(sales). This will ensure the accuracy is between 0 and 100. On the other hand, IF the SUM of all forecasts are grater than the SUM of all sales, then SUM(sales)/SUM(forecasts). This will also solve for weighted items and keep accuracy between 0 and 100%.

Can anyone offer why this might be a bad idea?

Thank you kindly",11n355r,a_coupleofwavylines,1678394472.0,3,1.0,"[""What's the problem with 102%? If you don't like the numbers over 100%, divide sMAPE by 2.\n\nWhat happens if you forecast 0 and observe 0?"", ""Thanks for the reply.\n\n102% would suggest we overforecasted by 2%. That's fine, and we can **BIAS** calculations for that (forecast of 100, but sold 102), but to be 102% forecast accuracy would suggest we were so exact on our forecast (100%) that an additional 2% needed to be added to convey how awesome our forecast was lol. With my equation, even though we overforecasted by 2, the accuracy would be 98%.\n\nAlso, if I recall correctly, we would have items with sMAPE over 200, which would leave us with the same issue.\n\nWe specifically want to use weighted accuracy for when we have multiple items like this:\n\nItem A forecast 1000, sales 1000. 100%\n\nItem B forecast 2, sold 1, 50%\n\nThe old way suggested out forecast accuracy was 75% overall, despite accurately forecasting for 1002 units and selling 1001.\n\nWe have filters and exception reporting for the 0 concerns."", ""sMAPE is bound between 0 and 200%. The worst case is when you forecast x but observed 0, sMAPE=|x-0|/(|x+0|/2)=2. If you divide sMAPE by 2, you won't get values above 100%."", 'man thats interesting. I am working on forecasting project at my work and i havent made it to evaluation part yet.\n\nCan u share some of your resources about what is the best way to evaluate multiple product forecasts? I will have the exact evaluation problem you are facing rn and i want to start searching', ""That's my understanding as well, but I've seen JDA calculate some items' sMAPE well above 200; sometimes over 1000. Never understood why. Perhaps if I come across one tomorrow I can take a heavily redacted photo."", ""Thanks for responding.\n\nHonestly, research has been my friend. I think which method you use will be determined by exactly what it is you're looking for. Each methodology has its strengths and opportunities. \n\nThe basics are MAPE and WAPE. Given my dataset, i prefer WAPE. sMAPE (symmetic MAPE) might be good if you're assessing how accurately a historical forecast fit the sales.""]"
[Q] - Guidance for Causal Inference,"I am trying to learn causal inference as I find it a very fascinating and useful subject. I started reading Judea Pearl's book ""Causal Inference"" and I think it is very well written.   
I am still trying to grasp everything but I feel like a lot of methods are very methodical and can be programmed such as the do-calculus stuff. There is also the method mentioned in the book to verify whether a certain graph fits or not with the data. I haven't finished the book yet, but for now, from what I understand, causal inference is even harder with continuous variables.  


Do you have any specific framework, programming library, pro-tips and ways to approach problems that you would recommend?  
Is there a standard way to approach every causal inference problem?  
Any state of the art python library that is used in the field?  
Any exhaustive tutorial?  
Anything else I should be aware?  


Thanks for the help and guidance",11n1p4c,dimem16,1678391124.0,36,0.95,"['Hello, 3 books/resources come to mind, mostly in R:\n\n1) Richard McElreath: Statistical Rethinking a Bayesian Course in Stan and R (Has code translated to Python language too among others, plus online lectures)\n\n2) Nick Huntington- Klein : The Effect \n\n3) Babette A Brumback : Fundamentals of Causal Inference in R\n\nHope these help and cheers.', 'As a practitioner, this is one subject that I have strong opinions about. The one thing to be aware of is that there is no one method for causal inference that works for every problem. I will grant that Pearl\'s work on causality is an impressive intellectual edifice, but his methods just aren\'t very useful in applied, day-to-day work. Research, sure, but if you have an observational dataset you\'re not going to be breaking out the do-calculus juju or trying to fit a reasonable causal DAG from your data. (In fact, I would argue that there are vanishingly few instances where causal graph discovery is actually useful for what you want to do.)\n\nYou might want to focus on learning the bread and butter of methods for causal inference: treatment assignment mechanisms, matching, natural/quasi-experiments including DiD and RDD, and IV. Depending on the kind of work that you want to do, you can branch out from there. In terms of other starting points, I would argue that there are four ""schools"" of causal inference. Pearl\'s work falls squarely into the kind of algorithmic DAG/SCM approach that many CS researchers take to causal inference. Then there\'re the econometrics folks who take a regression-centric approach and tend to care a lot about identification arguments. Their work has more of a math stat flavor to it too. \n\nThe other two schools worth mentioning are the epi/social science, and observational study/design-based folks which overlap somewhat. Compared to the CS/econ camps, these folks do a lot more matching. A lot more. That\'s the one key difference. Beyond that, the similarities are harder to pin down. Particularly in the design-based approach, where you see a lot of fun stuff like sensitivity analyses and permutation tests based on sharp null hypotheses that you don\'t see anywhere else. Read the first two books on the below list to get a sense of both schools.\n\nIn terms of further reading, check out:\n\nHernan & Robins, *Causal Inference: What If*\n\nRosenbaum, *Design of Observational Studies*\n\nAngrist & Pischke, *Mostly Harmless Econometrics* (or *Mastering \'Metrics*. Scott Cunningham\'s Mixtape book is good as an intro as well.)\n\nImbens & Rubin, *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*', 'I recommend the crash course for causal inference on Coursera. https://coursera.org/learn/crash-course-in-causality', '>Is there a standard way to approach every causal inference problem?\n\nIt\'s all about being really familiar with the subject you\'re analysing, knowing all about the relevant science etc. Pearl and others provide tools for reasoning systematically about a set of assumptions that allow expressing causal parameters in terms of  estimands (at which point you can use statistical techniques to estimate them).\n\nSuppose you do all that and present your analysis and draw the DAG, and someone says ""so what about U. Doesn\'t that affect both Y and X""? or ""you only observe data if U, and U also affects Y"". If you can\'t justify the absence of these arrows in the DAG, no one will believe your analysis.', 'Another useful ressource to learn about the subject :\n\nhttps://matheusfacure.github.io/python-causality-handbook/', 'Thanks! I am familiar with Richard McElreath and I find him very helpful. Thanks for the suggestions', 'Pearl isnt about causal graph discovery. He admits it sucks. Its more about using a causal DAG from domain knowledge and then do calculus/identification to get the adjustment formula. Its way more generalizable. Methods like G comp used in epi causal are also related to this.', ""I'm not sure how you can dismiss Pearl while at the same time recommending Hernan & Robins. Pearl and Robins have collaborated many times over the years. Their approach is essentially the same, even if the emphasis is different (Pearl writes almost exclusively on identification, while Robins also has done much work on estimation). I agree that for most people, Hernan & Robins is a better read than Pearl's books. Nonetheless Pearl provided much of the fundament for Hernan & Robins. There are many DAGs in the book, much discussion of backdoor paths, collider bias, etc.\n\nIn fact, I would say the main difference between causal inference in epidemiology and econometrics is that the former think in terms of causal graphs, while the latter do not."", ""Thanks a lot for the exhaustive comment.  \n\n\nMay I ask some questions:  \n\\- Why is Pearl's work not often used in practice? What are the limitations?  \n\\- Could you please define some terms in the context of causal inference? matching, DiD, RDD and IV?  \n\\- What would be the most suited school of thought and book to start with given the case where only observational data was collected and we want to infer causal relationships between variables?  \n\n\nI understand that there is no free lunch, but I would highly appreciate your input on the most likely methods.  \n\n\nthanks again so much for your help"", 'Is IV really, truly a useful tool?  All the examples I see online use rainfall in a clever and cheeky way.  Hard for me to imagine what a useful IV is for business or manufacturing problems.', 'Thanks a lot, I will check it out', 'Yeah I agree, there is no free lunch. What I meant by the standard way was the way to attach the problem. For instance:  \n\\- understand the problem  \n\\- create potential dags  \n\\- test dags on data and eliminate the ones that do not fit with the data\n\n\\- etc...   \n\n\nYeah, I agree, there is no free lunch. What I meant by the standard way was the way to attach the problem. For instance:  \n\\- understand the problem  \n\\- create potential dags  \n\\- test dags on data and eliminate the ones that do not fit with the data', 'Absolutely. This and Bradley Neals lecture series on YouTube get you pretty far into grokking the basics of causal inference.', 'thank you so much', 'Hey, \n\ncould you please explain what is graph discovery?  \nAlso, could you explain why it sucks?  \n\n\nDo you mean creating the graph is always based on assumptions and these assumptions can be seen in the data but still be false?', ""Hey thanks for your comment. Could you please give me your opinion on Pearl's work?"", ""You're welcome! \n\n> Why is Pearl's work not often used in practice? What are the limitations?\n\n[This stats.SE thread](https://stats.stackexchange.com/questions/26437/criticism-of-pearls-theory-of-causality) has a lot of good discussion about critiques of Pearl's theory of causality. I would add that a major limitation is that irrespective of its utility, Pearl's framework is just hard to explain and to convince other people that what you've done is sound. If I carry out a propensity score match, I can use very simple match diagnostics to convince readers that my groups are comparable. Similarly, I can estimate a regression discontinuity and argue, again in fairly simple terms, that the two groups on either side of the discontinuity are comparable. Noah Griefer's answer [this stats.SE thread](https://stats.stackexchange.com/questions/544926/why-do-we-do-matching-for-causal-inference-vs-regressing-on-confounders) has a very good rundown of this argument in the context of matching on versus running a regression with, the relevant confounders. \n\n> The advantage matching has over regression, and the reason why I think it is so valuable and why I devoted my graduate training to understanding and improving matching and its use by applied researchers as the author of the R package cobalt, WeightIt, MatchIt, and others, is an epistemic advantage. **With matching, you can more effectively convince a reader that what you have done is trustworthy and that you have accounted for all possible objections to the observed result**, and can at least point to specific assumptions and explain how their violation might affect results. \n\n> Could you please define some terms in the context of causal inference? matching, DiD, RDD and IV?\n\nSure thing: [matching](https://cran.r-project.org/web/packages/MatchIt/vignettes/matching-methods.html) = (usually) propensity score matching; DiD = difference-in-differences; RDD = regression discontinuity design; IV = instrumental variables. These are pretty bog-standard methods that have tens of thousands of papers written about them so there's a lot you can read up on if you google. \n\n> - What would be the most suited school of thought and book to start with given the case where only observational data was collected and we want to infer causal relationships between variables?\n\nI think this depends on your background and what you want to get out of it. If you're been able to get through Pearl's book, I think Hernan & Robins would be a good next step and a nice change of pace. [The book does come with some accompanying Python code in this github repo](https://github.com/jrfiedler/causal_inference_python_code) which may be of interest but I would say it's not algorithm-oriented if that's something you're looking for."", 'Yes. [Preference-based IVs](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2719903/) are used all the time in pharma and by health insurers for RWE purposes, for example. There are also some pretty slick applications that combine IVs with matching that Ive seen used quite often. Not to mention that the IV framework also comes in very useful when analyzing broken trials or experiments with weird compliance patterns.', 'Graph discovery is an automated method that does things like AIC/BIC (score based) or chi sq (constraint based) tests for conditional independence relations between the variables and then adds edges or takes them away. Its data driven.\n\nIt sucks because its computationally intensive and most of the time the created graph is not accurate. The graph should be based on domain knowledge not data driven.\n\nWhere it can be somewhat useful is when you have some graph but dont know everything then it can suggest some possible edges and those can suggest some hypotheses for future experiments.', 'I appreciate so much your help. This is really valuable. I will start digging in what you recommended. Thanks ']"
[Q] Selecting the best experimental design for a project expansion,"I conducted n extensive research project where we designed an unbalanced factorial to evaluate the effect of 4 different factors (one continuous and 3 discrete). Now, I want to expand into a larger project, using some of the outcomes of the first phase but we are realizing that there might be more to the data we investigated. 

Traditionally we use full factorials as our experimental design but I think it is too inefficient. I'm thinking of proposing a Screening design, covering as many variables as possible, and after we have the results from that, create a new design to conduct all the experiments and generate a prediction model (we seem to be leaning towards a multivariate linear regression model for the prediction). 

With that in mind, can you help me think about potential experimental designs that might be more efficient (I'm thinking something like a Latin Hypercube but accounting for variability since it is for physical experiments).

&#x200B;

Happy to clarify more if needed. Thanks!",11n0wpa,ok_pkg,1678389334.0,1,1.0,['How many potential factors and how many levels?\n\nDo you suspect any curvature/polynomial terms in the relationships?']
[Q] How do I get the value for an 85% C.I. of a given sample if the 95% C.I. is given?,"Hey folks, I am a beginner and am studying for an upcoming exam. I stumbled upon a task which says:

given is the 95% C.I. for a sample of monthly mean wage in country x \[3122, 3856\]

**What is the upper threshhold for the 85% C.I?**

The correct answer is 3759. I have tried drawing it, cutting percentages by counting S.E. but it didnt amount to that answer. Help would be much appreciated!",11mzjaq,SmashyInc,1678386175.0,2,1.0,"['You need to know how the 95% CI was calculated. Is it based on the normal or the t distribution? Then, consider these hints:\n\n* The formula for the CIs is x  Q*SE. Q is the quantile of the normal or t distribution and you have all information you need to calculate it (or look it up in a table). SE is the standard error, which is the crucial missing information you need to calculate from the given information.\n* The CI is symmetric around the mean.', 'I got the S.E. by doing 3856-3122 = 734, then 734 : 4 = 184 (rounded)\n\nThen for x, it should be the median of the given C.I. (since its symmertrical) which is 3494 (rounded)\n\nThen I looked up the table for my quantile, which for the 85 % C.I. is 1,440\n\nThat amounts to: 3494 +- 1,44 x 184 = 3759 (rounded).\n\nThank you so much for trying to help me without giving everything away :D']"
"[Q] Linear Regression - Insignificant variables, what to do?","Hi all, I am running a Linear Regression for my analysis and I have some insignificant variables, and I am not sure which model should be my final model. I can think of 2 options:

1) Leave the model as is, but report the insignificant variables

2) Run the model model, but remove the insignificant variables.

What is the correct way?",11mzecu,No_Canary_5299,1678385864.0,6,0.81,"[""There isn't one correct way and what you should do will depend on your purpose. \n\nFor instance, if you are looking to publish the model in an academic journal and the variables are important control variables, then you might want to leave them in your model for the edification of readers. \n\nAlternatively, if you are planning to implement the model for regular use, then removing the variables may provide a significant improvement on resource expenditure, e.g. computing time, memory."", 'Why would you remove variables because their coefficient has a higher p value than some arbitrary threshold?', ""What is the purpose of your regression?\n\nAre you trying to make a predictive model or determine causality?\n\nIf you are making a predictive model, go with the one that fits the data the best. You could use a technique such as AIC which will tell you how well the model fits the data. You would compare models with combinations of your different predictors and find the one with the best fit.\n\nIf you are trying to determine causality it's a bit more complicated. You need to be much more careful about your parameter selection, ensuring you do not have conflicting parameters that will bias your estimations AND ensure you are using 'controls' properly (avoiding over-control bias). You need to draw out how you think the system works and generate your model based on that, it may require separate models for your various predictors of interest. [Here's a short-ish piece that might help.](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1554)"", 'You shouldnt remove terms post hoc based on significance', ""Removing insignificant predictors from a linear model is essentially data dredging, so unless you know how to correct your p-values for multiple comparison, do not.  \nHere's an interesting option: you could run Bayesian linear regression in jamovi with all the predictors you suspect to influence your outcome variable and report posterior probabilites of all your models. You then pick the model with the highest probability  \n\n\nHere is an [example](https://imgur.com/cVGhkpw)"", 'I suggest that you keep the whole model and include all variables. If you remove insignificant variables your adjusted R^ will be seriously biased.', 'Depends on your original hypothesis. Are you attempting to find a model which most accurately predicts your response variable? Use backwards selection. Are you attempting to construct a model to find out what is/isnt a significant predictor of your response variable? Keep them all in. What I wouldnt recommend doing is p-hacking which is searching for the best combination of predictors to estimate your response variable AFTER the model has already been decided. \n\nAlso - Im still fairly new to the field of statistics (grad student) so if any more experienced individuals finds errors in my logic, please point it out as Id be happy to correct it and learn.', '>  for my analysis\n\nWhat analysis is that? What are you using the regression to do, exactly? Just some hypothesis test? Prediction? something else?\n\nI would generally recommend 1. over 2.', ""1. If the two models (with and without the insignificant variables) are similar in terms of their main effects, choose one. It doesn't really matter.  You could also report the one with the insignificant variables, but use a footnote to quickly list the controls you added and that they weren't significant.\n2. If the two models do not have similar main effects, then you have more digging to do. There's often no clear solution in this case, although understanding more about causal effects will help you. Of course, understanding causality is a never ending task. You absolutely should (but many people don't) report that your results are not robust to the inclusion/exclusion of additional variables."", 'Simple things you can do: \n\n\\- Compare the two models for R\\^2, AIC .\n\n\\- Compare the models for over-fitting on CV errors. \n\n\\- Think about the question. Do these insignificant variables help with interpreting the model? Does transforming variables change the situation? \n\nJust looking at p value is not a good idea. \n\nFancy things you can do: \n\n\\- Subset selection. Start from ALL the variables. Exclude subsets of variables and see how your model changes.  Use AIC /adjusted R\\^2  as a guide to select a model. (This can be computationally expensive). \n\n\\- Apply Ridge Regression.', 'Backward selection and other stepwise methods are biased, better to use regularization']"
[q] is there an R package where I can perform (group) lasso to fit a conditional logistic regression ?,"I am using package clogitL1 but it doesnt allow for group lasso. Is there a way around or accommodate this?

Is there a way to manipulate glmnet for conditional/group using family=cox (stretch)

Thanks.",11mwzbn,maka2250,1678380265.0,1,1.0,"[""Interesting question, there's a plethora of packages out there but it was surprisingly hard to find one for doing this. \n\n&#x200B;\n\nIt looks like the grpreg package should be usable to do what you want here. It directly supports group lasso for the cox model, so you should be able to use the cox equivalent of your conditional logistic model."", 'What do you mean by ""group""? As long as you\'re referring to regular old conditional logistic regression used when the number of events is fixed before the study, e.g. most common use is case control studies, then use glimnet with the ""cox"" family option, with common follow-up time (set to 1, it doesn\'t matter). For example, suppose the data frame, DAT, contains variables D (a 0/1 response) and a bunch of named covariate columns. Then\n\n    library(glmnet)\n    D <- DAT$D\n    X <- DAT[, names(DAT)!=""D""]\n    Ti <- rep(1, nrow(DAT))\n\n    fit <- glmnet(x=X, y=cbind(Ti, D), family=""cox"")\n    tune <- cv.glmnet(x=X, y=cbind(Ti, D), family=""cox"")\n\n    coefficients(fit, s=tune$lambda.min)', 'Can u explain why logistic regression', 'Super big thank you for recommending this, I am going to check it. If I have any questions about the cox manipulation, would you mind if I reach out', 'This will be a bit ugly butGroup lasso vs lasso refers to how coefficients are reduced to zero ie variables dropped. In lasso all variables are treated Individually where as in group lasso you can treat a series of variables as a group, either all variables are included or excluded. Its important for certain types of responses/questions. \n\nhttps://www.r-bloggers.com/2021/10/exclusive-lasso-and-group-lasso-using-r-code/amp/', 'Seems like a very odd/intro level question to ask. But because my outcome is binary.', 'That would be totally fine!']"
[Q] Question about sample size calculation," I'm looking at a long term follow-up project and in the sample size consideration section it is described as such:

The total person-years of follow-up is approximately 9045 for the 1500 enrolled patients. This number of person-years of follow-up will provide 91% likelihood of seeing at least one event of interest, if the true rate per 15 years of exposure is at least 1:250.

I don't understand how the 91% likelihood is calculated. Can someone please explain?",11mvolx,Beaverine,1678377200.0,2,1.0,"['Most likely, assuming a constant event rate. Then the probability of observing 1 or more events is \n\n    1-exp(-h PY) =1-exp(-(1/(250*15))*9045)\n                 =0.9103642\n\n\nwhere h is the annual event rate and PY is the total person years.', ""Thank you, this is very helpful.  I didn't thought about using the survival and cumulative hazard function to solve this."", 'I know what you mean. We usually use counting process data to estimate population level functions, like the hazard or survival function corresponding to a single person. But poison processes are infinitely divisible, which means the aggregate is a poison process with a cumulative rate equal to the individual annualized hazard rate the times person years on study. The probability of 1+ events is the complement of no events, which is the probability written above.']"
[Q] Need some help with premium statista reports for a thesis!," 

Hello! I'm trying to get some data about digital marketing, specially in Spain, but the premium account is quite expensive for me.

I need these reports for now, wondering if someone can DM and send me the pdf's through email. Thank you!

These are some examples i need.

[https://www.statista.com/statistics/237974/online-advertising-spending-worldwide/](https://www.statista.com/statistics/237974/online-advertising-spending-worldwide/)

[https://www.statista.com/statistics/245585/distribution-of-the-global-digital-advertising-spending-by-region/](https://www.statista.com/statistics/245585/distribution-of-the-global-digital-advertising-spending-by-region/)

[https://www.statista.com/forecasts/1271893/spain-digital-advertising-spending-distribution-industry](https://www.statista.com/forecasts/1271893/spain-digital-advertising-spending-distribution-industry)

[https://www.statista.com/statistics/307005/europe-online-ad-spend/](https://www.statista.com/statistics/307005/europe-online-ad-spend/)

I'd be very greatful if someone could help me out.",11mu01m,MCMadeti,1678373101.0,0,0.5,[]
[Q] How to report my linear regression model with 50 dummy variables?,"Hi all, I am doing a thesis on my dataset using Linear Regression. There are a total of 50 dummy variables of 3 variables. How do I report my final regression model? It looks funny if the equation is half a page long.",11mq7yw,No_Canary_5299,1678362675.0,38,0.93,"[""Sounds like you're trying to apply linear regression to categorical variables.\n\n[Try something like this.](https://i.imgur.com/VMACh1c.png)"", 'Are you able to pare down what you are reporting by simply presenting the type 3 tests for your variables?', 'Simple answer: only report the main coefficients youre interested in in the table in the main body. You can put the full table in an appendix.', 'Are you sure this is not overparametrised? Or are those FE dummies?', 'One way to report your final regression model with 50 dummy variables is to provide a summary table of the coefficients for each variable, as well as the overall model statistics (R-squared, F-statistic, etc.), rather than writing out the full equation. You could also consider using a regression tree or other visualization techniques to display the relationships between the variables in a more concise way.', 'You cant report on a model with 50 dummy variables, thats way too many to interpret. \n\nYou probably need a different model. \n\nHow did you end up with those dummy variables? Im happy to help if I can, and others here probably too. \n\nJust dont try to report that model with 50 dummy variables.', 'Wow, bro! Off-topic but what is the sample number?', 'Good idea, OP  could specify their variables as factors.', '*pare (cut) down', 'Has anyone on Reddit already found out u are using chatgpt to answer? Nice social experiment btw', 'Thanks! Do you have any idea how to export the lm summary from R in a way that will be presentable in a thesis?', 'The dummy variables are for town names and types of houses.', 'Yeah, agreed. Something is up here. OP needs to do some factor reduction.', ""As soon as I read that first sentence, I could tell it was a ChatGPT answer. It's such a distinct style of of writing."", ""There's a package called broom which should help"", 'Check out the `{gtsummary}` package', ""There's a package called stargazer."", 'Im surprised that nobody has mentioned that, but you probably want mixed effects modeling (unless you are really specifically interested in the main effects for each individual town). Town name should be a random effect.', 'Is it possible to form groups based on your research question (ie. Instead of town names, rural vs. urban or some sort of population grouping)', 'You need linear mixed effects modeling then. It is linear modeling specifically designed for your case. \n\nDoing linear regression on your dataset as it is will give you nonsense results that cant be interpreted. Either pool over towns and/or houses, or use linear mixed effects modeling.', 'Now that Ive used it a couple times, they always start off as if its an essay.', 'This is perfect, thank you.', 'This', 'I would very much prefer it to remain as town names. Is this not viable?', ""While you're at it, you may want to check this https://www.reddit.com/r/Rlanguage/comments/11horrf/in_a_regression_summary_table_generated_by_tbl/"", ""If your individual samples for each town are of sufficent size, it's viable in theory.\n\nIn practice, you can't make any substantial interpretations based on 50 dummies for 50 different towns. What information is I supposed to take away from such a study?"", 'Very helpful! Another way I found out is to use the JAMO theme (but it replaces comma with to)\n\n    theme_gtsummary_journal(journal = ""jama"")', 'How the price differs for each town', ""I'll have to check that out."", ""Theoretically, you'd need at least hundreds of each house type, in each town, for your final estimates to be reliable. \n\nAn easier way than using 50 towns would be to use factors about each town, for example school performance rankings, that research has consistently shown have a significant influence on house price and can be a proxy for the town itself. \n\nOther possible variables would be either homeownership rates or vacancy rates for the locale of each house. Those would\n\nThis way, you can replace your 50 variables with one or two and increase the power of your model. It should also decrease the likelihood of Type I or Type II errors."", ""Is that your primary exposure of interest or is it type of house? If it's type of house, you could have towns as a random effect (again, this is all assuming appropriate sample size)"", 'I am seeing how price of houses differs for the different towns. I suppose I can remove the type of house variables. \n\nShould I do the same if I am using the model for prediction instead of statistical inference?', 'A better way to model...(may not be possible) would be determined the characteristics that relate to property value like median income schools etc. And then cluster the towns together based on them. Take the error range and based on the knowledge of the underlying stats rank them afterwords with appropriate percentile assignments', 'What is your actual research question/hypothesis? Do you actually care about each individual/specific town? If not you might find mixed effects models to useful.', ""This is a good point. If it's a predictive model, you may be more justified in keeping all towns.""]"
Struggling with % [Q],"Losing my mind over this basic sum of percentages that I am using to calculate arbitrage bets: there are 3 odds of the same match (1.91 + 16.50 + 10.50), their % sum is ~68,2%, so the profit amounts to ~31,8%. However, when I calculate how much (total) money should I put on each odd  to get 131,8 as a secure profit (should be 100) using this calculation ( image under this post  ) but instead  it gives me ~89,5 and not 100 as it should. What am I doing wrong?",11mpr5t,ThatsKori,1678361093.0,0,0.5,['[calculation used](https://ibb.co/XymGBDn)']
[Question] Why is correlation calculated using percentage change so different than using absolute values? What is the correct approach,"Suppose I have two variable (both in millions) and I want to see correlation between the two. I have two approaches and both give different results. What is the ""correct"" way to approach this problem and why do the two correlations differ?

Method 1: Calculate correlation between A and B using percentage change. (where percentage change shows in % how the value of A and B changed from the last quarter)

Method 2: Calculate correlation between A and B using absolute numbers.

The first method gives 0.25 (low) and the second method i.e method 2 gives 0.65 (much stronger). What is the correct way to go about calculating correlation between A and B in this case?

Sample Numbers below (Table in Method 1 is calculated from table in Method 2):

For Method 1:

&#x200B;

|a (percentage\_change)|b (percentage\_change)|
|:-|:-|
|59.10%|184%|
|53.7%|88.5%|
|\-10.8%|\-19.2%|
|\-1.1%|\-1.9%|
|0.2%|77.8%|
|150.4%|0%|
|\-35.6%|\-25%|

&#x200B;

For Method 2:

&#x200B;

|a (real value)|b (real value)|
|:-|:-|
|0.265133|301|
|0.407535|567|
|0.363445|459|
|0.359494|450|
|0.358887|800|
|0.898735|800|
|0.578721|600|",11mo7mc,Past-Ad8219,1678355370.0,2,1.0,"[""The denominator of Pearson's correlation includes standard deviation terms. When you converted your values to percentages the original scale of the data is lost, and hence the standard devs are different, because this is not a linear transformation."", ""Just go with good old Pearson's r. This uses the actual numbers from the data (and their means):\n\nr = [sum(xi - xmu) (yi - ymu)] / {sqrt[sum(xi - xmu)^2 sum(yi - ymu)^2]}\n\nSorry for the crappy formatting but just Google it. And sorry if this isn't at all what you're asking."", 'There isn\'t a single right answer; it depends on the circumstances, but ""neither of the above"" is a distinct third possibility\n\nI\'ll stick to a general comment: If your data are time series, it might be worth understanding the issues with Pearson correlation and time series; specifically you should be aware of \n\n1. spurious relationships between unrelated series if the series are non-stationary (indeed even stationarity is not of itself sufficient to avoid spurious correlation).  \n\n  For example, take two fair coins and toss them for many trials, and keep track of the number of heads for each after each trial. Clearly the coins are independent of each other, but as the number of tosses grows large the expected value of the absolute correlation between them grows high. This is spurious correlation; there\'s literally no connection but it looks like there is because the nonstationarity changes the behavior of the correlation coefficient from the behavior under the usual assumptions.\n\n2. in the case where the series are cointegrated, the considerations about suitable ways to model the relationship will change.', "">Thank you! This makes sense to me. One question though, the deviations are in the numerator as well right? My intuitive understanding is that we don't do percentage change because the deviations of the data change overall (might increase or decrease) and in a way that doesnt capture in the original distribution. Would that be correct to say?"", '> Its correct to use the original data,\n\nNo, theres no correct way to measure correlation, and there are some very likely scenarios where using the original data definitely isnt the best choice.\n\nConsidering that OP is talking about percentage change from one quarter to the other, its reasonable to assume that there is autocorrelation,  and computing a correlation with raw values would be questionable.\n\nUsing percentage changes may not be the best solution, but the standard deviations are not wrong just because there is a nonlinear transformation. Theyre just different.', 'Yes they are in the numerator as well - for whatever reason the terms in the denominator came to mind first. As the other commenter mentioned, the original data does likely have autocorrelation so this transformation may help to shed that - so ""not capturing"" that aspect can be beneficial for inference.']"
"[Q] Which statistical analysis would be appropriate for the project: reviewing different medications, their magnitudes and determining the link with disease outcome (not quite a chi-squared test)","Hi all, I'm trying to put together a retrospective research protocol at the moment and thinking ahead to the statistical analysis I will perform as I have to submit for ethics approval. i wont name actual medication and conditions but try to explain my dilemma as simply as I can.

&#x200B;

Data I will be collecting:

* Medications given to patients (A, B, C D... etc)
   * Patient 1 may be given A, B and C at different dosage magnitudes 10mg, 25mg and 50mg whereas patient 2 is given C, D and E at 30mg, 20mg and 10mg
   * Repeating this for all the patients in the cohort
* Each patient will then either develop disease X or not develop disease X - this is the outcome of interest.

Therefore, I am looking for a statistical analysis that can analyse the outcome of disease X, compared to being given the different medications and the magnitudes at which they are given. If I was just comparing Administration of Drug A/not given drug A and developing disease X and not developing disease X, I could use a chi-squared or t-test but when it comes to multiple drugs and involving a magnitude I am stuck.

If I could get up to that, that would be great but I also have one step further I would like to take it: grouping the medication administration into before hospital, at hospital before surgery and at hospital after surgery.

&#x200B;

Hopefully this is enough information, if I can clarify anything further please let me know and thanks in advance for any tips",11mndmr,Agent-MJae,1678352140.0,0,0.4,"['This is clearly a causal probelm and requires very careful considerations. The setting is not trivial at all, especially if the switches are not randomized. If they are you can ""more easily"" use Dynamical treatment models to estimate the outcome of interest.']"
[Q] How to calculate regression inferences without sample population dataset?,"So all of the tools my stats I and II class have presented me allow me to calculate regression and inference with sample population data to input. However, for many problems I do not have a dataset, but rather just my regression equation, along with my significance level and standard error of the slope.

Short of doing the work long-hand, is there a tool that allows me to input this info to calculate other statistics?",11mlwz5,TheCrystalFawn91,1678346688.0,7,0.82,"['Your use of the terminology is confusing which is why you arent getting good answers. The phrase ..to calculate regression and inference.. doesnt mean anything. We will have to extrapolate what you wrote to figure out your question. My guess is that your provided regression equation includes the estimates for the intercept and slope. If you also have the standard errors for these estimates, you can perform inference for the estimates related population parameters, often represented by beta_0 and beta_1.\n\nThat is to say, you dont need the data to do these problems because the statistics have already been calculated from the data for you. As far as calculating other statistics without any of the data, you probably cant in general. What other statistics did you have in mind?', 'What exactly are you trying to calculate?', 'Hello. You could generate data under certain assumptions in R or Python if you have your parameters set. This data could then be used for other calculations.', 'I guess you are looking for contrasts?', 'You need to use the  formulas!', 'I think they\'re trying to get you to know the algebra inside and out of how these final statistical outputs line up vs. the summary statistics you\'re describing.\n\nSo ""doing the work longhand"" is kind of the point, I would assume.', ""> Short of doing the work long-hand\n\nPretty sure this is what your prof wants you to do.  He or she is probably trying to help you understand the relationships between these concepts by making you use your mark-1 brain and mark-1 hands to calculate a few of them.\n\nWhen you're done, you shouldn't end up mystified by a request to put together a confidence interval from information including a regression coefficient, its standard error, and the sample size.  This should eventually become second nature."", ""I apologize, it was late last night for me.\n\nI know my terminology is probably a nightmare to wade through, and I'm understanding of the fact that statistics is an incredibly complex subject, and still in my arm floaties trying to learn how to doggy paddle. Haha\n\nBut yes, I think you did extrapolate what I was I was trying to get at.\n\nIn our current problems we have a regression equation based on theoretical sample data (which we are not given, just the equation for intercept and slope) along with the standard error, and a significance level (alpha?).\n\nFrom that, I can work it all out in long form, but we are encouraged to find tools to make the process more efficient. We are primarily given examples using the Rossman Chance applets, but regression inferences require a sample population dataset to then give you the equation, standard error, etc, but no where to use that as an input. I'm hoping to find something similar to Rossman Chance that has that as an option.\n\nIn this case, I'm trying to find p-values, test statistics, critical t scores, confidence intervals, and I'm sure we'll be looking at ANOVA before the quarter is out."", ""P-values, confidence intervals, test statistics, basics 200 level stats stuff I think.\n\nWe're pretty much doing hypothesis testing and confidence intervals."", 'Refer to one of my replies. We already use formulas, but are encouraged to find tools to make the process more efficient.', 'Refer to one of my replies. We already use formulas, but are encouraged to find tools to make the process more efficient.', ""I can do all the work by hand, but he actually encourages us to learn the tools available to us, and not just grind formulas. Once we know the formulas and how they work, he wants us to find the tools to make the work quicker/more efficient so we have layer on more complicated topics quickly. Honestly this is hands down one of my favorite classes I've ever taken because his approach.\n\nEssentially the primary tools he focuses on is Excel and the  Rossman Chance Applets. The applets are fantastic, but for regression (and multiple mean inference) require a sample population dataset. I am pretty exceptional with excel, and have built all my own calculators on it for the most part, but it does take time to really fine tune it, and having an external source to be able to check my excel sheets against is exceptionally useful in that development.\n\nMostly just hoping to find something like Rossman Chance, but a little different I suppose?"", 'You can program your formulas using R or Excel or Python', 'sounds like you could just make a set of ""fill in the blanks"" tables in excel where there\'s one tale for each possible unknown parameter.\n\nIf you know how the formulas are structured you could build the formulas such that you can populate each table with the known inputs and spit out the unknown input into the appropriate cell.']"
"[Q] When do we say that the difference is significant when the computed p-value is less than or EQUAL to the critical p-value, but for the computed test statistic, it's JUST if it is greater than the critical test statistic?","I've searched all day and couldn't find a single source that says that if your computed test statistic is equal to the critical value, then the difference is significant. Is there a reason for this?

EDIT:

 I think I failed to make my point. When it comes to test statistics, there is usually just:

""If a < b, reject; if a > b, accept"", with no mention of the equal situation.

But when it comes to p values there would usually be:

""if a  b, reject; if a > b, accept"".",11md5vz,TaylorBrow,1678321225.0,1,0.6,"['1. Beware, not all rejection rules are ""greater"". e.g. see [1]\n\n2. By the most common convention the critical value is *within* the critical region (and so equality is part of the rejection rule), though this is only consequential for discrete statistics. (also see [1]). For a conservative test you then choose that critical value so that the probability of rejecting under H0 is as large as possible without exceeding alpha. \n\n  You can define your critical values as just outside instead if you wish, but you must clearly indicate this since people will tend to expect otherwise; the required property for a conservative test is still that you must keep the rejection rule such that you don\'t exceed rejecting H0 at greater rate than alpha when it\'s true. Doing it this way has the slightly weird property that there\'s an interval of values of the statistic, just inside the boundary, that you say are in the rejection region but which never occur. \n\n3. The reason ;p-value exactly equal to alpha\' should be in the rejection region follows directly from the same convention; you want to reject as often as you can (for more power) while not exceeding a rejection rate of alpha when H0 is true. This implies rejecting p=alpha. If you stick to defining p in terms of the smallest significance level at which you\'d still reject, everything corresponds as it should between conventions.\n\n---\n\n[1]: https://math.usask.ca/~laverty/S245/Tables/wmw.pdf\n\nScroll to the bottom where it says: ""If Uobt  Ucrit, reject H0""\n\nThis illustrates two of my points; first that some rejection rules are ""<"" not "">"", and that with discrete statistics, the common convention is that the quoted critical values are part of the rejection region.\n\nMany more examples may be found', ""Why are you asking?  It doesn't matter in practice.  The chance that the test statistic is exactly equal to critical value happens so rarely that you don't need to consider this boundary case."", ""Never happened and never will, sleep relaxed. Anyway put the = where you want it's exactly the same"", 'Thanks. But I think I failed to make my point. When it comes to test statistics, there is usually just:\n\n""If a < b, reject; if a > b, accept"", with no mention of the equal situation.\n\nBut when it comes to p values there would usually be:\n\n""if a  b, reject; if a > b, accept"".', 'In my earlier comment I addressed where the ""="" case should go in detail. \n\nNaturally with actually continuous test statistics, exact equality should have probability 0, so it shouldn\'t matter very much. \n\nWhen it\'s not continuous, you can always work out what the rejection rate would be (under H0) when the equality is in the rejection region and when it\'s outside it, (at least by simulation, if its not feasible algebraically; just simulate the proportion of rejections under H0 for each scheme).']"
[Question] Same inter-quartile ranges of two unpaired groups but significant p-value?,"Hello! I am comparing two unpaired groups and found them to have the same inter-quartile range but when I ran a Mann-Whitney U test the p-value was significant. Why is this? If they have similar distribution based on the IQR why would the p-value suggest otherwise? Sorry if I'm not being clear enough!

Thank you!",11m8i7m,YoungYeungYung,1678310348.0,9,0.91,"[""1. The Wilcoxon- Mann-Whitney doesn't compare interquartile ranges. They might be similar while the thing the statistic measures is fairly different\n\n2. With large sample sizes, even very similar looking distributions can nevertheless be  different enough that it can't be explained as random samples from identical population distributions"", ""Weird. It won't let me edit to fix whatever is causing the two 1s"", 'Thank you so much for the explanation! \n\nIs there a test that would be useful to compare medians? So even though these two groups may have a similar distribution in-between the 2nd and 3rd quartiles, it is possible the rest of the data distribution is what causes the significant p-value?', ""There are some tests that can compare medians, but in many situations the power is typically low. \n\n1. What was the original question of interest here (was it originally *specifically* about medians? Or did you decide to think about medians *after seeing the data*?) \n\n2. What is the variable you're comparing here?""]"
[question] Multiple partial correlation coefficient for linear regression model?,"I am working on my Master's thesis and currently trying to perform a post-hoc Power analysis for my (full) linear regression model in SPSS. I have to enter a multiple partial correlation coefficient as the effect size, but I'm not exactly sure what this means/ how to obtain that value (is it the same as r\^2, adjusted r\^2, f\^2, or). Can someone please provide an explanation?

I know g\*power might be a better option, since it works with f\^2/r\^2, but I am unable to do so because it is not compatible with the screen reader software I am dependent on.",11m4o5j,Alicia-Emily,1678301727.0,2,0.75,[]
[Q] Power calculation for novel topic,"Hello everyone,

I need a power calculation for my research. I have already found an example from UCLA but there (partial) R squared was already known from previous research. My topic, however, is a quite novel one and doesn't offer so many numbers.

I have two continuous predictors and one dependent variable (at first we planned three IVs yet we decided to do an exploratory analysis).

I used g\*power with:

F tests - LMR; Fixed model, r squared increase

f square: .15

alpha: .05

Power: .95

Two tested predictors

Total number of predictors: 2

If I select determine, what do I have to fill in? How do I know my variance or partial r squared without previous numbers? Is there a default I have to use?

Sorry if my question may sound stupid, I am still new to this field.

Thanks in advance.",11m3yhr,Fl1p1,1678300096.0,1,0.66,"['If you don\'t have previous results on which to base your required effect size, you can justify some number based on what you think would be reasonable.\n\nSince you\'re doing ""exploratory"" analysis, this approach is fine. Looking for a ""medium"" f-squared of 0.15 should give you a ballpark number. Modify that with pragmatic concerns like how much time you have or how much data collection costs.', 'You make an educated guess now, e.g. based on the most similar type of study you can find, and then next time your current study is the literature you or others can based their power calculation on :)', 'You might start by considering what the smallest effects are that could be practically relevant across a variety of possible (but not impractical) situations and see what effect sizes they correspond to. That would give you a ballpark on a small effect. Then you might consider the other end - what would be the least effect that should still ""hit people between the eyes, a major impact"" (across a number of practical  scenarios), and that then would give a sense of a large effect size. That gives you a basis then to select what the smallest effects are that you want to be pretty sure to be able to pick up which presumably will lay somewhere along tahat scale between the small and large that you identify']"
[E] What would be the repercussions of dropping my Theoretical Math Course?,"Hello, Masters' Biostats/Bioinformatics student here again. Brief Re-cap: I was a Biology Undergrad. Worked for some time, and came back for an Masters' in Biostats/Bioinformatics (joint program at my school).

1st semester of MS program was s\*\*\*, but I bounced back after that. I did well in in the 2nd semester, and last semester I got around a 3.6 GPA.

Current Situation:

I'm taking 3 Graduate Stats Courses and 1 Intro to Theoretical Math Course (Undergrad Level) and trying to complete my Graduate Project.

Long story short....the Theoretical Math Course is sucking up almost all of my time. Breakdown of weekly work:

1. 12-15 proofs/week (10 assigned for HW + 2-5 supplementary for quiz prep)
2. Quiz on same day that homework is due
3. Rinse & Repeat...

My university has 2 campuses. My Theoretical Math Course is later on one campus, and the Grad Stats Courses are all on the other one. I only have a 10 min. window between my Math Theory class and my Stats Class, and by design end up getting there 8-10 mins late (talked about this w/ my prof already).

I'm thinking of dropping my Theoretical Math Course, but at the same time PhD programs I've spoken to require some solid Theoretical Math background, and I feel I'd just be pushing things out further and further. I'm already in my mid 20s...

What would be the repercussions of dropping and having a 'Resign' on my transcript?",11m1xrj,Yakima42,1678295515.0,16,0.86,"['What is ""theoretical math""? If it is real analysis then most statistics PhD programs require you to have analysis done. Biostats PhD at lower ranked schools are generally more applied and may not required real analysis. All (bio)stats programs will require multivariable calculus and linear algebra. The good programs want as much math background as possible. So much so that any statistics background seems largely optional if you have the math background.\n\nThough you said your program is biostats/bioinformatics... So if you are looking for bioinformatics PhD then real analysis is not needed. They would prefer more CS background as many departments list that as one of the biggest hurdles bioinformatics PhD students face.', ""OP, I've been in your shoes, and if I could give my past self advice it is this: the course won't matter at all if you can't have the energy to finish it, and if it ruins your ability to learn in your other classes then it is a net drain on your education.\n\nIf you find that you can't really handle the proofs clas right before stats, then drop it.  Take it slower. Complete your 2 year degree in 2.5 years or 3 years.  Because if the consequence of taking that class right now is your burn out, then you won't be able to get anything done.  Put yourself first.\n\nOnce you decide to put yourself first, then decide if you should take it later, take a different course, or eliminate it completely."", ""To me, it doesn't sound crazy to drop it. You have a lot on your plate right now even without that course."", '> Im already in my mid 20s\n\nI graduated from college in my late 20s. You are still young and a year here or there will be easily made up. Your career wont get off track. \n\nLike another poster said, if its taking too much time, its a drain on your whole education. A single withdrawal isnt going to hurt you, especially if you can take the class or its equivalent after you graduate. Put off applying to a PhD for a year or two and go work while taking a couple theoretical math classes on the side. Youll be better off for it', ""My advice is to either soldier on and finish it now, or drop it with the intention of taking it later. \n\nThe main thing is to get some practice with figuring stuff out for yourself. This is a skill which is going to be useful over and over in every context.\n\nBy the way, whether you dropped this class and took it later, or how old you are, are details that are unlikely to show up on anyone's radar. Even your GPA is likely to be unexamined if it's greater than the threshold."", '>What is ""theoretical math""? If it is real analysis then most statistics PhD programs require you to have analysis done. Biostats PhD at lower ranked schools are generally more applied and may not required real analysis. All (bio)stats programs will require multivariable calculus and linear algebra. The good programs want as much math background as possible. So much so that any statistics background seems largely optional if you have the math background.\n\nIt\'s not real analysis. Just teaches the different types of proofs, logic, and basically a ""how to"" class on proofs.', ""I would say that is important if you are set on (bio)stats PhD. You'll need to show you have at least the ability to do proof based math (e.g. analysis). As I describe in previous comment this might not be as strict of a requirement for less methodologically driven biostats programs."", ""Gotcha. Do you think it's better I drop now and take it later down the road? Or is this level of pressure/stress necessary to prep for a PhD level rigor?"", ""I don't have enough knowledge of your background, goals, etc to give more in depth advice. I would consider deeply if you need a PhD for your career goals or if Masters would suffice. Talk with your advisor(s) and people that are in positions that you want to be in. I am starting a biostats PhD next fall and work with mostly bioinformaticians PhD students and staff in a lab. I personally enjoy the mathematics and think you need to at least be able to tolerate it for a PhD. The bioinformatics PhD students often have more bio background, computer science, etc. I don't think a single one of my lab mates took a course like the one you described besides the (bio)stat PhDs. \n\nStatistical methodology is going to be much more math heavy than the course you described."", ""Gotcha. Thank you for the advice. I do like the math...but at the same time I feel like I just don't have any time to properly master the concepts and do well. I don't even feel it's a time management thing, b/c I do my best to start assignments the day they are assigned, and start learning concepts the day we cover them in class w/o break. The concepts however are just very dense, and take time.""]"
[Question] Calculating probability of NOT being selected in a small lottery (fantasy football),"Im trying to adjust our lottery system for fantasy football. There are four participants in the draft lottery. 

8th place gets N8 balls in the hat
7th place gets N7 balls in the hat
6th place gets N6 balls in the hat
5th place gets N5 balls in the hat

When your ball is selected, all of your balls are removed. You can only receive one position.

Currently, we draw for the best position first, so you want your name selected. As such, 
N8 is the highest at 4 balls, N7=3, N6=2, and N5=1 (lowest).

What Im proposing, to build suspense, is that we draw for the worst position first, and build up to the best. We would adjust how many balls in the hat each manager would get. As such, N8 is the lowest and N5 is the highest. You DONT want your name selected. Im calling this a survival lottery but itd love to know the actual name. 

How can I calculate this? Itd like to continue to use variables to adjust the number of balls in the hat.

Edit1&2: Format",11lzdha,Nsmith1881,1678289666.0,0,0.5,[]
Z/Z' factor and high-throughput drug discovery [Question],"Howdy all, first time visitor to this community, looking for some stats help with a specific application. I work in high-throughput drug discovery where researchers will routinely test millions of compounds in some biological assay, trying to discover some beneficial activity.

There are statistical measures to determine the reliability of these assays - you're testing millions of drugs, you expect the VAST majority of them not to be active, and you can only screen once. So a reliable assay is a necessity. One of the most widely used parameters is something called a Z factor or Z' factor, which is not the same as Z score (publication describing this below).

My question is this - other than 'this is good enough' what else can a Z factor tell me about my assay?

In a sentence, how would you describe what the Z factor actually means?  
Can I use the Z factor to determine what my false positive rate is likely to be for a given sample size and a given number of replicates?  
Can I use Z factor to to determine how many replicates of each drug I aught to use? If not, any ideas on how I'd go about that?

Thanks!

[https://pubmed.ncbi.nlm.nih.gov/10838414/](https://pubmed.ncbi.nlm.nih.gov/10838414/)",11lxufj,Ro1t,1678285918.0,17,0.95,"['First time here too. At a glance, it seems that Z can only be interpreted indirectly, through a transformation W = (1 - Z) / 3. Assuming that signal values for positive and negative controls are independent and normally distributed, with ""positive signals"" coming from one normal distribution and ""negative signals"" from another, W is approximately normal\\*, characterizing the ability of the assay to discern positive from negative. If the positive and negative distributions had the same standard deviation (not necessarily assumed, but maybe reasonable), then W is half the number of standard deviations separating the means of the two distributions.\n\nAdmittedly this is pretty unsatisfactory, because an interpretation of Z only comes through W. For example, Z > 1 is equivalent to W > 6. [Wikipedia](https://en.wikipedia.org/wiki/Z-factor) calls such an assay ""excellent"", which seems reasonable enough if 6\\*2 =12 standard deviations of separation is ""excellent"" (though such terms are basically arbitrary). But even this interpretation is not precisely correct, unless the two distributions have equal standard deviation. And then the asterisk:\n\n\\*W is asymptotically absolute value of a normal distribution under the stated assumptions, but the scale is unusual because Z involves the sum of standard deviations. If I were allowed to define Z, I\'d scale by the square root of the sum of their squares instead, in order to make W *standard* normal. So Z seems largely heuristic, without a precise interpretation.\n\nEdits: Corrected my bad arithmetic and some statements following from it.']"
Between groups design [Q],"I am doing a report which is a between groups design (2 groups) and testing 2 dependent variables. When doing histograms/q-q plots , do I need to do a separate one for each group and each dependent variable, or can I do both groups together and one dependent variable. Thank you so much in advance

Edit: I will gift anyone that helps 
Please I am desperate",11lwb0g,celadonium,1678281804.0,1,1.0,"['[deleted]', 'Thank you so so so much. Do you reccomend I add all the figures in my results section, or just my appendices']"
[Question] Graphing moderation analysis,"Hello, stats wizards!

Bear with me here, Im very new to this-

I ran a moderation analysis with three IV, one being the interaction between one of the IVs and the DV. The analysis came back with a huge effect of the interaction, so now I need to unravel it. I need to graph it that shows the different levels of the IV towards the DV for high and low on the moderator, but Im not sure how to do that. If it helps, I can use Excel, Google Sheets, SPSS, or PSPP for graphing. 

Any guidance greatly appreciated!",11lvmk4,deplorable_word,1678279997.0,1,1.0,"[""Is the moderator continuous, or categorical? It's unclear from the post."", 'Oh sorry, my mistake. Its continuous', ""In that case there is zero point in testing/graphing the effect of the IV at arbitrary levels. Why not graph Johnson-Neyman intervals instead? It's much more informative and actually in line with the original research design.""]"
[Q] An alternative non parameteric test for Mann whiteny test,"In the field of medical research, if the data is not normally distributed, they jump and do the Mann whiteny test. However, I don't think that Mann whiteny test is testing the difference in the median like some sources say. Is there a non parameteric better option better than Mann whiteny test?",11lvbvt,Rosehus12,1678279159.0,5,1.0,"[""It's not testing the difference in the medians. Who says it is? What is the null hypothesis you want to test?"", 'You could do a randomization test of the difference between means (Mann-Whitney is just a rank randomization test). [Here](https://www.lock5stat.com/StatKey/randomization_1_quant_1_cat/randomization_1_quant_1_cat.html) is an online calculator. .', 'Quantile Regression may be what you are looking for! or moods median test.', ""You don't need to use any non-parametric test if your dependent variable isn't normally distributed. Just learn how to use GMM or contact someone who knows how to use it. Thorough GMM you can fit your model to different distributions according to the nature of your dependent variable. I could help you, but I'm short on time..."", ""\nThere's an infinite number of possible nonparametric tests -- but what hypothesis are we supposed to be testing?\n\nYou're right that the Mann Whitney is not a test of medians... but why test medians? If they had not been concerned about normality, wouldn't they have tested means? Why should the hypothesis change depending on what they think they see in the data? That makes no sense -- a hypothesis comes before you see data, it should not be contingent on it.\n\nSo what was the original hypothesis meant to be?"", ""There are lots of options for testing for differences in two samples.  It really depends what hypothesis the analyst wants to test. (Mean, median, stochastic equality, 75th percentile, and so on).\n\nThe \\[Wilcoxon\\]-Mann-Whitney test is not usually a test of medians.  It is a test of if values in one group tend to be higher than in the other group.  This is often meaningful.\n\nIf the median is of interest, Mood's median test can be used.\n\nIf the means are of interest, but the data don't meet the assumptions of *t*\\-test, a permutation test can be used.  Perhaps the easiest to cite would be the Fisher-Pitman test.  \n\nThis test will, however, also be sensitive to differences in the variances of the groups."", 'You could use the kruskal Wallis test', '> It\'s not testing the difference in the medians. Who says it is?\n\nIt\'s very common in ""applied"" statistics textbooks (e.g. for the social sciences) to describe the MW as a test of medians, which it is in the specific case of a pure location shift alternative (which is probably not a reasonable assumption in the majority of cases people use it for).', 'under the correct assumptions, mannU can determine the difference in medians. But only in that scenario.', 'The hypothesis is to test difference in the means but it is not normally distributed', ""https://www.ncbi.nlm.nih.gov/books/NBK560699/ a lot of papers says it and this is one of them. You might be lucky if you haven't encountered anyone says that"", 'This. Try a simple quantile regression.\n\nEstimate the regression model for the 50% quantile, with a dummy variable indicating whether the observation is from sample A or sample B. Test whether the regression coefficient on the dummy is significantly different from 0.\n\nThe [quantreg ](https://cran.r-project.org/web/packages/quantreg/index.html) package for R has extensive documentation and a great vignette to help you get started.', ""Why not use Mood's median test?"", ""You are correct. I think we shouldn't change the hypothesis after we see the data. But somehow, I learned conducting tests by trial and error from colleagues and they used to do this. The concern is that someone critiques the paper and says the results could be biased because the data is not normally distributed and that we didn't check the assumptions, and the results cannot be generalized to the population."", 'Good suggestion. I used to do it for more than 2 samples', ""MW test doesn't test the differences in the means. Perhaps look at bootstrapping?"", 'I have, I just wondered specifically in this case who did.', 'Non-parametric tests usually preprocess your dependent variable data and hide meaningful information. You\'ll want to use non-parametric tests when your sample is less than the expected/needed. The real question should be: "" why not use GMM?""', "">  I learned conducting tests by trial and error from colleagues and they used to do this. \n\nThis has consequences, including screwing with the very properties they want to guarantee by operating this way (like the fidelity of the chosen significance level, alpha).^([1])\n\nThe problems that lead to this behavior are many-fold but I'll mention a couple of major aspects:\n\n1. No thinking carefully about the model until *after* seeing the data but instead just trying to smoosh every square peg into one fairly round hole and if that doesn't seem to work (albeit they're usually doing the wrong thing to decide it doesn't work), just banging it into one decidedly triangular hole instead. Typically a great deal is actually understood about variables already, often enough to formulate a sensible model.\n\n2. Lack of awareness of alternative analyses. For example, choosing more suitable parametric models^([2]) at the beginning rather than just the single one you find in an intro stats book, or using a non-parametric analysis that don't make you change your hypothesis to suit it (make the analysis fit your original question, not the other way around)\n\n[1]: I'm not suggesting model checking has no place, it still does, but it serves only to see if you need to cast doubt on the results after having carefully chosen the model at the start, not to choose the model. If you *must* you data to choose the model (e.g. that somehow nothing even *similar* to your variable has ever been seen before and nobody knows anything about it^([3])), you need to use different data to choose the model and conduct tests on it.\n\n[2]: which I highly recommend where feasible. It's very often feasible but it requires a lot more work at the front end while many people don't even start considering the statistical issues until they have looked at data, where it's  too late\n\n[3]: in this situation you should nearly always be doing exploratory work rather than hypothesis tests in any case; you're not really in a position of formulate proper models nor hypotheses."", ""Because I work with beginners in research, I don't want to confuse them with stuff that they never heard of. I didn't study GMM in biostatistics classes. I just Googled it sounds it is used in economics more than medical field?"", ""Oh, I see. Well, GMM (generalized mixed models) are just regressions with the use of random effects along fixed effects, also different distributions to fit the data (while linear regression is restricted to the gaussian distribution), and custom link functions to estimate the parameters. In R it's implemented mainly in the package {GAMLj}. It's very weird that you didn't heard of it. Most clinical trials end up using this kind of modelling.\n\nSome books (edit1):\nApplied longitudinal analysis, by G. Fitzmaurice\nGeneralized, linear and mixed models, by C. McCulloch"", 'Oh sorry I thought you meant generalized methods moment because you didn\'t spell out the acronym in the beginning. Yes I did take that class, they didn\'t dig deeper to the topic but I wil read about them. I have a book ""extending the linear model with R by Faraway Thanks!', 'Sorry for that! Hope it helps!']"
Is this a Kruskal-Wallis? [Q]," So to compare the effect of receiving treatment on a dependent variable measured using a Likert scale (0-7),  using such data:

|\#|Group|Rating at baseline|Rating at endline|
|:-|:-|:-|:-|
|1|Treatment|2|5|
|2|Treatment|3|3|
|3|Control|4|7|
|4|Control|5|9|

I was told to use the following formula:  
\[(%  of respondents in the treatment group choosing a rating at endline - %  of respondents in the control group choosing a rating at endline) - (%  of respondents in the treatment group choosing a rating at baseline - %  of respondents in the control group choosing a rating at baseline)\]

He built a frequency tables to carry out these calculations, and said that this is a Kruskal-Wallis. However, when reading up on Kruskal-Wallis, it seems to be something different, and not applicable for this case. I would appreciate it  if anyone can help me understand what is happening here.",11lqqku,theunwillingdentist,1678263881.0,14,0.99,"['Why go the Kruskal Wallis route when you have paired data (2 repeated measures) from two groups and you want to apply a test for data not assuming the data is normally distributed? You need the [Wilcoxon signed-rank test](https://en.m.wikipedia.org/wiki/Wilcoxon_signed-rank_test)', 'Still learning myself, but if I understand you correctly and you want to compare all those different groups (more than 2), a Kruskal-Wallis sounds fine to me. Because you cant use an ANOVA which youd normally use to compare multiple groups, since your dependent variable is not interval data.\nWithout knowing more Id also say Kruskal-Wallis. But maybe someone more experienced can add their expertise.', 'You was told to use the formula to achieve what?', 'See also duplicate question: https://stats.stackexchange.com/questions/608808/estimating-treatment-effect-on-a-variable-measured-on-a-likert-scale-at-two-poin', 'To estimate the mean difference l, but no idea how to calculate p now.', 'What you are looking for is Average treatment effect (ATE) that is not what you are doing in general', 'Difference in Difference (DiD) can also be an option. Check assumptions.', 'Sorry it was to estimate DiD.', ""Makes sense. Anyway DiD although in many communities is used as a standard tool I'd suggest to also investigate other ideas since assumptions are often in empirical settings non plausible. Good luck!""]"
[Q] How do i do a mann whitney u test by hand?,"for my high school stats analysis paper, i need to do a mann whitney u test by hand but i'm having trouble. i know there is a formula but i'm getting really confused bc my data has tied ranks, and a sample size of 50. every time i do the math by hand, i end up getting a different answer than what the software i use to check says?? i am confused lol. 

can someone please send me an example of mann whitney u by hand for tied ranks?? if someone could even PM me i would send over my raw data so they can just do it for me/show me how to do it that would be so helpful because it's making me stuck",11lmnb6,fartmaster900,1678250921.0,6,0.6,"['https://www.youtube.com/watch?v=BT1FKd1Qzjw\n\nThis example has ties in the data.', ""That's awesome you're learning that in high school! I think the other post has your answer (sort, label, count labels, lookup statistic)"", 'step 1 is sort the data all together but keep the group labels with the numbers -- easiest by staggering the rows, like so:\n\n     A:    3          5      7  8              23\n     B:        4  4     5           9  10 \n\nThen write the corresponding ranks (when tied, you average the ranks):\n\n     A:    1            4.5       6 7              10\n     B:        2.5 2.5     4.5           8  9\n\nThen the easiest way to calculate the statistic is to use the Wilcoxon rank sum form; you can convert to U by a simple shift afterward if you must have U rather than W. Then with n=50 and tied ranks, use the normal approximation but make sure to adjust the variance for the ties.', ""Here's an example from my text book. I am an old man learning statistics!\n\nhttps://imgur.com/a/hLnoCCP"", ""that's awesome!!! it's very challenging though. my dad is also just learning stats as part of his masters program""]"
[Q] easiest non parametric significance test?,i'm a highschool student and i need to a statistical analysis for my class. i need to do a non parametric significance test by hand ... so i need something easier. i tried to do mann whitney u test by hand but i'm having a lot of trouble and i'm confused because i have ranks.,11lmia7,fartmaster900,1678250525.0,8,0.75,"[""The easiest one I know of is the Tukey Quick test, a.k.a Tukey-Duckworth test.\n\nhttps://en.wikipedia.org/wiki/Tukey%E2%80%93Duckworth_test\n\n> i'm confused because i have ranks.\n\nHaving ranks doesn't make it hard.\n\nYou mean you have *tied* ranks, right?\n\n\n\nIt's not going to get all that much easier than a Mann Whitney, to be honest.\n\n(If you're able to choose a different data set, get one that's *way* smaller and where there's no ties. )"", ""I think the Mann-Whitney test is a good candidate.  It's fairly easy to calculate and understand the results.  The only thing I would caution about is what the test is actually testing for.  It's not usually a test of the median.  (Even though this is commonly stated on websites.)  It's a test about if the values in one group tend to be higher than those in the other group.  (Or more simply, if the distributions of values are different between the groups.)  If you can use data that doesn't have tied values, the procedure will be slightly easier.  It's also easy to double-check your work with free software or online calculators."", 'See: https://www.reddit.com/r/statistics/comments/11lmnb6/q\\_how\\_do\\_i\\_do\\_a\\_mann\\_whitney\\_u\\_test\\_by\\_hand/', ""LOL i'm so sorry i meant tied ranks"", 'thank you so much', ""I meant to mention that the Tukey Quick test assumes no ties; if there's lots of ties you have to adjust the critical values for the pattern of ties, which would make it more effort than the Mann-Whitney (for which an adjustment is already available in large samples).""]"
[Q] How to rate players in 5-a-side football?,"Hi everyone!

I hope this is a good place to ask such question. If it's not, I am really sorry. Anyway, I have a group of 21 players, we play 5-a-side football (futsal) every Tuesday and we're looking for a perfect, but simple way of dividing players into teams so the odds would be 50-50.

We started by rating each other in Google forms and we got some ratings. The best player is rated 8.7 and the worst one 3.9. It's pretty simple to divide people into 2 groups with similar sum, but it may not be perfect. Maybe some player is underrated and he's actually worth more... Maybe someone injured himself and his rating should decrease... There are many factors, but right now we only use these:

+0.1 rating if you won
-0.1 rating if you lost
-0.03 rating if you didn't come
-0.01 rating if it was a draw.

We are currently looking for a better model and I wonder if you guys have any ideas. Thanks!",11lh739,nightmare_stage,1678236497.0,0,0.33,"[""A good team is greater than the sum of its parts. Some people might work well together, others might not. But if we ignore that:\n\n* I wouldn't subtract rating for a draw.\n* If the (predicted) stronger team wins, maybe change the rating less.\n\n> Maybe some player is underrated and he's actually worth more...\n\nThen teams with him should win more than they lose, which means over time his rating will increase. That could take a long time, however."", ""Thank you for the answer!\n\nI definitely agree with the first part, but we have to ignore it.\n\nI agree for draws.\n\nHow much should I change ratings for example if team with rating 40 beats team with rating 39.6?\n\nThe last part is probably the most problematic. We have a guy who most of us didn't know well and he got himself rating of 7.0, while he deserves at least 8.0. So now most of the games team with him wins, but it will still take some time until he reaches his normal value."", ""The Elo system for chess is a good example for such a system, although it's a 1 vs 1 game. You can use its adjustment formula but apply it to the summed strength of a team. New players can get a larger factor for Elo changes so they get to their right value faster.""]"
[Q] Are cross tabulation tables simply tables with labels on both the row headers and column headers?,"Something like this (a distance table):

&#x200B;

`-------|-US---|-Brazil`

`US-----|-0----|-4500--`

`Brazil-|-4500-|-0-----`

&#x200B;

Since it has labels on both the row headers and column headers, is it a cross tabulation table?",11lcduq,TaylorBrow,1678225458.0,2,1.0,"[""Maybe...  Cross tabulation simply presents the data in categories or sub-categories. For example, if you are measuring the mean income of some population, cross tabulation might present the mean income for each gender category, or for each combination of, say, gender category and age category.  The results could be presented in a two-way table in this case, say, with gender on one axis and age category on the other axis.\n\nFor example, the following table, which I assume are counts, not a variable like *Income*:\n\nhttps://www.researchgate.net/profile/Hina-Jaffery/publication/286239437/figure/tbl1/AS:669232815673371@1536568945745/Age-and-Gender-Cross-Tabulation\\_W640.jpg\n\nFor your example, you could consider your results to be cross-tabulation data, across two categories of *Nation*.  But you wouldn't need a two-dimensional table.  I'm not sure why data would be presented that way."", ""Your table could be a crosstabulation if you wanted to calculate the (weighted bird flights or whatever) distance From and To a particulate country\n\nIF you had more countries in the lists, it would be more informative. Nonetheless, I believe it could be seen as a crosstab. I've built something like this with the number of people who live in region A but get their vaccination in location B. This would allow for some more interesting figures to occur in the table. \n\nOf note: if you have the same exact countries From as well as To, and the numbers for From>To are the same as To>From, you only need the upper or lower triangle of that table, optionally excluding the diagonals are they're all 0. Edit to add that in your case, you only needed the 4500 value as a distance from US to Brasil."", 'Well, there can be more countries in the variable, and then we can see the distances between the countries. I used a 2x2 just as an example.', 'Thanks!', 'Then it sounds like what you are presenting maybe wouldn\'t be ""cross tabulation"".  Maybe ""pairwise table of distances"", or something like that.', ""Does increasing the number of elements in a variable change the type of the table it's being displayed in?""]"
[Question] International Beerio Kart Championships of the World: Power Rankings Help!,"**TL;DR: My friends and I have a stupid hobby that's getting out of control and I need your help spiraling it further. Please help me create a fair power rankings system (using the attached spreadsheet for reference) for the Beerio Kart tournaments we host.**

[**https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc\_jHWbTZ\_khS/edit?usp=sharing&ouid=114408781303577995971&rtpof=true&sd=true**](https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc_jHWbTZ_khS/edit?usp=sharing&ouid=114408781303577995971&rtpof=true&sd=true)

Dear members of the Statistics community,

I call humbly upon the statisticians, mathematicians, programming aficionados, excel experts, sports analysts, and power rankings enthusiasts of this great community to assist me with a vital task -- creating a fair and representative power ranking formula for the International Beerio Kart Championships of the World.

A little background: my buddies and I were trapped at home Thanksgiving of '21 for a fourteen day COVID quarantine. We were saddened by a missed opportunity to see our families, but with competitive spirit running through our veins and a surplus of leftover PBR from a party we threw (which was undoubtedly what gave us COVID), we found solace in roughly two weeks straight of fierce competition in the best drinking/video game pair to ever exist: Beerio Kart. For the uninitiated: Beerio Kart is Mario Kart, however, you need to finish your beer before the end of each race, and you can't drink and drive (i.e. chug and control your character simultaneously). Our version of the game has many extra rules and sub-rules, however, that's the basic premise of the game.

After two weeks of this, we needed an outlet to determine who was truly the best of us, and thusly the International Beerio Kart Championships of the World were born. It started with a modest eight competitors, but interest has increased steadily over the past three years and in recent events we've had as many as 58 competitors fighting to compete in a 32 person bracket (surplus competitors play in Play-in Prix's for entry into the main bracket). We've now had 75 people play in official brackets and obtain power rankings, and close to 100 participate in the events overall. For a little context into how the tournaments are run, four competitors participate in each Grand Prix, and the top two competitors advance from each round until the championship. In the preliminary rounds, players must drink a beer on races two and four of each Grand Prix, and in the finals all four races are drinking rounds, thusly the final four competitors must drink a minimum of 10 beers to win the tournament.

As tournaments got larger and more intricate (and people started complaining that they were seeded unfairly), we realized we needed an objective ranking system to seed players so that the Prix's leading up to the championship were fair and quantitative. This background brings me to the hallowed undertaking I beseech your help with: **please help me figure out how to do this.**

We've tried a few formulas, but we are but amateur statisticians and none have felt like they effectively capture a player's skill level.

First we tried the following formula: ibkc power ranking = 0.33t/60n + 0.33z/60 + 0.33y/60, where:

1. 60 = the maximum number of possible points scored in any given grand prix
2. t = total points accrued over all past tournaments attended
3. n = total number of grand prix held in all official tournaments
4. z = average points scored per prix, per tournament, in all tournaments attended
5. y = average points scored per prix, per tournament, in all tournaments attended this calendar year

It was a good start, but it unfairly biased players who had played in more tournaments, and wasn't an accurate reflection of *current* skill level. It would be like baseball power rankings putting the Yankees are at the top because they're an ancient ball club and have won 27 World Series', even though the last time they won was 2009, or the Astros low down on the power rankings because they didn't win their first Series until 2017, even though they've won twice in the past 5 years.

We then created a formula based on Pythagorean expectation, where a players skill level is calculated by averaging their (points accrued in a prix)/(points accrued in a prix + total number of possible points in a prix). Each round of a tournament was weighted heavier than the last, and tournaments with four rounds carry more weight than tournaments with three rounds. The player's Pythagorean expectation was then averaged over all tournaments they've participated in, averaged over the last four tournaments held, and averaged over the last two tournaments held. Their power score was then calculated by averaging these three numbers together with the intention that more recent tournaments would be weighted heavier than older ones. **This is the formula that the attached spreadsheet uses.**

This new formula was better than the first but has an inverse problem -- it weighs recent tournaments too heavily and doesn't account for any rank decay from missing tournaments. For example, you can see that BAT has won 6 of 8 tournaments, but after a huge upset in the semi's, BAT did not make the finals of the last tournament, and was booted from first place overall to third. All the while, Squirt4Boyz advanced from second place overall to first, even though Squirt4Boyz didn't even participate in the last tournament.

There's all sorts of hidden columns and rows and whatnot in this spreadsheet so please dm me with any questions you might have, but please, I beg of you fine and glorious proprietors of the world's most stressful game, help me create a ranking system that makes sense. **Ultimately we need a system that reflects how many points a player is expected to score, considers that player's tournament wins, podium finishes and finals appearances, accounts for rank decay, and like in global tennis or golf rankings, has some bias for recent events.**

Thank you, friends.

Your servant,

The International Beerio Kart Championships of the World League Commissioner",11la74u,zakarm22,1678220649.0,3,1.0,[]
[Q] How to prove the Poisson link function is a canonical link function?," So I'm a 3rd year undergraduate doing my thesisin football score models right now. In my thesis I want to include a proof of what the link function for the Poisson distribution is and why it relates the mean to our linear predictors. I'm almost there, but there is one part that most literature seems to gloss over.

So we have our linear predictors  =0+1\_xi1++p\_xip and our natural parameter . Now I know why if  =  then our link function is canonical, but my question is how do we prove that we have  =  in the case of the Poisson distribution? Most literature just state that the canonical link sets this equality, or we can assume this equality for distributions that are members of the exp. family but don't actually prove why. Can anyone help?",11l63tz,BantaPanda1303,1678211567.0,15,0.94,"[""Just use the definition.  Also you don't need your formula for linear predictors.  You can do just one Poisson random variable.  Then any linear submodel is also exponential family (a fact about exponential families).\n\nLook at the log likelihood for Poisson.  To be exponential family form it must be (function of data only) * (function of parameter only) + function of data only + function of parameter only.  From that you read off the canonical statistic and canonical parameter."", ' There are many resources for this and this is above my pay grade. Theres a proof here it looks like\n\nhttps://pages.stat.wisc.edu/~st849-1/lectures/GLMH.pdf#5']"
[Q] Monitoring a Year-Long Sample: How to Determine When Stratum Subsample Thresholds Have Been Significantly Exceeded and Should Therefore be Adjusted?,"I'm in charge of a year-long, stratified sample where the population is estimated based on previous years but which is also quite dynamic, changing in unpredicted ways every year (furthermore, past population data are aggregated by year, making it impossible to know in advance if there are seasonal spikes or drops).

Obtaining a sample response is very resource intensive, so only the bare minimum sample size is sought and the stakeholders are hyper-interested in monitoring the sample daily to ensure no single stratum subsample has 'taken off'.

Sometimes a stratum subsample will exceed its annual estimated total early (e.g. Stratum #3 has hit it's annual expectation of 200 samples even though we're only halfway into the year). When this happens, they immediately want to shut down all sampling for that stratum, even if other strata are lagging behind expectations (thus putting at risk obtaining the overall sample size goal).

What is the proper way to monitor and govern an in-progress sample like this? Are there any objective, statistical 'threshold' tests to govern such decisions (e.g. Stratum #2 has reached 125% of the estimated annual stratum subsample before the 8-month mark in the sample therefore further sampling in that stratum should be shut down)?",11l58bh,coip,1678209551.0,8,1.0,[]
[Q] How do you get 90% confidence interval &. 10% level of significance? Do you use t-test?,,11l4sln,feellikepooping,1678208562.0,0,0.2,"['Confidence interval for what? Significance test of *what*? What are you trying to do?', 'Compute a 90% [confidence interval on the difference between means](https://onlinestatbook.com/2/estimation/difference_means.html) and a [significance test on the difference between means](https://onlinestatbook.com/2/tests_of_means/difference_means.html ). BTW, the question uses sloppy language since you dont draw conclusions about significance. Significance is used to draw conclusions about the population.', 'A t test can test at that level of significance, and can be inverted to get a confidence interval at that level. But so can many other tests. The question is which is appropriate for your problem, which is impossible to know without more detail.', 'if population variance is unknown, use t-test, if population variance is known, use Z-test, for samples that are large, i.e. > 30, u may use central limit theorem and use Z-test', 'Ask ChatGPT even the teachers at Stanford are in on the craze. Ha ha', '2.    I need to Perform an appropriate test of hypothesis using 10% level of significance. Then im finding appropriate 90% confidence interval then from that,, i would depict it intowords', 'I have this significance test of comparing higher profit', 'Hello, can I message you about this? Thank you', 'Hello, can I message you about this? Thank you', 'Hello can i message you?', 'Hahahahaha! Idk how lol', 'What hypothesis? And confidence interval *for what quantity*? You need to explain your data and research question.', 'OK', 'definitely :)', 'Im not making you answer it im just curious if i should use two sample t-test for it?', 'I cannot explain it anymore sorry but thanks for trying to help', 'Do you want me to paste the question here,,', 'The management team wants to know if it is reasonable to conclude that ABC has a significantly higher average profit compared to XYZ. Perform an appropriate test of hypothesis using 10% level of significance. Support this finding by constructing an appropriate 90% confidence interval. Write a brief report (at most 50 words) interpreting the results to the management team.', 'Is this homework?', 'Assuming that the question gives you the average profits, the standard deviations, and the sample sizes, this would be an independent samples t-test for the hypothesis test portion of it. Im guessing you also need to do a confidence interval for the difference in average profits?', 'Yep,, but I just need to know what method should I use. Can i message you instead?', 'I honestly dont know  Im stuck in math i really find it difficult', 'probably the methods covered in your course?', 'Idk honestly im so confused', ""Fair, but if this is homework they probably have a specific approach in mind. I see you've mentioned t-tests elsewhere -- I would argue that the assumptions for this test probably aren't met, and therefore the test isn't valid. With that said, I know the business world generally couldn't care less about whether they actually do valid analyses, so I wouldn't be surprised if their courses teach this too..."", 'Can i message you instead?']"
[Q] Eurostat question,"
I have a Eurostat data Distribution of income by quantiles. But the percentages are different per quintile. Question: if its quintiles, why is it not 20% each? What is a quintile in this case? (I read their explanation, but they are unclear to me). Would GREATLY appreciate the answers, thank you!!",11l3ijw,imwaalkinghere,1678205686.0,5,0.86,"['Maybe you can share a link to the data set ?', 'See: https://www.reddit.com/r/AskStatistics/comments/11l3blv/i\\_need\\_reddit\\_wisdom\\_this\\_is\\_a\\_eurostat\\_data/', 'With pleasure!', 'https://ec.europa.eu/eurostat/databrowser/view/ilc_di01/default/table?lang=en\n\nThis is based on Share of national income and not Top-cut off point. Can not send you the exact table cause I adjusted it manually', 'Each quintile contains 20% of the population. The data here shows what share of total national income is earned by the population in each quintile. For instance the top quintile will have the largest share (and that share will be greater than 20%), since by definition the people in that 20% of the population will each earn more than someone in a lower income quintile.', ""Maybe it's just me, but I'm having trouble understanding the question without seeing what you're looking at.  Maybe share a screenshot of part of the table exemplifying what you're seeing ?"", 'Aaaah, yesss, that makes total sense and you put it in very comprehensible way. Thank you very much!! That is really helpful', 'Rules do not allow. Shall I send you the screenshot in a private message?', 'Nah.  Maybe mock up something with different numbers ?  Or maybe someone else here can address the question.', 'Ahah, shame, you were so active I thought you would not mind looking at it. But thank you anyways!']"
"[Q] Factor analysis, eigenvalue cutoff > 1","Hi everyone,

newbie to factor analysis here, I learned about it, but in real life the numbers are not as clean cut as in the profs examples.

I have three samples for the validation of a questionnaire in a new language. In the original, the authors identified only one factor, as only their 1st eigenvalue is > 1 and the screeplot has the distinct kink after the 1st factor.

In our analysis, the structure and magnitude of the eigenvalues is extremely similar, the only difference seems to be that the eigenvalue for the 2nd factor is > 1 (e.g. 1.04).

I would like to know if there is a rationale to exclude the 2nd factor in spite of being > 1, and if there is one, what reason would/could I give. As far as I can see, this dubious 2nd factor would be extremely difficult to interpret (i.e. no idea). Is there anything else I need to consider?

thanks in advance",11l1stk,alpaka4054,1678201742.0,2,1.0,"[""Kaiser's criterion is famously unreliable. Perform Parallel Analysis instead, and see where it takes you."", 'The >1 rule of thumb often retains too many factors so if your second factor is not interpretable even after rotation I would not keep it.', 'When validating the questionnaire in a new language fit a confirmatory factor analysis to see if the factor structure originally identified (ie what the original questionnaire had) holds. You are not trying to create a new measure you are seeing if an existing measure works in a new language/population.\n\nYou can then do measurement invariance testing to see if the structure, intercepts, factor loadings and errors differ across samples.', 'Interesting, I will have a look, thanks for the suggestion.', 'This is the way. Or a minimum average partial test.', 'Thanks, I was hoping for this to be true. May I follow up with the question how to argue this in a paper? Is no interpretation enough of a reason or do I need a mathematical argument?', 'I think interpretability should be sufficient. There is little mathematical basis for the <1 rule, only that 1 is the average eigenvalue in Principle Components which doesnt strongly imply that that should be the rule for the number of factors.']"
[Q] How exactly do I use prior information in a real example?,Suppose I do a clinical trial with a small number of subjects and run my model and estimate an effect size with standard error. Is that information the prior so that I can use to update the posterior as I release the drug out to public and gather more data?,11kzsrt,sonicking12,1678196862.0,1,1.0,"[""You certainly could use past studies (e.g. an RCT) as a means of developing a prior to then further study a drug.  I did this a lot in my PhD.\n\nYou just need to be a little careful.  For one, RCT's are mostly homogeneous when it comes to demographics; they are mostly men, usually north american, and a least in the studies I read usually healthy.\n\nWhen studying anything in the real world, there is a certain endogenetity that is omnipresent.  Sure, in the RCT the average treatment effect was X, but in the real world there is a reason these patients are needing to take this medication, and that may change  the average treatment effect in ways that are not wholly predictable.\n\nSo while prior information is useful and you can take previous studies as prior information, you need to be aware of the transportability of the estimate from those prior studies."", '1) If you are comfortable, could you share any paper you did using this approach?  \n\n2) Would you inflate the prior precision (not sure how, but using gut feel?) to account for the homogeneity issue your mentioned?', 'This is the sort of thing that makes weakly informative priors a good idea....', ""I didn't write this out explicitly in the papers I wrote, but yes its probably fine to inflate the uncertainty a little.\n\nA better approach would be to do a prior predictive check to see that your model is giving credence to plausible scenarios before seeing data"", ""BY weakly informative, what do you mean?\n\nWeakly informative priors (assuming they are in the neighbourhood of observed effects and are not just diffuse priors for the sake of being diffuse) are great, but if you're not going to use prior information and your model is relatively simple I don't see the benefit of being Bayesian."", 'I am learning all of these.  Are you aware of any paper or example would work out what I am asking???  Thanks again', 'Consider the context of what I was responding to:\n\n> When studying anything in the real world, there is a certain endogenetity that is omnipresent. Sure, in the RCT the average treatment effect was X, but in the real world there is a reason these patients are needing to take this medication, and that may change the average treatment effect in ways that are not wholly predictable.\n\nWhen such situations hold, a hyperprior that admits a broad variety of possibilities very different from what we\'re likely to see in a clinical trial is only prudent; the real world *is  in fact* different from what we\'ve seen repeatedly in the lab, it is more heterogeneous and our estimates of the heterogeneity based on the data from our clinical trials *is way too small*. Such experiences have been seen many times across many different medical situations (the list is long and worrying -- heart medications tested only on middle aged men that result in treatments very unsuited to  young women, or treatments tested largely on caucasians that were ineffective or even dangerous to non-caucasians)\n\nIf you want to have overly-informative priors based on the experience from say previous lab studies, or the pilot of the current study, you may well end up harming or even killing people. Not a particularly good reason to insist on that highly informative prior, I\'d have thought, even if that means you no longer see the point in ""being Bayesian"". Personally I\'d say that it shows us one of the values in being Bayesian - that - suitably set up so that the uncertainties are in the right kinds of places - that it is really possible to express a high degree of doubt about how our parameter information should generalize. Decision making under uncertainty is the Bayesian strong-suit but it doesn\'t do to underplay that uncertainty.', 'I don\'t think you\'ve really addressed my point of what ""weakly informative"" really means.  I\'m not saying we should only be using informative priors, but if you\'re just going to slap a normal(0,10) prior on the effect I don\'t really think we are *that* uncertain on the effect.\n\nI\'ve likened the approach to be like estimating the acceleration due to gravity from a dropping object.  I know its around 9, but I forget the second two digits.  I know its not 9.01 m/s but I also know its not 1000 m/s.  \n\nSo if by ""weakly informative"" you mean kind of placed around the effect size we saw but really uncertain then I stand by my point.  But it doesn\'t sound like you do, and I think we actually agree with one another.']"
[Question] MUltiple Correspondence Analysis MCA,"Hello World! I am building a thesis based on the MCA analysis but my supervisor is not satisfied with the way i am describing the way Eigenvalues work as well as Total and Explained Inertia. I will attach the parts in case someone can provide any help. I have tried to elaborate as much as possible but i cant find that many papers that describe how MCA works and youtube is not usefull at all. Thank you very much for your time.

[https://imgur.com/7DfuWgp](https://imgur.com/7DfuWgp)

[https://imgur.com/nYE931E](https://imgur.com/nYE931E)

[https://imgur.com/0YOlRhU](https://imgur.com/0YOlRhU)

[https://imgur.com/lC32rY2](https://imgur.com/lC32rY2)",11kxg8t,Capital_Marsupial517,1678190473.0,1,1.0,[]
[Q] How do you get the probability of model (prior) in Bayesian statistics?,"In reading about Bayesian methods, I often see the following:

P(model | data) is proportional to P(data | model) \* P(model). 

where model are more specifically model parameters like mean and variance. But how do you get P(model) in practice? 

Also is it possible to interpret ""models"" as distributions (normal, t, cauchy), or more generally neural networks / random forests, etc.? I assume it would not be possible to get a closed form solutions for the latter, so how would I get the P(model) in these cases?",11ktf93,No-Result-3830,1678177099.0,54,0.92,"[""This is where domain knowledge comes in. You have to supply priors in the form of PDFs for all the parameters in your model. In practice it often doesn't matter, and people often check how their priors affect the posterior by fitting the same model with different priors. \n\nThe reason that most Bayesian models don't have a closed for solution is the part you've left out: P(data). For most things that are interesting to model, this contains an integral we can't solve analytically, so we bypass it with MCMC."", ""Maybe a few extra words on domain knowledge and priors. People are often 'afraid' of priors because they think they may be biasing the model, or don't quite understand how they could have a prior without previous experiments. But in general, it is very rare that we don't have some prior knowledge about the possible values of our parameters. I like Aki's example. If I ask you to guess how much money in euros I have in my wallet, you have no previous measurements of my wallet, have never seen it, and have no idea how much money I make. At the same time, any reasonable person would put the probability of there being more than 10000 in any wallet as extremely low, and would rather put most of the probability at between 0 and 1000 (or around those numbers). \n\nThis is to say, we usually have so idea of what we can expect from our models, and what reasonable and unreasonable values are."", 'The prior encodes your beliefs about your model parameters before considering your data. It\'s ultimately subjective but there are several common choices.\n\nIt\'s common to use a uniform distribution between limits that gives equal probability to any plausible value of the parameters. There\'s other more complex solutions to this but the general idea is to ""let the data speak for itself"" and minimally bias your posterior.\n\nOr perhaps you have previous measurements? Suppose someone else published their own values of these parameters from another experiment. You could use their posterior as your prior.\n\nOr maybe you just know some stuff which you could loosely put a distribution to. Maybe you know the effect of one parameter is much smaller than another so you alter your basic uniform distribution to give no probability to parts of parameter space where they\'re close in value.\n\n\n\nSo that\'s how you choose a prior and then your other concern is how do you get the equation. Well, it\'s often not necessary. It\'s rare to do these things analytically and it\'s easy to code up functions that do things like check parameter A is at most half of B, returning 1 if it is and 0 probability if not.', '>P(model | data) is proportional to P(data | model) \\* P(model).\n\nThis isn\'t quite true, as uncertainty about the model parameters is distinct from uncertainty about the model being the true model. Let P be the parameters, D be the data, and M be the model. Then the correct formula is\n\nP(P | D, M) is proportional to P(D | P, M) \\* P(P | M) \\* P(M). However usually we just assume that P(M) = 1, and doing so otherwise leads to more complicated Bayesian model averaging.\n\nP(M) is simply your prior belief about the model being correct and isn\'t something that you can calculate. However if you want to calculate P(M | D) then this is exactly what Bayes factors or pseudo marginal likelihood values are for.\n\n>Also is it possible to interpret ""models"" as distributions (normal, t, cauchy), or more generally neural networks / random forests, etc.?\n\nAny model with a likelihood models the data with a probability distribution. Neural networks can also be modelled this way using distributions such as the Normal or Bernoulli, but the maximum likelihood estimates do not exist because the model is over-parameterized and not identifiable. However you can calculate parameter estimates using something like gradient descent and L2 penalties, and the resulting model + coefficients provide a distribution for the data.', 'I feel like there is some confusion here concerning the formula in the OP. Generally, when one starts learning about Bayesian methods Bayes\' formula is given as \n\nP(parameters|data) = P(data|parameters) P(parameters) / P(data).\n\nHere, P(data|parameters) captures the model, as it gives the likelihood of observing the data given a specific set of parameters. \n\nNow in the OP, ""parameters"" is replaced with ""model"" which leads me to believe that they may be reading texts dealing with Bayesian Model Averaging (BMA), where one tries to mix over different model specifications. In the simplest case, this boils down to something like ""which covariates do I include in a linear regression model"" but can also be something more complex like ""which likelihood is best suited to my data"".', 'Typically, people choose P(model) to make computations as tractable as possible. Then post-hoc and ad-hoc justifications are made.', 'Bayesian statistics is based on Bayes theorem that was discovered centuries before Bayesian inference could be achieved. Nowadays there is a wide set of tools for posterior distributions estimates, since most of the time there is not analytical form for them (see conjugate prior). \nA huge class of these methods is the Monte Carlo methods, specifically MCMC and more recent version as HMC and NUTS. \n\nAnother huge class relies on variational inference.', 'You decide it subjectively. Ideally, you take a prior that is sensible regarding your beliefs, if there are people with experience or domain knowledge...etc... The idea is to try to encode the prior knowledge in a distribution, which is then used as a starting point for the bayesian ""process"".  \n\nRead other comments for details.', 'Along with what other posters have said, you can also use a so-called ""objective"" or ""reference"" prior (both somewhat misnomers), Jeffreys priors for example.', '> Is it possible to interpret ""models"" as distributions (normal, t, cauchy), or more generally neural networks / random forests, etc.?\n\nSo the probability model isn\'t a single distribution - it specifies all relevant conditional and unconditional probabilities.  It specifies your hypotheses, their probabilities, and conditional probabilities of evidence on them.  It contains many distributions.\n\nTo my knowledge, there isn\'t overlap between probability models and machine learning (such as neural networks).  If you wanted, you could specify a probability model on the range of parameters for your machine learning model before training it - but that would be a lot of work to no benefit.  \n\nIn general, machine learning isn\'t a Bayesian practice.  In ML, you don\'t update a probability model.  Instead, you train a prediction model.  Both involve data, but the methods are different.  \n\nThere is a kind of neural network called a Bayesian Neural Network, which looks like a generalization of standard neural networks.  You might dig into this kind of model.  See if it answers your question.  If so, you should do an update, or a separate post.\n\nVery simple intro: https://towardsdatascience.com/why-you-should-use-bayesian-neural-network-aaf76732c150', ""Priors are for gut feelings/previous understandings of the causal relationship you're trying to model.\n\nAnd then you weight the prior based on how strong you believe in that understanding."", ""Just to clarify - are you talking about bayesian \\*hypothesis testing\\*, wherein you have a probability of \\*models\\*, and you're defining both within-model priors (for model 1, priors on linear coefficients) and between-model priors (e.g., prob. of model 1 being true)? Bayes factors and such?\n\n&#x200B;\n\nI think some comments here are assuming you mistyped, and are talking about p(data | parameters)p(parameters); but there \\*is\\* a world of bayesian model comparison and hypothesis testing that very literally does use p(data | model)p(model).\n\n&#x200B;\n\nCould you clarify?"", ""In simple cases, it's handy to choose a prior that is convenient and plausible. A conjugate prior yields the same distribution form for the posterior. E.g., for binomial data, choose a beta distribution for probability of success. An beta \\* binomial gives you another beta, with updated parameters."", ""In your example, the prior might be a uniform distribution between 0 and 1000 euros. But as I collect more data, would the posterior still be a uniform distribution with a narrower range? Ideally I'd like my posterior after much new information to be something resembling a gaussian or cauchy distribution. Is that possible mathematically or in practice?"", "">It's rare to do these things analytically and it's easy to code up functions that do things like check parameter A is at most half of B, returning 1 if it is and 0 probability if not.\n\nI don't follow this part. Could you elaborate on how this relates to bayesian priors and posteriors?"", '> which covariates do I include in a linear regression model\n\nDo you know of any good resources that touch on this?', ""Yes I was originally asking about the probability of \\*models\\*.  If I have a number of models, say, a neural network, naive bayes, gaussian, and a random forest, one question that arises is how do I know if I'm using the right model? And so what would be the prior for models?\n\nAnother question I had is what is the probability that the feature set is correct given some model prediction accuracy. How would one describe the feature set space as a prior?"", 'Your posterior will have whatever shape your likelihood has. You can fit a lognormal model, for example. The prior can also be lognormal with a mean at 0 and a very large sd.', ""Ok so let's say there's two parameters (a and b) that I'm trying to infer. I sit down and summarize what I know:\n\n- 0 < a <1\n\n- 0 < b < 1\n\n- a < 0.5 b.\n\nI need to encode my knowledge in my prior. How do I write my prior?\n\nAnalytically, I have no idea. I can't think of a function that is uniform between 0 and 1 in two dimensions but then is 0 where a>=0.5 b.\n\nFortunately, nobody does Bayesian inference analytically. They do it via numerical sampling. I'm going to write some code that samples the posterior using Bayes' theorem.\n\nSo all I need is a code function that given an a value and b value, sends back the value of the prior. \n\n    def my\\_prior(a,b):\n        if a>1:\n           return 0\n        if b>1:\n           return 0\n        if a>=b*0.5:\n           return 0\n        return 1\n\nOk so this function will send back a zero for my prior if any of my conditions at the start of my comment are broken. Otherwise it returns a 1.\n\nSo despite having no analytical prior, I have still managed to assign zero probability to all values of a that I know are bad and equal probability to all the good values."", '[Something like this?](https://journals.sagepub.com/doi/full/10.1177/2515245919898657)', ""Interesting, I didn't know you could do this. So this would be generated using a monte carlo simulation, and then you would look at the picture and try to fit a distribution to it or do you bin it to make it discrete? How would you go from this to the posterior?"", ""Judging by these questions, you probably want to read some intro tutorials on Bayesian inference. You'll get a much more comprehensive view than you'll get from Reddit comments.\n\nHowever, to answer you, we tend to use Markov chain Monte Carlo sampling to sample from the posterior.\n\nYou can then do whatever you need with that sample. If you want to show people the posterior distribution, you can histogram it. If you want to give people a mean value, you take the mean of the sample. If you want to report a credible interval, you can report a percentile range.\n\nThere's no need to fit a distribution because there's nothing you could really do with a distribution that you couldn't do with a big sample.""]"
[Q] Multinominal Probability with Value Given to the Trial Results," So, I'm trying to figure out some statistics for this game I'm playing. This is it how it works. Every time you buy an item, you'll have a 50% chance of getting a common worth 11, 30% chance of a rare worth 16, 15% chance epic worth 22, or 5% chance legendary worth 44. The total of all your items value equates to the amount of resources per second you get. So high value is good. Max items you are allowed to have is 150, so players will often compare their 150 item totals and see who's lucky and who isn't.

Average item value will be 15.8, so at 150 items the average should be 2370 between all players. Hence, that should be the 50th percentile and then all other results should follow a bell curve. I'm trying to figure out how to figure for percentile, or I supposed I should say chance, of scoring higher or lower than a specific value that's not the average. For example, what percent of possibilities are above/luckier than a total of 2400?

Doing research I learned how to use binominal probability to figure out the chance of getting a specific number, or more/less of a specific number, of a certain item rarity. So then I tried multinominal probability but I could only figure out the chance of a specific result as the chances having values make it confusing. Anyone able to help me figure this out?",11kqwix,Arn964,1678168581.0,7,1.0,"['> Hence, that should be the 50th percentile and then all other results should follow a bell curve. \n\nThe *mean* is **not** the *median* in most cases. Certainly not here.\n\nI don\'t know what you mean by ""bell curve,"" but most people interpret that to mean ""Normal distribution"" which this **is not**.\n\nNow, it is true that the larger the number of items, the more symmetric and Normal-like this distribution will become. So at a certain point it may not be a huge issue to apply a Normal approximation as the other comment suggests. (More on that to follow.) But you cannot take that for granted.\n\n> I\'m trying to figure out how to figure for percentile, or I supposed I should say chance, of scoring higher or lower than a specific value that\'s not the average. For example, what percent of possibilities are above/luckier than a total of 2400?\n\nThere are three approaches to ""what is the probability of this range of values?"" for a distribution like this, or to asking the reverse question of ""what range of values has this probability?"" (this second form is about quantiles/percentiles).\n\n1. Brute-force computation. We know how to write the *equation* which you want an answer to, it is the sum over all sets of items which have the desired range of values (greater than 2400 in your example, but we could also ask about values between 2200 and 2300, or anything else) of the probability of getting that set of items. Generally the math does not work out nicely with things cancelling out to get simple, but we can always make a computer do this for us. We can make a program which enumerates all item combinations which produce a value in the range of interest and sums up their probabilities. This gets hard/slow/burdensome when the number of items gets large, because there are many, many item configurations which could produce the value. But if you wanted to know about the probabilities for, say, a few dozen items, this would get you the exact answer.\n\n2. Simulation. You\'ve got a nice straightforward set of rules for how the items come about, so you can write code to simulate this. By simulating many many times, you get samples from the distribution you want to know about, and you can answer questions by simply counting how often that happens in the samples. Simulating comes with some error (called Monte Carlo error in the literature), but you can estimate and control that error by choosing the number of simulations carefully. Or just choose a really big number and hope for the best. Here are a few lines of R code which allow you to simulate this:\n\n    nitem <- 150\n    nsim <- 1e6\n    p <- c(0.5,0.3,0.15,0.05)\n    v <- c(11,16,22,44)\n    tots <- sapply(1:nsim,function(x){\n      sum(v[sample.int(4,nitem,TRUE,prob=p)])\n    })\n\n\n3. Find an approximation. The other comment suggests appealing to asymptotic normality, which would mean you could compute the probabilities in anything from Excel to stata to python. There will always be error in this case, called approximation error, and it cannot be controlled, it\'s just a fact of approximating. I simulated one million times and compared that to a Normal approximation. The Normal approximation is not *terrible* around the center of mass, but there is still some asymmetry that it can\'t capture. The Normal approximation has the 25th, 50th, and 75th percentiles being 2307.5, 2370, 2432.5 while the simulations have those being 2306, 2367, and 2431. (Note that the mean, 2370, is not the median, 2367 here, but the Normal forces it to be.) If we ask what the probability is of >2400, the Normal says 37%, while the simulations say 36%. The less-probable the values in question, that is, the more extreme and into the tails, the worse the Normal approximation will be. (To be fair, the simulations will _also_ have trouble with particularly rare events unless you use a really large number of replicates.)', 'You have already calculated the expected value of 15.8. So you still need to calculate the variance for your discrete random variable. You can do that in this way: [https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable](https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable)\n\nThen you can apply central limit theorem (sample mean is approximately normally distributed with mean 15.8 and the variance divided by n which in this case is 150). Then you can calculate the probability of getting a sample mean greater than 2400/150.', 'That helps make a lot more sense of it thanks. Been awhile since I used R but I\'ll mess around with it to fire up a simulation. \n\nI do have two follow up questions. First, apologies for the use of terminology ""bell curve,"" high school statistics was a long while ago for me lol. In the normal approximation is the 50th percentile different than the mean because the odds of getting on one side of the mean value isn\'t truly 50%? For example the odds of getting 75 or more common is actually 54% and not 50%. \n\nSecond, between simulation and approximation by using a normal distribution, which one do you think would be considered better? In actuality there is no where to know how many items in total have been bought, but we know how many resources were cashed out and the top 500 in the country and your own state. Using that knowledge, I could approximate that there\'s a minimum of at least 1,000,000 items bought upwards to maybe 2,000,000. Would the way of figuring out how normal this would be, be to simulate it? As for people with 150 items, which is often what\'s compared, I would approximate there\'s 3,000-5,000 people.', 'Thanks for the link. That should be very helpful.', ""> In the normal approximation is the 50th percentile different than the mean because the odds of getting on one side of the mean value isn't truly 50%? For example the odds of getting 75 or more common is actually 54% and not 50%. \n\nYou're conflating something like three different things here.\n\nIn *truth* the mean is not the median here. The mean is higher, you can think of that as being because of the low-probability expensive legendary items. This drags the mean up more than it affects the median.\n\nThe Normal approximation *assumes* that the mean is the median. Any time you make a Normal approximation and ask what the median and mean are, you will get the same answer. Because the Normal is one of those places where the mean is the median, which is also the mode. So, the Normal approximation must be wrong about two of these values. And since we make Normal approximations using the mean and variance, this means that the Normal approximation will be wrong about the median (which should be 2367, not 2370) and the mode (which should be 2354, not 2370).\n\nNeither of these have to do with the odds of getting 75 or more commons, which the Normal approximation is unconcerned with. The Normal approximation only approximates the *total value* of a given number of items, *not* the composition.\n\n> Second, between simulation and approximation by using a normal distribution, which one do you think would be considered better? In actuality there is no where to know how many items in total have been bought, but we know how many resources were cashed out and the top 500 in the country and your own state.\n\nIt seems like you're changing the goalposts here a bit. \n\nSimulations give you a powerful approach for getting answers when there are not good approximations available. As soon as you have to do a lot of simulating, such as cases where the value depends on something which is unknown so you do the simulation repeatedly for ranges of values, the cost of it can quickly get prohibitive if you're not careful.\n\nIn the situation we've been considering where you know the total number of items, simulating gets you very precise answers at little cost as long as you're not asking about extremely rare events (though you might be able to enumerate those and get them exactly). So the Normal approximation seems to me like more error than it's worth here. I personally turn first to simulation for problems like this and only to approximation if that is too slow/burdensome to work.\n\n> Using that knowledge, I could approximate that there's a minimum of at least 1,000,000 items bought upwards to maybe 2,000,000. Would the way of figuring out how normal this would be, be to simulate it? \n\nNow you're going from probability to statistics. \n\nSimulations from knowns (values of items, probabilities of getting each, independence per-item, number of items) to unknowns (probabilities of total values). Probability in general does the same, you ask about unknown outcomes of known data-generating processes. Statistics asks about the process of generating the observed data. It turns that on its head (which is why you need to know probability *before* doing statistics).\n\nThis is very much a regime where simulations can become incredibly burdensome. You can do statistics entirely with simulations, approximate Bayesian computation (ABC) comes to mind. But it can be more effort than it's worth, and with thousands or millions of items a Normal approximation is probably going to work pretty well.""]"
[q] How can all numbers show up with equal frequency?,"I once heard a statistician say that all numbers ultimately show up with equal frequency, and this is why he himself chooses to play lottery with numbers that have NOT been drawn as frequently, to increase his chances of winning. The logic behind this is: numbers that havent been drawn will show up next because all numbers are supposed to be equal. 

Ive never understood this and still dont. Ive tried googling this too, nothing. For the sake of argument, lets keep the lottery scenario as an example: how is it possible all numbers are supposed to show up in equal frequency? If true, Id imagine this would happen in an unlimited timeframe, so in this light, is the statistician really increasing his lottery winning chances by filling in least-frequently drawn numbers in hopes of them showing up next? 

This is lottery example so number 0 doesnt exist.

EDIT: 13h into the post and I'm finding myself somewhere between the paradox of
a) the Gambler's fallacy (belief that the next random event if more likely/less likely to occur due to past events)
b) law of large numbers (events balance out over large amount of trials/tests)
c) law of small numbers (cognitive bias referring to the tendency to draw broad conclusions based on small data)",11kjgye,tuesdaycocktail,1678148928.0,4,0.75,"[""In the long run ( thousands of drawings), each number should be drawn an approximately equal number of times... but that doesn't mean they will come up with exactly equal frequency.\n\nFurthermore, the statistician you mentioned is mistaken to believe that numbers that haven't been selected recently are more likely to come up.  That is an example of the gambler's fallacy (https://yourlogicalfallacyis.com/the-gamblers-fallacy)\n\nEach number has the same probability of being drawn at the beginning of each drawing."", 'Seems like a statistician of dubious quality if theyre making these kind of claims. This is a great example of the [gamblers fallacy](https://en.m.wikipedia.org/wiki/Gambler%27s_fallacy).', '> how is it possible all numbers are supposed to show up in equal frequency?\n\nThey aren\'t.  They only have equal *expectation* of showing up in upcoming draws.  The idea that a random process ""remembers what happened"" and doesn\'t do that in the future, is literally named *The Gambler\'s Fallacy*.  Your friend is incorrect.', ""> I once heard a statistician say that all numbers ultimately show up with equal frequency\n\nThis is a (bad) misunderstanding of the law of large numbers. Are you sure they were a *statistician*? (i.e. someone with an actual stats major?)\n\nI ask because there's pretty obvious reasons why this statement doesn't make sense. \n\n> this is why he himself chooses to play lottery with numbers that have NOT been drawn as frequently\n\nThis is straight-up Gambler's fallacy caused by that misunderstanding. I'm tempted to call bull. Either (a) this person isn't a statistician at all, or (b) they are one (somehow) but sound like a pretty poor statistician\\*, or (c) they once knew their stuff but have somehow forgotten it (*drugs are a terrible drug* so the wise man said) or (d) they're pulling your leg.\n\nhttps://en.wikipedia.org/wiki/Gambler's_fallacy\n\n---\n\n\n\\* I mean, getting the LLN and its implications wrong is one thing but how does a statistician fail to recognize plain old gambler's fallacy? It's not even slightly disguised. Someone who thought that the Gambler's fallacy was correct reasoning would send our clients broke. \n\n[For that matter, how does a statistician fail to recognize the problem with a regular bet which in just about every place I've heard of pays about 60 cents on the dollar? If you want to gamble, there's way better bets than that. Even within that style of lottery there's way better schemes than that.]"", ""Others have pointed out the problem in the strategy as you state it.  \nThere is some advantage in picking an unusual set of number though. Some combinations are very popular (e.g 1, 2, 3, 4, 5, 6) so a lot of people pick them. In the case that it's the winning draw, you're sharing your payout with a lot of people.\n\nSo it's possible to pick your numbers in order to increase your expected reward (by avoiding popular combinations) but not really possible to pick numbers to increase your chance of winning."", 'Is the statistician improving his chances of winning? Well 100*0 = 0, so no. \n\nHis premis is that the numbers all have the same chance of being drawn, meaning a ""uniform random"" distribution. So given a sufficiently large data set they should be drawn the same number of times - with some associated variance. So EVENTUALLY a given number WILL get drawn. \n\nAnother way to ask the question is if I flip a ""fair"" coin what will my next flip be? If we define the coin as ""fair"" (50/50), then this one flip will always have a 50/50 chance. But if I say: ""I flipped it 10 times and got 7 heads, therefore the next one is more likely to be tails"" because I expect it to even out eventually. \n\nI am very likely to be wrong on this one bet (nearly but not quite 50/50), but if I keep betting on the small number, I should win more often than not. \n\nKeep in mind - eventually- can be a Very Long Time. The more numbers/outcomes that are available, the smaller the chances of being right on any individual draw.  \n\nA weird thing about the lottery: in a choose 6 numbers between 1and 60 game:\n1,2,3,4,5,6\n55,56,57,58,59,60\n10,20,30,40,50,60\nAnd any other sequence are all equally likely as something like: \n7,13,26,45,56,59', 'He is not a very good statistician', ""The reasoning is backwards if the lottery uses actual physical balls, which I believe they do. Slight manufacturing defects could potentially make some balls in a set more likely than others to fall. \n\nBut if the lottery draws n=5 or 6 balls once or twice per week, it'll take many years before the posterior distribution isn't nearly equal to your prior."", 'He is wrong, example with a coin toss: imagine out of a 100 tosses you have 99 times head and 1 tail. So head 99% and tail 1%. Because the chance is 50/50 he then expects more tails. Now imagine that for all the next tosses you alternate between head and tails. After 100 tosses you have 549 head and 451 tail (54,9% and 45.1%) after 10000 tosses 5049 head and 4951 tail (50.49% and 49.51%) after 1 million its already 50.0049% and 49.9951%. And so on.\n\nSo for #tosses going to infinity you would expect ~50% heads and ~50% tails even though the difference in frequencies may be large in the beginning, after millions of tries these differences become relatively small.', 'Any chance he was being sarcastic?  This is about as far away from good stats  understanding as you can get.', 'All numbers do not show up with equal frequencies.    There are lots of domains where some physical/logical constraint creates non-uniform distributions of numbers.     There is a famous example of this in forensic accounting... benfords law if I recall? Bentham\'s? Bennyboob? B-something Law regarding the distribution of leading digits.  \n\nAlso, your statistician friend is either pulling your leg, or needs to revisit some fundamentals.   \n\nA statistician plays darts. The first dart veers wildly to the left.  The second dart veers wildly to the right.  The statistician exclaims, ""bullseye!""', '> In the long run ( thousands of drawings), each number should be drawn an approximately equal number of times\n\nThis is not so (at least not quite the way you said it, for all that you might have intended something else); choose two of the numbers (say 11 and 34 for example). Then the difference in the *number* of times those values appear will diverge as n goes to infinity.\n\nSpecifically let B11(s) be 1 if ""11"" appears in draw s (0 if not), and define B34(s) similarly. Now let   \nX11(t) = B11(1)+...+B11(t) and let X34(t) = B34(1)+...+B34(t) ...   \nso X11(t) and X34(t) are the number of times that those two numbers (11, 34) appear in the first t draws.\n\nNow as t grows large, the average value of |X11(t) - X34(t)| *grows* in proportion to t^(1/2). That is, the *number* of appearances actually diverges as t grows, heading off to infinity (albeit slowly).\n\nThe same applies for any such pair\n\nWhat converges is the *proportion* of times each one appears.\n\nLet P11(t) = X11(t)/t and similarly for P34(t)\n\nThen for large t, on average |P11(t) - P34(t)| decreases in proportion  to t^(-1/2) (i.e. it converges to 0, though again slowly).\n\nThe distinction between the proportions and the *raw* *numbers* is crucial, and it\'s the scaling by *t* that does it.', 'Also it ignores any problems with the drawing method that could favor certain numbers.', 'hmm interesting. this was a dinner conversation and i simply assumed he was right (i\'m not good in math/stats myself so didn\'t even know where to start with asking more questions), then came home and got curious cause i didn\'t understand and started googling... which leads me here. \n\none other thing he mentioned in why he chooses bigger numbers apparently is because most people choose birthdays etc as their \'lucky numbers\' which often limits them to 31 (max days of months), therefore has a smaller chance of sharing the wins if his line has bigger numbers +31. This made sense to me so didn\'t bring it up here (although not sure if that is statistically correct). It was this and the ""all numbers show up with equal frequency"" that his argument was based on and i didn\'t understand the latter. I\'m still confused about this, even after reading about gambler\'s fallacy, it feels trippy that there is no \'eventual balance\' of values - especially since if you look at lottery stats, all numbers do show up with an approximately equal frequency (in 7-digit lottery the frequency is around 15-19%) over history. If genuinely random frequency, wouldn\'t there be more deviation between the numbers?', '> But if I say: ""I flipped it 10 times and got 7 heads, therefore the next one is more likely to be tails"" because I expect it to even out eventually.\n\n> I am very likely to be wrong on this one bet (nearly but not quite 50/50), but if I keep betting on the small number, I should win more often than not. \n\nNo, the odds are still exactly 50/50. There is no advantage to predicting the next flip, regardless of what we know about the previous flips (assuming a fair coin).\n\nThe reason that things even out in the long run isn\'t because the sequence ""corrects itself"" in any way. It\'s because any initial sequence of abnormal activity gets washed out by the large number of flips that follow (eventually, as n -> inf).', ""you're talking about the law of large numbers right"", 'ah crap, thanks for this, i wish i was better in math/statistical thinking so i could understand what you\'re trying to explain, i still don\'t. This is what i\'m starting to form, please tell me if  understand the following correctly:   \n1. there is a difference between the \\_expectation\\_ vs actual \\_frequency\\_ of how often numbers are drawn   \n2. expectation of all numbers are equal statistically speaking, meaning each number 1-40 has the equal amount of probability to be drawn at the beginning of lottery  \n3. each lottery is a separate event and doesn\'t have ""memory"" of past events, which means the actual frequency of numbers being drawn is...completely random? if this is true, then numbers 1,2,3,4,5,6,7 (let\'s imagine it\'s a 7-number lottery) can be drawn 30 times on a row, correct?   \n4. if all points 1-3 are true, then how come lottery numbers do actually have a very similar frequency of being drawn if you look into their stats? in a 7-digit lottery of values 1-40, each number is historically being drawn approximately with a 15-19% frequency. If it was completely random, wouldn\'t deviation between each number be significantly bigger e.g number 3 is drawn with 5% frequency and number 28 with 30% frequency over history?   \n\n\npoints 3 and 4 mess with me the most.', ""sure thing. for the sake of understanding this statistically, i'm intentionally leaving out all the physics of the lottery machine, balls etc. in reality, many more things affect the numbers being drawn i'm sure."", '> one other thing he mentioned in why he chooses bigger numbers apparently is because most people choose birthdays etc as their \'lucky numbers\' which often limits them to 31 (max days of months), therefore has a smaller chance of sharing the wins if his line has bigger numbers +31\n\nThat strategy (choosing numbers other people don\'t) will certainly work (in that it should improve the average return per unit of money spent). It probably won\'t be enough to make it profitable (but it should be possible to estimate it if the lottery people publish information  about numbers drawn, prizes, and number of winners for each draw, as long as you have many years of data, so you should be able to see roughly how much gain there should be over the usual rate of loss).\n\n>  it feels trippy that there is no \'eventual balance\' of values - especially since if you look at lottery stats, all numbers do show up with an approximately equal frequency (in 7-digit lottery the frequency is around 15-19%)\n\nHold up there, you\'re misunderstanding.\n\n[Frequency](https://en.wikipedia.org/wiki/Frequency_%28statistics%29) means *counts* of occurrences (how frequent they were in the whole data), which *do not converge* toward equality (nothing makes them be more even) -- see my detailed explanation about why they actually diverge in response to another person\'s comment. But when you start quoting percentages you\'re talking about *proportions* which *do converge* toward equality. The difference between frequency (total count) and proportion is important.\n\nThere\'s a discussion [here](https://stats.stackexchange.com/a/136879/805) on the divergence in counts of heads vs tails in coin tossing at the same time as convergence of  proportion of heads vs tails -- with plots showing the traces of 1000 sets of simulated coin toss experiments (each for 1000 tosses).\n\nThe same thing happens with the lottery frequencies (diverge) vs the proportions (converge).\n\n\n> If genuinely random frequency,\n\nI don\'t know what you mean by ""genuinely random frequency,"" but I don\'t think I made any claim that this happens.', '>why he chooses bigger numbers apparently is because most people choose birthdays etc as their \'lucky numbers\' which often limits them to 31 (max days of months), therefore has a smaller chance of sharing the wins if his line has bigger numbers +31.\n\nThat part is actually sound advice. If you are going to play the lottery, at least choose numbers that don\'t follow a pattern that other people are likely to copy. It doesn\'t affect your chance of winning: Every combination is equally likely to be drawn (assuming there is no foul play or some peculiarity of the machine at play here). But if you manage to hit the winning combination, you have to share your winnings with the others who managed to hit it as well in most lotteries.\n\nSo the chance of winning stays the same. The payout if you do win, doesn\'t. However I would advise to choose the numbers randomly. Because ""numbers above 31, because many people use their birthday"" is also a pattern that some people may use.', "">It's because any initial sequence of abnormal activity gets washed out by the large number of flips that follow (eventually, as n -> inf).\n\nwhy is this? (i'm not good in stats so please dummy down for me)"", '>  i still don\'t.\n\nThe discussion was technical because I was trying to briefly explain it to the person answering you; the formal setup would allow us to discuss proof if it were needed.\n\nFor simpler explanation (and a simpler context), see the diagrams in the discussion I linked to in the comment just a few minutes ago in the other sub-thread.\n\n> there is a difference between the _expectation_ vs actual _frequency_ of how often numbers are drawn\n\nNaturally, due to random variation.\n\n>  expectation of all numbers are equal statistically speaking,\n\nYes, if the draw is fair (and they go to some effort to make sure it\'s as close as reasonably possible).\n\nThe average distance of each actual count from its expectation grows as the number of draws increases. However, the *proportions* of times they appear get closer to their expected values (because you\'re dividing by the number of draws and the divergence in counts is slow, only proportional to the square root of the number of draws; that makes the proportions converge to their expected values).\n\n> each lottery is a separate event and doesn\'t have ""memory"" of past events,\n\ncorrect\n\n> which means the actual frequency of numbers being drawn is...completely random?\n\nI don\'t know what this means. Can you clarify what you\'re saying there?\n\n>  if this is true, then numbers 1,2,3,4,5,6,7 (let\'s imagine it\'s a 7-number lottery) can be drawn 30 times on a row, correct?\n\nIt could ... but that would be astronomically unlikely.\n\n>  if all points 1-3 are true, then how come lottery numbers do actually have a very similar frequency of being drawn if you look into their stats?\n\nYou\'re conflating frequency (number of times each occurred) with proportion (fraction of times each occurred). Even though one is just a scaled version of the other, they behave very differently! (The reason being that the scaling amount grows as the number of draws grows)\n\nSee my discussion at the other comment.\n\n>  If it was completely random, wouldn\'t deviation between each number be significantly bigger e.g number 3 is drawn with 5% frequency and number 28 with 30% frequency over history? \n\nI don\'t know what you mean by \'completely random\'. But the issue here is the distinction between the frequency (counts) and the proportions (counts to date divided by total number of lottery draws); they\'re related but have different behavior in terms of convergence. The counts spread out more (around their expected value) as you draw more times, but the proportions squish in toward their expected values.', 'It\'s very simple: if you observe an ""abnormal"" sequence of length 10, that individual sequence will be completely inconsequential to the overall total after 1000 flips. An ""abnormal"" sequence of length 1000 will be inconsequential after 1000000000 flips. Basically, the amount of contribution from the abnormal parts of the sequence approaches 0 in the long run, as more and more flips are added, so the actual totals are dominated by ""usual"" behavior.']"
[Q] correlations with multiple entries per participant,,11ki9hh,majorcatlover,1678146132.0,10,0.92,"['I would include the multiple observations in my model in your cat example. Id call the owner or household a random variable.', ""Yes, it's a problem. What to do about it... I hope smarter people comment here...\n\nAs long as you know which participants responded, and how many times, I can think of two possibilities:\n\n1. Only  use each participant's first entry. Then you're good. Normal correlations (if the data supports it).\n2. Model everything as multilevel data, run a simple model, generate polychoric correlations or some other effect estimate."", 'How many multiples are we talking about here and do you have a way to identify them? \n\nIf it is small enough relative to your sample size, you might want to consider omitting them if you cannot run models controlling for the many to 1 relationships.', ""I see, but you wouldn't be able to run a simple correlation, you would have to model the nested nature of the data. That was what I was thinking."", 'I agree. The owner/household/resident food and scratch provider should be addressed as random effects.\n\n\nAlso, keep in mind that you can transform your data before applying the model. Not sure, if that would make sense in this case. Check your distributions, maybe pull a log or sqrt transformation.', ""Thank you! Is it possible to extract correlations from linear mixed models? I know linear mixed models using packages like lme4 provide correlations but I always ignore those unless they show evidence of problems, e.g., they are 1 and -1 or something. I know we can do it in bayesian by generating those measures, but I am not fully confident in my advanced bayesian stats knowledge as I don't use it often enough."", ""Yes, they are all identified, there's an identifier per participant and cat."", 'Exactly. Youll have to account for the multiple observations', 'You wouldnt get correlations. Youd get regression coefficients. Thats because a correlation, by definition, is the relationship between two variables. A regression equation usually contains more than one predictor, hence it does not capture a pure relationship. Rather, each effect is adjusted for others in the model.', ""I think you can extract those kinds of things from lme4, but it's been a bit since I used it, and I don't have time to go mess with it right now to find out. And I'm definitely not an authority on anything Bayesian."", 'Reading above is your data structured so that 1 owner can enter multiple cats. (1 row of data per owner multiple columns for multiple cats)? \n\nYou may need to restructure/ reformat your data so that each cat is 1 row of data and the owner ID is duplicated across the rows.', 'Thank you anyway!', ""That's what I have, the problem is analysing it since the rows are not truly independent since the cats share the same owner."", 'Got it. So there are tests like multi level modeling or within-subjects models that could be used but unless the owners have a huge sample of cats the model might violate that tests assumptions. Plus; you have a mixed bag of individual and dependent cats. So some of those models will not work without excluding some of the data. \n\nI would almost tweak the analysis and look if count of cats with same owner (partitioned by the owner) has a moderating relationship on the variables. If there doesnt appear to be an moderating impact, you would have a theoretical reason to violate the assumption. That is - if owning multiple cats doesnt influence the data, we can treat the cats as independent.', ""I see, with multilevel modeling you are referring to something like a linear mixed effects model where ID is a clustering variable? Is there a way of extracting the correlations from those? I know how to do it in bayesian, but not in frequentist like lme4. Maybe I shouldn't even care about the correlations and just run a multilevel regression instead.""]"
"[q] If I have yes/no/unknown survey, is it proper to recode unknown to missing?","A know its a dumb question. A little more detail, case-control study where one question has 8% unknown. 

What to use multiple imputation, if okay to recode to missing, as that 8% would remove to my strata if dropped.",11jyo1z,maka2250,1678108564.0,26,0.9,"['No, because you are blending two different things.', 'Its not missing data, though. They answered the question.', ""I've noticed a lot of questions recently about removing data or recoding data. The answer is just about always no. You can't remove data because you don't like it or because it makes your analysis more difficult."", 'It depends. You\'d have to give more details. Could you argue/prove ""unknown"" is missing at random? Would imputation give sufficiently good results? Would it make analyses unreasonably more complex to use the variable are categorical variables with three levels? Would it hurt to remove the 8%?', 'No ', 'If you are asking whether it\'s okay to leave out ""unknown"" respondents from your study, then the answer is yes, slightly dependent on the exact nature of the research question.\n\nIf you are asking whether it\'s okay to replace the missing values with something else, like say the mean etc., then please don\'t. It\'s not uncommon it\'s done sadly, including by people who should know better, but it makes little to no theoretical sense.', 'You have 4 values, y/n/unknown/null', 'What are your thoughts on handling the unknown then?', 'The first part: this is my opinion. That is, sometimes it makes sense. You need to be aware of the research domain, the larger context of how other researchers have approached this, and (especially) what your effective research question has now become, if you consider ""unknown"" to be missing. \n\nIn some situations, that will badly distort your findings or not respect what you\'re actually studying. In other situations, that might not be the case at all.', 'It is a nominal variable with 3 levels.', 'Thanks.', 'This is the way', ""If you want to do something less advanced it would also be legit to do a logit model where you're predicting whether people answered yes, vs no/unknown.""]"
[Q] Need help w/ an odds question,"Let's suppose that for each iteration of a dice roll, there's a 40% chance of rolling X (this is a hypothetical dice - maybe a d10 - where it doesn't really matter what X is).  

What are the odds that after 35 rolls, X actually appears 80% of the time rather than 40%?  Also, what is the formula that calculates that so I can try to figure this out for myself in the future?  I've got a decent background in stats but I can't figure out how to math this one.",11jslji,KungFooShus,1678090017.0,3,0.81,"[""You can model this as a biased coin, 40% heads and 60% tails.\n\nYou can use the normal approximation to the binomial distribution so:\n\nvar(p) = p(1-p)/n\n\n= 0.4(0.6)/35\n\n= 0.00686\n\nand the standard error is the square root of that, so:\n\nse(p) =0.08281\n\nIf the observed value for p is 0.8 with a true underlying value of 0.4,\n\nz=(0.8-0.4)/se(p)\n\nYou should be able to take it from there.\n\nBear in mind that if this question has been prompted by observing an extreme value once out of many tries, the probability of this occurring isn't very interesting. If you give rare things enough chances to happen, they will happen. If, on the other hand, you said that someone was cheating and tested it by observing their next set of dice rolls coming up X 80% of the time, that would be compelling evidence that they were cheating (absent any other explanation)."", 'This is distributed Binomial with probability of success p=0.4. So P(Y=28) [because you want a 80% of the rolls out of 35 to be a success: 0.8*35=28 successes]\nP(Y=28) = (35 choose 28)* [(0.4)^28]* [(0.6)^7]= 0.00000135643\n\nPlease correct me if Im wrong. The formula is just the PDF of the binomial distribution. I gave the probability that you roll an X 80% of the time in 35 rolls with the probability of rolling an X being 0.4.', 'Thank you very much - I appreciate it.', ""I thought I had a general feel for statistics, but clearly I was mistaken. I guess we'd put me about high school level maybe. I will definitely take your word on this one"", 'So, just plugging in your number, the odds of this happening are about 1 in 735k, correct?', 'This is a probability problem. Youre given information about the population: that the number you want to roll (X) has a probability of showing up 40% of the time. This tells you that if you roll this dice n times, as n approaches infinity, the probability of rolling X asymptotically approaches 0.4. You then asked about a sample from that population (what are the chances I roll that number 28 times if I roll the dice 35 times? {0.8*35=28}). Here the sample size is 35. In probability, were given information about the population, and we want to answer something about a sample. In statistics, youre given a sample, and want to make statements on the population. For example, you can test a hypothesis of whether or not a value X bar is a good value to represent the mean of the entire population. I hope this makes sense', 'Yes. I also typed .828 on accident. I meant to write 0.8*35=28 successes.']"
[Q] Which statistical test to compare differences in time series data?,"Hello, I am interested in testing whether the difference between monthly values in two financial market indices is statistically significant. I have only taken a few statistics courses and would greatly appreciate some guidance.

I have 452 months of paired values and the differences are close to being Normally distributed. The distributions of the values themselves are unimodal and right-skewed. I initially thought that a paired t-test might work, but the monthly data points might not be independent (testing for autocorrelation?).

Thanks.",11jgci3,andrewscheernutsack,1678056144.0,3,1.0,"[""You're going to need some form of model for the observations in order to make any progress. I agree that dependence is very likely to be present (though it's hard to say much more with almost no context); expertise in the specific area may well be important to coming up with reasonable models."", 'If you have censoring (drop outs) then you may have to do use an exponential or Weibull distribution to do a log rank test.', 'if youre interested only in the main effect of the two financial index differences, you may try fixing time using fixed effects linear modelling (check out the felm or plm package in R) to control for time invariant effects', 'Survival analysis methods such as log-rank test or cox proportional hazards regression models', 'No need for censoring for a log rank test.']"
"[Q] About to get a bachelors in MIS with only intro stats taken, but figure I’d like a MS in Statistics because it is more interesting. How do I prove to graduate admissions that I am competent with prerequisites? Would I need to spend more tuition money for formal statistics + math college classes?","Also, in case you may ask, how do I find statistics more interesting if Ive only taken introductory statistics? 
I believe I got discouraged from statistical coursework due to a bunch of formulas and Greek letters that intimidated me, even though I previously took calculus. It wasnt until much later in my academic study where I talked to an alumni that works with data analytics a lot in higher education that suggested I may have learned statistics poorly, where if I had first learned conceptually such as statistical intuition, I would have appreciated the field more. As I am working on data analytics projects to put on my GitHub profile, I found my interests to increase in statistics as a way to view the world rather than just a means to getting an entry level analyst job, such as consuming news or scientific articles and questioning their findings based on intuition on data sets, how they collected data, presence of holes in their data, statistical significance, etc.

Edit: with my recent luck posting questions in subreddits it seems Ive appeared to be asking dumb or annoying questions, but before you downvote, please understand I did my best to look through this subreddit and even other subreddits for someone else asking my exact question.",11jbyxy,Pere_X,1678047589.0,1,0.67,"['>I believe I got discouraged from statistical coursework due to a bunch of formulas and Greek letters that intimidated me, even though I previously took calculus.\n\nThe highest ranked statistics programs require a good understanding of theory.  Dong well in calc 1-3 and linear algebra hold more weight in admissions than doing well in basic statistics and analytics classes.  There are some programs that have de-emphasized math skills, mostly down further in the US News rankings.  That does not mean that you cannot get a good education from those programs, just letting you know that the top statistics programs are probably not what you are interested in.  Have you thought about a degree in data science or analytics?  It sounds more in-line with what you are interested in.', 'Thank you for your reply. I have considered a masters in data science and analytics, I just put Statistics as the preferred after reading older posts on this subreddit about statistics being a better degree.\nUpon further reflection I probably can see myself going for an Analytics Manager route than a data scientist due to the disparity in math level and the business aspect of my undergrad.']"
[Q] Should I keep the outliers in my dataset?,"I'm using a multiple linear regression model with three predictors and a continuous outcome. When I run the model with the complete dataset, I see no significance, however, when I remove the outliers (+/- 2 standard deviations of the mean) I see that predictor #1 now shows significance. I have a feeling this is a false positive and the outliers should actually be included in the model, or is there maybe a different way I should detect outliers in this problem?",11j6vgr,snacks_in_my_pocket,1678041101.0,2,0.75,"[""I don't know the context, but usually I'd say that outliers are expected to exist and we should keep them.\n\nI'd only exclude outliers when I have a good reason to do so, not because of the way they are impacting significance. For example, if the outliers don't make sense, maybe there was a measurement error."", 'if something was procedurally off for that replicate, the data for that whole replicate have to be removed and redone.  If its a real result though, the data (all) have to stay in as far as I know.  If one datapoint changes the answer, people where I work say that means either not enough data were collected, or the wrong analysis was used to evaluate the data.', 'Unless you know theyre bad data reports, keep them. Just try using models that are robust to it.', ""You need to keep them unless you think that they are result of a poor measurement or some kind of error. It's typical for a lot of data sets to contain outliers that are meaningful data that tell you something useful about what's going on.\n\nJust as an example, say that you're working with some economic data relating to income. Maybe most people are within two standard deviations of the mean income but some people are Bezos, Gates, etc. Even though those are outliers, they are really important data and any economic model that doesn't account for the fact that some people are really, really rich is not going to be a good model.\n\nYou can't start removing things from your data set to try to make a parameter seem significant. If it isn't significant, then it isn't significant. The model that you're using may not be the best way to represent this data."", 'Hello, maybe try using a robust regression with t distributed resiudals to account for outliers.\n\nHere is a guide in python: https://austinrochford.com/posts/2015-03-08-robust-regression-t-residuals.html', ""Report both sets of results, and feel free to discuss whether you think one deserves more attention than the other.\n\nIt's hard for readers to evaluate statistical results without information on how the results were arrived at, how robust they are, etc. Giving both sets of results helps, even if you just mention one in the footnotes or appendix."", 'IMO, you should definitely remove outliers, but +/- 2 standard deviations from the mean is not a reliable outlier screening at all.  At the very least, the criteria should be based on residuals rather than values of the response variable.', ""I don't remove outliers in general, only when there is a reason. Like maybe a value is always between \\[90, 100\\] and now it suddenly is 0 cause of a performance issue in software."", 'Outliers are not a problem perse.  Influential cases however can absolutely warp the results. \n\nFor my thesis in a sample of 165, I had two cases that completely changed my results.  And going to source on them, was found the data was really anomalous, as it was verbal data from two students with limited English skills. \n\nI would see if first if they are influential cases, then look over the data closely to determine if, one the results are accurate (sometimes miskeys sneak through) and two, do they make sense?  If something is really funky, I would remove them.  What I did was reported the results with all my cases, and then without the two problematic ones I found, and small section in the text on why I thought they were problematic.\n\n///\n\n\nEdit:  As other mentioned mentioned below, also look into robust regression.  Was off the table for my thesis, as it was a different chapter for a larger work, but it will solve your outlier problem.', 'Yes, but justify why you kept them in.', 'Depend on', 'There is still value in running the tests excluding the outiliers so that you can say ""X hand a significant impact on Y when outliers were disregarded."" Depending on the reasoning for the outliers,  this could still be an important claim to be able to make.', ""Thanks! I'll keep 'em."", 'Thank you! Seems reasonable.', 'I generally prefer not removing outliers but if you do  maybe try  +/- 3 IQR from the median', ""Removing outliers this way is not a good idea.  What if the extremely high values are well-described by the predictor variables?  You're fitting a model to measure causes of high blood sugar, you screen out the highest blood sugar results, which are linked to characteristics (undiagnosed diabetics) well-measured by the variables, and you miss out on critical insights.  \n\nAt the very least, you should identify outliers by the residuals (high blood sugar that's not explained by any of the predictor variables).""]"
[Q] Publication reviewer wants me to run a post hoc power analysis. How should I proceed?,"Hi all, 

I am publishing my MA psychology thesis in a peer reviewed journal. Part of the revisions requested was to run a post hoc power analysis to determine if I had enough power to justify the number of hierarchical linear regressions that I conducted. From what I understand, post hoc power analyses are a bit rubbish? Can someone please explain why they are rubbish; and if I was to do the post hoc analysis, would that be possible in SPSS? 

Thank you.",11j17hq,TheEudaimonicSelf,1678033655.0,70,0.97,"[""These references show why they are rubbish and the reviewer will almost certainly back down if you send the references to them, perhaps with a few relevant quotes.\n\nHoenig, John M. and Heisey, Dennis M. (2001), The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis The American Statistician 55:19-24. DOI:10.1198/000313001300339897\n\nLevine M, Ensom MH (2001) Post hoc power analysis: an idea whose time has passed? Pharmacotherapy 21:405-409 DOI: 10.1592/phco.21.5.405.34503\n\nGoodman SN, Berlin JA. (1994) \nThe use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. Ann Intern Med 121:200-6. doi:10.7326/0003-4819-121-3-199408010-00008 (Erratum in: Ann Intern Med 122:478. doi:10.7326/0003-4819-122-6-199503150-00029)\n\nThomas L (1997) Retrospective power analysis. Conservation Biology 11:276-280. DOI: 10.1046/j.1523-1739.1997.96102.x\n\nYuan K-H, Maxwell S (2005) On the post hoc power in testing mean differences. Journal of Educational and Behavioral Statistics 30:141-167. DOI:10.3102/10769986030002141\n\nWalters SJ (2008) Consultants' forum: should post hoc sample size calculations be done? Pharm Stat 8:163-169 DOI: 10.1002/pst.334\n\nMiller, J. (n.d). What is the probability of replicating a statistically significant effect?. Psychonomic Bulletin & Review, 16(4), 617-640.\n\nGreenland, S. (2012). Nonsignificance plus high power does not imply support for the null over the alternative. Annals of Epidemiology, 22(5), 364368.\n\nWilkinson, L., & Task Force on Statistical Inference, American Psychological Association, Science Directorate. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594604. https://doi.org/10.1037/0003-066X.54.8.594\n\nHow post-hoc power calculation is like a shit sandwich | Statistical Modeling, Causal Inference, and Social Science [Article here](https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/)"", ""I would do the analysis since the software is free, but then include a wall of text with citations about why it is not good practice to do this in a post hoc manner and you would greatly prefer to not have this as part of your manuscript as such and merely are providing the analysis for curiosity's sake.  \n\nShould be able to do this with G*power, and if not, R.  Both are free.\n\nHold your sources incase you have to reply to the editor about the post hoc analysis.  Frankly, the reviewer should have been able to do this themselves as well if they cared."", 'You are right about it being a bit rubbish. Some view it as a method of finding the power necessary to find statistically significant differences for your initial hypothesis. But applying this after your data has been seen and analyzed removes the randomness the analysis relies on. Since the goal is to find the likelihood of finding a statistical difference this is meaningless after the fact. \n\nIf you want to do more reading on it Id recommend: https://gpsych.bmj.com/content/32/4/e100069, and its first four references.', 'Additionally, if I do not run a post hoc power analysis, what other ways, if any, are there to justify the number of hierarchical linear regressions that I conducted?', 'the reviewer is not totally off in its request. I mean post hoc power analysis is not the best practice, but, I think that the reviewer is concerned with the fact that you fitted several models on the same data set which is ill advised. Discarding post hoc power analysis will not solve the issue here. You should check how to support what you have done. May be apply some type of correction for multiple comparisons?', ""Use G*power, it's free and specialized on power"", 'Are you sure this post hoc? I thnk they want you do the analysis.  They might want you do more experiments based on the analysis.', 'There are many rubbish stuffs that are asked by reviewers. There are things that are very rooted in some journals and/or specific fields and take time to update sort of ""tradition"" about what people expect to find in a given paper. Talk with your supervisor I\'m sure he/she knows how to deal with that', 'I am saving this post just for this answer. Thanks for sharing', 'This is amazing, thank you. I guess a follow up question to this would be: What does this say about the quality of my study given that I did not do power analyses prior to collecting data/conducting stats? Is it still publishable, despite the lack of initial power analyses?', 'How post hoc power analyses are like a shit sandwich lmao', 'This is a great suggestion. I plan to do just this. I conducted 5 seperate hierarchical models, each with 2 to 3 variables. Is it possible to calcualte power in Gpower for the \\*number\\* of hierarchical models I used. Or is it only possible to calculate power for the number of variable per model? I am a bit confused on this.', ""Power analysis is more necessary when you are conducting an experiment and, for instance, you need to figure out how many people you have to survey for a survey experiment.\n\nOn the issue the reviewer says, I think you need to go to Gelman and Hill, the book on Multilevel Modeling. They have some parts you can cite about what N you need for hierarchical modeling and how many hierarchies. The book also has a section on power analysis; maybe the reviewer got the idea from there? It's in one of the last chapters.\n\nIs the problem with the reviewer that you have way too many models on the same data? Or that you have too many hierarchies in the same model?"", 'It depends on what your purpose was. Fitting several models is fine if you are doing causal modelling. You typically fit several models and find the ones that best explain the data. \n\nIf you are doing hypothesis testing, then you should consider adjusting the significance level to maintain a good family-wise error rate. There are several methods to do that. I would see if you could find any other papers that do that with your linear model technique.', ""> the fact that you fitted several models on the same data set which is ill advised\n\nIt's very useful if you have no prior reason to discard the other models. You should use it as an estimate for the systematic uncertainty."", 'Yes!', 'Thanks lol, great advice.', 'The main purpose of a power analysis is to keep you from doing a costly experiment which has little chance of being conclusive even if your expected effects are real. It doesnt reflect on the validity of your results. Nonetheless, its true that if most submissions to a journal are either very low power or have true null hypotheses, the number of significant low power articles may not be that different from the number of articles with type I errors. However, increasing the number of low power experiments without increasing the number of articles with type I errors obviously lowers the type I error rate in the journal.', ""My apologies for the late response. The problem with the reviewer is that I ran 5 seperate hierarchical models on the same data. There are only 2 variables within each hierarchical model, with the exception of one model that has 3 variables. \n\nI don't even know how I would calculate post hoc power in GPower for the \\*number\\* of hierarcical regressions I conducted, if that is even possible.""]"
[Q] Movement Ecology: How to determine which Hidden Markov Model?,"I am modeling the movement of an animal using hidden Markov Models (HMM). The animal is in one of two modes (traveling and foraging). In each mode, I have a HMM consisting of two states, parameters generated from moveHMM in R using the movement step length and turn angles. The model fits well, and I can use it to generate movements.

Now, I want to observe the animal moving and then determine what mode they are in. I have a sample of, say, 100 steps, and I can compute step length and turn angles for that sample. How do I decide that it fits HMM1 (foraging) or HMM2 (traveling)? The way I have been doing it is to take the cumulative distribution function of step length and comparing it to the CDF of the different HMMs, using either Wasserstein (earth movers distance) or Kolmogorov-Smirnov. It works ok, but of course I'm completely ignoring turn angle. I could do the same with turn angle. But how do I calculate the joint distance, incorporating both step length and turn angle?

(Here's a youtube video discussing the topic in general: [https://www.youtube.com/watch?v=WELTpbB5BuU](https://www.youtube.com/watch?v=WELTpbB5BuU))",11j0nqr,beezlebub33,1678032421.0,2,0.75,"[""Based on your description, this sounds to me like an [xy problem](https://xyproblem.info/).  Why can't you build one HMM where your hidden states are traveling and foraging, since those seem to be of primary interest to your research?""]"
[Q] Mean Residual Life,"I see that the definition of the Mean Residual Life is the integral of a Survival function and then divided by another Survival function.  I understand the denominator, but why is the numerator the way it is? Why not the standard integral for continuous pdf?",11ixgwe,sonicking12,1678023814.0,2,0.63,"[""Are you asking why write  **^() S(x) dx / S(t)  \nrather than **^() (x-t) f(x) dx / S(t) ?\n\nIf so, that's because they're equivalent expressions; it's an\nidentity sometimes called the Darth Vader rule. Its proof is fairly a standard undergrad probability exercise\non expectation for non-negative random variables (where here (x-t) is the non-negative quantity)\n\nThere's a proof here: \nhttps://math.stackexchange.com/a/4000018\n\nor here: https://thirdorderscientist.org/homoclinic-orbit/2013/6/25/the-darth-vader-rule-mdash-or-computing-expectations-using-survival-functions\n\nAlso see:\n\n https://www.researchgate.net/publication/234465628_The_darth_vader_rule\n\n(This may have been the first use of this name for it but the result is considerably older.)"", ""Because it's conditional on X reaching a certain value. The denominator you are seeing is the conditional event."", 'Thanks!  This is what I was looking for!', 'Is there a similar trick for the variance?', 'Can you elaborate???', 'x-t =  int_{t}^{x} du    change order of integration', ""Interesting question. Not that I've seen but maybe there's a simplification there of some sort."", 'One more follow-up:  do you know whether there is generic way to draw from the Residual Life Distribution, without deriving its closed-form first?  My way of deriving the closed-from of the Residual Life Distribution is to use the ratio of Survival functions, S(t | t > s) = S(t+s)/S(s), then differentiate with respect to t.  I really like to know if there is generic way to draw from it with just the unconditional lifetime distribution.  Please let me know.', '>  do you know whether there is generic way to draw from the Residual Life Distribution\n\nIf you know S(t|t>s)\n\nthen let g(t) = S(t|t>s).\n\nGenerate T = g^(-1)(1-U) where U is standard uniform (if g is such that g^(-1)(0) is finite then you don\'t need to flip U (i.e. you could use U directly rather than subtract from 1)\n\nThen I believe T should be a random draw from the conditional distribution of t given t>s.\n\nIt\'s important to be careful to distinguish between t and t-s though; if you want to know about the distribution of ""years of lifetime left when I\'m s years old"" you\'re looking at t-s, not t. If you want to know about the distribution of age at death given I\'m s years old"" then you\'re looking at t, not t-s.\n\nThis will affect what you do: if you\'re generating from that conditional survival function, you may need to subtract s (depending on which quantity you were after) -- but in many cases you\'ll actually find yourself generating not T|T>s but  T-s|T>s  in which case you won\'t subtract s (it\'s already subtracted); but you might then need to *add* s if you want that conditional age at death\n\nYou\'ll see exactly what you\'re doing, because if your generated values start at 0 you\'re looking at time left, if they start at s you\'re looking at age at death (total life).']"
"[D] Need RCTs or Observational studies that explicitly mention ""statistically significant but not clinically significant/meaningful"" to dispel a misunderstanding","I  am having an argument with my dad, who is a clinician. I said  interpreting results solely based on statistical significance is  unwarranted because with enough sample size, anything will become  statistically significant. I have shown him paper after paper explaining  the difference as well as a systematic review actively utilising the  concept. He remains obstinent and continues to argue uncharitably.  Anyway, his current requirement is for **primary studies** that have  explicitly utilised the concept within their study design and reported  it in that manner.

Does anyone have any examples?",11in628,AstralWolfer,1677989629.0,36,0.95,"['https://pubmed.ncbi.nlm.nih.gov/10485679/ is an example. There\'s certainly more recent studies to be found though, this was just the first I found on a quick search.\n\nThe effects are all (very!) statistically significant:\n\n> mean annual rate of episodes of acute otitis media, 1.4 vs 2.1 (P<.001)\n\nBut the conclusion is essentially negative, because in the context of the side effects and costs, averting half an episode a year is not ""clinically significant"":\n\n> Our study showed limited and short-term efficacy of both adenoidectomy and adenotonsillectomy; given the risks, morbidity, and costs of these procedures, these data suggest that neither operation should ordinarily be considered as a first surgical intervention in children whose only indication is recurrent acute otitis media.\n\nAs a clinician, I\'m surprised by your father\'s position. Clinical vs statistical significance is something I hear very often from other clinicians. If the pharma rep comes to peddle a drug costing $100,000 with NNT 5,000 (odds ratio 0.98, 95% CI 0.970.99, ""it\'s statistically significant!""), you\'d tell them to fuck right off.', ""Not sure if this tackles your request, but this [article](https://www.jto.org/article/S1556-0864(20)30477-9/fulltext) has some interesting content and examples of statistically significant vs clinically significant.\n\nI like that the approach of the author includes two sides of the coin. When the statistical design of the trial is sort-of design to detect modest effects, and another one when a stringent threshold failed to capture a clinically significant effect.\n\nThere's a bunch more out there I am sure."", 'being a clinician as well as a PhD in Epidemiology and Biostatistics, I would say that you are on the right path. Clinically meaningful difference is a very nuanced concept. We use it all the time.\n\n""I would change your statement, ""I said, interpreting results solely based on statistical significance unwarranted because, with enough sample size, anything will become statistically significant"" to, ""IF THERE IS A TRUE DIFFERENCE, No matter how small, with enough sample it will become statically significant.""  \n\n\nClinically meaningful is a convergence of opinions of many physicians and patients in the area of interest./disease so it will be hard to find primary evidence in quantitative research. You might want to look for qualitative research. It will likely be interviews of multiple providers arguing on what is ""clinically meaningful"".. and then a mixed methods study that leads into quantitative analysis to prove if such clinically meaningful difference exist between treatment 1 vs treatment 2.', ""I feel like it's going to be a publication bias in observational studies. Most wouldn't focus on something that isn't clinically meaningful. Whereas RCTs likely wouldn't power off the primary aim with and effect size that wasn't clinically significant. So you probably wouldn't have a situation where you are detecting a clinically insignificant small effect by design. I'd say maybe it was a poorly designed study if that happens to a primary aim in many cases as you might have wasted a lot of time and resources.  \n\nIt definitely comes across in projects I work on a lot (non RCT), where age or length of stay or costs might be significant with a tiny effect size due to large sample size. We just don't really touch on it in the discussion usually because it's not a finding worth talking about if it adds nothing to the literature."", 'This [xkcd](https://xkcd.com/882/) might be enough to convey the message.', 'If you have a large sample size, you could find a statistically significant difference between BPs of 120/70 and 119/71 just because you have enough patients in your study. With enough people, you should have p < 0.000001. Ask him if that difference is clinically notable? Most clinicians I work with seem to get this concept pretty easily, so it may not be worth your time.', 'I would say, in practice, it is relatively rare to see clinical trial results that are ""statistically significant but not clinically significant"". There might be a high profile example somewhere I\'m not aware of, but overall it\'s rare for the following reasons:\n\n(i) Regulators give sponsors some guidance on what a needed effect size would be to gain approval, and that effect size would normally imply a certain level of clinical significance. As a rule, regulators do not tend to approve drugs that have unclear benefit (although some very controversial exceptions have occurred, perhaps most notably aduhelm). But even in the case of adulhelm the study was nowhere near meeting it\'s primary endpoint. Part of the controversy was that it was approved based on post-hoc analysis in a certain subgroup \\*after\\* the study was discontinued, ironically, over lack of efficacy.\n\nEvery drug has side effects. So no regulator in the world is going to approve a drug with a 1% effect size, even with a very ""good"" safety profile. Good safety profile isn\'t a perfect safety profile. So clearly the risk benefit still probably would not be favorable for a 1% clinical effect. But at 10%, 20%, etc. you can see how you might run into some gray areas. It is very standard  in an end of phase 2 meeting with FDA/EMA etc. to get written agreement from a regulator what the minimum effect they would want to see is to support approval.\n\nIf you don\'t hit that effect size then you normally aren\'t going to gain marketing authorization anyway, regardless of statistical significance.\n\n(ia) Even if you did get a regulator to approve you, nobody would pay you for the drug anyway. Ordinarily to get reimbursement you need to show that the benefit conferred by your treatment is cost effective. In most of the world, that is the state healthcare system. Although the system works differently in the US you still need to conduct similar studies to get insurance to reimburse it.\n\nSo good luck with demonstrating that cost-benefit if your effect size is that small.\n\n(ii) Clinical trials are expensive for sponsors and generally not powered to detect a ridiculously small effect size that would not have clinical meaning. Normally if the effect size is truly that small, the sponsor would not invest enough money to detect it.\n\nGranted, you could have a case where you estimated (spuriously) a larger effect size in phase II and then estimated a smaller one in phase III. Possible, but if you still don\'t clear the bar the regulators set for you, you are up the creek without additional studies.\n\n(iii) Clinical endpoints are rarely completely, 100% objective. And it\'s very rare that a singular clinical endpoint is the only way to establish clinical benefit in a given field/indication. Anatomical/physiological endpoints are sometimes close to 100% objective, but these are not usually used to establish clinical significance.\n\nSo even if you measured a 1% effect in some given clinical endpoint, it would be extremely unlikely you would measure the exact same effect in a different, but mot often valid, other endpoint.\n\nAn example of this is progression free survival vs overall survival in oncology. A drug with a 5% increase in PFS would be \\*extremely\\* unlikely to confer an OS benefit, and so would never get approved in the first place.', 'Not sure how statistically literate your dad is, but Ive been working with Antonio Morgan Lopez and some others on updating the 1991 old Jacobson and Truax paper on clinically significant change within patients over time. The rhetoric in JT1991 might help explain more than anything, but also we have done some comparisons of statistical significance to clinical significance using our new models. Results are pretty illustrative.\n\nOriginal Jacobson and Truax paper:\nhttps://psycnet.apa.org/record/1991-16094-001?doi=1\n\nRecent work by AML:\nhttps://doi.org/10.1016/j.beth.2022.04.007\n\nhttps://doi.org/10.1002/mpr.1906', 'This article seems to have some good information on the topic:\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4197528/', ""> There's certainly more recent studies to be found though, this was just the first I found on a quick search.\n\nCould you tell me what terms you searched for, and on which platform? I would like to be able to gather more evidence!\n\n>As a clinician, I'm surprised by your father's position. Clinical vs statistical significance is something I hear very often from other clinicians.\n\nI cross-posted this on r/medicine. You can take a look at the top responses of the thread here, its not very encouraging: [https://www.reddit.com/r/medicine/comments/11in7r3/need\\_rcts\\_or\\_observational\\_studies\\_that/](https://www.reddit.com/r/medicine/comments/11in7r3/need_rcts_or_observational_studies_that/)\n\n(of course! there could be a chance Im totally wrong, or misunderstanding something simple. In that case, Im happy to learn or be corrected!)"", ""A closing parenthesis in the link gets interpreted as part of the markdown. For anyone who wants to follow it, the link is:\n\nhttps://www.jto.org/article/S1556-0864(20)30477-9/fulltext\n\nTo make it show up, you'd need to as a slash as an escape character: [Like this](https://www.jto.org/article/S1556-0864(20\\)30477-9/fulltext)\n\n`[Like this](https://www.jto.org/article/S1556-0864(20\\)30477-9/fulltext)`"", '>""I would change your statement, ""I said, interpreting results solely based on statistical significance unwarranted because, with enough sample size, anything will become statistically significant"" to, ""IF THERE IS A TRUE DIFFERENCE, No matter how small, with enough sample it will become statically significant.""\n\nThank you, this was a misunderstanding on my part earlier', ""I've designed several RCTs and agree with your comments. I normally would identify the minimum clinical significance and adjust the statistical properties so that the effect size corresponding to the boundary where the p-value would be exactly =0.05 would be clinically meaningful. Otherwise, we would need to be prepared for the outcome where OP describes, which would be statistically significant and not clinically meaningful. I can't imagine getting such a trial funded if such an outcome was possible..."", "">Whereas RCTs likely wouldn't power off the primary aim with and effect size that wasn't clinically significant\n\nI understand, but for an RCT like that, if the final results have an effect size that was *less* than the estimated minimum/clinically significant effect size used for the power analysis, that would be an example of *statistically insignificant* but *clinicically significant* right? Are you aware of studies that explicitly mention it like that"", 'I have done all of the theoretical explanations and thought experiments, he says if I was correct there would be evidence in terms of papers that back up my claim. However, he rejects textbooks, commentary papers, and even meta analyses that present this information. He wants primary studies (which I think is a meaningless/absurd request - there is no relevant difference between these primary and secondary media)', '>Could you tell me what terms you searched for, and on which platform? I would like to be able to gather more evidence!\n\nI went looking for systematic reviews on clinical vs statistical significance then went looking through the articles they reviewed favourably.\n\nOver in /r/medicine they made the reasonable comment that big pharma-sponsored trials might not be explicit about clinically insignificant results. You could try searching on Twitter (#MedTwitter), they are usually a bit more merciless there about dodgy results.\n\nOncology might be a good field to look. Contrary to the stereotype, the good oncologists tend to be pretty against making patients miserable to achieve nominal gains in survival.\n\n>I cross-posted this on r/medicine. You can take a look at the top responses of the thread here, its not very encouraging: \n\n:(\n\nI suppose ""anything"" was a bit vague, obviously you meant ""arbitrarily small effects"". I don\'t think it was worth making a fuss about.', ""Here's a sneak peek of /r/medicine using the [top posts](https://np.reddit.com/r/medicine/top/?sort=top&t=year) of the year!\n\n\\#1: [Pharmacist here - I posted several months back about starting a pharmacy geared toward lowering the cost of meds by filling just generics and pricing them just above my cost + a dispensing fee. Many showed interest. We are up and running in Virginia. AMA](https://np.reddit.com/r/medicine/comments/um6d5p/pharmacist_here_i_posted_several_months_back/)  \n\\#2: [Michigan Medical Students walk out of their White Coat Ceremony to protest speaker who has fought against a womans right to reproductive health care.](https://np.reddit.com/r/medicine/comments/w7sumj/michigan_medical_students_walk_out_of_their_white/)  \n\\#3: [Leaving medicine for good and leaving the country.](https://np.reddit.com/r/medicine/comments/yooyoy/leaving_medicine_for_good_and_leaving_the_country/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)"", ""Sounds like you have a dad problem not a stats problem.\n\nAnyone who understands statistical significance can see that given a large enough sample size, you can confidently detect things that are of no practical significance.\n\nThe fact he won't listen to any explanation and demands more and more ridiculous evidence says he's not interested in learning this, he's interested in not being wrong in an argument with his kid."", '>obviously you meant ""arbitrarily small effects\n\nWould it be wrong for me to say that, if I could arbitrarily increase my sample size, I would be able to find a statistically significant difference between 2 randomly sampled and assigned groups taking the same placebo pill?', ""Well if we're going down that rabbit hole, you wouldn't have to increase your sample size to do that, just run your experiment 20 times!\n\nI don't think your example is the best example, it feels more like it's about p hacking/type I error than clinical significance. In the placebo vs placebo case, I would expect you to find a difference 5% of the time, regardless of your sample size.\n\nI would speak of clinical significance when there is in fact a difference in the population (not merely due to sampling variation), but the difference is not meaningful.\n\nIn the placebo vs placebo case, one imagines there is no difference in the (counterfactual) infinite population of possible placebo-takers, and if you detected a difference it would only be due to sampling variation.\n\nWhat is true in your placebo vs placebo case is that, as you increase the sample size (and continue p hacking by repeating the experiment 20 times), you expect the spurious differences that you purport to detect to become arbitrarily small."", 'Ah, I see your point. What about this then - I randomly sample and assign a group of 2 million people into 2 placebo groups. Would you be comfortable in saying then that  I would observe a very trivial statistically significant difference, purely due to the enormous power? This way, I perform the experiment only once, but with a ridiculous sample size', ""I see where you are coming from, but I'm not convinced  I would still expect you to find statistical significance in only about 5% of experiments.\n\nYes, your study with N = 2 million will be powered to detect very small differences. But with N = 2 million, you will expect your random partition to produce only small inter-group differences in the first place.\n\nWe can confirm this by simulation:\n\n```\nimport numpy as np\nfrom scipy import stats\n\nnp.random.seed(0)\n\nnum_significant = 0\n\n# Repeat the experiment 100 times\nfor _ in range(100):\n\t# Randomly sample 2 million people\n\tparticipants = stats.norm().rvs(2_000_000)\n\t\n\t# Randomly partition into 2\n\tnp.random.shuffle(participants)\n\tgroup1 = participants[:1_000_000]\n\tgroup2 = participants[1_000_000:]\n\t\n\t# Do a t test\n\t_, pvalue = stats.ttest_ind(group1, group2)\n\t\n\t# Was it significant?\n\tif pvalue < 0.05:\n\t\tnum_significant += 1\n\nprint('Of 100 trials, I found {} significant differences'.format(num_significant))\n```\n\n> Of 100 trials, I found 7 significant differences"", 'This is the one example where, if you randomize and blind properly, the null hypothesis of no difference at all is exactly true, by design. \n\nChange your example a little bit: one group gets a placebo pill and the other a placebo injection. Now you will probably find some very small and cinically irrelevant difference.', 'To clarify on your answer , the intuitive explanation that youre making is: with larger sample sizes, the outcome values for each group will reach some very stable baseline values - sort of the like the law of large numbers. And that both groups will tend to the same value, since they are identical. Is this correct?\n\nSo the increase in power is accompanied by a corresponding decrease in the magnitude of difference between both groups? Such that obtaining a statistically significant result is just as likely with a smaller sample like 100 VS 100 as it is for 1 million VS 1 million.', 'What would be the mechanism that causes a difference to arise in this example? That the injection elicits a stronger placebo effect on the patient and expectations for treatment?\n\nHow would you feel about 2 placebo injections - 1 group gets normal saline and the other dextrose. Would there be exactly no difference, or would I find a small & clinically irrelevant difference? \n\nThanks for the discussion!', 'Yes indeed, if there truly is no population difference between the groups.\n\n(Now if there *is* a systematic difference, which is not due to sampling, then no matter how small it is, you can detect it with a large enough sample size.)', ""Placebo effect is a finicky thing, e.g. a big hard to swallow placebo does better than a small one, one you are told is expensive does better than a cheap one etc. So I'd expect the injection to work better. But I wouldn't bet strongly either way, but they are different so very likely the results will not be identical.\n\nRe your question, I don't know, depends on what outcome and if patients can tell a difference between the two somehow. \n\nThere's a cool little trial where they give both arms a placebo, but tell one arm it is an actual treatment while the other arm knows it is a placebo. The arm that were told they got a treatment saw better results. https://pubmed.ncbi.nlm.nih.gov/28727701/\n\nAlso a good example of statistical significants not being clinically meaningful because we don't generally find lying acceptable as part of practice."", '> if there is a systematic difference, which is not due to sampling, then no matter how small it is, you can detect it\n\nWhat would be some causes/examples of a systematic difference, say for a double-blind RCT? What about if the RCT was ""high quality/well-done"", would it still be possible to have systematic differences?\n\nThank you very much for the discussion btw! I\'ve learned a lot :) !']"
[E] Time Series or Econometrics Course/Book,"Hi I am look for some recommendation for book on Time Series!

I am hitting myself in the head for not taking a course on Time Series given my interest in financial economics.

I've looked at the book by Hyndman but didn't find it quite rigorous. I was recommended the book by Tsay and another by Chris Chatfield and Haipeng Xing; are those any good?

Any course with videos are also much appreciated!",11i5kig,kailiu19,1677948576.0,12,0.93,"['Another book you might consider is \nHamilton, analysis of time series\n\n\nIt seems more rigorous than tsay.', ""Tsay is decent if somewhat unclear at times. (Pros: Complete, quickly covers the main points and provides data that you can use to verify your understanding.)\n\nYou can supplement it with Shumway Stoffer (though their focus is not strictly financial time series). \n\nI do not recommend Hamilton because it spends way too much time deriving formulas, the notation is attrocious (imo)  and if you are familiar with stats it won't add a lot to your understanding."", ""Duckduckgo.com - > forecasting principles and practice fpp3 hyndman\nYou're welcome"", 'The book on time series applications by shumway is pretty good', ""Recommend Tsay and some others over Chatfield and Xing, honestly. Chatfield and Xing's errata page no longer even works online. Some of its provided solutions are not very clear and at that point, would have rather not had the unclear solutions at all. \n\nI find [Time Series Analysis by Ord and Kendall](https://www.amazon.com/Time-Charles-Griffin-Maurice-Kendall/dp/0195207068/ref=sr_1_1?crid=3PDWZS11UZWWW&keywords=time+series+charles+griffin+book&qid=1677996602&s=books&sprefix=time+series+charles+griffin+book%2Cstripbooks%2C157&sr=1-1) (which Chatfield and Xing cite) to be a good one with a good balance of rigor and intuition. The exercises are dated, obviously, but the fundamentals of frequentist time series analysis are motivated very well.""]"
[Q] what statistical package to use to compare average annual percent change?,"What package on RStudio is best to use to pool and analyze AAPC in a meta analysis? I figured it would be metamean since Im comparing different averages?

Thanks!",11i187a,industrialshallots,1677942095.0,1,0.6,[]
[Q] Do you need to take theoretical statistics courses for grad school?,"So I am currently doing an undergrad in economics, and I am taking a lot of math courses offered by the math department. However, the statistics courses I took are applied statistics for business/economics offered by the business department, would this hurt my application eligibility for a master's in statistics?",11hz337,Yang_Nyima,1677936335.0,33,0.91,"[""Lots of folks who go to grad school for stats had few (if any) stats classes in undergrad. Many were undergrad math majors. As long as you've had a full sequence of calculus and a course in linear algebra, you should be fine. For a PhD in statistics, you'd also ideally have real analysis."", 'I will be taking a full year of linear algebra and calculus 3 in the upcoming academic year. But calculus 1 and calculus 2 I took were also focused on business and economics applications only where trigonometry and geometry are not taught. Would that be a disadvantage?', 'I think youll be fine as long as your GPA is decent (at or above the listed requirement for the specific program), and you meet the other prereqs given by the school. Of course a good application with decent LORs is important too. \n\nId check out r/gradadmissions if you havent already.', ""Here's a sneak peek of /r/gradadmissions using the [top posts](https://np.reddit.com/r/gradadmissions/top/?sort=top&t=year) of the year!\n\n\\#1: [This should be common practice!](https://i.redd.it/9uhgf2z8agga1.jpg) | [56 comments](https://np.reddit.com/r/gradadmissions/comments/10uir49/this_should_be_common_practice/)  \n\\#2: [Finding out decision with my daughter](https://v.redd.it/t2am80pfksm81) | [81 comments](https://np.reddit.com/r/gradadmissions/comments/tbviqd/finding_out_decision_with_my_daughter/)  \n\\#3: [I only applied to four PhD programs. Three super elite ones and one safety. Finally heard back from the safety: rejected](https://i.redd.it/9c40wfndroia1.jpg) | [50 comments](https://np.reddit.com/r/gradadmissions/comments/11460de/i_only_applied_to_four_phd_programs_three_super/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)""]"
[E] Resources for Kernel Regression? (Kernel Smoothing),"I have to give a presentation about Kernel Regression for a Regression II class, but I can't seem to find good resources. Is there any book you guys have found useful in understanding this topic?",11hsnlw,dibasixx,1677913821.0,1,1.0,"[""Many references are good on various aspects of the topic (there's a lot of things one might talk about), but chapter 5 of Wand and Jones' *Kernel Smoothing*, and chapter 6 of *Elements of Statistical Learning* are perhaps places to start.\n\n\nThere's many sets of notes and talk slides around on this topic; some are quite helpful.\n\n\nAdditive models can be based on different smoothers, including kernel smoothers, so some of those resources may be helpful as well."", ""You might try looking in textbooks relating to nonparametric statistical methods. Larry Wasserman's All of Nonparametric Statistics is a good resource, and content in chapters 4-6 might be of interest to you. Chapter 4 gives an intro to smoothing, chapter 5 is about nonparametric regression, and chapter 6 talks about density estimation. Good luck on your presentation!"", 'I loved Applied nonparametric regression (1990) by W. Hrdle', ""This sounds like it's just the thing I need, thank you so much!""]"
[Q] Kruskal-Wallis and Mann-Whitney different p-value?,"Hello everyone! I would like to start this post by saying I am very statistics illiterate  I am evaluating some data (3 groups) to see if the method used to generate the data was statistically different between each other at a 95% confidence. Ive attached a photo of an example group of real data I have. The sample size is small and cannot be increased. I am using an online calculator to do a Kruskal-Wallis test (from statistics kingdom) and am really looking into the group comparison p-values (i.e x1-x2, x1-x3, x2-x3). However, I am getting a p-value > 0.05 for 2/3 comparisons. When I individually compare the groups using a Mann-whitney U test the p-values are < 0.05. Can anyone explain why this is? I appreciate any help!!!!

Photo of sample data for reference:

https://imgur.com/gallery/0El2uMB",11hpyia,grilledpork3,1677905660.0,18,0.96,"['Your p-values cant all be smaller than 0.05 with a Mann-Whitney. You have two groups with 3 observations each, the smallest achievable p-value is 0.10 for them.\n\nDunns and Mann-Whitney are different tests, its normal that they give different results.', ""So here you're comparing 3 populations.\n\nIf you compare populations one by one against each other using three different t-tests (Mann Whitney U) then you will introduce error.\n\nThe correct test would be a non-parametric ANOVA - which the Kruskal Wallace should work. So the ANOVA will compare all three groups in 1 test and will spit out a p-value which will determine whether or not you reject the null hypothesis that there is no difference between groups. If you reject the null hypothesis the test isn't showing which groups are statistically dif from each other, but that a significant difference exists between at least two groups."", 'For three categorical groups as the predictor or independent variables, and one continuous response or dependent variable, the correct test would likely be a one-way ANOVA (though you also need your data to [fit the assumptions for the ANOVA test](https://www.statology.org/anova-assumptions/)). If you want to see individual comparisons, corrected for multiple comparisons, you need a post-hoc test like Tukeys honest significant difference.', 'I think there\'s some confusion in some of the responses because the question isn\'t quite clear.  As far as I can tell from the website you reference, you asking about the difference between the results when using the Dunn (1964) post-hoc test for Kruskal-Wallis and when conducting three individual Mann-Whitney tests.\n\nThere\'s an additional complication in that, from your image, you are using the Bonferroni correction for the Dunn test, and presumably not using a *p*\\-value correction when conducting the multiple Mann-Whitney tests.\n\nSo, there are likely two sources of differences between your sets of results: 1) The Dunn test is not the same as multiple Mann-Whitney tests; 2) Using the Bonferroni *p*\\-value correction will give different results than not using this correction.\n\nIn general, in your situation, the Dunn test is a better approach than multiple Mann-Whitney tests.  \n\nWhether you want to use a *p*\\-value adjustment is up to you.\n\nOn the website you reference, you have the option to use either the Dunn test or multiple Mann-Whitney tests as a post-hoc test, and you have the option of using either ""no correction"" or Bonferroni for the *p*\\-value adjustment for multiple tests.\n\nAs noted in other responses, when comparing a group with three observations to another group with three observations, you\'re behind the eight-ball, if you will, since you won\'t be able to get a *p*\\-value less than 0.05.  This result may tempt you to make a misleading conclusion, unless you keep in mind what the hypothesis test is actually saying.\n\nPractically speaking, for the data you have, there\'s not much point in conducting a hypothesis test at all.  The values in each group are clearly similar within groups and clearly different between groups.  A simple plot of values will tell the story better than will the results of hypothesis tests, especially given the small sample sizes.', 'This response will deviate from the original question, but...\n\nRank-based tests like Kruskal-Wallis look only at if the value of an observation is greater than the value of another observation, ignoring how much greater one observation is than another one.\n\nYou probably want to do an analysis that captures the magnitude of the differences.\n\nAs mentioned in a comment in my other response, a plot of values may be sufficient to convey the information without conducting a hypothesis test.\n\nI understand you are doing analyses with online calculators.  So, if you want to do a hypothesis test, to keep things simple, you might use Welch\'s anova followed by post-hoc pairwise Welch\'s *t*\\-tests.  (And then adjust the *p*\\-values for multiple comparisons.) I assume you can find these tests with an online calculator.\n\nThese test *do* assume normality of the errors of the model, which you can estimate by examine the residuals from the model.  Your residuals are not terribly far from a normal distribution.\n\nHowever, these tests do not assume homoscedasticity. \n\nFor reference, below is the appropriate code in R.  You can run it at [https://rdrr.io/snippets/](https://rdrr.io/snippets/) without installing software.\n\n*A = c(393, 399, 369, 391, 387, 352)*  \n*B = c(148, 142, 143)*  \n*C = c(5.01, 5.45, 5.39)*  \n*Y = c(A, B, C)*  \n*Group = c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"", ""C"")*  \n*Model = lm(Y \\~ Group)*  \n*hist(residuals(Model))*  \n*plot(residuals(Model) \\~ predict(Model))*  \n*oneway.test(Y \\~ Group)*  \n*t.test(A, B)*  \n*t.test(A, C)*  \n*t.test(B, C)*  \n*p.adjust(c(1.52e-07, 5.02e-08, 0.0001652), method=""BH"")*', ""If whatever program they used employs the asymptotic Normal approximation for the Mann-Whitney test, then it would produce a p-value that is *just barely* less than 0.05 even for the group 2 vs group 3 comparison.\n\nThe value of U would be 0 or 9, and the asymptotic mean and variance are 4.5 and sqrt(63/12), respectively. This gives a z-score of 1.964, or a two-tailed p-value of 0.0495.\n\nNot that an asymptotic Normal approximation should be used for two groups of size 3. But it could explain OP's results/comment about that."", 'how do you calculate the smallest achievable p-value?', 'Okay okay Im understanding what youre saying  the part Im confused about is after it spits out a p-value (which is < 0.05), it does group comparisons between x1-x2, x1-x3, and x2-x3. 2/3 p-values are > 0.05 and Im just wondering why they are not statistically different between the groups (when visually the values are clearly very different). Im sorry if this doesnt make sense', ""What's wrong with using Kruskal-Wallis with a Dunn (1964) post-hoc test, as the OP has done ?"", 'Thank you for the response! Your last bit is kinda the conclusions Im coming to, cuz visually with a bar graph its obvious the values dont overlap or anything', 'Wow thank you so much for taking the time to post this!! I really appreciate it  Ill def have to read up on some of these tests', 'Look at how the Mann-Whitney U test works. Two samples of 3 observations are small enough that you can enumerate the possibilities (with a computer).\n\nIf you do so, youll see that (under the null) U will have the following probabilities:\n\n* p(0) = 5%\n\n* p(1) = 5%\n\n* p(2) = 10%\n\n* p(3) = 15%\n\n* p(4) = 15%\n\n*  (the centre of symmetry is 4.5 so probabilities for U=5 to 9 are the same as above)\n\nSo you can never have a p-value below 10% for a two-tailed test (or 5% for a one-tailed test) because even the most extreme values of the test statistic (0 and 9) are achieved 5% of the time.', 'There are ^(6)C = 20 arrangements of 6 values into 3 groups of 3. The smallest possible one sided p value for any permutation test is therefore 1/20,  but here we have a two sided symmetric p value, so 2/20 is the best possible for any 2 sample  permutation test at those sample sizes', 'Nothing wrong with it. The parametric ANOVA test has a clearer interpretation and usually has higher power, making it easier for statistical novices to work with when their data meets the assumptions. In my experience you dont often get very different results when using K-W or ANOVA, so I recommend reaching for ANOVA if at all possible. \n\nTukey HSD is the appropriate *post hoc* test following ANOVA; similarly, Dunns is appropriate following K-W.', ""One thing you have to remember about the rank-based tests you are using is that they look only at if one observation is greater than another, discarding how much larger one observation is than another.  In order to capture this degree of difference, you may want to instead use a test of the means.  I'll post another response about this approach."", 'Ah makes sense, thank you', 'My data is not normal which is why I used KW instead of ANOVA, although interestingly ANOVA with Tukey post hoc gave far different results. Maybe because of small sample size. Anyways I appreciate the answer!', 'Parametric tests usually do better with small smaller sizes. I imagine your data would cause problems based on the widely-differing variances.']"
[Q] Looking for help,"I'm doing something out of my own curiosity. I'm having a hard time making sense of this information. Language problem maybe.

its from site [https://www.fbi.gov/file-repository/2015-ncic-missing-person-and-unidentified-person-statistics.pdf/view](https://www.fbi.gov/file-repository/2015-ncic-missing-person-and-unidentified-person-statistics.pdf/view) 

Looking at the table on page 3 

total file transactions: 1,813, 755. - so I assume this means the total number of reports

entries: 634k (is this the ""active"" list of missing persons?

Cancelled & cleared: 579k (is this the number of  reports that were dismissed, or report is cancelled? ie false alarm?)

Locates (LM): 54.9l (Assuming this means persons that went missing and were found.)

Modified (MM): (no idea)

Modified supplemental : n/i

Cancelled supplemental: n/i

Queried missing person file: n/i

Queried NCIC system wide: n/i

Looking at this table what's the math to find the total number of missing persons whom were not found?

&#x200B;

Thank you",11hj0ec,realpawel,1677886983.0,1,0.55,"['I think the number you want is the number at the top: Active entries. \\~85k. That is the number of currently missing people in the list. Entries is just the total number of entries in the list, which includes people who have been found or entries that have been canceled for another reason.\n\nThe specific number of total missing persons who where not found may not be possible to calculate here. Depends on how they maintain the list, and what sort of events can de-activate an entry: being reported as found, yes, but time might also do it not sure how the list works.', 'Hmm thanks. May have to ask folks that specifically deal with this']"
[Q] How would clustering help in this study?,"Let's say I have a study that wants to understand how crime activity and the economy at different geographic unit levels (county and zip code) are related. How could cluster analysis help? We already conduct a Spearman rank correlation, and later on, if there's enough correlation, will begin a causal analysis.

The data (which are highly skewed):

* 4.5 million anonymized crime data points across the United States collected from 2011 - 2022 labeled with the time, coordinates, the type of crime, and who committed the crime.
* Each county in the United States is labeled with average income, median income, and some other economic variables for 2022.",11he9cv,Abstray,1677876076.0,2,1.0,"['Hierarchical/multilevel SEM can do the job. Please don\'t be confident that a ""correlated enough"" data can be sufficient to proceed with causal inference. Are really two different realms', ""You might find there are two clusters (just an example), one which is high crime <--> small economy, and the other is high crime <--> blooming economy.\n\nThe criminal profile between these two clusters might be different. So if the government want to take measures to reduce crime rate, your cluster analysis's result could guide targeted intervention strategy."", ""Cluster analysis is helpful for two reasons. \n\nFirst, Spearman rank correlation is not going to be optimal, since I doubt the relationship between your two variables is monotonic (but you didn't specify the variables you put in that Spearman rank correlation, so I don't know for certain--I'm just speculating).\n\nSecond, and perhaps most importantly, multilevel models are *made* for situations like this. I did a lot of work like this for my master's thesis. I'm assuming you're running a regression model to determine causality (which, by the way, is going to be very difficult with cross-sectional data, but that's a different discussion). One of the assumptions of regression modeling is independence of observations. Your observations are **not** independent, since they were collected from the same neighborhood, which could exert an influence on the observations. (I mean, that's part of your hypothesis, right?). If you don't account for the clustering, your results could be biased."", ""Thanks for the caution and correction! The multilevel SEM sounds like a good idea, however, we don't have individual-level data per se. Our crime data points mirror events Someone (group host) who has committed a crime (hosted an event) can be charged with many at various points on the map (multiple events), but we can only connect the crime to the person---a single column for unique ID's.\n\nIf we pulled the column for unique ID's we could derive a count of crimes and each crime type count.\n\nIf I understand your recommendation correctly? I'm a research student working at a  lab, so while I know implementation, I'm still getting up to speed on statistical reasoning and design."", 'This sounds promising! Would this tell us perhaps which variables are properly well-associated with each other?', 'Well you were right. We did apply spearman; now considering Kendall. If you mean neighborhood as in geographical overlapping (I assume you dont), yes. If you mean similar sourcing, no. Im in undergrad l, and this using a VERY recent social capital analysis of the US. Each county has attached social capital variables (mostly economic in nature), and then we PLOT THE POINTS in each. We then repeat our analysis at the zip code level as well.', 'You could incorporate that into the model, yes. Just make sure you select a suitable model. Cluster analysis is honestly a broad term. The other commenter mentioned SEM, but that is just one approach to go about it.']"
[Q] Statistical chance of winning in a bot game simulation,"I am programming an elo system and I want to make bots that play a generic game, but I want the bots to be able to have a skill level so that elo makes sense. My goal is to be able to calculate the probability player1 will win over player2 given a their skill levels. My idea for how the bots play a game is as follows:  
Assign a skill level to each player where the possible skill level values are between 2 and 99. (2 to 99 is chosen to avoid games where no player can win)

Generate a random number between 1 and 100 for each player and compare that number to their skill level. If the random number is less than or equal to their skill level, they get a ""yes"", otherwise they get a ""no"". A winner is determined when one of the players has a ""yes"" and the other player has a ""no"". Otherwise, we generate a new set of numbers and compare again.

Here is an example:  
Player1 has skill level 70, player2 has skill level 40

A random number is generated for player 1. If that number is less than or equal to 70, player1 gets a value ""yes"", otherwise a value ""no"". A random number is generated for player 2. If that number is less than or equal 40, player2 gets a value ""yes"", otherwise a value ""no"".  
Suppose player1 is generated the value 50 and player 2 is generated the value 10, then the game will repeat since both got a ""yes"". Now suppose player1 is generated the value 80 and player2 is generated the value 72, then the game will also repeat since both got a ""no"". Finally, suppose player1 is generated the value 10 and player2 is generated the value 80. Then player1 wins because he is assigned the value ""yes"" while player2 is assigned the value ""no"".

I created a test program that will simulate this game for the two players 100k times for different skill levels just to see what the averages look like. Here is some of the data: (I'll make player 1 more skilled for readability)

|Player1 skill level|Player2 skill level|higher skill player win %|
|:-|:-|:-|
|20|10|\~68.8%-69.5%|
|90|80|\~68.8%-69.5%|
|75|70|\~56.2%-56.4%|
|99|98|\~66.7%-66.8%|
|70|10|\~94.5%-95.5%|
|70|70|\~50%|
|62|39|\~71.6%-\~71.8%|
|68|62|\~56.2%-56.8%|

My question is, how would you go about calculating the probability that the higher skilled player will win given player1 skill and player2 skill levels? I tried calculating it by letting P = higher skill player wins, Q = low skill player loses and then doing P\*not(Q), but that's as far as I've gotten.",11gwp5k,xx1HawkEye1xx,1677836782.0,10,0.91,"['I think you just divide by all the terminal states. Take 20/10 as an example P(20 wins) = .2 * (1-.1) / ((.2 * (1 - .1) + .1 * (1 - .2)) = .69', ""In an ELO system the probability of a win can be determined by:\n\n    exp = (home_team_rating + home_field_advantage - away_team_rating) / 400\n    team_a_prob = 1 / (10 ^ -exp + 1)\n\nhome team advantage doesn't matter here, so we can remove it. The 400 as a constant just scales the values, it's based on a 1500 centered ELO system - if we reduce it to 40 we get reasonable ish predictions\n\n&#x200B;\n\n|A|B|Win %|\n|:-|:-|:-|\n|90|10|97.5%|\n|60|40|71.5|\n|55|50|55.7%|"", ""ahh okay thank you, I was over thinking it. I'm never sure when I need to go into combinatorics verses just sort of going through the problem like this verses moving to a distribution model."", ""cool thank you! I'll study it a bit more so I understand it better. Programming can only get you so far if you don't know the math.""]"
[Q] Negative Binomial Regression vs Zero-Inflated Poisson?,"I'm attempting to work with a dataset involving NFL statistics for quarterbacks. I want to find significant relationships between physical and collegiate attributes and professional, count statistics. However, there are many quarterbacks in my dataset who have never played professionally, meaning that there are an outsized number of zeros in my dataset. I'm wondering whether a negative binomial regression or zero-inflated poisson would be more appropriate in this instance.

I'm leaning towards poisson, because I believe that a theoretical reason behind all these zeros is the round the player was drafted in. Players drafted in earlier rounds are a bigger investment, and thus their teams would have more incentive to play them, whereas the patience for players drafted in later rounds would be lower. However, I'm not sure whether this is a solid-enough foundation to run zero-inflated poisson, or whether this is even what zero-inflated poisson is designed to measure. 

If anyone has any advice on this or any other aspects of my project, I'd be very appreciative!",11gn4sm,PoopyJoe420,1677806490.0,28,0.97,"['Just run a ZINB model or hurdle NB. Poisson assumptions are almost never met.', 'Poisson and Negative Binomial are two different count models that can both be zero inflated. To see which model to use, you need to see if your dependent variable is equi- or overdispersed. If the mean and variance or your DV are the same, its equidispersed and you use Poisson. If variance is greater than the mean, use negative binomial.\n\nAs for zero inflated or not, you need to think about the causes behind the zeroes. Does each case in your data set have the same chance of a 0 in the DV? For instance, if you wanted to predict the count of school buildings in each census tract, youre going to be zero inflated because theres no reason to put a school in a census tract with 0 people. Those 0s are qualitatively different.\n\nAnother example that Ive seen is with fishing. If I want to count the number of fish caught in 15 lakes but 5 of the lakes are too shallow for fish to live there, of course 0 fish will come from those lakes. I cant attribute the 0s to any other factors. Those lakes are qualitatively different.\n\nI know a lot more about statistics than I do football so I cant tell you from your description if I think your model is zero inflated. But hopefully this helps you think through it.', 'Check dispersion first.', 'Since youre dealing with count data with a preponderance of 0s, why not compare the results of a ZIP model and a ZINB model? I wouldnt be surprised if they yielded very similar results.', 'Following!', 'my understanding is that you use negative bionomial over zero inflated poisson when the variance>>mean. Also can you not just do a log (x+1) transformation?', 'Can you still obtain RR from a NB regression?', 'Does a ZINB model work with indepedent variables that represent percentages i.e. are on a scale from 0-1?', 'In the situation the lakes are too shallow for fish would you fit a ZINB or poisson. You didnt really specify', 'Yes, just exponentiate the coefficient. Same interpretation as Poisson.', ""IV scaling doesn't matter. The GLM just makes assumptions about the DV."", 'Because theyre two different things to consider. When deciding between Poisson and NB, you need to think about overdispersion. When deciding between zero inflated or not, you need to think about whats causing the zeroes in the DV. Either count model could be zero inflated. The examples were just hypotheticals! :)\n\nThat being said, I think OP may be better off with a binary outcome model?', 'What about if the DVs are percentages?', 'I meant zero inflated vs not', ""Then no count model works. There's a whole discussion about whether you can just apply a logistic model to them, or whether you need something else like beta regressoin. I don't have a strong intuition about what's right there."", 'Oh, yeah with the too shallow lakes youd do zero inflated to account for those structural zeroes! Sorry for missing that point', ""If you have the numerator as a count, couldn't you just include the denominator as an offset term? Since we're dealing with a  log link function, it's mathematically equivalent to modelling percentages.""]"
[Q] Odds and Probabilities: What Am I Doing Wrong?,"I learned the following probability concepts:

\- Fractional odds for an event = P(E)/(1-PE)  
\- Probability for the event to occur: P(E) = a/a+b

\- Fractional odds against an event = \[1-P(E)\]/P(E)  
\- Probability against the event: P(Ec) = b/a+b

Out of curiosity I wanted to look at the implied probabilities of a sporting event.  
Online bookmakers here in France give decimal odds.

Say the bookmaker provides the decimal odds of 1.3 for team A to win.  
Now, unless I am mistaken:

\- The implied probability of team A to win is 76,9%  
\- The implied probability of team A to lose is 23,1%  
\- The implied fractional fractional odds for the team to win is 3/10

For some reason, when I try to run a check on the implied probabilities using the formulaes at the very beginning of this post I get:

\- Probability for the event to occur: P(E) = a/a+b = 3/13 = 23,1%  
\- Probability against the event: P(Ec)= 1 - 23,1% = 76,9%

Long story short, I am getting the opposite results?  
It's actually driving me crazy and I was hoping someone here could help me see what I am doing wrong.

Thanks!",11ge7ui,ButtersMamba,1677791602.0,1,1.0,"[""~~Bookmakers often quote odds-against not odds-for. If something had a 1/4 chance, 'fair' gambling odds would be given as  3.0 (3-1 against) not 0.333, ie [1-p]/p not p/[1-p].~~\n\n~~Note also that implied probabilities from  bookmakers odds won't add to 100%, since they're trying to make a profit~~\n\nedit: I missed that you had *decimal odds* (I saw the word; I misunderstood its implication -- I didn't know that it was a term referring to one of the sets of odds that are distinct from fractional odds). The implied probability for decimal odds is different. So ignore what I originally wrote above"", ""I have no idea what you're talking about. \nIf odds in favor is \n\n     O=P(E)/(1-P(E)) \n\nthen probability in favor is \n\n     P(E) = O/(1+O) \n\nso in this case, 1.3/2.3 = 0.565"", 'I see. Well, if that\'s the case and your ""implied probabilities are correct, then it looks like the 1.3 means 1 to 0.3 in favor\nSo\n\n    P(win) = 1/1.3\n    P(lose)=0.3/1.3\n\nor am I still missing something?', 'P(E) * U(E) + P(not E) * U(not E) = 0 \n\n-> P(E) * U(E) + (1 - P(E)) * (-1) = 0 \n\n-> U(E) = (1 - P(E)) / P(E). \n\nThe decimal odds are U(E) + 1. Let x = U(E) + 1, then x = 1 + (1 - P(E)) / P(E) = 1 / P(E).\n\nSo converting from decimal odds to implied probabilities is as trivial as doing 1 / odds. Using your example of odds 1.3 gives us P(E) ~= 0.769.', 'Thanks! \n\nSo given my example, what would the odd of 1.3 mean? \n\nSince the odd is low, it means that the team is believed to be the favourite to win, hence the probability of 76,9% right? \n\nIm having trouble seeing how these are odds against a team.', 'A decimal betting odd of 1.3 includes the original stake so the odds for are actually 0.3, or 3/10. \nTherefore the correct way to solve for the probability is 3/13 = 23.07%.', 'Your intuition is correct, however, P(lose) should rather be 1/(1/(1-1/1.3)) = 1-1/1.3 (which is the same as 0.3/1.3).', ""My apologies; I misinterpreted your original post.\n\nYou have a different form of odds than the one I was discussing.\n\nFor the odds you're dealing with, bookmakers quote money back on a stake of 1 (rather than money back *above* the original stake as I was discussing). Such odds are higher by 1.\n\nThe implied probability for this form of odds is (1.3-1)/(1.3) = \n0.2307 or about 23%  (but again, the implied probabilities for all the possible outcomes won't add to 1 because they're trying to make money)."", 'No worries. \n\nBut isnt that counterintuitive?\n\nWhat I mean is, with odds of 1.3 for Team A and odds of 8.00 for Team B, doesnt that mean that probabilities are in favour of Team A winning, hence it should have the 76.9% probability of winning and not the 23.1% you solved for?\n\nIm not saying youre wrong, I got the same result but that seems contradictory and I cant explain why Im getting those results.', ""Yeah, you're right; I fixed the issue of the difference in definition between the two kinds of odds but reintroduced the other problem, going back to thinking in terms of odds for rather than against.""]"
[E] Current or recent TAMU MS stats online students,"Hey 

I'm considering applying for this program and I've got a few questions I'm hoping some current or recent students can answer.

1) How accessible are the professors? How much did you interact with them directly? Did it feel more like a traditional classroom experience or a MOOC?

2) How much of the coursework is project-based? 

3) Did you get to know any of the other students in the program?

4) What programing languages did you use?

5) How applied vs. theoretical did you find it to be? Was there much ETL learning or was it primarily analysis in a setting where you already had all of the data you needed in the format you needed it in? Maybe a better way to put it was: How data science-y was the program?

6) I noticed that there's a new ML course being offered. Did ML show up in other courses?

Thanks so much and feel free to DM me if that's easier.",11ge13b,internetname20,1677791165.0,18,0.87,"[""I attended 2017 - 2019 and I imagine it has changed since then. I think some of the professors I had are no longer there and the coursework I also imagine has changed. With that said, it was a great program and I was overall really happy with it. I had started out wanting to work as a statistician but post-graduation I got a data science internship and now work as a data scientist. During my time, I had taken 1 course that had python in it and had never done any thorough ML projects; that I learned in my internship and on-the-job training. \n\nSo let's see if I can take a stab at these:\n\n>1.How accessible are the professors? How much did you interact with them   \ndirectly? Did it feel more like a traditional classroom experience or a   \nMOOC?\n\nIt varied, but each course had a 1-hour virtual office hour session each week. These were great and well-attended. Some professors were also really active answering questions on the Blackboard forum. These forums were usually broken up by homework assignment or lecture topic, so asking about a formula or something got good responses. \n\n>2. How much of the coursework is project-based?\n\nVaries course-to-course. A few of the courses I took had a final project; one of these was the time series course. Most of them were standard courses/exams, so you can navigate electives as you feel works for you.\n\n>3. Did you get to know any of the other students in the program?\n\nSomewhat. I'm just naturally reserved and never went out of my way to meet friends in undergrad courses, so saying I never did it at TAMU could be misleading. But, we had a slack channel and I connected with some people to study for the comprehensive exam. I hear there's a Discord channel now and if you want to be more active/social, the opportunity seems to be there.\n\n>4. What programing languages did you use?\n\nAside from that one course of super-basic Python, I used mostly R and SAS. I still regularly use SAS at my current job, so it worked out well.\n\n>5. How applied vs. theoretical did you find it to be? Was there much ETL   \nlearning or was it primarily analysis in a setting where you already had  \n all of the data you needed in the format you needed it in? Maybe a   \nbetter way to put it was: How data science-y was the program?\n\nSome courses were super theoretical, most were a healthy balance. If you're looking to avoid theoretical topics, this isn't for you. But if you're looking to get into data science, then I would say stick to the theoretical. You can teach yourself how to build a GBM, but it's more difficult to teach yourself the theory behind a GBM, and in a data science role, you need to do both.\n\n&#x200B;\n\n>6. I noticed that there's a new ML course being offered. Did ML show up in other courses?\n\nVery high-level. I remember one course we used SAS which had a click-and-drag feature to build decision trees, GBMs, Neural Nets, and we compared the results. Again, we mostly used to it discuss the concepts behind them. That one Python course had a bit deeper, but not much, and certainly nowhere near a model build that you can find on Kaggle. But, that said, they may see where the field is going and may have created some more ML courses.\n\nHope this gave some help and feel free to DM me if you have any other questions. I have no regrets and really enjoyed my time there; it was a great program and I wouldn't be where I am now without it/the great professors I had."", '1) professors and TA are quite accessible via office hours and email \n\n2) very little if not 0 for me so far \n\n3) you could but I havent since I work so much \n\n4) R but sometimes I sneak in Python for fun \n\n5) not much ETL so far which I get way more than enough of at work anyway. Most stuff is applied statistics and a good amount of theory \n\n6) so far no but Im only in the core courseload so the ds focus classes are coming up', '1. Traditional classroom. Weekly assignments. Exams. Lectures to watch.\n\n2. Maybe an elective or two.\n\n3. Yes.\n\n4. R, SAS\n\n5. Traditional statistics with some data science electives. Mix of theory and practice. Mostly pre re-sorted data.\n\n6. ML was only for PhD students when I went. There are two courses that most people would call ML.', 'I graduated from the program in December 2022. I was working full time while attending and finished in 2.5 years (started Summer 2020). I was able to move from Engineering into a DS role as I was finishing up.\n\n1.\tI personally did not interact with the professors all that much since I did not attend many live sessions due to my schedule. They were definitely available if I would have needed them though.\n2.\tHighly depends on the classes you take. I think I ultimately had 4 classes that I would consider project based, 2 of which were data science focused. One building a prediction model for NBA games, the other trying to project the remaining useful life of rechargeable batteries.\n3.\tThis was the one thing I missed when compared to my undergrad. Everybody I worked with in group projects worked really hard, but I never got to really know them or get to be friends with them. Just hard when everybody is living in different states and you only meet for an hour per week. Like others have mentioned, there is a fairly active Discord server that has different channels for each class.\n4.\tR is the heavy use, although if you go down the biostat route you will take classes with SAS from what I heard.\n5.\tFrom what I could tell, based on new courses that were starting to be offered as I was finishing up, they are actively trying to grow the applied course list. Outside of the projects, almost all of the data provided was pre-cleaned so that the actual methods could be practiced. \n6.\tI took two ML specific courses as electives. They were essentially two sides of the same coin where one was the theoretical backbone of the methods and the other was the application. They were both taught by the same professor,  Dr. Homrighausen, who is fantastic teacher. Other than that, the only other spot ML really showed up (outside of Linear Regression and the Penalized versions) was in my final project class.', ""A bit late but I'll chime in.  I just finished my first semester in the program.  They just revamped the programming course to be exclusively R and Python - SAS is now an optional stand-alone course.  I cannot emphasize how important it is to join the student Discord channel!  I was about to give up the first semester and got so much support from the other students.  I've only taken one course (Probability and Math Statistics - 630) so far and, tbh, it was the worst class I've ever taken - poorly taught, the homework was overly excessive (at least 20 hours a week).  If you're currently working, whatever you do, don't take 2 courses in the first semester.    \n\n\nBut the other students promise it gets better after that, so we'll see.  I can say that it's definitely very rigorous based on that class!"", ""This is so helpful, really appreciate your reply. I'll for sure Dm you with follow ups if I think of any."", 'Super helpful, thank you!', 'Thanks! Mind if I ask when you attended?', 'Really appreciate the reply. Super helpful information.', ""DM'd a few people in this thread if you'd be kind enough to share materials on STAT630!! I am keen to sign up for the program but also a little anxious based on comments. Would love to go through the material ahead of time so I feel prepared.  \n\n\nu/H_Badger , how did the exams work? Were you able to proctor via Zoom and in your own personal space? Also, was it a set time, or were you given a window of time to complete the exam in?"", 'They sponsor a great DS competition with Humana. Do it regardless of what school you end up at.', 'I should have mentioned all classes have active discussion boards on canvas, and while the professor sometimes doesnt reply the same day, they always do reply. Discussion boards for STAT 630 were heavy with student and professor activity before exams and during homework assignments. \n\nSometimes the professors respond to discussion boards very quickly (even in the middle of the night lol).', 'I graduated 2 years ago.', 'It depends on which professor you have. Cline teaches it in the Fall and I\'m not sure how he does it.  Aburweis is the spring ""minder"" - you\'re given all the videos of Cline\'s lectures, and Aburweis and a TA are the graders.  For Aburweis the exams are open book open note and you have 24 hours to complete them (no proctor)', 'Awesome, highly appreciated! Yup I am aiming to start Spring 2024 so hopefully nothing changes, because that sounds reasonable.']"
[Q] Will a month+ vacation after graduation hurt my chances of finding work later?,"Sorry to post this here, i just want insight from people in the field that ill be going into.

My parents really wants to take me back to their native country for vacation after graduation. It sounds exciting for me, but iv been really worried that i wont find a great job after graduation, so i really dont want to hurt my chances further by taking alot of time off. I figured maybe it might be better to work for a couple years and save some of my vacation days to do that instead.

Im getting a bachelors in Statistics and minors in Economics and applied math, with average grades. That sounds cool to an employer, but personally i feel holey unprepared. Im worried a vacation might make me feel that even more so.

There is a job fair happening at my uni soon, im not sure how to let them know that i wont be immediately ready for work after graduation. I wonder if just mentioning that i want to chillax a bit will make me undesirable. I graduate this spring.",11gc3yo,ThrowThisCloudAway,1677786738.0,0,0.5,"[""You're about to finish four years of investing in your education/career. Give yourself a month off to invest in your happiness. The jobs will still be there when you get back, but you can apply while you're on vacation as well if you want. For most positions, employers expect to wait at least two weeks after you accept an offer so you can wrap up at your previous position. They may not expect you to have a job that you're leaving since you're coming out of college, but different schools also graduate at different times in the season, so similar effect."", ""At least in the United States, it's very common for new graduate jobs to start a few months after most students graduate."", 'Yeah, vacation, sure\nYall are getting hired within a month as a new grad???', 'It wont make a bit of difference to getting a job. As long as moneys ok for a few months, enjoy your life!', 'You can also apply to jobs now and negotiate the start date with the company. In my experience employers are flexible on this front.', 'No', 'No impact.', 'If you can get a job before and ask for a later start date that would be the best. One month will turn into many more in reality if you start applying once you get back from vacation. But I highly recommend taking time off before starting a job.', ""This is a no-brainer. You're young. Take the time off. In fact, take more than a month off if you have the means. Enjoy!"", 'This. Im 47. Take the time off, if you dont you will regret it later.', 'Thanks for the insight. For the job fair, sometimes employers ask me when i am graduating, especially if they are interested in me as a candidate.  Do i let them know i wont be available immediately after graduation? Do i just wait until they extend a formal offer, and let them know then? How do i go about this?']"
[Q] Single Response Significance Testing,"I'm embarrassed to post this but I've been researching for quite a bit and cannot figure out the answer.  For context, I'm a marketing professional who knows enough about statistics to be dangerous, but would be considered intermediate at best in the real world. 

Let's say I have a simple single-response question: ""Which is your favorite ice cream flavor?"" People can only pick one answer, and the options are Chocolate, Vanilla, Strawberry and Something Else. 

I survey 100 people with this single question, and end up with 50% choosing Chocolate, 40% choosing Vanilla, 10% choosing Strawberry and 0% choosing other. 

What I'm trying to learn is whether the difference between Chocolate (50%) and Vanilla (40%) is statistically significant at 95% confidence based on my n=100. For the purposes of this question, I have no idea what the actual distribution of this answer is among the general population - hence why I'm surveying. 

Weirdly, I know how to do this if the question is structured as a multiple response: ""Which ice cream flavors do you like? Check all that apply."" In that case, it's my understanding I can conduct an independent samples T-test. It feels like it's plausible that I could do the same thing here. 

TL:DR - Within a single response question, what test do I run to tell whether the difference between the % who chose A and the % who chose B is statistically significant?",11g97ud,NeilAnnwn,1677779835.0,2,1.0,"['Keyphrase: categorical outcome', ""You'd normally do a proportions test, which is easiest done by conditioning on choosing only one of those two options; of those that expressed one of the two preferences you're comparing, compare the proportion of one of them (say chocolate) to 1/2. \n\nThis can be done as a z test or a chi-squared test, or (more exactly) as a binomial test. It could even be done as a t-test.\n\n\nThere's also the problem that this sort of data doesn't tell you about Chocolate/Vanilla preference among people who ranked strawberry first (so if you're trying to say that people prefer chocolate to vanilla, you may be misled; if you add the preference between the two options of the people that ranked them 2 and 3, you may well flip things). (This is the difference between a first past the post election and ranked choice type voting; if you want to say which of two options would be preferred, a first past the post election won't give it to you)\n\n[Beware also of this smaller paradox: the mere *presence* of a third choice may alter some people's preference between two options. (e.g., presented with only two options, someone may be unable to pick a preference and essentially choose at random, and say 'chocolate', but merely adding the option of 'strawberry' may resolve their chocolate/vanilla dilemma in favor of vanilla; this is not a mere hypothetical, it sometimes happens).]\n\n> In that case, it's my understanding I can conduct an independent samples T-test. \n\nWell, not quite; (1) the responses are counts; (ii) a subjects choices in the multichoice are dependent (dependent on subject), and (iii) that data wouldn't tell you about preference between options that are both selected or both unselected."", "">Well, not quite; (1) the responses are counts; (ii) a subjects choices in the multichoice are dependent (dependent on subject), and (iii) that data wouldn't tell you about preference between options that are both selected or both unselected.\n\nThank you very much for your thoughtful reply. I really appreciate it!\n\nAnd you're definitely right on with the callouts on the question constructs. Obviously if the goal is to get to the truth the answer is far more nuanced.""]"
"MS program with no comprehensive exams, what’s the catch? [Q]","I have gotten into two MS programs in statistics which are both funded. Wake forest, and Miami (OH). The Miami of Ohio program requires a comprehensive exam and a thesis. The wake forest program has a choice of thesis or no thesis, and has no comprehensive exams at the end of year 2. 

Right now Im leaning towards wake forest solely because they have no comprehensive exams. However, I wonder if theres a catch to this. Do you think they just make the actual course curriculum way harder? Is it a red flag of sorts if a MS stats program doesnt require comprehensive exams?  These are both not online by the way.

EDIT: These are both *not* online. I made a typo",11g96jh,AdFew4357,1677779747.0,10,0.92,"[""It's whatever. The thesis/no thesis distinction is more important. Nobody ever asks how your comps went."", 'Our program just dropped the comprehensive exam. Why? Cause its dumb. We all know a single massive exam is not a good measure of your real understanding of the content. Its an example of your ability to memorize.', 'The Miami of OH program allows you to skip the exam if you get a high enough GPA in the classes it covers.\n\nMy thesis got a publication out of it which was nice.', 'Is there any chance youd want to go back for a PhD later? Do you have any desire to publish and build a solid program of study attached to your name?', 'Is a thesis helpful for jobs?', 'How it should be', 'Thats great to know! Can I pm you more about your experience at Miami?', 'I mean, Ive thought about pursuing a phd, heck I applied to PhD programs, but I dont know about this for sure, and frankly dont know how much the phd will benefit me in industry yet. I was hoping in my MS to figure this out.', ""It varies. The thesis itself may not, esp outside of academia. I work in government and have a PhD thesis - ain't nobody want to see it. The process of writing a thesis helps no matter what you do. It's really more about having had research experience than the writing itself, and while research can happen without a thesis it's probably not as long or defined as the research that goes into writing a thesis. \n\nYou don't need a MS with thesis to get into PhD programs. Undergrads get accepted into PhD programs. Remember that all experience is helpful experience (an MS thesis will make a PhD thesis feel more approachable) but there is a time cost associated with collecting more experience, so you have to decide what is best for you."", ""Not really needed for normal industry jobs, but if you are pursuing an academic career (research, PhD, etc) then you'll need a thesis"", ""Depends.  I think an applied thesis where you spent a significant amount of time solving an unsolved practical problem is useful because it shows you can work on tough open-ended problems, and because it'll give you a project to talk about during interviews."", 'An MS thesis *could* get you hired for roles that ordinarily would go to a PhD, but a scenario like that will likely only come up if you wish to be hired specifically to do research and youre a great fit for the employer in question. The important part is having something significant that demonstrates your ability to do whatever it is youre trying to do professionally.', 'Yeahhh its tough. I think if youre looking at industry jobs and considering that statistics is more of an applied field youd probably be ok without a PhD. I got my PhD in experimental psych but am a data analysis now. I would have been much further in my field if I had realized sooner I just enjoy statistic and data so Im not a great person to ask. But Ill share my 2 cents nonetheless lol \n\nIf you want to consider a PhD later definitely do the thesis option (then you might be able to skip the first 2-3 years of a PhD program). The comps exams will also help you get into a PhD program but really having a completed thesis is whats going to matter. \n\nIf you just want the skills and ability to jump into a career, both programs are probably equal. In that case I would set aside the comps and thesis considerations and look at the profs for each program. Who has similar interests as you? Who is really leading in the field? What is their current research focuses? And choose the one you like the best based on that.', 'Okay, well as it stands Im right now trying to be hirable for industry jobs out of MS. Im only wondering if doing a thesis could help bolster my resume for jobs. Theres an option where I can just do a semester long project and not do a thesis to get experience with a project.', 'Okay thanks a lot for the advice! Ill think about it more. Lol I just hope with an MS in stats I dont have a hard time finding a job.', ""Yeah, I think it's fine to skip then. I haven't seen any job postings that specifically ask for a thesis."", 'I dont think you will! My current job only requires a Masters degree. Good luck!', 'Do you do lots of data analysis?', 'I do but I do a lot of other things. I would like to do my statistically analysis but my job is as research support for a university so its a mixed bag on what projects Im assigned.']"
[Q] Help me understand Two Way Anova,"Hey!

So i need to understand how apply a Two Way Anova. Is my setup (see below) totally effed?  
The data i have is:   
10 patients with Measurement of 5 different brain areas  (Dependent variable?).   
During Fasting/Fed states (Independent variable?)  
And then again after X intervention i have measurements of the same brain regions during the fasting and fed states.

Set up;  
I was thinking: ""Fed/Fasting state"" on the 2 rows. 5 diffrent brain areas in 5 subcolumns (matched across row) and the columns representing before and after.   


I feel really lost. first day using statistics software. wanna bash my head in so help is much appreciated :)  Im using Graphpad Prism. to do a Two-way Anova to get the interaction effect.",11g4zna,Openpentagon,1677769584.0,0,0.5,"['Guessing from the information you provided, you would want to do a two-way repeated measures MANOVA. You have multiple dependent variables, so you need to do a multivariate analysis. I think nobody here will be able to guide you to how to do that with your software and dataset. Your supervisor/tutor should be helping you there', 'Well, people study years to use stats, so no surprise you are struggling. By the way, you have measures in the same patient, violating independence assumption of anova so this is not the appropriate model for you anyway. Seek support of a statistician irl.', 'So would this work:   \n2 main columns for before&after (Independent variable = time).  \n5 Rows for brain areas (Brain activity = Dependent variable).  \n2 matched subcolumns for fed/fasted (so that same brain area during the same state can be compared before and after intervention. Independent variable = Fasted/fed).  \n\n\n?', '>you have measures in the same patient\n\nWhat do you mean? i said i have measures from 10 patients..', '>10 patients with Measurement of 5 different brain areas\n\nI mean this. You have 5 measures in the same patient. Those measures are not independent. They are nested.', 'Yes they are nested meaning they are paired , right? So If i were to use subcolumns A:Y1.and BY:1 could be matched/paired meaning they are measurements of of the same area before and after?']"
"[Q] Layperson here. Is it possible to run a multiple regression when the independent variables are ""unequal""? Not sure how to explain this properly"," [https://imgur.com/a/kWMSzch](https://imgur.com/a/kWMSzch)

This is representative of the content set I am working with. With each variable, Leads is my dependent variable but as you can see the number of categories in the independent variables is different from each other (which is what I meant by ""unequal"").

Can I run a multiple regression on these three variables? I want to see if all three variables taken together are descriptive of Leads.

If I am missing any information, then I can provide more detail as needed.

Thanks all.",11g3y68,getthedough9191,1677766841.0,1,1.0,"['Yes, you can run a multiple linear regression!', 'Do you have leads broken down by each *combination* of your three variables?', 'This *table* is not very useful for (joint) statistical analysis (unless you want to think of these as multi view data, but you just dont have enough columns to get a lot out of that), but if you have the full data row-wise in a spreadsheet elsewhere, where each customer has their associated Tier, State, Age, and Lead indicator in a row, then you can perform logistic regression.']"
[Q] Causal inference with dynamic treatment?,"Hi guys,

I'm studying causal inference with dowhy package. I'm reading the example of membership reward program [\[Link\]](https://www.pywhy.org/dowhy/v0.5.1/example_notebooks/dowhy_example_effect_of_memberrewards_program.html). The notebook takes the example of signup month i = 3

But, what will happen if everyone has different signup months? Do we run the model independently for every different signup month? Or we can have a dynamic model with dynamic signup month?",11g13ua,footballfanabc,1677758687.0,6,1.0,"['Tbh i would just control for fixed effects, but that is what makes me applied and not a statistician', 'Remind me', 'The outcome is time-dependent so as long as you don\'t think that the specific eg season is a confounder you can use only one lagged model. This is not a ""Dynamical treatment"" as we refert to DT as treatment varying in a time manner for example dose escalation of a drugs for the same patient.', 'wdym by control for fixed effects?', 'Can you elaborate more about lagged model? Why it works well in this case?', 'https://bookdown.org/mike/data_analysis/two-way-fixed-effects.html', 'Instead of using the ""month"" you can use the lag so ""for how long"" and you can now use it as a standard feature. Easy easy. For more complex stuffs you need to carefully write down your CSM', 'so iiuc, if we apply fixed effects, we just run causal model for different signup months (so 12 models), then causal coefficient is the weighted average?', 'It\'s still difficult for me to fully understand. It\'s like take the lag (of spending) as a feature? but how this indicat ""for how long"" as you mentioned?', 'No man very easy. If you are assuming that longer subscriptions have even a stronger causal effect on spending, instead of using the month itself use the length of subscription. The treatment is sub/not sub the length can be treated as a feature.']"
"[Q] How are flux, divergence, and curl of vector fields used in statistics?",,11fzw9p,lightsnooze,1677754502.0,42,0.89,"[""I never needed those concepts in statistics. They are useful to know(ex: modelling systems behavior) , but if you focus on stats only, it won't be useful.\n\nThere might be some niche exceptions, but that's what they are. Exceptions."", 'Those objects have a more physical and geometrical interpretation that isnt used much in statistics. The differential objects you encounter in statistics are mostly things like the gradient, Jacobian matrix and the Hessian matrix.', 'I have never seen them applied in Statistics.', 'The generalizations of these concepts (exterior derivative and differential forms) show up in some areas of probability theory. But this is mathematical statistics / theoretical statistics and not something you would use in applied stats. \n\nhttps://en.m.wikipedia.org/wiki/Information_geometry', 'Ive never seen curl used in statistics.  Flux and divergence both appear in the derivation of the Fokker-Planck equation which is useful for the analysis of Markov chain Monte Carlo algorithms, notably the unadjusted Langevin algorithm.  The Langevin and related algorithms such as Hamiltonian Monte Carlo are among the primary methods used for Bayesian inference (e.g. I believe Stan uses HMC).', 'Flux, divergence, and curl are mathematical concepts from vector calculus that are primarily used in physics and engineering to study fields such as fluid dynamics, electromagnetism, and thermodynamics. However, there are some applications of these concepts in statistics and data analysis, particularly in the fields of spatial statistics and machine learning. Here are some examples:\n\nSpatial statistics: In spatial statistics, flux, divergence, and curl can be used to study the spatial distribution of a random field. For example, if we have a dataset of measurements of temperature or air pollution at various locations in a city, we can model the distribution of the field using stochastic partial differential equations (SPDEs) that involve flux, divergence, and curl. These models can help us understand the spatial correlation structure of the field, and make predictions about the values of the field at unobserved locations.\nMachine learning: In some machine learning applications, we may want to represent a dataset of vector-valued features as a graph or network, where nodes represent the data points and edges represent the relationships between them. In this context, flux, divergence, and curl can be used to define graph-based convolutional neural networks (GCNNs) that can learn representations of the data that capture the spatial relationships between the features. These models have been applied to tasks such as image classification, 3D shape recognition, and drug discovery.\nOverall, flux, divergence, and curl are not commonly used in statistics, but they can be useful tools in some specialized applications.', 'You use them to read stokes theorem for the fifth time and then switch your major from math to statistics', ""Never, and I'm someone who has been both an electrical engineer (microwave electronics) and a statistician at various points in time."", 'In a reductive sense I guess you could consider gradient descent as sequentially finding the local direction of maximal (or minimal?) work.', 'Yeah- its more physics and engineering. Maybe someone will find a clever application for knowledge propagation in nnets or something?', '**[Information geometry](https://en.m.wikipedia.org/wiki/Information_geometry)** \n \n >Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)', 'This answer looks like it was generared by chatGPT.', 'Or keep your primary major *as* Stats', 'It was.  The continued use of the triad flux, divergence, and curl instead of identifying specific applications of the three components in their own way is a tell.']"
"[Q] Assumptions for MANOVA failed, what are my options?","I want to conduct a one-way MANOVA with many DVs (around 9) and one fixed factor as IV that consists of six groups. Since I first have to check for many different assumptions, this is where I am having trouble.

First I removed outliers, then I checked the normality and almost half of them showed deviation from normality based on Shapiro-wilk and/or Kolmogorov test. By log-transforming the data I still have a couple of non-normal variables.

If I were to ignore it, there still are quite a few other problematic results:

1. Correlations  some of them are in the range (0.2  0.9), while some are very low.
2. Box's M is < 0.001 so this test also failed
3. Levene's test of equality of variances has also showed that half of them are significant, while the other half is not

My question based on all of this is what are my options? Firstly, can I ignore that normality test and just say that we looked at Q-Q plot which does seem ok.

Regarding correlations, can I maybe separate DVs into two MANOVAs where I only put those which do show correlation?

What can I do about Box's M and Levene's test?",11fzey8,gremlin665,1677752692.0,3,0.81,"['MANOVA is obsolete. SEM has taken its place. See e.g., Huang, F. L. (2020). MANOVA: A procedure whose time has passed?. Gifted Child Quarterly, 64(1), 56-60.', 'If you can formulate a null hypothesis you should be able to cook up a monte carlo resampling test', 'Or https://quantitudepod.org/s2e09-manova-must-die/']"
[Q] What tests/parameters describe the variances within differently distributed sample groups?,"For example, I have data on how tired people are when they wake up in the morning. Each participant logged their level of tiredness on a scale from 0-100 each morning over a few weeks, and ultimately I will look into determinants recorded.

However, initially I just want to examine the variance between each individual's readings for tiredness each morning to see the general variation for each participant. For an individual obviously the range and difference in range, mean and median are useful, but median and mean won't effectively look at the overall population as each person will have a different 'baseline' for general tiredness e.g. someone might regularly get 8 hours and wake up feeling awake each morning, so would generally be around 90 each day, whereas someone might get less sleep and struggle with insomnia so would have readings closer to 10 each day.

Standard deviation and coefficient of variation seem logical, but otherwise I'm unsure what to look into. I'm not sure ANOVA is relevant as I'm not interested in the difference between different individual's data.

I am hoping to determine whether imputation can be used to replace missing values, so ideally am hoping differences are not significant.",11fzdtt,hazza6696,1677752567.0,2,1.0,[]
[E] The Brier Score Explained,"Hi guys,

I have made a video [here](https://youtu.be/BiaebXlgfNQ) where I explain what the Brier Score is and how it is computed

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)",11fyg3x,Personal-Trainer-541,1677749207.0,1,0.57,"['""Hi, I\'m spamming my videos across dozens of likely and unlikely subreddits. There\'s no downside, so who cares! Any sockpuppets want to give me a hand, I\'ll scratch your back if you scratch mine.""', ' Following!', 'That was pretty brutal, but I guess I deserve some of that critique. Thanks!', 'Thank you!']"
[Q] Is it worth making the jump from spss to stata?,"I am currently working on several databases. However my pi plans to add more data years that will expand the record to around 100 millions entries with around 200 variables.

I am currently handling 30m via spss. Is it better to shift to stata?",11fxnln,dancedanceevol909,1677746355.0,12,0.87,"[""Why specifically are you considering Stata? It's what i was initially taught, i would not recommend it to anyone over R today for my area."", 'Yes.\nIts better to go with R. \nCause free.', 'R is free, constantly updated, and can read spss and stata files.', ""Always shift from SPSS to something, anything else.  \n  \nStata is proprietary, but if it's common in your field, then probably has useful libraries. Otherwise, R."", 'SPSS < Literally doing stats by hand < Stata <<< R', '[List of R packages giving access to different databases](https://cran.r-project.org/web/views/Databases.html)', ""Stata keeps all data in memory (or did, when I used it, which was a while ago). This means you need enough memory on your computer, or it stops. SPSS will swap data to disk so as long as you have enough disk space, you are good. \n\nAgain, I could be out of date, but stata was faster than SPSS for large datasets (as long as they did fit in memory).\n\nI don't know much about this stuff any more because there has to be a good reason not to use R."", 'Youre not going to be able to load that into memory in any program. A few suggestions:\n\n- explore and validate your hunches using a much smaller sample. \n- pull in someone with programming experience to write code that will run a select set of predefined tests while managing memory.\n- use a relational database and sql', ""If you're currently balls deep in proprietary stats software then do yourself a favour and learn something open source. You've already scored proficiency in the language required be much of industry. Stata use is much more niche. Especially outside of academia. Pick up R or Python now."", 'Thank you. I have seen that most studies that use the database of my interest have used stata for big sample. I will check if R is popular or has the modules needed to run the database in question.', 'R also has some good libraries for handling datasets too large for memory', 'Lol i feel prehistoric now as you ranked spss as doing stats on cave wall haha', ""It will definitely have the tools available, i would be more concerned about whether you need to rely on others code. If not, R will be able to handle whatever you wish to use it for, given it's open source nature."", 'you could try if after adding another 70m  (or however much) it will run in R. Or it might when you add another memory bank, which may be more cheap than a software subscription', 'Came here for this warning. R alone wont parse large datasets. Or at least it was easier to parse with Python. So I would agree that you need to specifically look at how youll handle databases that big in R.', 'Id liken it more to riding a bike with training wheels. R is a $14,500 Specialized S-Works Tarmac SL7 bicycle in comparison.', 'I currently have 64gb ram. Should suffice i think']"
[Q] Mean vs median,"Hi guys, I have a question.

If the mean GPA of a sample is 3.5, and the median of that sample is 3.6. If a student is randomly chosen from that sample, what would his expected GPA be?

Just made up this question out of curiosity. I'm inclined toward the mean, but I'm not sure how the median would be any worse.

Thanks!",11fvt40,Go_1707,1677740150.0,1,0.67,"['The ""expectation"", as is defined in probability and statistics, is equal to the mean. \n\nIt can be confusing terminology for some distributions, including skewed ones like in your example. The ""expectation"" is not necessarily the value you\'d expect to get if you randomly sampled. Another example: the expected value of a dice roll is 3.5.', 'If you actually mean to ask about expected value, simply look at the definition of expected value\n\nhttps://en.wikipedia.org/wiki/Expected_value', ""I think the author was trying to explain that the median is a robust statistics but the mean it's not being very sensitive to outliers."", 'I\'m reading a psychology book and in it the author poses a problem in which the gpas of a student sample were divided into percentiles (..,50th,...,90th, 100th). One is supposed to pick a random student in the sample, and the concerned question is what we should ""predict"" his gpa to be. He said if there is no other information, the best guess is the 50th percentile value. I was a bit taken aback, since I thought it would be the mean. But here there\'s not enough information to determine the mean. I\'m not sure it\'d be fair to say ""if there is no other information"", the median is the best guess.', 'Btw, thank you for your help!', 'Thank you very much!', ""Thank you! Actually I don't think that's what he was trying to say. He was making a point about regression to the mean and its application in psychology. Sorry I didnt provide the full context so it might be a bit confusing about the topic of that problem. "", ""Often statistics can be very fuzzy for a number of reasons, usually because people are not very explicit in their terminology. You yourself asked for expected value, when you wanted something quite different.\n\nIf I had to predict what the value would be, I'd take the mode. That maximises my probability of being correct in my prediction.\n\nIf I was asked to predict what the value would be, and was awarded points for being close to the correct value in a linear fashion, I'd do as the book author suggests and take the median. The reason for this is that we want to give the value, x, which minimises the mean absolute difference between itself and the samples.\n\nIe. we want x such that the sum(| GPA_i - x |) is minimised, where GPA_i labels the GPA of student i. This turns out to be the median.\n\nThe mean, on the other hand, minimises the square difference."", 'Thank you so much!!! You\'ve just brought so much clarity inro this problem. I have a question.\n\n""If I was asked to predict what the value would be, and was awarded points for being close to the correct value in a linear fashion""\n\nI get why you would use the median here, but I\'m not sure how much ""[...] in a linear fashion"" factors into this decision. I\'d believe you were just giving an example of a way to punish/reward deviations from the correct value, but I\'m not sure if there might be more going on here. Please explain! Thanks so much again!', 'The decision to use the median very much depends on ""in a linear fashion"". The median minimises the sum of the distances to the data points. The mean minimises the sum of the squared distances to the data points. These statements can be proved with basic calculus.', 'The linear part is the reason for picking the median. The motivation for minimising the absolute differences (and therefore using the median) is that the reward is proportional to the absolute differences.']"
[Career] What is needed for a career as an environmental statistician?,I have a BS in applied statistics and am looking to pursue a masters degree in biostatistics. Are environmental statistics considered public health? Would an MS in biostatistics help me towards a career as an environmental statistician or should I look into MS programs in environmental data science/applied statistics? Are environmental statisticians just statisticians who work in environmental contexts?,11fofz7,VasNest,1677719789.0,16,0.94,"['A stats MS program might be better. There are some spatial people I know in biostats who have done research on air pollution, but most environmental stuff happens in stats departments.', ""I think it's a mildly unusual career path for someone who has studied *only* statistics; a lot of environmental types will have done their undergrad in geology or biology, then switched to statistics (or, perhaps, done the opposite, done undergrad in statistics and then grad in some specialized area like hydrology.)\n\nIt won't be impossible if you have the science background to understand the type of environmental statistics you're doing. But it may be hard to get an interview if they don't see anything environment related on the transcript or resume."", ""My friend works for the US govt and counts wildflowers and writes papers.\n\nI can't remember which department, but it's not public health."", 'I was a military statistician with just a BS in biostats.', 'Biostats, public health, & epidemiology are human-based; environmental programs are much more varied and have different concentrations to choose from depending on the university. Env programs Ive encountered cover many subjects including statistics, because it does help dramatically to have the scientific background when choosing a statistical setup. Env fate & transport (in air, surface water, or groundwater), toxicology, chemistry, ecology, risk, treatment, econ, policy all possibilities to choose from. Env stats needs more professionals! Statistical methods are transferable from human-focused fields, though they might not be as varied. Also see [this other relevant post](https://www.reddit.com/r/statistics/comments/x17778/q_environmental_careers_in_statistics/?utm_source=share&utm_medium=ios_app&utm_name=iossmf).', 'Not sure the path, but make sure you look at the lit.  Environmental Stats uses traditional methods but often has to modify them to fit the data they have to work with.  Things like working with low sample sizes are a lot more important there.', 'Im a statistician at an environmental consulting company. A masters in stats, biostatistics, quantitative ecology, data science, or similar would probably get you an interview. Your actual credentials wouldnt matter much if you could demonstrate statistical proficiency and at least an openness to learning the domain expertise. So my advice would be to do something that you can get excited about. weve hired people with economics backgrounds, and lost quantitative ecologists to pharmaceutical companies.', 'what company?']"
[Q] Which Statistical Test to Use?,"I'm working on a project currently with both the independent and dependent variables being binary. For example, the data compares sex as the independent variable (male or female) vs existence of symptoms of \[disease\] (yes or no). Which statistical test would be best to use for this case?",11fji30,CompetitiveHat2510,1677707997.0,10,0.78,"['Chi-square test, most likely.', 'If you want an OR, you can use a logistic regression', 'Depends on your hypothesis. If youre trying to see if theres any significant association between gender and existence of a disease, chi square would be your best bet. If youre trying to see if gender is a potential predictor of said disease, dichotomous logistic regression would be an option', ""Look up Chi square test of independence and Fisher's exact test (better for small n)."", 'Chi square test', ""If you require a test, nothing more: chi square test (and Fisher exact if you have small expected values)\n\nIf you require a model: I would try log linear modelling if you expect other categorical data to be added, but if you ass in continuous data as well I'd suggest logistic regression"", ""Mutual information is the clear answer, imo. It seems like a straightforward comparison: compute MI between sex and symptoms, and compare that to a null distribution comprised of shuffled surrogates. If p < alpha, you've got yourself a correlation.\n\nMI is great because it is sensitive to non-linear relationships between variables in a way that many of the usual tests are not. \n\nFor binary data, the computation is also stupid easy: I(X;Y) = H(X) + H(Y) - H(X,Y) (where H() is the Shannon entropy). The small number of states means you probably don't even need to correct for the bias in the naive entropy estimator."", ""There are many different options. It really depends on the kind of assumptions and what kind of answer you are looking for. With more context I'd be happy to help"", 'Are you able to tell us what hyptotheses you are testing? Null and alternative.', 'Question from random reader. Is the Chi square test an algorithm?', ""I'm looking at the relationship between sex and the existence of disease symptoms. Both are binary with biological sex vs whether the disease symptom exists or not. There's about 11,000 data points, and the data is coded in 0 for males, 1 for females, 0 for no disease symptoms, 1 for existence of disease symptoms."", 'Null: There is no relationship between sex and existence of disease symptoms.  \nAlt: There is a relationship between sex and existence of disease symptoms.  \n\n\nThat is ideally what I am testing.', 'There are a couple of chi square tests. The one people mentioned here is the chi square test of independence. There is a fairly simple equation you can use to get a chi square test statistic, which you can then use to calculate a p-value from a chi-square distribution. There are a lot of programs, and even graphing calculators, that will do it for you. Just make sure you know the necessary conditions for doing the test and how to interpret a p-value.', 'I ask you a very crucial question now. Are you looking for relationship or causality. Do you want to say at the end that biological sex causes more symptoms or that you observe more symptoms in a group?', 'Relationship. Hoping to see if  we observe more presence of symptoms in one sex over the other', 'In this case the chi2 test of independence makes perfectly sense to be used']"
Question about multiple comparisons [R],"Hi, Im stumped on how to analyze some data, and I suspect that others in my field may be incorrectly correcting for multiple comparisons. Ill try and briefly describe the situation below: 

We record 21 spatially separated channels of brain activity while presenting a stimulus a few hundred times. We then use a circular statistic, the Rayleigh test, to say whether the distribution of phases (one phase value for each time the stimulus was presented) of the brain activity, in response to the stimulus, is significantly different than a hypothetical random phase distribution. So we end up with a lot of p-values from lots of Rayleigh tests (21 channels of brain activity). 

What, if any, corrections should be made for multiple comparisons?

Ive tried to read up on this, and have been doing stats for years, but its just not clear to me.",11fgz1p,darbyhouston,1677702484.0,3,1.0,"[""Yes, you should correct for mc. The EEG (I presume) channels are typically  correlated though (as sources project to many channels), making bonferroni and friends too conservative. I'm much better versed in the literature for time resolved analyses, but for phase you probably could just use fdr. \n\nWhat exactly do you think people misunderstood in the field?"", 'Thanks for the input. FDR looks pretty good. The misunderstanding Im referring to is multiple people recommending that I correct for the number of stimulus presentations (e.g alpha/#trials) which seems incorrect as those arent hypothesis tests.', ""Indeed, that does not make any sense. You correct for the number of tests.\n\n From a theory side, the troendle mc would be a little bit more powerful than FDR, it works only if you use permutationtests, which you do. It is implemented in the permuco package in R - but I haven't seen anyone use it in your use case, might be harder to publish"", 'Interesting. Ill look into it, thanks for your help.']"
[Q] how to calculate interclass correlation,hello all! i love using the psych package in R to get the ICC which i understand to be intraclass correlation. is there a way to also extract the inter? or another package that gives both? ty!,11f92xe,mickmars51,1677690275.0,5,0.86,"[""I think that's just your between-groups correlation.\n\nThe statsBy function might do that for you."", 'oh so literally like a pearsons! that makes sense', ""actually I may have mis-phrased, I'm looking for intra and inter-rated reliability which I guess is not ICC!"", 'You use the same formula for calculating both types.  If the values you estimate your ICC on comes from the same rater -> intrarater.  If they come from different raters -> interrater reliability. \n\nBoth types are intraclass correlations.', 'this blew my mind and solved my problem, thank you so much!', ""I'm happy it helped you. You might want to read this paper. I think its one of the best on the subject.\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/""]"
[Q] Help calculating the Pre-post Δ (Standard deviation),"Hey all, 

I'm working on a meta-analysis and I'm kind of stuck. I'm trying to calculate the delta of a given group's pre- and post-intervention standard deviations. Can someone please help out? Very much appreciated!

For example: 

Preintervention 22.29 (8.39)

Postintervention 19.29 (8.21)

Pre-post delta -3.00 (4.73)

How does one calculate and arrive at 4.73?",11f6u3y,BitBaby6969,1677684981.0,4,0.7,"['Assuming 4.73 is the standard deviation of the change score, you can\'t calculate it without having the correlation coefficient between pre and post. The formula is shown [here](https://training.cochrane.org/handbook/current/chapter-06#section-6-5-2-8) (scroll down to the subsection ""Imputing a change-from-baseline standard deviation using a correlation coefficient"").\n\nTo summarize: In order to calculate the SD of the change, you need the SD from pre and post as well as the correlation coefficient between pre and post.', 'Thank you so much! I missed that sub-chapter']"
"[Q] G*Power - Difference between ANOVA Repeated measures, between factors and ANOVA: Repeated measures, within-between interaction","Hi,

I have a mixed-ANOVA design: 2 groups (experimental & control) x 4 time points (pre, post, post2, post3), and am using G\*Power to calculate the required sample size.

My understanding is that the ""ANOVA Repeated measures, between factors"" should be used if I am interested in the sample size required to detect the main effect of the group factor, and that the ""ANOVA Repeated measures, within-between interaction"" should be used if I am interested in the sample size required to detect the interaction effect (group\*time).

However, the sample size required when I used ""ANOVA Repeated measures, within-between interaction"" is lower than the sample size required when using ""ANOVA Repeated measures, between factors"" in G\*Power, which is weird as we generally require larger sample size to detect an interaction effect.

Am I missing something here?",11f1ifo,whotakemy,1677670552.0,5,1.0,"[""G\\*Power is working properly.\n\nCheck out pages 18 and 19 of the paper (which i copied the following from) [https://www.journalofcognition.org/articles/10.5334/joc.72/](https://www.journalofcognition.org/articles/10.5334/joc.72/):\n\nG\\*Power's recommended sample size is only valid under the following conditions:\n\n1. the interaction effect is as big as the main effect (i.e., the equivalent of a fully crossed interaction), \n2. you compare designs in which you only have one observation per condition (so, more levels of the repeated measure gives you more information), and \n3. you are only interested in the overall interaction effect of the omnibus test, not whether the pattern of pairwise differences agrees with what is in the population."", 'I dont know but I hope G*Powers calculation includes an epsilon correction such as Huynh-Feldt? If a correction for the almost-certain violation of sphericity is not used then the Type I error will be inflated.', 'Thanks so much, this helps a lot!']"
[Q] Is it bad if my two-way ANOVA between subjects factors aren't even?,"I have triple-checked my assignments for the groups and there's no way they are wrong, and yet I end up with 32/30 and 32/30 in my IV groups for between-subjects factors. I see every example with even numbers in each so I'm wondering if this is a problem",11f0wot,MidnightMei,1677668528.0,3,1.0,"[""You're probably fine. Those are pretty similar group sizes and ANOVA is robust to minor violations like this."", 'ANOVA does not assume equal sample sizes although power is greatest with equal sample sizes. The examples you see probably have equal n because the calculations are much more complex with unequal n and unequal n requires one to choose the type of sum of squares you are using.']"
[Q] Basic question about confidence intervals,"Hey everyone! I'm a PhD student that is
INCREDIBLY bad at stats (yeah, this should go well), and I have a question about confidence interval. Since my undergrad, Ive always been taught that a confidence interval that includes 1 is considered to be not significant, but in my current course (and I believe the course I took in prep for this one), my professors have said that the data suggests significance unless the CI ""crosses"" 0. I was even having a conversation with one of my attendings the other day about a study we reviewed and I commented that the data was significant because the Cl didn't include 0 and she corrected me to say ""doesn't include 1"". So now l'm all sorts of confused. We're using SPSS for this particular example, and it's even suggesting significance with a Cl of 0.51-21.16 with a p=.04. So, which one is it?",11esfz7,AGACNP,1677639913.0,36,0.93,"['It depends what is the parameter of interest and what is the null hypothesis. Usually if youre interested in a mean difference, your hypothesis test will specify the null: mean difference = 0 and so if your CI does not include 0, your result is significant. But other times you might be looking at odds ratios where the null is instead that OR = 1 (because OR = 1 implies equivalent odds between the two groups in question and is thus a natural choice for the null).  In this case, your results are significant if your CI does not cross 1.', 'Others have resolved your confusion, I\'m just here to add a more general tip.\n\nYou can\'t learn statistical methods by rote, you need to understand the underlying ideas. It can feel like a scary subject but it\'s just ""advanced common sense"" with a little bit of maths needed to express some of the ideas precisely enough to be useful.\n\nHere, we\'re interested in whether the confidence interval is consistent with the null hypothesis. A 95% CI which does not contain the null value is (typically) deemed inconsistent with the null hypothesis (and will usually also have a p-value <0.05, as they\'re usually both computed from equivalent methods). The null value is usually 0 for a difference in means, or 1 for a ratio statistic.\n\nWhile I\'m here, it would be remiss of me not to suggest you also try to get to grips with what a p-value actually is (and isn\'t) and why the p<0.05 standard is inadequate in many (most) contexts. This is a very useful explainer which might also give you some ideas on how to turn the often rather abstract ideas you will be taught into concrete examples you can understand and therefore remember and apply properly: [The p value and the base rate fallacy](https://www.statisticsdonewrong.com/p-value.html).', 'Are you testing an odds ratio or an absolute effect?', 'A CI of a ratio would be not significant if it included 1. This is because if a/b ~ 1 then a must be similar and n value to b.  \n  \nSimilarly, a CI of an absolute effect or a difference would not be significant if it included 0. This is because if a - b ~ 0 then a must be similar in value to b.', ""If you're calling something *significant* you're doing a hypothesis test. In that case the value you look at as causing you to reject H0 when it's outside the interval *depends on the hypothesis*.\n\nIn some situations that might be 1 in other situations it might be 0. In still other circumstances it will be something else.\n\nWe can't tell your what your hypothesis is, and you have not given us enough information to guess \n\n(I will, say, however, that if you're 'bad at stats', don't use CIs to do tests, since it does often happen that they cause confusion/errors when trying to carry out tests.)"", 'Thanks, everyone! That was all very helpful in clearing up my confusion.']"
[Q] Looking for a bit of guidance about statistics and bets," Hi, I know the title might appear not that serious, but i am looking for a legit response.

What statistical subjects should I master in order to bet with a controlled risk?

What would be the learning roadmap for this?

Is there a specific programming language I should learn to develop models?

I apologize if the post looks out of place but again, I am serious about this, thank you for your time.",11ert31,kharnaval,1677638176.0,1,1.0,"['>What statistical subjects should I master in order to bet with a controlled risk?\n\nDecision theory (expected utility, the ""Kelly criterion"") etc tells you how to make decisions under uncertainty in an optimal way. Optimal means there is a clearly defined objective. You need to reflect on whether popular objectives are in line with your goals though. Maximising wealth growth isn\'t going to be the same as minimising the risk of a bad loss.\n\nApplying these theories at minimum requires assessments of what events might happen, and usually assessments of probabilities that they happen. If you feed bad assessments of how likely something is into your expert decision model, you get back a bad decision. Where do you get good assessments from when making bets? You need to have as much information that helps you predict events as possible. And you need to turn these into probabilities. Regression models are the subject to learn for this. The world\'s best model used on easily available public data will be less useful than someone\'s shrewd guess based on private, insider info.\n\nOnce you have all that, you need to take into account fees when making bets and other constraints (eg bet size with a bookmaker, or not getting to bet if you are too successful). You should be able to bet on the best odds available for an event to maximise your return, taking these constraints into account. \n\nIf you appropriately take that into account, you can think about whether you\'re doing any of this much better than other bettors. If no, it is not likely you\'ll be a consistent winner. Maybe buying a government bond yields a better risk adjusted expected return for you.', 'I would first get comfortable finding and manipulating data. This can often be the most time consuming. \nAfter that basic probability, including conditional probability and Bayes law. \nI use R predominately and it works great but there are many that prefer Python for languages. \n\nIm not suggesting that is your best end game, but it would be a great place to start.']"
[Q] Difference between Masters of Statistics and Masters of Science in Statistics?,"Apologies in advance if this is a repetitive question - I've tried looking for the answer and can't seem to find it.

What's the difference in degree with programs that are called ""Master of Statistics"" versus ""Masters of Science in Statistics""?

I've heard mentioned in some places that Master of Statistics is considered a ""lesser"" degree than a MS, but I don't have concrete reasons why. Can anyone help me understand the difference (if there is one)?

Thank you!",11eph0z,khops287,1677631854.0,2,0.75,"['These types of distinctions make no difference at all. \n\nMasters is a masters. People who care are likely gatekeeping.', 'Where I study, the MS Stat degree is more in-depth than the Master of Stat degree.\n\nFor example, the Master of Stat degree includes courses like: introductory probability, introduction to statistical inference, applied regression analysis, and applied multivariate analysis. Note the use to introductory and applied.\n\nWhile in the MS degree, courses are: probability theory, parametric inference, linear models, multivariate analysis.\n\nMS stat also requires a masters thesis to complete, while master of stat requires a simpler research paper.', 'At least in the US, I thought they were all masters of science in statistics at accredited schools', 'can you link those degrees? we can look at the course requirements and stuff and can prolly answer better', 'That just sounds like an abbreviation imo', 'It depends on the school, but you\'ll sometimes see non-thesis degrees that do not have the ""of science"", although most don\'t bother with a distinction and offer a non-thesis track.\n\nAt my last school, the Master of Applied Statistics was just a rebranding of the non-thesis MS track in the beginning. It seems that they\'ve switched that up so that the MS can be completed with a thesis or an exam, and the MAS can be completed with a capstone project. The MS program has several theory courses that are optional and probably geared towards PhD track students. The Statistical Science track of the MAS has a lot of over lap with the MS, while the Data Science track has a few classes from a different department\'s (Computer Information Systems) MS program.\n\nAs you can see, even within one department the difference can be quite wide or it can be as small as the choice between an exam or a capstone.', 'Nah, it varies quite a bit school to school. You can get a Master of Arts in stats from Boston University, for example.']"
[Q] Please explain alpha equivalence like I'm 5,"Have recently covered transformations to stabilize the variance in SLR. The transformations are all stated such as ""If var(ei) is alpha equivalent to Xi\^2..."". I understand that transformations are necessary in some instances to correct a model, but what does alpha equivalence entail? I have tried looking up relevant articles/videos but none of their explanations are really making much sense to me. Something about the bounding of named variables? Thanks in advance - just want to be able to wrap my mind around it.",11elwpo,GottaBeMD,1677622961.0,2,1.0,[]
[Q] What classes are the general prerequisites for Stats/Data Science Masters/PhD programs?,"I have heard generally about probability, linear algebra, calc, and real analysis. Is there anything I should add to this list? Also, what kind of math classes should I focus on that would be the most helpful for grad school? I am doing okay in real analysis, but it is definitely difficult and I am not sure if I want to keep taking a lot more proof based classes since I don't need too many more to graduate.",11el10u,ewoodnc,1677620927.0,0,0.5,"['it varies by grad programs... take a look at the programs you want to apply to and see what they require for admission.\n\nOne gap you should look at is programming. For DS, you would want to pick up phyton for example.. while Stats, perhaps R or SAS or STATA...', 'Calc I-III and Linear Algebra is pretty much the bare minimum, other programs may want more - PhD programs will value the Real Analysis course work', 'A big one missed here: Probability.', 'Struggling with your math?  Please check your chats once I have shared some valuable information that can help .', 'Master: General Cal (usually offered for general life science), Linear Alg, statistical theory\n\nPh.D.: Mathematical route Cal (up to multivariate), Linear Alg (intro + advanced), Real Analysis, Discrete mathematics, Probability + Statistical Inference, computational statistics, statistical methodologies (regression, anova, time series analysis, GLM, etc.),  capstone project, and REU.\n\n**This is the US standard**. However, the requirement can be less stringent in the EU.', 'What would a statistical inference class/computational class be like? We have to take stochastic modeling and a good amount of programming classes', 'In a sense, you can expect the statistical inference to be a level of Casella & Berger or John Rice (the latter is more applied). \n\nComputational statistics can vary depending on the program. Usually, they are involved in advanced simulations or optimizations.']"
[Question] Sample sizes are not the same,"I am working on an app and trying to understand how many people need to make it through my feature in order to feel confident in the conversion data. My constraints are:

95% Confidence Level  
20% Margin of error  
Using 50% for population cause i'm not sure  
Unlimited population

This tool says i need a sample size of 25:  
[https://www.calculator.net/sample-size-calculator.html?type=1&cl=95&ci=20&pp=50&ps=&x=70&y=18](https://www.calculator.net/sample-size-calculator.html?type=1&cl=95&ci=20&pp=50&ps=&x=70&y=18)

This one says i need a sample size of 2900:  
[https://www.optimizely.com/sample-size-calculator/#/?conversion=10&effect=20&significance=95](https://www.optimizely.com/sample-size-calculator/#/?conversion=10&effect=20&significance=95)

They are not exactly the same calculator since one takes into account baseline conversion, but i am not sure i understand why the sample sizes are 100x different.

What is causing this discrepancy?",11ekc9a,KaufNation,1677619302.0,1,1.0,"['There are two big differences between these two calculators.\n\nOne is that the first calculator is aimed at the case where you collect a single sample and compare it to a fixed target. It told you that, if the true rate is 50%, a sample of size 25 is sufficient to ensure that your sample will usually give you an estimate between 30% and 70% (5020%).\n\nThe second calculator is aimed at A/B testing, where you collect two samples and see if they differ from each other. As a rough rule of thumb, if you need N samples to compare against a fixed target, you\'ll need 2N samples in each of two groups to compare them against each other with the same precision.\n\nThe other difference is your second calculator\'s effect size is *relative* change in conversion rate. It told you that you need two groups of size 2900 to distinguish between a 10% and a 12% conversion rate (p=.12 being 20% more than p=.10.)\n\nYou don\'t say what conversion rate you actually expect; whether your ""20% margin of error"" is relative or absolute; and whether you have a fixed target you are trying to meet, or are comparing your app to another one to see which is better. You\'ll need the answers to those questions to decide which calculator is right for you.', 'Now if you\'d only used one, you\'d have no doubt, right? This is why online calculators are dangerous; to get a number out you only need to know enough to put viable numbers is, not whether it fits your specific use case.\n\n1. Baseline conversion rate is certainly one issue, yes. \n\n2. The first link is for a *confidence interval for a single proportion*. The second is for a test comparing two proportions.\n\n3. The use of the word *relative* in ""minimum relative change in conversion rate"" concerns me. What do they mean by *relative* there? (Clicking the ""?"" was no direct help, since the link is dead and you just end up at a generic looking page). I can make assumptions about what they intend, of course, but I would want to see *exactly* what they think it means.\n\n using the wayback machine ( web.archive.org ) does seem to give some\ninformation, but not all of the page properly renders (it seems to rely on other assets that aren\'t showing up). It\'s probable there\'s an explanation for any remaining difference in there.\n\n---\n\n""Statistical Significance\n95%""\n\nOkay, important tip: Beware any tool that is written by people who *don\'t even understand what a significance level in a hypothesis test is*. They are very, very likely to get less basic things *even more wrong*.']"
[Q] Does it make sense that the R-squared on three variables combined is substantially less than the R-squared for each variable individually?,"I am not a statistician and I only have a vague, long-ago understanding of these concepts :)

I built a very simple predictive model based on three variables. I used Excel to calculate the R-squared for each variable and they came out to be 67%, 96%, and 83%. But when I combine all three variables into one ""super-variable"", the R-squared comes out to be 21%. Just wondering if that is an unexpected outcome.

My layperson's point of view is that if each variable is highly predictive then all three combined should be similarly predictive.

EDIT:  To clarify what I meant by ""super variable"", suppose my independent variables were Zip Code, Industry, and # of Employees.  Each of those on its own generated the R2 values above.  My so-called super-variable combined all three together, such as ""90210+Construction+25 employees"".  When I ran the combined variable against my dependent variable (which happens to be sales), then that's where I got the R2 of 21%.

If it matters, because not every combination is represented, there were a lot of zeros.  But when I took them out, the R2 went to 20%",11ek3do,getthedough9191,1677618745.0,12,0.88,"['> I used Excel to calculate the R-squared for each variable and they came out to be 67%, 96%, and 83%.\n\nIt sounds like you were running three separate models:  ""x1 and y"", ""x2 and y"", ""x3 and y""\n\nYour next step should not be to combine the variables into one \'super variable\'.  Your next step should be a *multiple linear regression* where ""x1, x2, x3, and y"" is your model, and you can see all three variables together. \n\n> But when I combine all three variables into one ""super-variable"", the R-squared comes out to be 21%. Just wondering if that is an unexpected outcome.\n\nI think so.  Your formula to create the \'super-variable\' is not working.  If you think that some relationship between the variables might be helpful, then try adding a fourth variable like x4 = x2 * x3.  But you want a good reason for this!\n\nIn general, *adding variables to a model, not combining them* will increase r^2.  Also look at *adjusted* r^2, because if that doesn\'t go up, your added variables are less useful.  \n\n> My layperson\'s point of view is that if each variable is highly predictive then all three combined should be similarly predictive.\n\nNot necessarily.  Depends on the how the variables themselves are related, and then how they are combined.', 'OP summed the independent variables into a single column, and then ran a univariate regression.  Your intuition is sound, but your execution was flawed and resulted in a very noisy single predictor.   \n\nIf only there were a way to find the ""best"" combination of your predictors?   Well the answer to your question (in a particular sense of ""best"") is MULTIPLE LINEAR REGRESSION.  \n\nother replies are confused because they assume OP ran a multiple regression and obtained a lower R^2.', 'Imagine a model y ~ x1 has an r sq of 0.9, and then you have x2 = -x1 + e. The model y ~ x2 may have r sq of 0.8 because of the relationship between x1 and x2. Now we create a super variable called z = x1 + x2. Guess what, the model y ~ z only has r sq of 0.05', 'Is this an *adjusted* R-squared? Is it being calculated on the test set?\n\nEDIT: And what on earth is a ""super-variable""?', 'Define ""supervariable"". Did you add them to your model (y = x1 + x2 +x3) or did you literally add up the values of the three variables into one column and regress that on the dependent variable? Because if its the latter, its no wonder because its wrong.', 'Yes, this could easily happen depending on the variables and how you combine them. I just created a regression problem where y~x1 had an r^2 of 0.88, y~x2 had 0.89 and x3 = x1 + x2, y~x3 had an r^2 of 0.07.', ""Isn't due to the overlapping properties of the prediction power of the variables?\nmulticollinearity"", ""I'm not familiar with Excel's method for computing R-squared. Could it be giving you the adjusted R-squared?"", 'No because R squared should not decrease as more predictors are added. It should only stay the same or increase.', ""It does not make sense.  There is something not right somewhere in your calculations.  Can you describe your variables a d what you are trying to model?  Also how did you calculate your 'super variable', and why not just include all three iv's in the model instead of a super variable?"", ""Yes.  Maybe this will help.  https://en.wikipedia.org/wiki/Simpson%27s\\_paradox#Simpson's\\_second\\_paradox"", 'This is a perfect explanation.  Id save your excel as a CSV, use read.csv() to get it in to R, and then lm() to fit a linear model of your Y on x1, x2, and x3. \n\nHeres a good primer: https://www.datacamp.com/tutorial/linear-regression-R', ""Spot on, this is probably the next level of stats I don't know."", ""Yeah you created nonsense, likely getting to granular as well with the combinations, look for patterns that aren't there.\n\nIt also creates heteroskedasticity I believe bc the variable you created isn't a reliable predictor. Your combining 3 independent variables of different types so while you know they match-up to the dependent now if you simulate this I'm sure you'll get nonsense."", 'I think he concatenated, not summed.', 'yes, yes, yes. if the direction of the relationship between x1 and x2 differs, your super variable will come to grief. as others have said, multiple regression should being more joy', ""It is entirely possible, usually as the result of poor decisions.\n\nFor example: order number, manufacturing day, and calendar day may all have a strong correlation with kW hours used that year. Combining those four features in a model makes less sense than any correlation that exists individually.\n\nClaiming a higher confidence in knowing the kW hours based on the calendar and manufacturing day an order was received with it's associated number is a bad model no matter how well the individual values may correlate."", 'Thats just wrong and you could easily prove it for yourself in R', 'Ooohhhh maybe !  I read the + signs as addition.  Also if he concatenated the three variables into a single column, where did he get the additional dependent observations from?  Seems like the concatenation would produce a matrix with more rows that the dimensions of the data. \n\n Either way, this is a ""fuck around and find out"" approach to multiple linear regression lol.    I wish I had students that did what OP is doing because he created his own problem where the actual solution is given by OLS.  That creates a very powerful and personal connection to the methods in practice.']"
[Q] [E] [C] Resources for reading research in HR/People Analytics/IO Psychology,"Can anyone recommended journals or resources for research in the area of IO psychology/people analytics?

I want to better understand how researchers in this field determine ""acceptable""/""significant"" correlations, testing methods, factors in estimating a type II error rate, determining 1-tail vs 2-tail t-tests in practice (I know the difference on paper, but I understand it is often subjective), etc.",11ejk7i,werdunloaded,1677617528.0,2,1.0,['You could start with the Journal of Applied Psychology.']
[Q] Interpreting negative differential entropy,"It's well-known that differential entropy can be negative - does anyone have any good heuristics for interpreting this case?

For instance, we know that, for a Gaussian RV X:

h(X) = ln(\sqrt(2e))

Obviously, any  < 1/(\sqrt(2e) will results in h(X) < 0. Is there significance to this value? It seems like a highly peaked distribution will have negative differential entropy, but it's not obvious why this should be the case, or the interpretation in terms of uncertainties.",11ei3c3,antichain,1677614042.0,8,1.0,"['I think the take-away is negative differential entropy on average means your random variable is \'tighter\' than the continuous uniform distribution on \\[0,1\\].\n\na) So differential entropy is actually not strictly what you get when you take the limit of the riemann sums of discrete entropy, it\'s missing a -log(dx) term. Informally, if we take the Riemann sum of discrete entropy, we get lim\\_{|P| --> 0} \\\\Sigma\\_P -p(x\\_i\\*dx)log p(x\\_i\\*dx), which is equal to  \\[\\\\Sigma\\_P -p(x\\_i\\*dx)log p(x\\_i)\\] + \\[\\\\Sigma\\_P -p(x\\_i)\\*log(dx)\\] = \\[differential entropy)\\] - \\[log(dx)\\]. Anyway.\n\nb) one way I think about it is in terms of the pdf of the ""corresponding uniform"" distribution, eg if a random variable has differential entropy H(X), then the uniform distribution with entropy H(X) has constant probability value 2\\^{-H(X)}, so we see that eg a uniform distribution on \\[0, 0.1\\] will have p(x) = 10, and H(X) = -log\\_2 (10). So then any continuous random variable that\'s on average ""tighter"" than \\[0,1\\] has negative differential entropy.\n\nc) another way to intuit it is maybe that the entropy (when it\'s computed using log\\_2) corresponds to the average message size, or number of bits or binary decisions that we need on average to single out or uniquely specify a sample from a distribution. So the discrete uniform distribution on the integers \\[\\[1\\],\\[2\\]\\] has entropy 1, the continuous uniform distribution on the real subset \\[0,2\\] has entropy 1, so one binary decision for both of those. \\*But\\* the discrete uniform with all mass concentrated on a single value has entropy 0, and similar for continuous uniform on real subset \\[0,1\\]. So again, I guess anything tighter than \\[0,1\\] has already been narrowed down to only needing one binary decision and to specify it any further than that gets us negative differential entropy.\n\nI\'m not saying this is super insightful, but maybe different ways to intuit it.']"
[Q] What to do when 2/3 Quartiles are the same?," I have a 500K data set that counts customers and their trips into Cabernet Sauvignon. I working to replicate a report I saw that groups your best customers by the quartile of their spend and the quartile of their trips.

The problem I'm running into is when I calculate the quartiles using Excel, I keep getting 1,1,2, ad when I exclude single visits, I get 2,2,4 ...

my gut is telling me to make my quartiles 1,2,4 but I don't want to fudge the data... I'm attempting to make this analysis repeatable, so how do I solve for the duplicate quartiles? Do I leave them in and my resulting scatter plot will only have 6 sections?",11ehtp9,Multiverse_Madness,1677613398.0,1,1.0,"[""There is no solution that does all the things you want to do / avoid at the same time. Fixed quantiles don't work so well as distributional  summaries with highly discrete data (like counts with small means)\n\n\nI didn't follow what you meant about 6 sections at the end. What are you doing?"", 'What do you mean by solving the issue? If the lower 75% of trips taken was one, then naturally the lower 25 and 50 as well. Just leave out the 25 and 50 then.', '&#x200B;\n\n|3|Potential customers|best customers|best customers|\n|:-|:-|:-|:-|\n|2|Infrequent customers|Potential customers|best customers|\n|1|worst customers|Infrequent customers|Potential customers|\n||1|2|3|\n\nWith the Y-axis as Trips/Visits quartiles, and the X-axis as Spend quartiles, I want to create a distribution of customer IDs', ""I ended up doing 1, 2, and 3+ trip shoppers, but I'm going to revisit if trips is the right measure I'm looking for"", ""That really should go up in your post, for any other person who's trying to understand what you're asking."", 'Thats then a conditional quantile. \n\n\nMaybe start with the question (the exact formulation) of what you are actually interested in. Then translate that into a formula. 80% of the difficulty is formulating the correct question.', 'I\'m looking to isolate Wine customers at a retailer to determine if the ""best customers"" (as called by the merchants) are actually shopping the niche vineyards or if they\'re just buying Josh, Decoy, and 19 Crimes like the rest of America.\n\nSo I want to know who are our most valued customers in share of dollars (how much of their retailer wallet is being spent on Wine) and in trip frequency.\n\nIf I can isolate them using quartiles, then I can create a different view that looks at just those customers and see what\'s in their basket.\n\nConversely, I can also then see the lowest quartile customers and see what\'s in their baskets (and what low velocity items can be cut from the assortment)']"
"[Q] How do you decide when a logistic regression model is ""good""?","Hi, I didn't really cover this in school but it's now coming up in my job... I've looked online without much success, I think I just need someone to explain it to me.

One view I've seen says that basically the only thing that matters is the classification rate on the test data: we use the model to get probabilities, and define a cutoff for what the response should be. Then the model is good if we correctly guess a lot of them right. (In my case the cutoff is just 0.5, and I've been seeing if we correctly guess more than half of them)

That makes sense, and it's easy to follow, but I feel like there should be other things we should be using. 

For example, in R I can use the summary() function on the model and it tells me the residual deviance and the AIC. All I know about these stats is that lower is better, but how much lower?

If it says that the null deviance is 3000 on 2340 degrees of freedom and the residual deviance is 2800 on 2335 degrees of freedom, then is that good?

Are there other stats I should be calculating?",11efku4,yoyomangi,1677608066.0,58,0.96,"['Most answers provided thus far are from the prediction point of view. ""Goodness of fit"" is assessed through the discrimination capacity of your currently fitted model. There are other Goodness of fit assessments which are not aimed at prediction, rather goodness of fit, and does not require test-validation samples, very alike of the goodness of fit of traditional linear regression model. graphical inspections of residuals and so on.\n\nI think the most straightforward method to assess goodness of fit for these models is by means of the likelihood ratio test. In the traditional approach you compare your model with one that does not have covariates, only an intercept (a NULL model) .\n\ncheck [this](https://stats.stackexchange.com/questions/6505/likelihood-ratio-test-in-r) out.', ""> One view I've seen says that basically the only thing that matters is the classification rate on the test data: we use the model to get probabilities, and define a cutoff for what the response should be. Then the model is good if we correctly guess a lot of them right.\n\nThat is the sort of view that is (in my experience) common among machine learners, but uncommon among statisticians.\n\nWe use the model to get probabilities. The model is good if the outcomes that actually occur are the outcomes the model says are most probable.\n\nBut imposing a cutoff and just counting right and wrong answers is throwing away information. Getting a 51%-chance prediction wrong is a much smaller deal than getting a 99%-chance prediction wrong. \n\nIt is also a bit unfair to judge a model by a criterion it did not seek to optimize. If you fit a model by maximum likelihood, you judge its goodness of fit by some likelihood-related method, not by counting correct predictions. (If you fit models by maximizing the number of correct predictions, sure, judge them on that basis... soon enough you'll soon run into a pathological case that will scare you out of ever using that fitting method again.)\n\nYou aren't going to find a one-size-fits-all formula for what constitutes a good fit. (You won't for linear regression either - you'll just find more standard options.) All of the usual comparison tricks are available, and that is where things like AIC shine. If a simpler model does much worse, but a more complicated model fails to do much better, you have some confidence you've done the best you can do with the data at hand."", "">If it says that the null deviance is 3000 on 2340 degrees of freedom and the residual deviance is 2800 on 2335 degrees of freedom, then is that good?\n\nYou can't use this for binary data goodness of fit. Look up Hosmer-Lemeshow for an alternate test.\n\nAIC is useful for comparing models. In that case, lower is better. Alone it's not useful. \n\nYou can use k-fold cross validation to improve your current classification testing strategy. \n\nI realize now my responses are all out of order with your original questions. Lmk if unclear."", ""If you know the cost of false positives and false negatives, you can use specificity and sensitivity to choose between models and classification thresholds. Overall classification at 0.5 has appealing simplicity, but imagine you're trying to predict whether someone has a 1 in 10000 cancer. Most likely, your model will just give <0.5 predictions for everyone, no matter what your data is. But the model may still be very useful, if it gives higher probabilities to those who do have it (as a false negative is very costly, someone dies, while a false positive sucks but can be cleared up with a further test). This is even if the probability was low, even sub-10%."", ""The two most common metrics used to assess separating power of the model are KS (Kolmogorov-Smirnov) and Gini coefficient.\n\nYou also may need to check how well the model is calibrated. If all log odds double, KS or Gini won't change, as they only check rank ordering.\n\nFinally, no single number will tell you the whole story. For example, you may only care about separation at the lower end of the range."", 'Look at your conf matrix, model stars, etc. You can have holes in your model based on imbalanced classes and resulting sampling, or various other dynamics with collinearity. You can perform pca and get into the weeds of component analysis if you want to make inferences. If you just want to predict then get collinear as fuck. But you might be predicting a class a null all the time because your alternative is rare, so it gets high accuracy because it just always predicts not sick or whatever. Type 1 and type 2 are both important, so you can cheer yourself on for a high accuracy but it may not be terribly predictive over the null. Also make sure you define your null, calculate mse/loss, etc.', '[deleted]', 'The most important metric when it comes to classification problems is the AUC ROC.', ""The classification table approach with the cutoff value is fine, using an ROC curve is better, as it doesn't collapse the values of the predicted values into one cutoff value. Once you use an ROC curve for predictive capability, then use the concordance index. Analysis also depends on the format of your data, if the format of your data is rows with binary 0/1 and covariates, then the residual deviance test statistic for goodness of fit is unadvised because the deviance residual test statistic doesn't follow a approximate chi-squared test statistic (because the residuals should follow a approximate standard normal distribution, but since the sample sizes is 1 because of the way the data is formatted, the residuals don't have that distribution). For the comment on the hosemer and lemeshow test, that's used if you have quantitative explanatory variables."", 'Glad you posted this. With the popularity of ML I think people have fallen into the trap of seeing likelihood on held out data as the be-all end-all of model validation. I highly recommend people check out chapter 6 in BDA3 on posterior predictive checks and test statistics then I believe chapter 15 or 16 where they have the election regression example. \n\nBasically once you see your model as a generative process of your data you can do much richer queries than just predict the average of a 1D outcome. You can then test your models ability in answering those queries using test statistics and posterior predictive checks that capture not just mean behavior but distributional behavior. Also as OP said if your test statistics are ancillary statistics like say the fifth percentile of some outcome as in the speed of light example in BDA3 then its sort of already built in held out information since.', ""Does this work with conditional logistic regression? My understanding is the intercept term drops out which implies there is not a null model (unless you're specifically comparing nested models)."", 'Could you point me to resources on using maximum correctly predicted as the optimization criteria instead MLE?', 'Deviances absolutely can be used to compare nested models', 'you can always compare to a null model tho ...', ""That's helpful, thank you. I'll look up that test.\n\nCan you please explain when the deviance would be used then? Would it be if I was interested in the rates/probabilities the model gives, instead of just using them to classify?"", 'Thank you for the response! Can you please elaborate on your second point about calibration? (Or will it become obvious when I research the tests you suggested?)', 'Not in half, an 80-20 training-test split is standard.', ""That's a terrible way to assess whether not a logistic model, or any model in general, is a good fit or provides prective power."", 'My only advice on the subject is ""don\'t."" (And not just because discrete optimization is hard compared to continuous.)', ""Let's say you are building a credit risk score like FICO. You compare its performance on development sample and on the hold-out sample from a different time period, and KS is the same. But default rate associated with a given score band is different - the model has drifted, and instead of predicting 5% probability of default for, say, FICO 640 is now predicting 10% probability of default.\n\nIf your business is profitable lending to customers with 5% probability of default but is losing money lending to customers with 10% probability of default, you are interested in that difference, even if the model's separating power is still the same.""]"
[Q] When should one compute RMS or average of an oscillating non-negative time series?,"I am working in a lab and we are trying to compare an oscillating quantity over a time window (it is more or less a sawtooth pattern, but is biological data so not a perfect sawtooth) to some other baseline quantity over time (which is constant during the time window).  Thus, we want to calculate some overall value for the fluctuating time series and compare that to the constant baseline value. 

&#x200B;

I was wondering if under this circumstance taking the RMS over time would be better than simply taking the average. I know for sinusoids that oscillate around y=0, taking the average yields a value of zero which is not helpful. 

&#x200B;

I also know that RMS is a metric for dispersion (so not good for central tendency?) so I am not sure if it is better than a non-zero average computation. However, we are basically doing a t-test where one distribution has some variance (oscillating sawtooth) and the other distribution has effectively zero variance (constant over time). So in that case wouldn't dispersion also be important?

&#x200B;

Any insight is appreciated!",11efd0c,surf_AL,1677607522.0,4,1.0,"[""Without knowing exactly what your question is, it's a bit tricky to know what the best metric is. \n\nIf you want a holistic measure of dispersion, why not compute the differential entropy? It has some nice intuitions behind it, and since it's based on probability distributions rather than direct values it behaves nicely in many contexts. You can compute it easily using Gaussian assumptions, or if you want a non-parametric estimator, the Kozachenko-Leonenko method is one I'm fond of.""]"
[Q] When should I use Mann-Whitney U test vs. Wald-Wolfowitz test,,11ef6uu,Akilis72,1677607128.0,4,0.84,"['Those tests are used to answer *completely different questions*. The opening paragraphs of the respective wiki articles ([Wald-Wolfowitz](https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test) and [Mann-Whitney](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)) do a good job of explaining the general use cases.', ""Im glad this question was asked as I've never heard of the Wald-Wolfowitz test"", 'I totally forgot about that test, I saw that in my bachelors, and have never used it in real life.', 'A.K.A. the ""runs"" test']"
[Q] Can a logistic regression result be imported to R?,"I have a logistic regression that was run in a program that uses R for the stats. I don't have access to the individual level data, as the program takes care of that. But is there a way to take the output from this and put it in R to be able to use the predict() function on it in R?",11ec15z,7r0gdor,1677599629.0,9,1.0,"[""Probably not what you're looking for, but it's pretty simple to just type the equation. P(y|x) is just 1/(1+exp(-1\\*(b0 + b1\\*x1 + b2\\*x2 + ...)))."", 'You can save the model as an R object.', 'If it gives you the weights of the model, yes. If it just gives the predictions, no.', ""`predict` operates on an object with a specific class (a class that has a `predict` method). You'd need to have that specific object that's returned in order to call `predict` on it, not just the output that is put to screen (typically the output is generated by calling `summary` on the same object that you would need to have).\n\nSo ... possible, but not just by grabbing the screen information (unless you extract the information from that output yourself, either automatically or by hand, and write your own prediction function, but you won't have the ability to get standard errors that way, for example)\n\ne.g. you can save the glm object to a variable by `myresults <- glm(...)`, then if you have that object when you want to predict (which can be made available in a new session in several different ways), then `predict(myresults)` should work just fine."", 'Pearl diver?', 'I will try this thank you!', '> `-1*(b0 + b1*x1 + b2*x2 + ...)`\n\nunary minus ... `-(b0 + b1*x1 + b2*x2 + ...)` should work correctly', 'Unfortunately, I cannot as this is a different program that uses R and gives me the output, but I am not able to use it like R. I can only use the functions they allow.', 'It gives me the coefficients, odds ratios, variance inflation, and variable matrix', 'Yeah exactly, I was thinking of creating a risk calculator based off of the logistic regression I did.', 'Note, b0.bn in this equitation are log-odds.', 'Thanks - I was being over-cautious.']"
[S] Changing Axes Range in Sigma Plot,"Hey y'all!

I'm currently using sigma plot to create some graphs, and I am having a bit of an issue with scaling the axes. Currently, the axes are set up such that there is a starting/baseline value and an upper value, with the bars being positioned at the starting value.

I am wondering whether there is a way to change the axes such that it shows a range of values above \*and\* below the starting/baseline value? E.g. if zero was the baseline it would show both positive and negative values above and below, respectively. This way my bars could ""point"" above and below the baseline value, if that makes sense.

Thank you!",11ec14q,selena_x,1677599627.0,2,0.75,"['This is not really a statistics question, it is a software question. And, I doubt that many ""statisticians"" use it... I have only seen Chemists use it, personally. Previous posts to /r/statistics relating to SigmaPlot haven\'t gotten any responses in the past 5+ years.  I see a few responses in /r/chemistry , /r/biology , and /r/labrats, but this  seems to be a very niche product.']"
[Q] Is Huffman Encoding suitable for fair dice-rolls using coin tosses?,"Assuming a fair coin, if I am simulating a six-sided die roll, are the following rolls an even distribution of probability as if it were a fair die?

1 - T/T

2 - T/H

3 - H/T/T

4 - H/T/H

5 - H/H/T

6 - H/H/H

The part I am not quite understanding is if the required H-prefix for 3-6 affects the overall probability so this would be weighted in a biased manner.

Use case is using Diceware for a game but wanting to use coin flips instead of dice but maintain the 1/6th probability whether it's the 2 (compressed) or 3 flips.

The other option that I don't like is to always flip 3 coins and discard any HHH or TTT flips.

How can I achieve this?",11e2fxm,akayataya,1677578864.0,1,1.0,"['This would not be a uniform distribution, as the probabilities of 1 and 2 are both 1/4, not 1/6.  You would need to re-roll (re-flip) certain outcomes like your last method, even if that seems undesirable.', ""No finite sequence is guaranteed to work as you have 2^N equally likely option after N rolls and 2^N is never divisible by 6.\n\nA simple algorithm that's still optimal in terms of the expected number of rolls:\n\n* 1 - TTT\n* 2 - TTH\n* 3 - THT\n* 4 - THH\n* 5 - HTT\n* 6 - HTH\n\nIf you get HH, start over. Compared to your algorithm this has the benefit of saving a (useless) third flip in the 1/4 case that we need to start over."", ""If you insist on a fair die, you will *always* end up discarding flips at some point, since there's no positive integer m such that 2^m is a whole positive multiple of 6. If you want a finite number of steps to generate a number, there's no option but to discard at least a few results (either 2 or 4) out of the 2^m.\n\nNo matter how clever you get with use of coding schemes, you can't get around this issue.\n\ne.g. toss 5 coins (or one coin 5 times); you can get 32 distinct sequences, all equally probable (assuming a fair coin and independent tosses). The next smallest multiple of 6 is 30 (5 outcomes each), so two of the 32 results must be left out.\n\nOdd m means no fewer than 2 outcomes must be discarded, even m means no fewer than 4 outcomes are discarded. e.g. toss a coin 20 times, then after you allocate the 2^20 outcomes evenly into six bins (by whatever mechanism) there will be 4 outcomes left over."", 'Okay awesome thanks', 'Okay cool thank you', 'Okay this makes sense thanks. The part I was not grasping was the notion of lossless compression as it applied to these rolls distribution.\n\nThank you.']"
[Q] How do I make a ratio to distinguish the rows from each other?,"https://i.imgur.com/pYmY1Wq.jpg

I have three columns (G,H,I). In columns K and L Ive created two ratios (K represents G/H), while L represents (H/I). 

The ratios are my attempt to distinguish each of the numerical sets in each row from each other. ie: assigning a numerical value to the ratio of the values in row 168 to tell it apart from row 171, 174, etc. Is there another way to relate the values in G, H, I according to a correlation type formula?

Im basically looking for ratios to classify each row of values and distinguish them from each other.


Lets say columns G represents red marbles, H represents blue marbles, I represents green marbles, and each of the rows represents a jar. I am trying to classify each jar by placing a number on it, with the number (or two separate numbers) being a coefficient found by involving addition/subtraction/division of the numbers in each of the jars.

 Im trying to find numbers to classify each of the jars that are different enough from one another such that you can distinguish the ratios of colored marbles in the jars clearly and with no overlap.",11e1u24,LukeGoldberg72,1677576520.0,1,1.0,[]
[Education] MSc Biology in Biomedical Data Science + MSc (Bio)statistics?,"**TL;DR:** offered a fully-funded Masters position with a CS/ML faculty prof, with a project in my area of interest (AI/ML in biomedical science). However, as a biomed major, Id have to be doing the masters as an MSc in Biology, rather than MSc in CS. How transferable is this to industry, provided Im okay with another 1-year MSc in (bio)statistics/data science if needed? Alternate plan is to do an accelerated BSc in CS.

Hey everyone. I finished my Bachelors in Biomedical Science in Canada, and for a year after graduation, had worked in wet lab research. [After taking the time to explore my careers and interests](https://www.reddit.com/r/statistics/comments/108ltqp/education_computer_science_biostatistics_health/), I was planning on doing an accelerated bachelors in CS + a master's in biostatistics. Through this, my goal was to do research using data science within a biomedical/clinical context, while also having robust credentials for industry if things go awry.

However, I recently got offered a fully-funded Masters position by one of my professors. They are primarily appointed in the CS department, with research in ML for drug discovery, cancer biology, etc. The project aligns very well with my interests, however, as my current credentials are in biomedical science (not CS), he is only able to offer the position through a Master's in Biology, as opposed to a Master's in CS.

That being said:

* How transferrable would this MSc in Biology be towards the general CS/DS/ML industry? 
* Overall, provided my interests, would this be a wise decision career-wise?

Im also fine with doing another internship-based MSc in (bio)statistics or data science after this program, if need be (e.g. if the MSc in Bio isnt recognized). I would have enrolled in those directly if it werent for missing prerequisite courses (which I plan on taking in the BSc in CS or MSc in Bio).",11drjbv,itsmarq,1677543535.0,6,1.0,"['The name on the degree doesn\'t matter nearly as much as your project portfolio at the end of the program. If you can point to a Github and/or published projects you should be in good shape. \n\nAlso, while you shouldn\'t  *lie* on your CV, you could probably get away as listing the Masters as something like ""computational biology"" depending on what classes you take and what your advisor says. My PhD is *technically* in Psychology (since that\'s what my University offers), but my CV reads ""computational neuroscience"" because all the classes I took were computational, all my research is computational and published in journals like PLoS CB or Interface, etc. I\'ve never actually taken a psychology course. So far, no one has ever looked askance at it. \n\nIn general, I would definitely take this deal. The mentor clearly thinks highly enough of you to make the offer, it sounds like it\'s relevant to your interests, and the real-world difference isn\'t huge.', 'If the worst case scenario is a funded MSc compared to an accelerated bachelors in CS + a masters in biostatisticsit seems like if everything goes extremely poorly you could still get a second MSc in biostatistics just like you originally planned. Im not Canadian so there could be nuances, but there seem to be no downsides with the funded MSc - and I agree with the other poster that what you do matters more than the name of the degree.']"
[Q] is per capita income and total income continuous or discrete variables?,"Question says it all. I have to build a model for class, and Im getting mixed opinions online. Sorry if it seems trivial lol. Still a novice at this.

Thanks!",11dpzvo,NickyK66,1677539568.0,2,0.75,"['Continuous until it gets binned into categories like ""small"", ""medium"", ""large"", then it\'s discrete ordinal.  Don\'t bin it.', ""I would treat them as continuous. While not strictly continuous, very few real world situations will have truly continuous values.\n\nA quick question I ask myself is 'are there often multiple entries per value' to determine if I should treat the variable as continuous. For example, I would treat 'number of children' as discrete because I would expect to see multiple people with 2 or 3 children. Total income I would treat as continuous because I would not expect more than one or two people to make exactly 74251 per year."", 'What definitions do you have for *discrete* and *continuous*?', 'Interval-censored', 'Id say continuous but Im studying the difference between the two in probability currently sooo not positive Im right ', 'Money (the medium of exchange) is on a continuum. It can take on any real-number value. So I dont see why it wouldnt be a continuous r.v. Whereas discrete r.v.s are for thing like measuring how many heads you get in N coin tosses, where you are restricted to integer math. More specifically if the set of *possible* values is countable.', ""If you have millions of people then you'll get multiple people with an income of 74251, but the difference between 74251 and 74252 (or 74251.01)  is irrelevant so it doesn't matter if they have exactly the same income or something else, for all practical purposes it's continuous."", 'To be fair, every continuous variable in practice is interval censored']"
[Q][D] Probability of choosing 4 tiles all at once vs. one at a time,"My girlfriend and I were playing Azul the other night, and a friendly conversation was sparked from the game. For those of you unaware of the game, Azul, it has a component that requires you to choose tiles from a bag at random (without replacement) to play. Naturally, classical questions of probablity tend to form when playing this game. Before I ask the question, here's the basics of the bag of tiles and the random choosing method: 

- There are 100 tiles in total placed in a bag.

- There are 5 different tile colors (blue, red, orange, white, and black) and 20 of each tile color (so the color types are evenly distributed). 

- All tiles are exactly the same size and weight.

- To start a game round, you pick sets of 4 tiles and place them in groups (for two players, you will do this five times, thus picking five sets of 4 randomly chosen tiles).


**Here's the questions:**

*Imagine I am trying to calculate the probability of one of my tiles being blue when I pull out a set of 4. Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?* 


*Furthermore, another question that came up. Again, imagine I am trying to calculate the probability of choosing at least one blue tile in a set of 4. Is there a way to choose 1 tile from the bag, not look at it, and calculate a new probability of at least one tile being blue with the information that I already pulled a tile before that (despite not knowing what color of the previously pulled tile was)?*

I think there may be a problem with the fundamental question being asked. Would anyone have some light on how you would approach responding to/answering these questions? I tried Googling this sort of problem but was lost. Any resources would be helpful too!

Thanks so much!",11dprua,jabberwock91,1677539009.0,1,1.0,"["">Imagine I am trying to calculate the probability of one of my tiles being blue when I pull out a set of 4. Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?\n\nNo. The only way it could possibly matter is if you looked at each tile as you drew out each one, and wanted to update probabilities based on what you observed.  Since you aren't doing that, then no.\n\n> Furthermore, another question that came up. Again, imagine I am trying to calculate the probability of choosing at least one blue tile in a set of 4. Is there a way to choose 1 tile from the bag, not look at it, and calculate a new probability of at least one tile being blue with the information that I already pulled a tile before that (despite not knowing what color of the previously pulled tile was)?\n\nNo. The only way the probabilities can change is if some information is learned about the tile that was removed, versus the tiles still in the bag.  If you don't look at it (or in some other way learn something about it), the tile might as well still be in the bag."", '> Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?\n\nNo, if they\'re well mixed, the probabilities (considering the full set of 4) are the same whether you draw them all together or one-by-one. \n\nNot looking at tiles as you draw will change nothing from drawing them all at once.\n\n(If you examine them as you go, you do get information about the outcome as you go but that doesn\'t change whole-of-draw probabilities like ""chance I draw at least one blue one in the draw of 4"".)', ""Ahh, perfect. This is what I was trying to explain, but just couldn't get at for some reason.\n\nI think she philosophically wants there to be a difference or something. But without the information, there just isn't any way to really get at what she's trying to determine. \n\nThanks so much!"", 'Perfect. Thank you!']"
"[Q] When reporting B and Beta in a hierarchical regression table, which value gets the asterisk?","I'm looking at my output for a hierarchical regression in SPSS, and I've been instructed to report B, Standard Error B, Beta, and Change in R Square.  On SPSS, I can tell whether the change in R square is significant because on the Model Summary, it has the last box labeled ""Sig. F Change"". However, when looking at the Coefficients output table, I see there is a significance value/box, but which value is this connected to when I'm reporting it in my table?

&#x200B;

I have my table as:

B (SE B) | Beta | Change in R Square

I know where to put the asterisk if the change in R square is significant, but for the Coefficients table, where does the asterisk go? B or Beta?",11dn4x6,HonoraryPopsicle,1677532584.0,0,0.5,[]
[Q] Help understanding a statement,"Hello!

I'm reading a paper and got puzzled by one of the statements in the analysis of a regression model. I was wondering if someone could please explain to me what the authors mean here: 

*""There is a moderate association between socioeconomic disparities and achievement gaps: the R2s from the models in figure 6 are .41 and .38 for white-black and white-Hispanic gaps, respectively (implying that the correlation between district-level achievement gaps and an index of racial socioeconomic differences I, is roughly .62.64).""*

Basically, I am not sure how they  assert that the correlation is ""roughly .62 - .64"" from the R2 (.41 and .38 respectively). How is the correlation calculated here?",11dloyw,Dr_Mokiki,1677529147.0,2,0.75,"[""take the square root of the R2. that's it"", ""They computed r from the square root of R^2 \n\n    > sqrt(c(.38,.41))\n    [1] 0.6164414 0.6403124\n\n1. That only works for simple regression; I can't tell from your quote whether that was the case here.\n\n2. It ignores the fact that correlation might be negative; you have to match the sign of r to the sign of the simple regression coefficient."", 'Thanks!', 'Thanks a lot, thats very helpful!']"
[Question] Help with GLMM in R," Hello,

I need some guidance in doing a GLMM in R and was hoping this subreddit might be able to help me out!

Background:  I'm a master's student working on the data analysis part of my thesis  and need to run a GLMM in R, but I have very little experience in R (or statistical analysis in general if I'm being totally honest). So I need  someone to explain this stuff to me like I'm 3 years old in the most  basic baby terminology possible.

My  data is on the impact of urbanization on bird foraging; my comparative  variables are site type (natural vs urban), individual site (9 different sites), observation time per visit (mins), and migration period (during or post); my response variables are # of bird species seen per visit, and # of bird-fruit interactions per visit.

I  get the general idea that it will be formatted as something like,  \[response variable\] = \[comparative variable 1\] + \[comparative variable 2\] etc, and that I'll need to specify which variables are random effects and which are fixed effects and so forth. My problem is that I have NO  IDEA whatsoever how to start doing this in R. The coding stuff is just so obtuse to me, and it seems like all the guides I see online for GLMM are assuming a base level of familiarity with coding in R which I just don't have.

If anyone has a suggestion for like, what packages to use, and/or tutorials on how to tell R what  to do with the data, I will be forever in your debt.

(also please no suggestions for me to use other software for my analysis, my thesis advisor wants me to use R)",11dlcvd,birdtrek,1677528351.0,13,0.85,"['If you are truly a total beginner to programming I would recommend doing an introductory online course in R (e.g. [datacamp](https://www.datacamp.com/courses/free-introduction-to-r)). Unfortunately if you have zero background in this area it is going to be quite difficult to jump straight into your analysis.\n\nIf you have managed to successfully load your dataset into R (and done any necessary pre-processing steps like removing or replacing missing values), GLMM is pretty easy to use. Regression models in R are specified with the following format:\n\n    DV ~ IV1 + IV2 + ...\n\nWhere DV is the name of the column in your data frame containing your dependent variable (DV), and IV1, IV2, etc. are the names of different columns, each containing one independent variable (each will be modeled as a fixed effect). Random effects in GLMM are specified in the following format:\n\n    list(~0 + RE1, ~0 + RE2)\n\nWhere again, RE1 and RE2 are column names corresponding to the variables you want to model as random effects (note you can have any number of random effects, each separated by a comma and including the ~0 prefix). If your dataframe is named ""data"", to use GLMM you just need to use the function glmm() after importing the library, and then use summary() to see the results:\n\n    library(glmm)\n    glmm_model <- glmm(DV ~ IV1 + IV2, random=list(~0 + RE1, ~0 + RE2), data=data)\n    summary(glmm_model)\n\nNote that **you cannot just copy-paste what I just wrote**. You need to change the variable names (DV, IV1, IV2, RE1, RE2) to match the column names in your dataset, which I am assuming is stored in a variable named ""data"". You can add or take away arbitrary numbers of IVs/REs.\n\nAll of this (and more) can be found in the [GLMM package documentation](https://mran.microsoft.com/snapshot/2017-08-20/web/packages/glmm/vignettes/intro.pdf).', 'I know youre not going to use other software, but while working on your R code I recommend using Jamovi as a sanity checker in the sense you can compare outcomes to be sure youre getting the data worked out like you want. They have a GLMM package (GAMLj) that is based on the lmer package, and which you can ultimately use in R if necessary.', ""Read these papers on GLMMs in ecology:\n\nhttps://peerj.com/articles/4794/\n\nhttps://www.sciencedirect.com/science/article/abs/pii/S0169534709000196\n\nIf your school's library (or PI) has it, this book is also a really helpful introductory resource with many real world examples and accompanying R code for similar research problems:\n\nZuur, A. F., Ieno, E. N., Walker, N. J., Saveliev, A. A., & Smith, G. M. (2009).Mixed effects models and extensions in ecology with R. New York: springer.\n\nFrom the information you provided, it sounds like you need a GLMM with a Poisson link (to model count data as a dependent measure) and maybe a couple of random effects, which are terms that account for hierarchical relationships, clustering, or dependencies/auto-correlation among observation units in the data (e.g., repeated measurements for the same individuals or locations over time). In R, the most popular function for fitting such a model is glmer() from the lme4 package, which is concisely covered, alongside the necessary theory you should know to be able to understand what you're doing, in the above mentioned resources."", 'If your thesis advisor wants you to use R, why are they not providing any training or resources for how to use R?\n\nI like lme4 package, btw', 'This a masters thesis. If your main tool will be R, you need to spend a week or two learning really well how to use that tool. You wont master it yet, but you need to reach at least the level of familiarity that you say those GLMM guides assume. Theres no way around that.', ""Chat.openai.com/chat\n\nIt's been working well for R so far."", ""Thank you!! This was a huge help - our lab's postdoc (who specializes in R) is now back from his fieldwork and is able to help me hammer out the details, but was a massive help in getting the ball rolling. Thanks again!"", 'Probably run into challenges just using chat gpt with no baseline knowledge in R. With that said, great way to get ideas.', 'Absolutely.  You need to know what to ask. But it gives you a good baseline. Sometimes the code and results are simple solutions,  sometimes very complex. But, play around with it.', 'Agreed, not only that, you also need to be able to figure out when its providing you nonsense. GPT loves to make shit up!']"
[D] Is Bruce Bueno de Mesquita legitmate?,"Some of the claims he makes about his models are quite drastic ie: ""predicting 90% of events"" and yet the methods and the way he describes them publicly are fairly basic.",11dkwh3,Competitive-Onion515,1677527244.0,2,0.75,"['He\'s a legitimate political scientist.\n\nAt the same time, predicting political events can be very easy. You can get 90% accuracy by just predicting that the next month will be the same as this month, e.g. ""will Putin still be in power next month? Yes.""\n\nMuch harder is predicting momentous events. Your accuracy metric has to take that into account.', ""https://en.wikipedia.org/wiki/Bruce_Bueno_de_Mesquita:\n\n> A declassified assessment by the Central Intelligence Agency rated his model as being 90 percent accurate\n\nIt's hard to argue with CIA statements, and they can make very good publicity.\n\n> yet the methods and the way he describes them publicly are fairly basic\n\nSo what? Simple models can be very accurate. The obvious one I can think of is Einstein's E=mc^2 formula. Basic yet accurate.\n\nAlso, describing them in a basic way just shows that he is a good talker.\n\nI don't know his work, but nothing seems strange to me so far."", ""Ya, I think his self-cited accuracy is misleading at best, per a TED Talk he gives he gets half a 1/4 of his predictions he makes wrong while saying his model has 90% accuracy, and it doesn't take into account past events. It just feels like a garbage in, garbage out model."", '>It\'s hard to argue with CIA statements, and they can make very good publicity.\n\nThose aren\'t official CIA documents...""The citation is not, in fact, an official CIA assessment of the performance of his methodology. It is an article published in an internal trade journal for analysts called Studies in Intelligence. The article, by Stanley A. Feder, entitled Factions and Policon: New Ways to Analyze Politics, examines the use of a pair of tools that political analysts used in the mid-1980s. Given the source and nature of the document, it is inaccurate for Bueno de Mesquita to claim a CIA endorsement for his work. Aggrandizing the source of his citations undermines the credibility of the rest of his claims.""', 'I am fairly skeptical simple models can subjectively predict complex political predictions. There is an inherent difference between predicting human behavior vs physics.']"
[Question] What are some go-to statistics resources beyond basic textbooks?,"I am searching for some intermediate/upper intermediate statistics resources to deepen and broaden my skills in statistics. My academic background is in psychology and experimental design, and I work as a data analyst. I am familiar with the 'basics' of statistics (e.g., NHST, distributions, linear and multiple regression) and have done some work with principal components analysis and factor analysis, multiple imputation (although I am not fully aware of how this works), and confirmatory analyses. But as I get further and further into my job I feel like I am reaching the limit of my stats and data knowledge and I want to further my skills. What are some resources you recommend? I'll include a list of books I have read and used below to give you an idea of what I have access to and what my knowledge base is like; hopefully someone who knows of these resources can recommend some deeper insights.

* Mediation and Moderation (Andrew Hayes)
* Learning Statistics with SPSS (Andrew Field)
* Statistics for the Behavioral Sciences 7th edition (Gravetter & Walnau)

I have been looking at the Elements of Statistical Learning by Toshibriani(spelling?) et al., 50 topics for data analysts, etc. I do not have a strong math background so I have run in to some limitations in that regard. Any recommendations are appreciated!",11dhyco,likeanoceanankledeep,1677520187.0,4,0.83,"['The statistics notes series in the BMJ is good: [https://www-users.york.ac.uk/\\~mb55/pubs/pbstnote.htm](https://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm) ; more for clinicians but written (mostly) by Doug Altman/Martin Bland.', ""ESL, by Tibshirani et al is a fair jump and the book focuses on statistical learning which is not likely to be directly useful to you (at least not yet), unless you're specifically  interested in topics like prediction and classification. (However I would suggest paying close attention to a couple of early sections that *are* quite relevant to a psych person,  specifically parts relating  to not using the same data for inference and/or prediction as was used to choose the model)\n\nIf you want to understand statistics beyond the level you you indicate (understanding something of the why and how and what else the might be) you probably will need some basic stat theory  for which you will need at least some mathematics (calculus would be a big help to get started but for regression/ anova, etc, some familiarity with vectors and matrices would be important)\n\nA basic book on probability  and mathematical statistics would then be my suggestion (some mathematics, mostly algebra, but well below the level of ESL)... I'll come back with some possible suggestions but there's dozens  of similar books\n\nEdit: maybe take a look at *An Introduction to Mathematical Statistics and Its Applications* (Larsen & Marx), it's about as simple as it gets while still actually covering topics that would help to give a start on some actual statistical grounding. It doesn't seem to mention the Neyman Pearson lemma (but *does* cover likelihood ratio tests, which is kind of odd), nor does it seem to cover Wald tests, nor score tests, so it's maybe *too* basic but if you can manage it, there's always the possibility of going back later on to pick up missing topics.\n\nAn alternative would be the book by Wackerly, Mendenhall and Scheaffer (edition doesn't matter with these books, grab an old one without much worry).\n\nI'd suggest maybe trying to get a look in a university library if you can, check out early chapters and see if that would be manageable.\n\n(What you will miss with these is exposure to simulation. Maybe Hardin and Cenka, *Introduction to Modern Statistics* might be gentle enough to get started there - available free online, if you can't find it let me know. An alternative would be to pick up some R, perhaps via Navarro's [*Learning Statistics with R*](https://learningstatisticswithr.com) -- a book on basic stats aimed at undergrad psych students though it does touch on some very basic simulation ideas -  and then try to go on from there with some deeper simulation notes, of which there are many; there's an R package for the book, by the way)\n\n\n\nAfter that probably a good book on applied regression. Maybe Fox? (Fox is a social scientist, specifically a sociologist). There's a companion book (Companion to Applied Regression) and package (`car` in R)\n\nThen after that basic grounding you should be in a position to read books on a much wider array of topics without getting lost.""]"
[Q] I need a help to find the source of the base of industrial inflation index data I'm using. Anyone can identify where this series comes from?,"Hi! I'm working with the data base of industrial inflation of the image below:

[https://imgur.com/a/XxrnuNW](https://imgur.com/a/XxrnuNW)

Does anyone know where can I find the rest of the historical data related?

It was said to me that this table data was obtained in the Bureau of Labor Statistics, and it's an Industrial Goods index. Searching the Bureau's site, I couldn't find a match.

I'd be very grateful if anyone can help me with that! :)",11dhxy4,lunarShuttle,1677520157.0,1,0.67,"['I checked bls.gov and couldn\'t find an ""Industrial Goods Index"" (didn\'t look every hard), and wanted to confirm you meant the US Department of Labor, and not some foreign country\'s?', ""[FRED](https://fred.stlouisfed.org/categories/32455) has over 10,000 Producer Price Indices.\n\nIt's probably one of them.....have a look."", 'Thanks for helping!\n\nI got this numbers from a report that stated that the source of the index was IMF, but it was discontinued, being replaced by the ""same"" index from the american Bureau of Labor Statistics.', ""I'm sure you've already noticed but the dates are in a foreign language and they're using commas to separate the decimal part, which makes it unlikely it comes from a US source. Of course, it could be translated from a US source, but the source might be non-US entirely"", 'Yes, well noticed.\n\nThe info is from an international report elaborated by Brazil and Paraguay together.\n\nHowever, in the report its stated that the source of the data in the table is the american bureau of labor statistics.  \n\n\nEdit: the data is intend to reflect the american inflation, because the report deals with dolarized quantities.', ""If you load data into Excel (or similar) and change the localization, it will reformat dates and numbers to conform with local settings.  That said, it probably is an unreliable indicator of the data's source.""]"
[Q] Compounding Interest Help,"Hey, I was wondering if there is a formula out there that can help me.

Lets say I do something 20 times and I have a 5% chance to get the result I want. Out of the 20 times I go through each result what is the percent that I would get the result?

Is there a calculator app out there where I can put a number of attempts and a percent for each attempt?

Wanting to use this for a ton of rpg games that I play but I wasnt able to google correctly to find an easy formula or calculator to help me out.",11dhr3q,coryandstuff,1677519675.0,0,0.5,"['This is more like a probability question, not a compound interest question. Since you are trying 20 times, the distribution of getting the result is ranging from 0 to 20. If your question is whats the probability that you get at least one result, the answer is 1 minus the probability that you dont get any result out of 20 trials, which is 1 - (0.95)^20 = 0.6415.', 'Not sure if I got my thoughts out well, but was just curious how many attempts I need to make at something before I get close to getting pretty good success', '> Lets say I do something 20 times and I have a 5% chance to get the result I want. Out of the 20 times I go through each result what is the percent that I would get the result?\n\n1. Do you mean ""get the result *at least once*""?\n\n2. Are the trials independent?   (Edit: oh, for it\'s TTRPGs? then yes, almost always independent)\n\nIf yes to both, then 1-(19/20)^20 (or roughly 1 - 1/e, since n isn\'t small and p = 1/n here)\n\nmore generally, for probability of success p in each trial and n attempts, P(at least one success) = 1 - (1-p)^n\n\n(This is just ""what\'s the probability I don\'t fail all of n times in a row?"")', 'Yeah, I wasnt sure how to word it. Hah this isnt anything formal, just to help me with video games.\n\nThanks for that!']"
[E] Help finding resources about Markov Kernels,"I am getting Markov kernels introduced for the first time in a causality course and I am struggling to understand them. We have exercises to practice like ""show the product of markov kernels is a markov kernel"", ""composition of markov kernels with gaussian densities""... but I just do not know how to apporach them. 

For example, I see the definition of the product of Markov kernels, and then a remark about it not being commutative. I get the idea of why the product of Q(W|T) and K(W|T) should not be commutative, but I can't mathematically write it down. This happens often, I get what the idea should be, but get completely lost trying to write it down.

The only resource we are given are some lecture notes with no solutions and most proofs are left as exercise to the reader, and after reading the definitions 10 times I still can't figure out anything. This new Markov kernel setting to generalize probability spaces is making me completely lost, and being unable to find resources to go little by little or at least some examples of how to solve some exercises is making me desperate. 

I am looking for either some videos/notes/books that go really slow about markov kernels, or some basic exercises with solutions about them.",11deqjd,Chus717,1677512203.0,4,0.84,"['https://www.jstor.org/stable/2290486\n\nThis paper provides an introduction to Markov kernels in a clear and concise manner, with several examples and exercises that are suitable for beginners.', 'thanks for the reply, but when I click the link I get a review of a book, is the link wrong? or are you reccomending the book it is reviewing?', 'Thanks for such a wonderful reply! TheGratitudeBot has been reading millions of comments in the past few weeks, and youve just made the list of some of the most grateful redditors this week! Thanks for making Reddit a wonderful place to be :)', 'Oh, sorry! Yes, ""Markov Kernels and Their Applications"" by Srinivasa Varadhan']"
[Q] Would this be a moderator or mediator?,"Lets presume that there is a positive relationship between the intensity of a stressful event and negative mood, such that the more intense a person experiences a stressful event, the worse their mood will subsequently be. 

Would distress tolerance (the ability to tolerate stress), mediate this relationship (i.e., explain it), such that individual differences in the ability to tolerate distress is the reason why stress intensity relates to mood. Or would distress tolerance moderate this relationship, such that the ability to tolerate distress would weaken the strength of the relationship between stress intensity and mood. 

Im having trouble wrapping my head around it, because they both sound plausible.

Thanks.",11dc5b0,TheEudaimonicSelf,1677505374.0,1,1.0,"[""The stressful event doesn't get explained away by the stress tolerance. \nYou can have a high stress tolerance and still show reaction with your mood.\nAnd at lower stress tolerance,  your mood may be more low.\n\nAlso sounds like a moderator to me"", 'Sounds like a moderator to me']"
[Q] What sort of time series model would be appropriate for a binary variable?,"Hey everyone, I'm just curious if there are any resources on time series models for binomial variables. For instance, analysing the landing page of a website and seeing how many visitors actually go ahead to make a purchase. Google wasn't very helpful for me!

&#x200B;

My initial thoughts were to construct a multi level model where time is just another variable.

&#x200B;

Thanks.",11d8uwl,poetical_poltergeist,1677494576.0,25,0.85,"['Is it really binary or is it binomial/poisson? Sounds more like you have count data, unless I am missing what aspect of this process you want to model.\n\nI.e. Do you want to model aggregate behavior (purchases over tien period) or individual behavior (likelihood a person makes a purchase after X time on site)', ""So if I understand correctly, you're interested in modelling how many users from a cohort of say 100,000 will get a binary outcome (e.g. a signup) over time?"", ""I feel like there's a way to transform these into a single binomial variable with mean (X-Y) and then test if that transformed distribution's p value is 0... my other guess would be some kind of chi-squared testing?"", 'Simple linear Regression with X as whatever variable (maybe time) and y as the binary of 0 = no purchase and 1 = purchase.', 'Logistic regression', 'Sorry, I meant binomial for modeling aggregate behaviour. Will update the question!']"
[Q] Percentile and sorting.,"I have two questions. Suppose I have a numerical dataset (n=100).
1. Is pth percentile the value whose ""magnitude"" as well as relative positioning is greater than p% of the data, and less than 1-p% of the dataset? (These two conditions should be simultaneously satisfied ?)
2. Is the only way, mathematically speaking, of finding a percentile to fully arrange all the data in ascending or descending order? I've searched online and almost all methods involve some sort of sorting. (Like partial sorting, full, heaps)

My apologies in advance, I'm not very bright when it comes to statistics. Just starting to learn it.",11d8gku,Aggravating_Data1489,1677493095.0,3,0.81,"['> 2. Is the only way, mathematically speaking, of finding a percentile to fully arrange all the data in ascending or descending order?\n\nNo, you can avoid sorting all the data with a suitable algorithm. But with n=100, just sort the data (you have a computer right?), unless you want a fairly extreme quantile (in which case just pull out the largest or smallest few values)', ""2. You don't have to sort all the data. Look up quickselect, e.g. You mention partial sorting yourself."", ""Well I am trying to find VaR(Value at risk) for a dataset, the definition states, it is the worst loss for a given confidence level. So if I'm correct, it means for a C.L of 90%, it must be the worst loss. Do i have to sort the full data to find this? I was thinking sorting only the relevant 90% of the data would do, but then the question is which 90%? Would appreciate any insight."", 'Yes, i was asking, whether there is a way to find percentile without sorting of ""any"" form.', 'It isn\'t hard to answer the question ""What percentile of my data does this response value fall into?"" without sorting - go through all of the data and find the proportion that falls on either side of your chosen observation, and you should be able to calculate the percentile from there. I\'d venture to say it\'s not feasible to answer the question ""What response value is the Xth percentile in my data?"" without sorting - percentiles are  proxies for order statistics, and you\'ll need to know the ordered form of all of the data/variable of interest to answer this. I\'d assume there are methods for pretty good approximations, though I\'m not personally familiar with them.']"
[D] Scaling dummy variables K means,"Hi. I had a discussion recently with a coworker. We are running a K means clustering algorithm. He said that when dummy variables are made into 1s and 0s,these columns must be scaled in a specific way. If variable A has 17 categories - names of cities. Then after turning into 17 columns of 1s and 0s (Python does it this way). He said that the sum of variances of those columns should be num_cathegories - 1. Thus 16. If the categories are 11, then sum variances should be 10. He proposed centering the dummies for each dummy column by substracting the mean and then dividing by the square root of the columns mean. I have looked into literature for this but haven't found anything. He is senior to me so I did what I was told without understanding it. Can you help.",11d53i6,AnyJello605,1677480385.0,7,0.89,"['He got me on the first half. The n-1 thing is right. If you think about it when we have sex (M, F) as a feature in practice its two categories but you end up with one column. The reason we do it like like that has do with the least square error method and linear algebra in general. I can go in details if you want. \n\nAlso forgot to mention but the are two types of categorical variables ordinal and nominal. What we are talking about here is nominal. An example of an ordinal categorical variable is Likert scale. Every time you see question about how much did you like this, its usually likert. \n\nHaving said that we dont normalise the nominal variables because it makes no sense.\n\nAlso I dont know if in K-means using n or n-1 columns makes any difference but in general have n-1 columns contains all the information and then nth column is redundant. In some methods that causes a problem in others it goes unnoticed.', ""What first half are you refering to? I understand that Python and R have dufferent number of created dummy columns. A variable with 12 city names will make 12 columns in Python and 11 in R. But still don't understand why they should be divided by sqrt(mean).""]"
[Q] How can i reasonably get access to JMP to learn/practice?,"My company (and some that I've seen job postings from) use JMP for data analysis, I'm interested in learning it, but i cant seem to find a way to practice. I have access to the virtual lab through an online course, but the VL is not able to open data files, handle excel files, etc. I highly doubt my company would let me use a license as is not really in my job responsibilities. Any way i can get some hands on practice? I dont mind paying a reasonable amount just not the $2-3k for the full license. Thabks",11d2u7k,FirstLast24,1677472672.0,1,0.67,"['[Here\'s the link to the free trial](https://www.jmp.com/en_us/download-jmp-free-trial.html).\n\n\nIt\'s fantastic software.  The price is justifiable based on actual time savings and based on conceptual leverage.  It takes less time to do something in JMP than in the alternative, so multiply your hourly rate by the time saved and you\'ll get to a cost-benefit analysis that is in favor of the annual expense.  For leverage, you\'ll be exploring new methods of analysis that you weren\'t aware of, will get better answers from a pile of data, design better experiments and will find more truths than with other software.  \n\n\nThe leverage benefit is harder to quantify, so you\'ll need an anecdote that comes from your usage during your free trial: ""I would never have seen this (fill in your result here) if I hadn\'t used JMP, and the fact that I have seen it has such-and-such economic benefit to the company.""  This can be a hard sell that depends on your status and influence, so my approach has been to sell JMP to management on the basis of time savings with the fan-boy enthusiasm that lets people see this is god for morale.']"
[Q] Advice on JMP versus R,"Hi everyone, I'm in my second semester of my Biostatistics MS, and I'm running into a problem. Due to the fact my undergraduate degree was in Biology, I'm fairly new to statistics. I did fine in my first semester (Probability I, Introductory statistics I) as math comes easy to me, and I wasn't required to use any software in these classes. However, now that I'm in introductory statistics II, my professor incorporates R into most aspects of the class. I plan on beginning to learn R soon, but as of right now, I don't have even close to enough experience with it to complete this class. 

Upon asking my professor, he said that although we use R, I'm free to use whatever software I want as long as I can answer the questions. I found that JMP (another software I have never used) was much easier to grasp, and I've been getting through the class with the help of online tutorials. Is this a trick that is going to last for me? Are there limitations to JMP that are solved with R that I will face in a class like this? Any information would be very helpful. Thanks!",11d06l5,NickyK66,1677464618.0,6,0.8,"[""Although coding may seem overwhelming at first, manipulating and visualizing data in R is just a few Youtube videos away. Don't limit yourself, don't waste your time, learn R now while you actually have a reason to use it."", 'JMP is a very capable stat program and it is doubtful that it would not be able to do everything covered in your second semester. JMP is the only existing stat program written from the ground up to have a GUI so, as you have discovered, it is very easy to learn. You could work on learning R while doing problems in JMP. You might also be interested in [JMP and R integration.](https://www.jmp.com/en_us/events/ondemand/mastering-jmp/jmp-and-r-integration.html)', ""Invest time learning R, it will going to bring you closer to a lot of tools used in a lot of fields: genetics, bioinformatics, ecology, you name it. You won't regret it."", ""If you're looking for a stop-gap Stats software while you learn R, try JASP. It's a free statistical analysis software which runs on R. \nhttps://jasp-stats.org/"", ""For me, JMP is for exploration of data and R is for modelling.  Both can be used for either, but JMP is for designing and analyzing experiments, choosing custom designs, making charts and graphs faster that I could ever learn graphing in R, quickly altering the type of analysis to understand visually what's happening in a data set.  I already know JMP so I go there first for manipulating tables, control charts, probability charts, reliability analysis, process capability analysis.  \n\n\nI use R for modeling of sampling distributions, simulating probability models, experiments and characterization of Monte Carlo analysis.  I switched to R for these tasks because to run random simulations in JMP is tedious and coding in R is more flexible.  Graphing in R is grunt work compared to JMP, but it's woth learning the rudiments so you don't have to go back and forth between R and JMP."", ""> Is this a trick that is going to last for me?\n\nIt's not a bad stopgap to get the current work done on time, but try to get going on R asap because it will keep coming up."", 'Thanks for the advice everyone!', ""Learning R will be worth the effort for you. It's probably useful to say you know JMP on a resume, but R is the place to go for novel statistical work.\n\nInstall R and RStudio and try following along to some tutorials. There's **so much great content** on YouTube for learning R. I'd imagine a lot of your classmates will also know R, so you can pick stuff up from them.\n\nA little dated, but a good book for learning applied stats using R is [Data Analysis and Graphics Using R](https://www.cambridge.org/core/books/data-analysis-and-graphics-using-r/E04AEC5BCEF09D2E51A63EB5A8CB0680). I took an applied stats course with a professor who used that book in her PhD at UCB. She used it at my institution and I thought it was a good baseline.\n\nA lot of the data processing in that book is outdated compared with the modern tidyverse suite of packages (visualization in ggplot and processing using dplyr/purrrr), but that's the type of stuff you can pick up from YouTube as you go."", 'Try Jamovi. It is like JMP but with R under the hood.', "" JMP is mostly used in Experiment design. If you are heading in that direction, then it can give you an edge for it. However, as long as you are not eager to become a statistical programmer in Pharam where SAS is heavily preferred, you can't avoid R under the hood of statistics. You have to invest your time in it and the learning curve can become steep as you progress. \n\nIn a way, R is somewhat harder than python (it varies from person to person). So you might also want to start learning python simultaneously to grasp the basics of programming. \n\nThere are so many free learning materials out there nowadays, so dig them up and take a crack at them."", 'JMP is fine, but learning how to code can really bring your skills to the next level.  Not just for the modeling, but the data preparation, reporting, app development, etc.  R is an amazing language - go for it.', ""Learning R is not hard at all and it pays handsomely, and whatever your class is going to be it will be reasonably simple like inputting commands on a text file and sending to execution, it's highly unlikely that it will be a design algorithms class at worst you'll get an introduction to algorithmic thinking from that class."", 'Use R and learn python slowly on the side.', 'Not to mention with chatgpt, learning almost language is a breeze.', "">JMP is the only existing stat program written from the ground up to have a GUI\n\nI'll politely suggest that you do some fact checking before making such a broad statement."", 'Do you have advice on how to use chat gpt to learn a language? I already know the generic one, I mean specific practices or ideas that youve found have worked for you.', 'Not only existing, but I suspect first (or only longstanding) would be accurate. Most, if not all, of the big names you can probably list off the top of your head (SPSS, Stata, Minitab, SAS, ) started off either on mainframes or on PCs running DOS. JMP, initially Johns Macintosh Project, was released in 1989, when System 6 was current. The highly regarded System 7 would be released two years later, and the 68k-PowerPC transition followed in 1994.', 'For me I find that the best way is to start on a small project, perhaps a project that you have done before in another language. Just reimplementing it in a new language that you want to learn. Once you hit a stumbling block, just ask chatgpt how to solve it. Because you already know the solution to the problem in another language, you can ask chatgpt something along the lines of ""Using python, i was able to do this.... how can I accomplish the same thing in R"".', 'I had already used both approaches (the parallel between languages, and the simple ask for help to chatgpt), but combining them sound simple and powerful enough, thanks!']"
[Education] SOHCAHTOA to Gradient Descent,"\[Education\]

Hello,

I am studying machine learning, and I am trying to piece some concepts together to align my understanding of them individually. Here they are:

Concept 1: All machine learning is a geometry problem. All machine learning is the fitting of a line/curve/plane/surface in dimensional space.

Concept 2: SOH CAH TOA, the penumonic for remembering the calulcation of the sine, cosine, and tangent of the acute angles in a right triangle. Note: right triangles are used to find the minimum distance between a point and a line in 2D space; the minimum distance between the point and line is the length of the tangent line from the point to the line. The tangent line forms a right angle with the line.

Concept 3: In simple linear regression, we find a ""line of best fit"" amongst data points (x,y) in a 2D space. The function of a line is y=mx+b. Thus, our predictions using this line are yhat\_i=mx\_i+b. The line of best fit is the line that minimizes MSE, the mean squared error. The error is the difference between the actual values (y\_i) and the predictions (yhat\_i). Thus, we effectively minimize the sum of (y\_i - yhat\_i - mx - b)\^2 across all i data points (x,y). To minimize a function with respect to a variable, we take its derivative, set this equal to 0, and solve. Note: our variables in the MSE function are m and b as x and y are realized values of our data points. Thus, we minimize the MSE with respect to m and b. When doing this, it is clear the values of m and b in this context are directly solvable using the data. Thus, we can directly solve the values of m and b which minimize MSE using the data points. The resulting line y=mx+b has value values of m and b such that the sum of the lengths of the tangent lines from all points to the line is minimized.

Concept 4: Maximum likelihood estimators. I somewhat forgot what these are but I know the MLEs for m and b in simple linear regression should result in the same values as what is found following the process in concept 3.

Concept 5: Where we take the derivative of the cost function, set it to 0, and solve for the coefficients of our line using our data points in simple linear regression, we use the gradient when work in higher dimensions or in nonlinear context. Why? What is the gradient? How is it related to the derivative and minimum distance of points to the function plane/surface?

Concept 6: The ""linear"" in linear regression refers to the relationship of the parameters/coefficeints (m and b) to the independent variable (y). y=m(x\^2)+b is still linear regression; it is polynomial regression which is linear regression. However, y=(e\^m)x+b is not linear regression. The relationship between any indepdent variable and the dependent variable can be nonlinear!

Concept 8: We often should, but do not necessarily need to, normalize the independent variables in linear regression so their mean is 0. In all other linear models, we must normalize the independent variables. We should always normalize the dependent variable(s) to avoid vanishing gradient. Why do we normalize in linear models? StandardScaler() vs MinMaxScaler() and use in linear models vs ANNs?

Concept 7: Multiple linear regression is different from multivariate linear regression. Example of multiple linear regression: y=mx+kz + b. Multivariate regression: multiple dependent variables. Multiple polynomial linear regression: y = mx + kz\^2 + b.

...

I feel like I am getting some grasp of what is happening theoretically, but I am hoping you can poke holes in my understandings and/or align them.

Thank you to anyone who provides input!",11cxsh5,basicdude13,1677457845.0,0,0.5,"[""Concept 1: I don't think clustering algorithms are equivalent to fitting a surface in dimensional space. Or at least, I haven't thought of them that way before. You're more separating your data points into multiple clouds, which I guess is fitting multiple surfaces in dimensional space, but the boundaries of those surfaces are sometimes fuzzy, and it's certainly a set of poorly behaved surfaces from a mathematical perspective.\n\nConcept 5: You want to take multivariable calculus to understand the gradient thoroughly. It's an extrapolation of a derivative in 2d (a slope), used in multidimensional space. The derivative of a three dimensional surface at any given point is a plane. That plane has many possible slopes. The gradient is the steepest slope of all those possible slopes. Similar logic extrapolates to higher dimensional spaces. One reason to care about it because we want to know which combination of x variables (which direction in X space) results in the fastest change in y. But there are plenty of other reasons to care too.""]"
[C] Master's in Stats: UWashington vs. NYU," Hi  stats community, I was hoping for some insight into potential master's  programs for statistics. I was accepted into three programs thus far:  UWashington M.S. in Statistics - Advanced Methods and Data Analysis,  NYU's M.S. in Applied Statistics for Social Science Research, and  Columbia's M.A. in Statistics. The final program that I am hoping for admission is UC Berkeley's M.A. in Statistics.

I  didn't receive funding for any programs, but will receive in-state tuition for UW (which is a nice bonus) and I will not need to go into debt for any of them. I've heard good things about UW's program  especially, and bad things about Columbia's on reddit (which is why I'm not considering it as strongly). I was wondering if anyone had insights into NYU's program, or if you have any general advice when considering various master's programs. I am currently leaning towards UW mostly because of the curriculum, but NYU is also of interest to me because it is a statistics program hosted within a humanities school (Steinhardt), and I think I am interested in the social sciences/policy aspect. The  price difference is quite large though, so I would definitely want to make sure I get my money's worth from NYU.

Thank you for any advice, and also if you think the Berkeley program is also strongly worth considering (should I get in), please do share!",11csgbr,bornskinnyhomo,1677445025.0,14,0.94,"[""None of these programs are better than UW's and all of them are more expensive. UW all the way."", 'UW for sure. One of the best Statistics programs in the world. Plus you receive in-state tuition.', ""UW has a legendary stats department. Best professor I've ever had went there."", 'Ive been at UW for grad program in a stats adjacent field and not only is the in state tuition going to be beneficial but UW is absolutely dripping with TA/RA funding so theres a good chance youll be able to get tuition covered. Also the courses and profs (Biostats anyways) have been excellent in my experience', 'I didnt do that MS at NYU, but did take 5 or 6 classes they do while in grad school at NYU in another program and have a bunch of friends from the MS. Happy to answer more specific questions if you have them, but given the general prestige of the programs and cost dimensions, Im not sure youd end up being able to justify the difference.\n\nThats not to say I think the NYU program is bad (classes/profs were strong, friends got job or PhD placements they were happy with mostly, the people from there Ive interviewed at my current job are good), just that UW is a great program and that NYU and New York are expensive without a really strong reason to come here.', 'Not sure about the NYU program, but I am extremely in favor of Cal. If you got a chance to step your feet in either Cal or UW, flip a coin, choose one, and go for it.\n\nTo my knowledge, the quality of the program at Columbia Uni is just horrendous. Apparently, it is more of a cash cow driven instead of a cultivating environment for advancing your training in Statistics.', 'How are you paying for UW? Also what did you hear was bad about Columbia?', ""Off-topic, but I'm curious: have you majored in statistics?"", 'Damn, I was aware that people liked the program, and in general it piqued my interest when I was applying due to the rigorous curriculum, etc. but I had no idea that it was viewed so highly! Seems like I made the right choice by applying haha, thank you for your input!', ""Is there anything in particular that makes it so good? The faculty? The curriculum? The intern/research opportunities? Or a combination of good qualities? I know that UW's statistics program is ranked very highly on, say, USNews, but I'm trying to take a more holistic approach to deciding on the right program for me. Either way, I do think UW's curriculum aligns quite well with my future career goals, and the fact that it's much cheaper means that I will get a return on my education faster, so that is probably currently my top option :)"", ""Any chance you can share their name? I'll be sure to take a class with them if they are still teaching!"", ""Honestly that's really exciting that there is a lot of funding to go around, as I am definitely in the market for one of those positions! Thanks for your input :)"", ""Honestly a big draw for NYU is the city for me, but I know that I should prioritize the actual program over big-city living! Also, I do love Seattle, so it's not like I'm going somewhere dreadful to save money lol"", ""Thanks for the input and your vote for Berkeley! And even aside from the reputation of Columbia's stat's department here on reddit, the fact that they award, by a massive margin (like 3x), more stats master's degrees than any other university makes me very hesitant..."", 'I just looked up ""Columbia masters statistics reddit"" on google and there were quite a few threads of folks saying to be wary of the program (some thought it was worth it, others were very disappointed and thought it didn\'t live up to the Columbia name). For UW, I can\'t discuss my finances here.', ""No I haven't. I am finishing up undergrad in math!"", 'I am a current UW stat grad student. The quarter system goes fast, but the first year is so rigorous that you will learn a ton. This program is very good and the profs here are great.', 'The faculty. I was in UW before and attended a couple of stats courses. But I wouldnt recommend any program with a quarterly system. You dont learn anything.', 'Fair enough. Congrats. Also when did you hear back from Columbia? I havent heard back yet.', ""I heard back one week ago, last Monday on President's day. And thank you for the congrats!""]"
[Q] How to integrate the sample size when building the average placement for characters in a video game?,"Hello,

I'm currently trying to build some stats for a video game (Hearthstone Battlegrounds). At the start of the game, the player chooses a hero, and their final placement is between 1 (they won) and 8 (they were the first to lose).

I want to build an ""average position"" stat for each hero, to give players an overview of how strong each hero is. This is done pretty easily.

However, some heroes are much more popular than others, which means that their stats should be more accurate (data points roughly vary between over 50k for the more popular ones, to below 4k for the least popular ones). Showing only the average position doesn't reflect this.

Is there a way that I could modify my final ""average position"" stat to have it reflect the uncertainty of the sample size?

I tried to look into the standard deviation, but I get really high values (over 2, while the means is at roughly 4.5), so I'm not sure how to use this value (I still would like the final value to be between 1 and 8, to be more easily understandable).

Thank you in advance for any advice :)",11cqu9o,sebZeroToHeroes,1677441246.0,2,1.0,"[""The standard deviation is showing you how wide the distribution of positions is. That doesn't decrease with larger sample size because this is the real spread of positions people get (your estimate of that spread gets more precise). To estimate your uncertainty of the average, divide the standard deviation by sqrt(N).\n\nhttps://en.wikipedia.org/wiki/Standard_error#Exact_value\n\nhttps://www.statlect.com/fundamentals-of-statistics/mean-estimation"", 'Thanks a lot for the explanation, and the links!']"
[C] Industries that often hire statisticians.,"What are some industries besides biotech, biostats or finance that commonly hire statistics MSc graduates?  Im getting more and more interested in engineering companies or some kind of urban planning. Is it common for people with formal statistics training to be offered opportunities in an engineering company or something similar if they dont have engineering training? If you work outside the above listed fields as a statistician please list your industries in this thread.",11cqnkw,see_2_see,1677440801.0,5,0.86,"['Lot of federal places hire statisticians such as Census, Marine bases, etc.', 'Lots of federal agencies hire statisticians. from recently looking for jobs, I saw openings at faa, dot, and irs. A lot of bigger companies have at least a few statisticians on staff too. I currently work at an aerospace manufacturing company, and we have a few dedicated stats people on staff.', '>Is it common for people with formal statistics training to be offered opportunities in an engineering company or something similar if they dont have engineering training?\n\nThen you need to take chemometrics if it is available. Survival analysis also will likely be involved in the aforementioned domain (engineering). As long as where there are data available, and the generating process or forecasting needs to be studied/performed in a rigorous manner, there will be demand for statisticians.', 'government apparently', 'Media and survey eg- Nielsen', 'Public sector - I worked as a statistician in a govt department.', ""Would you count medicine under biotechnology? I'm assuming yes, but even then. Lots of firms hiring to look at medical device failure etc."", 'Sounds interesting. Can you tell me more about what sort of statistics you use? What typical job title would be given and if youre comfortable what your pay is like? The reason I mentioned I wasnt interested in biotech was specifically because Im not really interested in clinical trials and the experimental design that accompanies it.  However Im not against the medical or bio-tech industry.', ""They tend to call it smt like a biostatistician if they're looking for it, but the job title tends to be smt less clear like project manager etc. They just denote payscales in the company I think.\n\nMine was remote (I'm in GA) and it was roughly 200K a year. I didn't want to stay for the longterm (it was all half programming and half doing basic stats, nothing challenging) as it would have bored me to tears. Everything was always an emergency because someone else messed up, so weekends and late evenings were the norm. \n\n I'm now a professor (I'm slightly below 100K but have a promotion coming up in a few months, and it's 40 hours a week, diverse, challenging (if you ask for a new challenge, they listen), and they respect my free time).\nI ended up becoming a weekend consultant instead, so weekends and evenings only.\n\nPeople always say i could have just cried in my money because they would have taken it for as long as they could.\n\nI'd highly suggest it for maybe a year or so. Get some money in the bank and take the time to find a job you actually like.\nI did it as a side job for a while (two years) and that was better, until the new boss asked me to start working during the week while I was working. Like, during my actual work hours of another employer. I said no, and they finally cut me loose, saying I wasn't available enough. Yeah, that's why I was retained as a weekend consultant  \n\nThere's a lot of turnover. The more challenging jobs are often all about programming and once they're done, you're moved elsewhere as you're no longer needed (since you made the program in a way that a child could run it successfully). And eventually even the most successful programmers are cut loose once their use is gone.\n\nAnyway, if you can do some decent programming (SAS seems to be the standard still), especially including sql, and can do basic stats and don't mind going insane of boredom, this is the job for you lol\n\n\nSo yes I'm negative but yes the pay can be terrific."", 'Sounds like you have a PhD and maybe even post doc training. Im only getting an MSc unfortunately.', 'No post doc. But yes, a PhD. The guy who got hired in my stead got a MSC though. I bet he got paid more as he actually negotiated']"
[Question] What statistical techniques can I use for binary output data where the input variables are categorical?,"Im used to performing statistical analysis when all input and output are continuous but now I have event count data (zero or one) for a dozen or so feature variables that are all categorical. 

I would like to do things like figure out which feature variables most correlated with the output like a sensitivity analysis or pca, but the input data is categorical and not necessarily ordered. I would also like to compute something like correlations between a variables but how do you do that when the variables are categories and have no ordering? Any suggested techniques to look into? The bottom line is I want an automated approach to figure out which categories are most important for the events of interest. 

One technique I know of is one-hot-encoding but the input space seems like it could blow up pretty fast since some of my categorical variables have 10 or 20 distinct values.",11cpoi2,purplebrown_updown,1677438469.0,1,1.0,"['> Im used to performing statistical analysis when all input and output are continuous but now I have event count data (zero or one) for a dozen or so feature variables that are all categorical. \n\nThis is unclearly worded. Do you have a single output variable that is binary? You also have multiple categorical predictors it seems. Is your output data a count that just happens to be 0/1 and could possibly be higher, or is it intrinsically 0/1 valued?\n\n> I would like to do things like figure out which feature variables most correlated with the output like a sensitivity analysis or pca, but the input data is categorical and not necessarily ordered. \n\nNot really sure why you mention either PCA or sensitivity analysis here.\n\n> I would also like to compute something like correlations between a variables but how do you do that when the variables are categories and have no ordering? \n\nIf you really need a correlation like measure between categorical predictors, Cramers V is typically a decent option.\n\n> Any suggested techniques to look into? The bottom line is I want an automated approach to figure out which categories are most important for the events of interest. \n\nThis is a totally different task than mentioned above. The simplest way to do this would be a simple regression of the outcome on the categorical variables coupled with effect size measures.\n\n> One technique I know of is one-hot-encoding but the input space seems like it could blow up pretty fast since some of my categorical variables have 10 or 20 distinct values.\n\nThis may or may not be an issue depending on how much data you have total and how many values are in each of the various predictor variables. Regularization may be an option if it does end up being an issue.', 'Thank you for this. For continuous variables regression and sensitivity analysis are deeply connected. So that is why I might have confused the two. \n\nIn my case I have a single output that is a failure yes or no. So f(x1, x2,   ) = 0 or 1 where x are discrete categorical. \n\nCan you provide a reference for this effect size measures? Are there other ways of regressing categorical inputs that doesnt involve one hot encoding? Thanks this is helpful.', '> Are there other ways of regressing categorical inputs that doesnt involve one hot encoding\n\nYou have some choices exactly how you do the fitting and model building. But if you have a category with 10 levels and no other structure, you will do something that is mathematically equivalent to estimating 9 coefficients.\n\nIn the old days that might have been manually grouping similar items into fewer categories; in modern times it might be regularizing (either forcing a certain number of those coefficients to be zero, or restricting the sum of those coefficients to some small number.)']"
[D] Do you think it's a good idea to first try some traditional statistical models when approaching a machine learning problem?,"Do you think we should give a try to traditional statistical models (e.g. Linear Models) before moving on to more complex machine learning algorithms when approaching a machine learning problem? IMO, traditional statistical models give you more space and flexibility to understand your data. For example, you can do many tests on your models, calculate different measures, do some diagnostic graphs, etc...

What do you think? Would love to hear your opinion.",11cnctc,AmirWG,1677433066.0,61,0.96,"['your first stab at a predictive modeling solution should always be some sort of moderately naive solution that can be achieved quickly. Operationally, this accomplishes several things:\n\n1. It quantifies the ""low hanging fruit"" in the problem. If you are able to capture enough of the variance with your naive model, there might be significant diminishing returns investing more time and effort in the problem. The naive solution might be good enough for your needs. Congratulations, the problem was easier to make headway on than you anticipated.\n\n2. The quick and dirty solution gives you a benchmark to measure against. You shouldn\'t just care how good your sophisticated model is: you need to justify the time you put in doing it the hard way by demonstrating how much more effective that approach was than modeling the problem an easier way might\'ve been.', ""It is absolutely meaningful and useful to make baseline comparison models, and to report results for more complex models relative to the baseline. \n\nMy experience has been that you can get about 80% of the way there (or more) with a simple linear regression / logistic regression / lookup table etc. That has been a sobering experience for me; I'm much less enthusiastic these days.\n\nMy conclusion has overall been to emphasize the large-scale organization of data and results. Try to get the big picture straight, then look for specific places where a specific model is called for. Good luck and have fun."", 'Transforming individual features to be palatable and well scaled for a NB or linear model should be the baseline.  \n\nI had such a problem, and with some clever algorithms for transformation of the univariate features plus an appropriately constrained linear model, the final one with a small number of features beat a tree ensemble on original data, and likely would be more stable through time.   The setting was a regulated application where the model had to perform stably many years without change.\n\nOften complex models might outperform in time, but out of time they can collapse. \n\nThe simple models can still be the core of the solution with isolated addons. With human effort, the nonlinearities might be feature engineerable back to new features which work in the linear model.\n\nPadawan:  uh stuff it into excel and fit a line\n\nNew Data Jedi: autoML with latest tree ensemble tweaks with self supervised deep learning for hidden representations, we need 200 GPUs to train\n\nData Yoda: wise features and logistic regression it is', ""Newb question, does it only apply to specific problems? Because when doing NLP, it's hard to simply use a traditional stat model to replace the underlying RNN or transformer architecture. In computer vision also, how do you simply do some traditional stat regression when you do have need for CNNs?"", ""Another distinguishing feature of the problem is whether we're talking about tall data (n > p) or wide data (n < p). The latter case series traditional methods and requires model averaging (random forest, RF) or regularization (lasso). These are usually considered machine learning. RF is nonparametric and can approximate any shape, and neural nets are also non-parametric. Lasso logistic (for binary classification say) is parametric being linear on the logit scale. Having said that, I would always try lasso first."", 'Absolutely you should try linear models first as a baseline.', 'Rather than using any one specific model as an addition, Id recommend using a model you are familiar with and understand well and which is relatively transparent/white box. \n\nFor most people (including myself) linear models  will fall into that category. \n\nBut theres little point in using linear models if you dont know how to interpret or use then well.', ""Depends on the problem you're solving. Is it actually linear?\n\nIf the underlying relationship is linear then a linear model will give you a better result (less overfitting) than more advanced methods."", ""I'm tired of people thinking linear models aren't machine learning. Where does this come from?"", ""Every ML approach is a stats approach. I've never seen any ML approach that is based on archeology. Anyway yes, high bias explainable approaches should  always been used as a good practiceand to have some sort of baseline. Of course if you are referring to a problem that involves absolutely impossible for any reasonable high bias hand crafted model to be deployable it becomes a less good idea."", ""Also, a model that you can make in a week or two that works pretty well can be worth a lot when the current solution is not model driven. Sometimes it's worth it to get the fast solution that mostly works to market, and then invest the time in a more sophisticated approach afterward if that's still the priority."", "">My experience has been that you can get about 80% of the way there (or more) with a simple linear regression / logistic regression / lookup table etc. That has been a sobering experience for me; I'm much less enthusiastic these days.\n\nFor context.\n\nIn some industries 0.1% of the way is millions of dollars"", 'Im also much less enthusiastic. I dont think I really trust model results other than from a linear regression or GLM and even then the assumptions must be assessed. Ive had neural networks perform well and thats about it from ML. Like for the Super Bowl, if anyone told you they had a model with more than 50% certainty it would just be ridiculous and wouldnt matter anyway, the game came down to the last moments and it was basically a coin toss - some stuff just cant be predicted any more accurately after a certain point and its not clear why some ML model would beat out LMs in general', 'Basically this. I have some limited knowledge of the quantitative hedge fund space and its not that uncommon for them to use basic linear regression. The real work lies in defining the problem, acquiring high quality data and subsequently creating a dependent variable that is relevant for answering the aforementioned problem in question and finally executing the trading strategy given by the model(s). But there are of course many ways to solve these problems, some more machine learning heavy than others.', 'This is a good point here', 'Not always apply, but TFIDF + logistic regression can serve as baseline model in text classification task.', 'Random Forest and Lasso were developed by statisticians', 'Linear models from the perspective of statistics are not nessesracly linear. What you are referring to is linear regression.', 'because it is not as sci-fi as neural networks', ""I guess so, but I was never lucky enough to work on such problems. I guess OP has to decide if it's worth the trouble."", ""   In some industries 0.1% of the way is millions of dollars\n\nDo you work in such an industry or are you a student?\n\nWhy don't you identify what sort of companies this applies to, and how many such companies there are"", 'I dont think just expecting ML to be more accurate is really the right way to think about ML models. The most common ML models are really more about expanding the space of functions that can be learned when fitting your model. Naive linear models are constrained  to additive linear effects and simple multiplicative interactions. If non-linearities are prominent or interactions are not approximately multiplicative, linear models may give poor results that can be improved simply by considering different functional forms.', 'Ive seen TFIDF with scam beat more complex models on text classification problems. No free lunch', 'True, but the discipline in which something is developed doesn\'t characterize the salient features of the modeling strategy. Leo Brieman, a probabilist and statistician, who invented random forest together with his student Adele Cutler, wrote the fundamental papers in machine learning. The relevant one here has the phrase ""two cultures"" in the title. In it he distinguishes machine learning from more classical statistics by saying that the goal of the former is black box prediction and doesn\'t (always) involve specifying a model for the data generation scheme whereas the latter is always  concerned with making inferences about the distribution of the data generation scheme. From this perspective, random forest is machine learning. Lasso poses a model but regularizes it and one consequence of this is that it is amenable to wide data. As a result of regularization, the parameter estimates are biased, but you achieve lower variance. \nDue to this fact, I see it as a step away from posing models and estimation and that\'s why I lump it into the category ""machine learning"". But at least in the case of lasso, you\'re right.  Lasso requires one to choose a linear predictor and for that reason you could say it\'s not machine learning, but I hope you see my point.', ""In Star Trek, TNG, wasn't Data's positronic brain a vastly complicated adversarial neural net?"", ""Large casualty insurer's risk models are an example. Models that estimate the expected loss on insurance contracts, and directly feed into pricing them.\n\nThese models are still, in general, generalized linear models, for regulatory reasons."", ""Financial services (who do you accept/deny for a credit card? What rate do you offer on loans?), Insurance... I mean there's a lot of room for skepticism in this discipline but man what you're poking at really isn't one of those places."", ""> Why don't you identify what sort of companies this applies to, and how many such companies there are\n\nThe number of companies doesnt matter when they are large employers like google."", ""High frequency trading is a good example (having a 50.1 accuracy rate on uniformity placed investments is profitable whereas 50.0 isn't. More difficult in practice but that principle is generally the same.) Insurance is another.\n\nThere the problem is closing any gap between product price and expected claim payout.\n\nLarge scale manufacturing could be considered another, assuming a model-based production system is used (my brother implemented one at a solar company, and it did lead to US$1M savings in annual production costs)."", 'I dont work in such an industry, but I know there are problems where going from being right 51% of the time to being right 51.5% of the time is quite a significant difference. A hint is that it happens in industries and situations where the model affects transactions that are many, thousands or tens of thousands per day, and large in dollar amounts.', 'Expanded functional forms are great. My disdain for ML and DS is that with the best neural networks and really all the clustering and a lot of them, is that the functional forms arent even though about, theyre just all captured by the model. At least with GLM or other more basic models you have to think about what youre doing, other than just maximizing different scores. But whats so disheartening is that Ill never be able to do anything better than OpenAI on my own at this point because theyre a great collaboration of smart people - this is disheartening bc theyve put together a black box that is so good, its just so complex like how could one improve on that framework? A whole team or startup would need to be engineered to compete.', ""Well my point ( as I think you see) is precisely that it's only the biggest companies - FAANGs etc for which this is true\n\nAnd  there are not so many data science roles even in Google ( lots of sales roles)..\n\n\nSo I am worried that it's an article of faith that a small improvement as created by a model change has millions of dollars effect ( and no replies seem to have given concrete examples)\n( And bigger effects are created by adding more data sources)"", 'I dont think this is true that functional forms arent really considered in ML work. Maybe so for Kaggle type of analyses where people are just blindly crunching algorithms with aggressive cross validation schemes, but its certainly not true for most ML work.\n\nE.g. in the deep learning domain a great deal of work goes into design of neural layers and architectures such as CNNs or transformers. These architectures are essentially imposing functional form constraints on the types of relevant features that may be worth consideration in certain problem domains. Additional work on interpretable AI and adversarial examples are especially concerned with these ideas!', '> And there are not so many data science roles even in Google ( lots of sales roles)..\n\nThere are a crazy amount of DS , statisticians, ML related roles in FAANGs. Saying there are not many of those roles is plain incorrect. Those roles are even some of the emphasis called out as investments in earnings calls.', 'The value of a statistical life, though varying, has been placed at $7.5m by FEMA. So, more or less any application where one additional life is saved (medical imaging or drug trials) falls into that category. Large casualty insurers were already mentioned though and you seemed to just disregard that as not concrete...\n\n""So I am worried that it\'s an article of faith""... so just to clarify, this worries you? You\'re worried that somebody on Reddit might be away to try to make a 0.1% improvement in the benign belief that they\'ll generate millions of Dollars from it?\n\nRelax, man.', 'I think CNNs and RNNs have logical constructions. Even some layers can be shown to meaningfully identify certain features like ears to distinguish animals, etc. My point is things like skip layers magically improve performance in terms of output and training time. I understand that DL is helpful and there is some science and logic to it, youve got good points. Im a bit behind on transformers too, things progress so quickly and its easy to get out of the loop unless youre on a team of researchers. Helps to make me feel extra worthless and just let OpenAI do their thing']"
"[Q] If I have a high sample size (2500 data per group of a total group of 3) and the data have no normality and unequal variances, should I then apply Kruskall wallis? Or what hypothesis testing method should I use in those cases where sample size is huge?","I have a time series data where at certain point the time series gets stabilized, what I want to do is prove statistically that in this stabilized zone, these 3 groups are different between them, yet I cant do an anova since the data dont have normality and their variances are unequal.

I heard that for these cases Kruskall wallis is appropiate, yet the data is huge and havent read what hypothesis testing method use when the number of data is huge.

I am plotting the trayectory data in root-mean-square deviation against time and in an interval of time the trayectory is stabilized and I want to prove statistically that these groups are different in this stabilized zone.",11cn2to,HardTruthssss,1677432485.0,2,0.76,"[""> yet I cant do an anova since the data dont have normality...\n\nThe lack of independence is probably a far bigger problem than the lack of normality.\n\nYou haven't explained what these data actually are, or *how* you're trying to compare the groups (are you specifically interested in *mean* differences? Then Kruskal-Wallis is definite the wrong choice), so it's hard to give advice. What exactly are your data and research question?"", ""Normality is the least of your worries here.\n\nYou don't have independence and you say yourself that you don't have identical distributions, both of which are more critical. \n\nThe data are also matched on time, which you seem to be ignoring. With that in mind, the marginal distribution of the data are not really relevant."", 'Im studing a covid spike protein and mutations on the covid spike protein.\n\nI calculated the root-mean-square deviation of the sequences of this protein which forms a time series of the values in Angstrom and time in nanoseconds.\n\nI identified a stabilization zone where you can distinctly see that all these groups differentiate, now I want to prove it in a statistic way yet the data which is about 2500 per group, dont have normality nor variance.\n\nMy research question is if in the stabilized zone the mean in angstroms of the mutated spike proteins differ from the mean in the no mutated spike protein.\n\nExample would be like this image (a) zone.\n\nhttps://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562\\_0008.gif\n\nI wanted to calculate the mean and put in my findings that these groups have a different mean but the mean doesnt have normality nor variance and I cant say in a statistically way that they are different, I know anova is robust for non normality but only if the variances are the same, which is not the case. So I believe my only option is using the median, yet I am not too much into statistics and dont know if there is a theorical obstable for this, if I am violating an assumption, I dont know, or if there is another hypothesis test that I can use which I dont know or other analysis. I am not specialized in statistics, thats why I ask.', 'Here is an image like what I am doing, in the picture (a)\n\n[https://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562\\_0008.gif](https://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562_0008.gif)\n\nfrom 100 ns to 200 ns stabilized zone, I want to say in a statistical way that they are different.\n\nYou can clearly see in its trend they are different, but how to say it in a statistically way?\n\nAre there special hypothesis testing from time series which I dont know?\n\nI wanted to use an anova but the assumptions fail.\n\nI know time affects independence, but once reached the stabilized zone it is supposed that time doesnt matter anymore nor affects the variable since it is now stabilized.']"
[E] ISL second edition solutions manual wanted,Anyone able to get their hands on what seems to be an extremely precious (if at all existent!) commodity and is willing to share? thanks :),11ckjto,tromiti,1677426889.0,1,0.57,[]
[Question] ways of calculating R^2," First time posting here, I'm confused about the ways of calculating r-squared, I've read some articles regarding r-squared and how to calculate it. some says R2 is just pearsons R squared, which makes sense. Also, I could just use this formula **R2** **= 1 - (RSS/TSS)**. Usually, I just calculate r2 using pearsons r. does using the pearsons r method to get the r2 applicable for multiple linear regression? because you cant calculate pearsons R with multiple independent variable. I'm just a student.",11c9ozm,kansha-,1677402154.0,9,1.0,"['For linear regression all those formulas should give the same value.\n\nFor generalized linear regression, for example logistic regression different definitions lead to different formulas that also give different values.', 'Yeah as far as i know, R is just r in linear regression. In mulitiple lineare regression you add the increments in R for each new predictor, using semi partial correlation. Dont know if this was what your asking, im first year statistics too. I can also look up the more percise formula if you like. Cheers :)', 'R is equal to the r obtained when a linear combination of predictors is chosen to maximize r. You get the same coefficients (up to an arbitrary multiplier) as you get when you minimize squared errors.', "">For linear regression all those formulas should give the same value.\n\n.... but only in the case where the regression has exactly one independent variable and a constant.\n\nAs the OP notes, with multiple independent variables, you've got multiple Pearson's r.""]"
[C] Bachelor in Statistics,"I would like to know what do you think about a about in a bachelor in statistics for both, industry work and academy, and how does this compares to a bachelor in math",11c351a,Unhappy_Passion9866,1677378923.0,7,0.74,"['Statistics is severely underused in the business world and I think a lot of businesses are starting to realize this. \nI work for one of the largest retailers in my country and there are very few statisticians on staff. \nI end up taking on that role often for other teams.', ""A bachelors of stats is much more useful than a bachelors of math. As a statistician you can provide value to many businesses using stats skills. The world is only getting more and more data and there is still value in analyzing it. A Stats degree is especially useful if you combine this with a little CS (e.g. you know SQL, how to scrape and wrangle data).\n\nMaths isn't so useful in and of itself until you get to the PhD level, unless you combine it with something like engineering."", 'Asking this question in a stats forum brings certain biases  Im surprised no one has yet to mention this given the sub. \n\nIm going to go against the grain and say a bachelors in stats is as (un)employable as a bachelors in math  because jobs where youll  be expected to do reasonably technical work involving stuff you learnt in undergrad will generally require graduate degrees.\n\nIve personally found the maths curriculum to be quite enjoyable due to its variety. We have topology, number theory, abstract algebra, complex analysis, various flavours of geometry, harmonic analysis, etc. each of them its own world, and often youll have enough electives to cover most of the required/important stats courses. \n\nIt might be helpful to glance at the course catalogue for both departments, select all the courses that interest you, and let the courses tell your whether to do maths or stats degree.', 'I graduated with a Bachelors in Statistics last year. As far as finding a job. I had two offers for work pending graduation about 7 months before I graduated. Meanwhile I had classmates who also where in the same program as me with no job prospects. I find that my most useful resources where school career fairs, networking, and persistence.', 'I am currently master student in stats and also did my bachelor in stats. Currently I am trying to get into computer science PhD to do ML research. I have to say that having stats background prepares you well for machine learning research, because it lays on statistics, probability theory and information theory.\n\nI say that stats bachelor is versatile and gives you freedom about your career choices. Think about what you want to do, finance, analysis, theoretical, ml engineering, ml research and take courses to support your goal.', 'The following is an unedited response from a notable AI Chat Platform after a couple clarifying iterations of a Question as to whether or not AI may one day completely replace human Statisticians ...\n\n------------------------------------\n\nYes, I can provide an analysis and logical inference based on existing data related to the original question of whether AI will render statisticians obsolete.\n\nFirst, it is important to note that AI and statistics are not mutually exclusive fields. Rather, they can complement each other, with AI being used to analyze large datasets and statistics being used to interpret and draw conclusions from that data.\n\nMoreover, AI algorithms typically require extensive training and validation from statisticians to ensure they are reliable, transparent, and free from biases. This means that statisticians will continue to play a critical role in developing and refining AI algorithms.\n\nFurthermore, there are many areas where human expertise and judgement are essential in statistical analysis, such as determining appropriate statistical models, choosing the right variables for analysis, and interpreting and communicating results. AI may be able to automate some aspects of statistical analysis, but it cannot replace the need for human expertise in these areas.\n\nAdditionally, there are many ethical considerations surrounding the use of AI in statistical analysis, such as privacy concerns, potential biases, and the need for transparency and accountability. Statisticians are uniquely positioned to address these issues and ensure that AI is used in a responsible and ethical manner.\n\nIn conclusion, while AI has the potential to revolutionize statistical analysis, it is unlikely that it will completely replace the need for human statisticians in the foreseeable future. Statisticians will continue to play a critical role in developing, validating, and interpreting AI algorithms and ensuring that they are used in an ethical and responsible manner.', ""Do yourself a favor and see if you can double major in both. In my university, about 30-40% of the courses for the stats major are Math courses. If you have a robust math foundation, the upper level stats work will become infinitely easier. So if I were you, I'd recommend double majoring."", 'Versatility is probably the most attractive from majoring in statistics. You (assuming youre an undergrad student) would need a change in your career a few times, and you can move on pretty smoothly with stat background, I would say.', 'Would you say that your title in stats gives you an advantage on getting jobs or getting raises?', 'I have the same background, industry, and work history. I can back this as well. Retailers are usually behind the times on investing in tech or STEM background work. This means you can automate their processes if they give you that ability.', 'thank you', ""This is definitely not true, I'm graduating in 2 month with a dual math and stats degree. So far on my job search the stats part of my degree is the only thing recruiters are interested in."", ""Also learning how to code and familiarity with all major libraries. If you don't have this you won't get hired. \n\nPretty much all students in my program who were somewhat decent devs got jobs and everyone else pivoted or went to grad school to postpone job search.\n\nI started as a stats major and switched to math because the upper level stat curriculum had a bunch of useless classes. Better to learn more math to understand more complicated statistical models. So if you truly want to do stats, major in math and specialize in stats."", 'Hey. Im actually a statistics bachelors student whose applying for MS in statistics, with hopes of pursuing ML research. I had thought I was at a disadvantage because of my non cs background that I couldnt get roles in ML research, but seeing as you are doing this, I wanted to ask you more questions about your experiences and path. Can I pm you?', 'thank you, I do not know why are you getting downvoted', 'I only graduated 1.5 years ago, so havent got much to go on, but jobs werent easy to get. \nNone the less, I got hired as an analyst, but when they discovered how much value I could bring outside of the roll they hired me for, things started to ramp up. \n\nI will find out in a month or so about a raise. \nIm Expecting a 30% raise over the next 3 years.', 'What positions are you applying for and what are they asking?', 'Hey, I\'m still in the first year of my undergrad in stats, what are the classes that you found to be ""useless"" in your course requirements?', ""Sorry, just saw your Message + thank you for your thoughtfulness .. TBH, I did not even know there was any reaction to the Post bcuz I'm not very active on the Platform .. Yeah, it doesn't make sense that people would not appreciate thoughts directly from the very technology that is going to replace virtually all jobs eventually over time ...\n\nThanks again + all the best ..."", 'Thank you', ""R advanced data analysis when I already knew python and pandas / scikit learn. A capstone project course when I was already competing in data science competitions. A class on experimental design which is really just pretty simple linear models (ANOVA etc). Also the senior level multivariate stats course course was basic multivariate distributions / linear models / PCA / mixture models / clustering. I had a stats professor who was extremely ambitious in her course work and taught me multivariate statistics so we'll to the point that this stuff was easy.\n\nBecause of switching I was able to take real analysis, advanced numerical methods, stochastic calculus, and graduate statistics courses. The math major was much more loose than the stats undergrad plan so I was able to design a more useful curriculum. Stats forced you to use R and take fluff courses."", 'As a result Im the only DS on my team without a PhD', 'My consternation with undergrad programs is this rigid curriculum you speak of, especially forcing you towards R instead of being at least agnostic to your language choice.']"
[Q] Distribution of Regression Coefficients (Bayesian vs Frequentist),"I was having a conversation with a coworker regarding the difference between a frequentist and bayesian regression formulation. My thought was, that in bayesian modelling, all coefficients in a regression are random variables that follow some posterior distribution, while in the frequentist model the coefficients are point-estimates of the population parameter. My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn't hold.

While my coworker may be technically correct, my intuition tells me that there is something more to this. For instance if you were to make a prediction in a frequentist model, you would use the mean beta(s) to calculate the response y, whereas if you were to make a prediction with the bayesian model you would sample from the random beta variable(s) & use those samples to calculate a posterior prediction. In the bayesian case it seems we are actually treating betas like random variables, whereas in the frequentist modelling we are not.

Is there something i'm missing here? What is the technically correct differentiation between this bayesian model and frequentist model?",11bz8hk,weareglenn,1677367966.0,35,0.97,"['> My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn\'t hold.\n\nYour co-worker is mistaken; under the usual assumptions, estimated coefficients follow a normal distribution (though with unknown variance) not a t. However, you\'re not comparing like-with-like anyway. *Conditional on the data* the estimated coefficients are not themselves random variables, while in Bayesian statistics, the conditional posterior distributions for the parameters are. So when you condition on the data for both, your original distinction holds up  --- but it\'s not clear to me what the point of the distinction is.\n\n> What is the technically correct differentiation between this bayesian model and frequentist model?\n\nYour use of the word ""the"" there confuses me. I would not label that distinction you made with ""the"". Possibly with ""a"", in some situations.\n\nThe fundamental difference is simply how you treat probability (as a notionally infinitely long-run frequency or as a formal way to summarize relative degrees of belief). Everything else pretty much follows from that distinction.', '>For instance if you were to make a prediction in a frequentist model, you would use the mean beta(s) to calculate the response y, whereas if you were to make a prediction with the bayesian model you would sample from the random beta variable(s) & use those samples to calculate a posterior prediction.\n\nThis seems to mistake estimating the mean function **E**(y|**x**) with estimating quantiles of ""future"" y values, which are not the same thing.  Sure, if you allow the Bayesian analyst to give you estimates from the entire *posterior predictive distribution*, and you only allow the frequentist analyst to give you the estimate of the mean function, the first will appear to be using more uncertainty than the latter.  But the analogous ask is for the Bayesian analyst to give you the MAP estimate or posterior predictive mean or something similar.', "">My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn't hold\n\nESTMATES of the coefficients follow a distribution, not the coefficients themselves in a frequentist framework"", 'For the frequentist the regression coefficients are unknown fixed quantities (they do not have a distribution), the *estimates* of the regression coefficients are random variables, and follow a t distribution, assuming you also had to estimate the variance.', 'There are two things to be said.  First the ""usual assumptions"" for linear regression satisfy the ""usual assumptions"" for both frequentist and Bayesian asymptotics.  So for large n the frequentist sampling distribution of the estimators and the Bayesian posterior distribution can be close.\n\nBut in this case it is also true that the exact sampling distribution of the point estimates (the marginals of which are t distributions) can also be the posterior distribution for some Bayesian with some (perhaps improper prior).  I am not 100% sure that is true but am assuming this is analogous to the two-parameter normal case https://www.stat.umn.edu/geyer/5102/slides/s4.pdf#page=56 and the following ten slides.   But there is no reason for anyone to choose that prior except to match the frequentist answer.\n\nBut philosophically *even if they have the same distribution* the frequentist and Bayesian theories of what those distributions are doing is very different.  For the frequentist it is the sampling distribution of estimators.  For the Bayesian it is the posterior.', 'For Bayesian regression, its a generalization of hierarchical models. You have priors set on each of the coefficients, where each coefficient can have a layer of priors for hyperparameters or a some sort of pooled priors where all the coefficients share the same mean and variance etc. \n\nThe nice thing about this in comparison the frequentist regression is that, in this framework we can get regularization over the parameters. The data will aid in shrinking certain coefficients.', 'The coefficients of a frequentist regression follow the t-distribution *across realizations of the regression.* For a single dataset the estimate is a fixed quantity, not a random variable, and doesnt follow a distribution.\n\nEdit: changed datapoint to dataset\n\n\nAs /u/efrique pointed out, the estimator is in fact normally distributed and not t-distributed.', ""Regression coefficients aren't random in Bayesian statistics, they are *uncertain*. The Bayesian posterior distribution describes the range in parameter space in which you'll be able to locate the parameter of interest with a chosen probability (say 90%, or 95%). It's still a point, but to know it with perfect accuracy you would need infinite amount of (sampling) information."", 'Whether the parameter is fixed or random. Frequentist adopts the infinite many samples and states the confidence interval in a resampling manner, whereas the bayesian employs the credible interval that may yield a satisfying conclusion.', 'The random sample estimates follow a particular distribution, not its actual observed or hypothetical values.', "">if you were to make a prediction with the bayesian model you would sample from the random beta variable(s) & use those samples to calculate a posterior prediction.\n\nYou have the right idea here, but I want to clarify the language. In a Bayesian model, one way to generate predictions is to sample from the *posterior distribution* of parameters and simulate the data-generating process for each element in the sample. The distribution of the results is the *posterior predictive distribution*.\n\nHere's an example from Think Bayes: [https://allendowney.github.io/ThinkBayes2/chap17.html?highlight=predictive#prediction](https://allendowney.github.io/ThinkBayes2/chap17.html?highlight=predictive#prediction)"", ""If I'm understanding correctly - are you saying I should be comparing the prediction from the posterior predictive distribution in the bayesian case to the prediction of a new point in a frequentist case, which itself follows a normal distribution dependent on error in estimating beta(s), and this is how the frequentist would incorporate uncertainty in the betas in its final prediction?"", '>The coefficients of a frequentist regression follow the t-distribution   \n>  \n>across realizations of the regression.\n\nnot true.\n\nthere are subtle things that are being confused or misunderstood. \n\nFirst thing : regression coefficient are parameters. In frequentist approach those parameters are fixed and unknown. In a Bayesian paradigm those are random variables.\n\nEstimators: when you fit a model in the frequentist approach you do so by calculating estimators. Those estimators are random variables which indeed have a distribution (t-distribution in this case). In the Bayesian context you use as estimator some parameter of the posterior distribution, like the expected value or the median.\n\nlet us set this clear with an example: estimating the expected value. in the frequentist approach you use the mean, and as you may know, in the case of normal distribution the mean have t-student distribution that you use to make confidence intervals and hypothesis testing. In this case, the expected value is the parameter (fixed and unknown), the mean is the estimator (with a distribution which allows us to derive CI and tests). I will let you to complete this example in the case of the Bayesian estimation procedure, you can find it elsewhere.', 'When you say ""across realizations of the regression"", do you mean that if you re-fit your regression with a different sample of data from the same data-generating process many times the fit betas would follow the t-distribution? So basically, for any single regression in the frequentist methodology the betas are in fact point estimates?', 'Im not exactly sure what you mean by prediction to Ill try to clarify what I mean.  There are two unknown quantities for a new y with associated covariate vector **x**: the conditional mean **E**(y|**x**), and the value of y itself.  The conditional mean is not itself a random variable, but we estimate it through a statistic (through the plug-in method, sure) which we call *y*-hat.  This conditional mean estimate has an associated standard deviation, and as our sample size N increases, this standard deviation decreases, meaning our estimate of the conditional mean becomes more precise.\n\nBut a prediction is typically not about the conditional mean, which is being estimated, but about the value of the next observation taking into account *its own* variance, contained in the epsilon.  In frequentist statistics, you can calculate a prediction interval which accounts for the uncertainty from your epsilon error.  When you use some nave estimators and go through the math, youll see this often arises in the form of an extra +1 in the square root of the conditional means standard deviation formula.  (Think that the s.d. of the conditional mean estimate decreases proportional to sigma times root-N, but there is always a non-decreasing sigma remaining from the epsilon.)\n\nFrequentist prediction intervals are to Bayesian posterior predictive credible intervals, what Frequentist confidence intervals are to Bayesian posterior credible intervals.', '> First thing : regression coefficient are parameters. In frequentist approach those parameters are fixed and unknown. In a Bayesian paradigm those are random variables.\n\nCoefficients is a pretty generic term used in many contexts. The true value of the coefficient is one thing, the coefficients of an estimated regression are another things. Further, and this is extremely pedantic, coefficients can themselves be random or fixed in either bayesian or frequentist contexts. In frequentist models we might have random coefficient models where the coefficients are themselves random. In bayesian models we can also have fixed coefficients (commonly called offset terms in some contexts).\n\n> Estimators: when you fit a model in the frequentist approach you do so by calculating estimators. Those estimators are random variables which indeed have a distribution (t-distribution in this case).\n\nEstimators are in fact random variables, but the realized value of an estimator conditional on the realized dataset is not. It is a fixed quantity just as a realized confidence interval is a fixed quantity. Making probability statements about the realized estimator is an error in exactly the same way as the common error about interpretation of confidence intervals.\n\n> In the Bayesian context you use as estimator some parameter of the posterior distribution, like the expected value or the median.\n\nWhile usually true it doesnt have to be this way exactly. You can define your estimator initially and calculate the posterior associated with that estimator rather than deriving it from the posterior distribution of the parameters. Bayesian IPW would be such an example where its not immediately clear how you would calculate this estimator from the posterior, but it can easily be defined.', 'Yes that is exactly what I mean.\nAlso yes to the second point. For each regression it is a point estimate and is not a random variable.', 'A few points of minor clarification on your response here, which it seems you are probably aware of, but may be confusing to others. \nit is very common to call the predicted value of the conditional mean using the fitted regression, Ehat(y|x) a prediction. \nUnder the standard linear regression assumptions, the predictions are also normally distributed (with the distribution being in the frequentist sense over possible datasets), as they are computed via linear combination of the normally distributed coefficient estimates. \n\n\nOne other point, it may be misleading to try to compare and contrast bayesian posterior predictive intervals and prediction intervals. \nThe bayesian intervals derive their characteristic probability statement fully conditional on the data. The probability statements for frequentist prediction intervals are not conditional on the observed data, but are taken jointly over both the regression datasets and new data.', 'Thanks! this makes much more sense.', 'From end to start, the main purpose of likening the intervals in this manner is not to say that credible intervals or confidence/prediction intervals are in fact the same, but that the two sets of intervals (in the manner I paired them) are analogous within themselves and disanalogous across themselves in the manner I was getting at.  If there is confusion about the mean function v. the response values in themselves, its helpful to recognize both the Bayesian and Frequentist approaches have ways of addressing these two separate entities.  This is why confidence intervals are to posterior credible intervals in the same manner predictive intervals are to posterior predictive credible intervals: the first set handles mean functions, the latter handles response values themselves.  And youre right about conceptual differences between Frequentist and Bayesian intervals (what they mean, what they are with respect to, so on).  But these dissimilarities are also shared in the same manner; confidence intervals are conceptually different from posterior credible intervals in a like way to how prediction intervals conceptually differ from posterior predictive credible intervals.\n\nSo yeah, the main issue I was addressing was this distinction between mean function and response values.  And to that end I figured it would be helpful to make a terminological distinction, especially since the phrases posterior predictive and prediction intervals are already native to the stats lingo; and are themselves suggestive of an intended distinction from the mere posterior or from confidence intervals, respectively.', 'We can be sloppy with the language here sometimes and it leads to the confusion. We call the distribution associated with the coefficient the sampling distribution because it is the distribution we get when repeating the sampling process and performing the regression many times.']"
[Q] What test to use to determine similarity between data sets,"I would like to compare two data sets representing self-report and partner-report values. We both took a personality test (I know they're BS but it was in good fun) and then took the test again through the lens of the other person. We want to see ""who knew the other person better"". Each test gave numerical values for 5 categories, then broke down each category into 5 subcategories with numerical values. I'm not sure how to phrase this next part, but it looks like I got higher values on both tests I took (rating myself and them). Is there a way to compare that as well? Formulas that could easily be entered into excel would be bonus. Thanks!!!",11bvztb,franzia5eva,1677359812.0,1,1.0,['Mean squared error maybe. \nConsider your attempt at answering for her as predicted and her answers as actual. \nDo the same for her and then compare MSE']
[S][R] Hidden Markov Model implementation in R and Python for discrete and continuous observations.," Hidden Markov Model implementation in R and Python for discrete and continuous observations. I have a tutorial on YouTube to explain about use and modeling of HMM and how to run these two packages.

Code:

[https://github.com/manitadayon/CD\_HMM](https://github.com/manitadayon/CD_HMM) (in R)

[https://github.com/manitadayon/Auto\_HMM](https://github.com/manitadayon/Auto_HMM) (In Python)

Tutorial:

[https://www.youtube.com/watch?v=1b-sd7gulFk&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=1b-sd7gulFk&ab_channel=AIandMLFundamentals)

[https://www.youtube.com/watch?v=ieU8JFLRw2k&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=ieU8JFLRw2k&ab_channel=AIandMLFundamentals)",11bt3hh,chess9145,1677352639.0,31,0.93,"['Cool.  Does it work for a single time series or multiple observations with many subjects (like a mixed model or multi-level model)?', 'Great job! Do you have others?', 'Oh spammer, please go away. Funny how the sockpuppet replies are always the same. ""Great work! How do you handle <detail xyz>? Can I subscribe to your channel?"" Can\'t wait until spammers are using ChatGPT to generate 500 word responses, that\'s going to be glorious and/or disastrous.', 'It does handle multiple observations.', 'How does it handle the heterogeneity?']"
[Q] Probability of 2 events happening together given a set of probable events,"My friend and I are making a maze game where we want doors to close at random as you play. Essentially every minute a timer goes off and sets off a probable chance that a door will close.

We want to know what the statistical chance of any 2 doors closing at the same time is. We both struggle with stats and couldn't solve the question so here is the question:

Given a set of 20 doors, each with a 1/10th chance to close, what's the probability that exactly 2 doors will close?

I tried mapping this to another problem: ""Add the numbers 1-10 to a hat and draw a number at random 20 times with replacement. What's the probability you will get 5 exactly twice?"" But I wasn't sure if this is an equivalent problem or not. I couldn't solve it either.. stats is my weakest subject.

How would this be solved? We would like know so we can fiddle with balance of the maze so we would like to see how this specific problem is solved, but also the general approach. What if I wanted to know exactly 3 doors? Or what about 3 or more doors and not just 3?

Thanks a bunch for any insight!",11bsef1,xx1HawkEye1xx,1677350876.0,5,1.0,"['This sounds like Bernoulli trials. The probability of exactly two successes in 20 trials given P_success = 0.1. We solve this by saying 0.1^2 * (1-0.1)^18 * 20!/(18! 2!) ~= 0.285 or about 29%', ""If you're good at coding but not stats, a solid thing to learn is how to simulate things with random numbers. It allows you to get the answer to arbitrarily complex stats problems without very complex maths.\n\nFor this problem, we can represent a door by drawing a random number between 0 and 1. If it's less than 0.1, the door is closed, if it's greater it remains open. That gives it a 1/10 chance of closing.\n\nFor twenty doors just pull 20 random numbers and count how many are less than 0.1. That's how many close in one simulation.\n\nTo get the chance of exactly two doors closing when twenty have a 1/10 chance: do the above simulation thousands of times and count what fraction of the time you get 2 doors closing.\n\nIt's super simple and whilst this problem is easy to solve analytically, simulations don't get much harder than this but some problems can get extremely hard to solve analytically."", ""Thank you! I'll research this a bit more to understand it better. I ran some sample generation and got around that value, but I didn't know how to calculate it"", ""awesome thanks! Yeah I have a math degree too which is funny, but I'm just horrible at stats. The way of thinking to me is so different than proofs. I feel like my intuition is always wrong for how to start a stats problem or how to visualize it. Thanks for this explanation"", ""the formula for the binomial distribution is exactly what you're looking for: [https://en.wikipedia.org/wiki/Binomial\\_distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n\nintuitively you can think about it this way, you have 2 doors that close and 18 doors that do not close so (1/10)\\^2(9/10)\\^18 \\[independent events so you just multiply there probability\\] but thats not all, you also have to consider every combination of 2 doors opening and 18 doors closing as well so you multiply it by the number of ways you can choose 2 from 20 objects which gives you the probability of exactly two doors opening. \n\nthis might be easier to think about in the three door case, if you are looking at the probability that exactly one door is open, then do the same thing and recognize that either the first door can be open or the second or the third (3 C 1)."", ""thanks for the explanation! This is close-ish to what I tried after a while, but I never felt confident my approach was right. I don't have a good stats foundation since I didn't really retain much from my stats classes. I don't think I was ready to think in the way you need to for stats when I took the class even though I made a good grade.. I just knew how to get the answers via formulas, but I didn't really understand it.""]"
[E] Why is Linear Algebra required for Statistics?,"I am enrolled in a MS in Applied Statistics program that did not require Linear Algebra. Most of my classes are heavy on the ""applied"" part of statistics where we are using code to perform statistical testing/analysis etc. The Statistics and Probability courses we are required to take are particularly difficult though. Most of the computations just utilize a lot of calculus, but the concepts are very tough to grasp.

I've not taken Linear Algebra so I am curious about if having taken that would have helped my understanding of the concepts, and in particular which concepts would be clearer with that background?",11bqhfw,anditgetsworse,1677346153.0,43,0.79,"['Linear algebra might be the most important traditional math course for understanding many modeling techniques. Your data and coefficients are a bunch of matrices.', ""The reason it's important OP is because when you take all the equations you know from basic statistics, and try generalise them to multiple dimensions, you need vectors. Instead of a single slope for linear regression, you have a vector of regression coefficients for each variable.\n\nLinear algebra gives us mathematical tools to deal efficiently with all those vectors. It's the next step up from high school algebra.\n\nAnd the reason we care about multivariate problems is because in statistics nothing is ever just a function of one thing. Everything is an interplay. Univariate statistics is not enough."", ""Yeah much of regression rides on it. It can hardly be taught without results from linear algebra. You can learn it as you go without a strong command of linear but it's really helpful for building intuition."", 'Used heavily in nearly every part of applied statistics and stochastic modeling', ""Try taking multivariate analysis without having any linear algebra background. And, in general, your model equations will be generalized into model matrices.\n\nYou're gonna want to know how to compute eigenvalues/vectors and what they mean. They'll come up in covariance structures, etc.\n\nYou're gonna need to become very familiar with sigma notation because in multivariate analysis, a lot of the computations involve blocked/partitioned matrices. \n\nMatrix algebra is fundamental.\n\nIdeas of positive definite, full rank concepts, quadratic forms are important too.\n\nYou don't necessarily need to understand all of the theory (depending on your program)."", ""Even in applied statistics, your program should teach you about what's going on under the code. The easiest example is multiple linear regressions - moving from the design matrix to the normal equation for coefficient estimation are big steps. Without that, it sounds more like a data science and tools program."", 'finish the semester and switch schools', 'Linear models and pca rely on linear algebra.', 'Regression relies heavily upon it, and matrices also just make regression equations much easier. Also used in a bunch of other places (unsupervised learning, stochastic processes, etc.), but regression is the main one for me.', ""Most of these responses are sensible and worthwhile, so I'll offer a simpler one.\n\nLinear algebra is the class that will get you thinking about data in the proper structure, which is vectors, matrices, rows, and columns. You will start thinking about tables as just big matrices. You will start visualizing general model structure as the headings of columns and the rows as the tuning of the regression.\n\nI'm a Data Scientist by title, Biostatistician by training, and I don't ever find myself doing long-form matrix calculations since software does that for me. But you need to know how to mentally structure the code or inputs for the software in order to retrieve an accurate result...which is where the Linear Algebra classwork comes in.\n\nHighly recommend you take one. Highly."", 'In my experience, I didnt get very far into my stats/econometrics journey before matrix algebra became absolutely necessary (in theory and in practice).', 'if you anything your working on touches on covariance matrices, multivariate distributions, stochastic processes for example, you might want to have some linear algebra under your belt.', 'If you are finding applying the calculus to statistics hard you will find working with multivariable data incomprehensible. It will look like an endless morass of formulas to you. \n\nThat is where linear algebra comes into play.', 'Try doing PCA without understanding linear algebra. That will motivate you real fast.', 'Linear algebra is really important for programming. You will likely be hired for your general programming/quantitative/problem solving set. Most jobs are not pure statistician roles you have to be good at working with data in general. Linear algebra is a great way to understand how the computer sees data. Id argue its more important than calculus. You dont need to know how derivatives work to know linear regression.', 'I think a way of understanding it is that In statistics you arent every really using just one variable, u have a sample of multiple draws of that variable. This becomes a vector. If theres more than one variable u have a matrix. To do anything with these u need linear algebra', ""MS in stat that doesn't require linear algebra... I think you should check again the requirements, I honestly think it's very unlikely that this is the case. Anyway without a deep foundation in math you will not be able to go beyond the most easy techniques, will not be able to customize your models, will not be able to understand why some are failing or why some are not appropriate for a specific problem"", 'Stochastic also uses plenty of LA\nIve been searching for a school that is heavily applied - my stats degree was almost exclusively theoretical. \nCould you share which is your school?', ""The problem with letting your computer do the thinking for you is that your computer isn't smart. It will run whatever you tell it to run whether or not that is appropriate or whether or not it makes any kind of mathematical sense. If you don't understand what you're telling it to do, then you can't count on the results.\n\nThe reason why you need to know linear algebra in particular is because your data is best understood as a data matrix and any regressions or other models that you do usually involve doing something ( a lot of somethings) to that matrix.\n\nYour MS program may also be preparing you for PhD programs, and if you're getting a PhD in statistics it's your job to come up with new ways to use that data. You are transferring between being a user and a producer of statistics. That requires you to understand the math behind every tool you use - how else can you improve it?"", ""So, regular math is adding values you know...\n\n...Algebra is adding values you don't know using relationships (formula/functions)...\n\n...Linear algebra is adding relationships (formulas/functions) for values you don't know yet?"", ""Just as you can use a computer without knowing how a transistor works, and you can drive a car without knowing how the engine works, you can do lots of useful statistics without knowing linear algebra or calculus.\n\nMany statistics classes depend on linear algebra and/or calculus, but that's because someone chose to design those classes in that way, not because that's the way it has to be."", 'I was not formally trained in statistics, so my wording may not be 100% on point.\n\nWhen you spend enough time on the subject, youll realize that there is geometric interpretation (multivariate calculus) to various statistical concepts.  LA is the way to navigate the multivariate world.', 'Understanding of normal distribution theory requires linear algebra.', 'Underneath all that code there are looots of linear algebra, I promise.', ""Yes - I can't imagine doing a proof in stats without linear algebra under your belt."", 'Years after starting my PhD I finally got around to learning linear algebra and I thought two things: 1) this is actually more straightforward than I thought and 2) where the hell have you been all my life?', 'By any chance could you recommend texts or resources for linear algebra? I like to study textbooks and would appreciate your thoughts.', ""100%. I am currently doing Bayesian machine learning and it's all vectors and matrices, all of it"", 'Please elaborate.\n\nFor example, ChatGPT was able to provide this information for OP:\n\n>Normal distribution theory is a branch of statistics that deals with the probability distribution of continuous random variables that follow a normal (or Gaussian) distribution. Linear algebra is used in normal distribution theory for a variety of reasons, including:\n > \n>* Matrix notation: Normal distribution theory often involves working with multiple random variables, which can be represented using matrices. Linear algebra provides the necessary tools to manipulate matrices, such as matrix multiplication and inversion, which are essential for solving problems in normal distribution theory.\n>* Multivariate normal distribution: In many real-world scenarios, multiple variables are jointly distributed with a normal distribution. This is known as a multivariate normal distribution. Linear algebra is used to characterize the properties of multivariate normal distributions, such as covariance matrices and eigenvectors, which are important for statistical inference.\n>* Principal component analysis: Principal component analysis (PCA) is a popular technique in normal distribution theory that is used to identify the most important features in a dataset. PCA involves finding the eigenvectors and eigenvalues of a covariance matrix, which requires linear algebra.\n>* Maximum likelihood estimation: Maximum likelihood estimation (MLE) is a common method used in normal distribution theory to estimate the parameters of a normal distribution that best fit a given dataset. MLE involves solving a set of equations that can be expressed in matrix form, and linear algebra is used to find the optimal solution.\n> \n>In summary, linear algebra is essential in normal distribution theory because it provides the necessary mathematical tools to work with matrices, multivariate normal distributions, principal component analysis, and maximum likelihood estimation.', 'Or implementing a model without using matrices', 'Why I only minored in stats and did not do a double major. I hate doing proofs.', 'linear algebra done right by sheldon axler. \n\njust go to /r/math for linear algebra book recommendations.', 'Yes, on Udemy theres an excellent complete and not expensive course by Mike X Cohen. His lessons are perfect bites, very clear and with good exercises and demonstrations in both Python and MatLab (I believe). Really targeted to math novices too. Well worth the money (often on sale for less than $25). \n\nHe also has a book on LA I believe. Guy is a legend imho. (Full disclosure: Im not him xD).', 'This sort of chat bot output always needs to be taken with a large grain of salt, because the bot does not ""know"" any of these things, so much as generate responses learned from material online.  But it will speak confidently about things that are ultimately not true.  There are hints of this happening in the bot asserting ""normal distribution theory"" is a branch in itself in statistics, despite the phrase not really being its own thing in the first place; and also, I think it would be more accurate to say that normality assumptions are popular with dimension reduction techniques, not the other way around, because the properties of the normal distribution make it generally appealing in complicated methods rather than those methods being embedded under the ""normal distribution theory"" umbrella itself.  Saying linear algebra is important for the normal distribution because you need it for PCA is besides the point; you don\'t even need the normal distribution for PCA.\n\nThe public ChatGPT bot gives the following very similar response to the query ""Why is linear algebra important for gamma distribution theory?"":\n\n>Linear algebra is important for gamma distribution theory for several reasons:  \n>  \n>1. Matrix notation: Linear algebra provides a convenient way to represent and manipulate large datasets. The gamma distribution can be parameterized in terms of matrices, which makes it easier to work with in a linear algebra framework.  \n>  \n>2. Maximum likelihood estimation: Maximum likelihood estimation (MLE) is a common method for estimating the parameters of a gamma distribution from data. Linear algebra is used to solve the system of equations that arise in MLE.  \n>  \n>3. Multivariate gamma distribution: Linear algebra is used to extend the gamma distribution to the multivariate case, where it is known as the multivariate gamma distribution. This extension allows for the modeling of more complex phenomena, such as the joint distribution of multiple waiting times.  \n>  \n>4. Linear transformations: Linear algebra is used to study the effect of linear transformations on gamma-distributed random variables. This allows for the transformation of gamma-distributed data to a new scale or location, which can be useful in certain applications.  \n>  \n>Overall, linear algebra provides a powerful toolkit for understanding and working with the gamma distribution and its applications. It allows for the efficient computation of the distribution\'s properties, as well as for the development of new statistical models and methods.\n\nMuch of this is nonsense.  The gamma distribution is not parametrizable in terms of matrices, matrices have no place in the distribution.  There is not any single ""multivariate gamma distribution"", especially not one that is both analogous to the normal distribution and can jointly model multiple waiting times.  If anything, the commonly accepted extension of the gamma distribution to multiple dimensions is called the Wishart distribution (not ""multivariate gamma""), and it models p.s.d. matrix random variables, not vectors.  By and large, I would also not say that linear algebra is used to study location-scale families merely because these are simple algebraic manipulations.\n\nWhile it\'s helpful indeed to have descriptive and thorough answers to questionshence I agree with your very first request to elaborateIMO we should refer to authorities on statistics instead of computers trying to mad-lib their own responses.', 'Look, one important way of understanding normal distribution modelling is to represent different hypotheses as Hilbert space subspaces. This works, since the normal distribution itself lends itself well to kernel space constructions and decompositions of these.', 'I saw on YouTube about how some math magicians wrote a proof for 1+1=2.   It was 400 pages.', 'This is nonsense, unless they were starting from axiomatic set theory.\n\n1+1=2 boils down to the definition of 2 as being the successor of 1 when constructing the natural numbers.', 'You are probably thinking of Russell and Whiteheads Principia Mathematica:\n\n>\t[From this proposition it will follow, when arithmetical addition has been defined, that 1 + 1 = 2.](https://i.imgur.com/PojpY6C.jpg)', 'Googled and found this blog post: \n\n[https://blog.plover.com/math/PM.html](https://blog.plover.com/math/PM.html) \n\n"" Whitehead and Russell\'s Principia Mathematica is famous for taking a thousand pages to prove that 1+1=2. Of course, it proves a lot of other stuff, too. If they had wanted to prove only that 1+1=2, it would probably have taken only half as much space. ""', '> unless they were starting from axiomatic set theory']"
[Q] What is a test to see if two groups of different dependent variables with a shared independent variable are possibly related?,What is a test to see if two groups of different dependent variables with a shared independent variable are possibly related?,11bcdpr,Hikaritoyamino,1677300754.0,15,1.0,"['Maybe look into the theory of causality and Bayesian DAGs for this one. I don\'t believe there is a simple test because it\'s hard to have access to the set of all possible sources of cause.\n\nOtherwise if it\'s simply correlation check you\'re after look at the sample data for p(a|b) and p(c|b) and use any relevant correlation check where ""b"" would be your ""shared independent variable"" and a and c are your groups of variables. \n\nYou can for example check for correlation between a and b and c and b, then between a and c and ""you can conclude"" b could be a common factor of effect between a and c somehow.  \n\nIt\'s really hard to give a precise answer because the writing of the problem is not so precise', 'A simple correlation matrix could show you if they have any relation to each other.', 'You might want to look into canonical correlation analysis. But, from your phrasing, you sound fairly new to stats so that may not be an option. Other suggestions here are good.', 'A simple test: suppose your two dependent variables are X1 and X2, and the independent variable that ""drives"" both is Y. \n\nIf I(X1;X2|Y) > 0, then there is dependency that exists between X1 and X2 beyond that which is provided by Y. \n\nYou can significance test this by shuffling Y a large number of time and doing a NHST of the true value against the null distribution.\n\nNB: this can get complicated if X1 and X2 also ""cause"" Y. If you can assume that X1 <- Y -> X2 then you\'re fine, but if X1 <-> Y <-> X2, then conditioning can get really difficult.', 'Maybe PCA would help? You could also use information values to see if a dependent variable can help explain the other one. My fair guess would be that, due to them sharing an independent variable, you kinda have to look for a somewhat powerful IV.']"
[D] Ordered vs un-ordered axes? Weird question.,"Ordered vs un-ordered axes?

My only point is to do a comparison, as to why if i plot a graph for ordered or un-ordered(randomly ordered) x axis, the former depiction is termed ""correct"" and latter termed ""wrong"". Why must ordering affect the interpretation of relationship between these two variables? Also, if i were to ever use this un-ordered x-axis to plot norm dist, would the area under a normal dist sum to 1? Would it even be a normal dist or just a meaningless cloud.
I guess I'm asking, why favour the ordered distribution? Because at the end the pdf or cdf or edf dictates the area under curve, and we know, changing the order of x axis changes the distribution shape, then why must we be biased toward saying the ordered x axis along with its respective probability density must give the correct depiction everything else is not accurate?",11bay6w,Aggravating_Data1489,1677296262.0,3,0.81,"['You use plots to help make sense of data. In the vast majority of cases, an unordered x-axis will not help you make sense of anything.\n\nInstead of musing in the abstract, make some plots and see for yourself.']"
[Q] Working with a known type II error rate,"I'm having a bit of trouble getting my head around this. I have two datasets. The first has my dependent variable and the explanatory variables from data collection with 18,000 participants. The second has a sample of participants where we double checked the validity of the dependent variable. We identified a 25% type II error rate.

Any suggestions on how we can proceed? Right now I'm at the brute force method of just randomly removing 25% of negative cases. Surely there's a better way, right?

[Edit for more details:]
In our large sample dataset, we sent a message to phone numbers we suspected used WhatsApp. 

If the message was marked as sent/read these were confirmed WhatsApp users. If it did not, we initially recorded these as NOT WhatsApp users.

However, when we explicitly call them to confirm the negative outcome, we find that 25% of those we call have WhatApp, but use a different phone number.

So, we can estimate (for now) that 25% of our large sample negative outcome cases are in fact positive.

There's no need to confirm our positive cases because we have a read receipt.

It's a study on factors that determine WhatsApp usage in developing countries by the way.",11ba2yg,ewokcommander,1677293681.0,1,0.67,"[""> We identified a 25% type II error rate.\n\nCan you explain what you mean by this sentence in completely explicit terms, in detail, please? I don't see how it's possible to do what the words say, which makes me wonder if you mean something else."", 'Okay, so you have\n\n- a known sample of WhatsApp users\n- a contaminated sample that (you estimate) contains 25% users, 75% non-users.\n\nCan you use that to correct for the estimation bias caused by the use of the contaminated sample?', ""Sure. In our large sample dataset, we sent a message to phone numbers we suspected used WhatsApp. \n\nIf the message was marked as sent/read these were confirmed WhatsApp users. If it did not, we initially recorded these as NOT WhatsApp users.\n\nHowever, when we explicitly call them to confirm the negative outcome, we find that 25% of those we call have WhatApp, but use a different phone number.\n\nSo, we can estimate (for now) that 25% of our large sample negative outcome cases are in fact positive.\n\nThere's no need to confirm our positive cases because we have a read receipt.\n\nIt's a study on factors that determine WhatsApp usage in developing countries by the way."", '""Type II error"" is not a synonym for ""false negative"". That\'s why it has a different (and woefully clunky) name.\n\nType II error is a concept used in designing experiments. It\'s the probability of missing a difference at significance level alpha if there is a true underlying difference of delta (D). It\'s conditional on there being a meaningful difference (D) to find, which is why we don\'t call it the false negative rate because that depends on the probability of there being a difference to find.\n\nThe false negatives in your dependent variable are not likely to be random; that is, people who use a different number for WhatsApp are likely to be different from people who don\'t use WhatsApp at all. So just arbitrarily removing some negatives won\'t remove the bias.\n\nIf you don\'t have the resources to systematically check all the negatives, I\'d start by working out how the false negatives are different from the true negatives and ponder from there.']"
[Q] Misuse of p-value wiki article is bugging me,"item 2:

https://en.wikipedia.org/wiki/Misuse_of_p-values#Clarifications_about_p-values

""The p-value is not the probability that the observed effects were produced by random chance alone.""

The text after it doesn't correct the sentence. In fact, I think the text after it is pretty useless and confusing. 

As we all know, the p-value is the probability of obtaining the observed result or a more extreme one if Ho is true.

But isn't ""by random chance"" a perfectly valid layman's way of saying ""if Ho is true?""

Doesn't that mean the sentence becomes correct as long as you change it to as follows:

""The p-value is the probability that the observed effects (or more extreme ones) could be produced by random chance alone.""

If so, I think they should at least add the above sentence to the wiki.",11b7723,SodaHackk,1677285683.0,0,0.31,"['> But isn\'t ""by random chance"" a perfectly valid layman\'s way of saying ""if Ho is true?""\n\nNo, it doesn\'t automatically imply that; random chance operates under both hypotheses. Indeed lay people tend to specifically *not* interpret it that way, which are largely the people the article is for.\n\nYou would need to specifically talk about the operation of random chance along with H0 being true -- indeed, note that H0 is not necessarily a *nil* null even if you would like to interpret it to be one.\n\n> I think they\n\nWho is ""they"", exactly?', '1. The null hypothesis is absolutely integral to the definition of a p-value.  ""By random chance"" does not acknowledge the critical role of the null.\n2. A p-value is not a probability unless you know the true distribution; we wouldn\'t be doing hypothesis testing if we knew the true distribution. I believe that Bayesians treat it somewhat differently, but in frequentist statistics a p value ~~never a probability~~ *is never a probability the observed result (or the null) is true*. Instead frequentists call it a likelihood.\n\n[https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)\n\nThere are extensive debates on how to describe the p-value so that laymen can understand it. The lack of precision about p-values is a common frustration among formally trained statisticians, and a regular cause for concern about non-statisticians using statistical models.\n\nThat said, there\'s a pretty good chance I misspoke about the true definition of a p-value in this comment. Hopefully a statistician with even closer ties to formal statistics will point it out.\n\n(Edited to continue my caveat in the first sentence of point 2 to the second sentence, because indeed, I was insufficiently precise.).', 'I have a different objection to that paragraph, which is that it throws in the words ""prior"" and ""posterior"" in a way that is (1) not useful, (2) introduces Bayesian terms that are not relevant, and (3) in the first instance, is not quite right.\n\nI deleted them; let\'s see if someone puts them back.\n\nIn general, I am a big fan of Wikipedia, but this paragraph shows some of the problems of writing by committee.', 'b pnl', 'The ""explanations"" about statistics in Wikipedia are pretty random. I wouldn\'t worry too much about it. The way it works is that whoever feels a need to write, puts down their opinions, and then others try to mold it into something coherent. This can eventually produce something valuable but at best it\'s a long slow process. \n\nThere are some extremely competent people working on WP, but competence is not any kind of requirement, only enthusiasm.', '[deleted]', '>Who is ""they"", exactly?\n\nThe Horde of Ecumenical Yodelers. THEY control the world.\n\nEven a lab mouse knows that.', '> No, it doesn\'t automatically imply that; random chance operates under both hypotheses.\n\nTo me, someone saying ""it happened by random chance alone"" absolutely means as opposed to being a real phenomena. In other words, it happened under Ho.', ""> but in frequentist statistics a p value is never a probability\n\nIt is definitely a probability. When the null is true, the test statistic has a specific distribution, and the p-value is the probability, under that distribution, of observing a test statistic greater than or equal to the observed value. Note that this interpretation doesn't depending on whether you're a frequentist or a Bayesian."", 'bot?', 'My question was whether my corrected sentence is correct. To me, by random chance alone is laymans terms for under the null hypothesis', 'As I already pointed out in my previous comment, not all nulls are ""nil""; that is, you can have ""real phenomena"" under both H0 and H1.', ""It is a probability under some specific conditions that we don't know to be satisfied. You cannot use the p-value to talk about a probability that something is real, because that's not what it is (but that's how people misuse it)."", '> It is definitely a probability. \n\nYeah I\'ve no idea what he\'s talking about. Anyway, is my ""corrected"" statement (at the bottom of OP) correct?', 'Ah, right. I put the ""p value is not a probability \\*unless\\* we know the true distribution"" in the sentence immediately prior to the one you quoted, but didn\'t repeat the caveat in the sentence you quoted. I\'ll change the quoted sentence to ""is never a probability the result (or the null) is true"".\n\nI can\'t tell you how many undergraduate and graduate lectures I\'ve sat through emphasizing that a p-value is not a probability of the truth of our coefficients/results. Or how many PhD statisticians I\'ve heard express frustration (anger, really) that people call it a probability. Or the number of times I\'ve had to catch myself (or be called out for) calling it a probability.', ""No, it's missing the mention of the null hypothesis."", ""See my comment above; I edited it to be more precise as a stand alone sentence.\n\nA p-value is not the probability of seeing the observed effects. It is only the probability of seeing the observed effects if the null is true. We never know if the null is true (and in fact are usually trying to show it may not be true). Therefore we never know if the p-value is a probability of the observed effects.\n\nPut into logic terms:\n\n* If A -> B.\n* So if (A) the null is true -> (B) the p-value is the probability of seeing the observed effect.\n* We never know if A is true. So we can't know (B) that the p-value is a valid probability.\n\nAnd yes, statisticians care about this, a lot."", "">  emphasizing that a p-value is not a probability of the truth of our coefficients/results\n\nThat's true. But it *is* a probability (always), just not of that."", 'That\'s what ""by random chance alone"" implies in layman terms. Random chance alone as opposed to being a real relationship. \n\n""You saw Mark at Disneyland, did you know he would be there? No, it was totally random!""', 'Agreed. Hence my edit.', ""If relying on implications would work you wouldn't need an explanation at all. You are trying to fix a misconception by relying on another misconception."", 'Im pretty darn sure no, it happened by random chance alone can only mean there is no significant relationship', 'Your last comment highlighted the problem of implied things - because you showed that you are implying the wrong thing.\n\nYour null hypothesis doesn\'t have to be ""no significant relationship"". It can be anything. It can be ""this drug is equal to this other drug"". It can be ""the probability of this process is as expected from theoretical predictions"". A different null hypothesis will lead to a different p-value, so it matters what you use.']"
[Question] Which is the right statistical test for a cost and benefit study?,"Hello fellow redditors,

I am collecting data about businesses and their costs and benefits that are associated with implementing sustainable practices in their business. 

I was wondering, which would be the right/best statistical test to use for a study of this type? 

Thank you in advance.",11b4ja6,Tradeoffer69,1677278887.0,0,0.33,"['I think this is bordering the field of decision analysis. David C Skinner has a nice introductory text. Hope you get something to guide your steps there.', 'You could test the hypothesis that the net benefit is more than a trillion dollars', 'Tests are consequences of hypothesis. No hypothesis no test. So there is ""not right test for the problem"" because you are not formulating any hypothesis. Of course ""tests"" are only one tool you could use for your problem.', 'Thank you ']"
"[C] As a statistics major interested in the health sector, should i go for a minor in public health or bionformatics?","Hi! I'm currently a Stats majour and i've been considering adding a minor. I'm interested in both working in the medical field or environmental field as a statistician or analyst. the issue is that i can't afford to go to grad school any time in the near future and plan to move from the east coast to west coast after graduation, so getting a job right out of school is important for me. at the same time, i know how some employers tend to be wary of new grads with only a bachelors in stats.

 i've already completed 4/9 classes required for the public health minor (did them in high school). the bio-info minor at my school is about 24 credits and has courses like coding in python, mathematical bio, and independent research (in genetics iirc) which have their own set of pre-reqs. i'd say the bii-info degree might be more useful; however, there's the emphasis on genetics which i'm not totally invested in. what are your thoughts?",11b3umb,COSMlCFREAK,1677277134.0,3,1.0,"['data analyst in medical and public health research here. I took bioinformatics in college and have never applied any of it. I found it very computational and specific. I think you cant go wrong with public health as it goes hand in hand with stat, especially if youre already interested in it!', 'Bioinfo is a pretty specialized field. Most bioinfo jobs would require ms or phd. If you dont want to become a bioinfomatician, just learn some python/coding skills on the side. Dont bother with the degree.', 'If you arent particularly interested in bioinformatics than go for the Public health minor. Public health is a wide field and there are never enough statisticians.', ""I'd recommend finding a professor doing research related to topics you're interested in. Research experience is a good substitute for job experience. If there's a professor in your department or a related one who does work that interests you, ask them if they can supervise an applied project or two with you. Those professors would also know directly or through their networks what minors could be helpful to you. They may also know people in the region you plan to move to who could help you land a job there.\n\nI'm biased as a PhD student, but I've seen this work out a few times for undergrads in my academic career. People always talk about networking in college, but they never emphasize that part of the networking is with the professors you take classes with. Those people can really kickstart your career for you if you're smart about finding the right people."", 'Also a DA in in healthcare, have my MS in Bioinformatics.  I agree and think that public health might suit OP better.  That said, I really enjoyed studying bioinformatics.  It may be a lot more wet lab oriented depending on where you go to school.', ""Thank you! I've been seriously considering and plan to speak with an advisor on a combination degree :)""]"
[Q] Admitted to Claremont Graduate University and University of Washington for MS Statistics - Difficulty Comparing and Choosing," 

Hello Statisticians,

I recently was admitted to both Claremont Graduate University MS Statistics & Machine Learning and the University of Washington MS Statistics - Advanced Methods and Data Analysis.

I am going into graduate school with the intention of academically preparing for potential pursuit of a Ph.D. in Statistics. If possible, I would prefer to work for 1-2 years before then proceeding to my Ph.D. Presently, I am having difficulty comparing the two programs in terms of academia preparation and in terms of career prospects.

I was wondering if anyone here has experience with Claremont Graduate University or the University of Washington?

I would love to hear about your thoughts!",11b0ka2,fanofhusky,1677268964.0,30,0.87,"[""Don't know much about Claremont. But I know UoW is a good school. Lots of interesting research and great lecture notes I've pulled from them. I have my MS but plan on submitting a PhD app there eventually."", ""Consider the lifestyle of each city too. In Seattle, you won't need a car to get around most of the time and there's a lot happening there to do in your free time"", 'As a UW alumnus I\'m biased, but it is an outstanding program and you won\'t lack for opportunities for pure and applied research (outside of the fact that an MS is very time constrained compared to a PhD).\n\nSome things to consider:\n\nUW ranks in the top 20 largest universities in the US, and is about 25x the student population of Claremont. \n\nUW consistently ranks among the top 5 universities for winning grant funding and in the top 2 for NIH funding (if interested in health/bio work).\n\nUW has a large number of micro grants, incubators (both technological and social), as well as mentorship and spinupp/startup programs across the university, some wholly within an academic/student body, some via private/public partnership. This is especially true of STEM departments which maintain close relationships with industry.\n\nBeyond The UW\'s statistics program (7th), the university holds high ranking colleges and programs across STEM.\n\nSeattle is home to multiple global leaders in ML research and implementation and not slowing down. Similarly, world class participants in health statistics, civil and industrial engineering, nautical engineering, aero-space, weather, physics, and others. Depending on your desired application area, the city alone would be why to come.\n\nTogether, the above means there is no comparison in opportunity of exposure to projects and access to resources and expertise.\n\nHowever, I would more consider the culture of the two institutions. I find The UW to be on the far end of requiring an ""entrepreneurial mindset"" in order to succeed. You won\'t likely be handheld into a lab, project, and thesis. Faculty are friendly (though time constrained), and you can approach them and ask all kinds of things, but you need to think for yourself about what you need and what you will ask for in any meeting. With few exceptions, your thesis work will be entirely your baby, and you will need to discover and plan out which skills you need to build, and then find classes, labs, and projects to participate in in order to build those skills and push your thesis and career forward.\n\nFor people who are comfortable doing their own leg work to build their support networks and project with little or no guidance, UW is extremely effective. But for people who want strong guidance, or to function in a team/system and leave with a degree, UW will leave them lost and feeling unsupported (as well as abused, since they certainly will be able to work in a lab, but a thesis won\'t come directly from that.) I believe the UW model is particularly harmful to people from backgrounds that rely on positive reinforcement, high levels of modelling ""right"" solutions, and high levels of ""superfluous"" engagement beyond the ""business at hand"", and/or who expect that their managers/mentors will reliably guide the student to the student\'s end goals (a degree) if the student serves the mentor faithfully. This means biasing towards higher rates of failure for many from disadvantaged backgrounds, many foreigners, and first generation students. I think many STEM programs at The UW are rather oblivious to how extreme they are in this regard, but they know they are not the norm. I\'d say their CS programs are especially wracked by these problems, and so have an abysmal rate of retaining female students. \n\nAlso, the size and complexity of UW, combined with being a relatively under resourced state school (STEM departments get lots of money, but core academic funding is actually quite tight), means that navigating UW bureaucracy can be, at best, annoying, and sometimes Sisyphean. They are always working to better things, but they are also regularly downsizing administrative staff support, meaning there are a lot of things you will have to figure out and execute/do, on your own that many other schools may gracefully take care of for you, or at least be able to explain and navigate you through. You need to budget bureaucracy into the timeline for everything you do at The UW.\n\nIn return for all of this you will receive massive amounts of freedom and, where you can convince people to work with you or share/give resources, opportunity to execute pretty much anything you can imagine. \n\nAlso, Seattle is a baller city, and Washington is one of the highest functioning states in the county with financial, human, and natural resources galore and sound (enough) leadership and plenty of scrappy (and non-scrappy) disruptors to keep the wheel from spinning off.', 'Is the UoW offer funded?', ""UW.\n\nin addition to UW having a significantly more reputable stats department: one of the things you will learn about your masters program is that it will be an entry point into the professional community. UW has strong ties to a rich tech community in and around seattle (microsoft, amazon, facebook, google, tableau...). CGU definitely can't compare on this front."", 'My wife and I went to Claremont Graduate University, and my tl;dr for you is go to UW.\n\nCGUs tuition is expensive for a private college. PhD students at automatically granted 50% tuition coverage (and could go up to 75% with applying to additional scholarships - not guaranteed), but that still put yearly cost around 20k with 3 years of coursework. I was admitted directly to the PhD program and left with about 65k in student loans.\n\nMy wife started in the MA program before converting to the PhD track for her degree, and loan coverage for MA students was abysmal. It was either 12.5% or 25%, which meant you were paying or pulling loans for about 30k-35k per year. My wife ended up with over 100k of student loans (since she also picked up an MBA along the way). I think it would have been better for both of us to have gone somewhere else, but hey, at least we met each other there!\n\nFor what its worth, the coursework and professors available are fantastic if you are interested in taking your skills to an applied setting over an academic one. When comparing to a friend in a UC, I generally had more coursework, but came out with a wider set of skills that likely helped me land internships and get my foot in the door with applied work. And I am now working at a large tech company as a result with all loans paid off. So it ended up being worth it, but I think any other path would have been just as effective.\n\nThis may have been particular to our department (which was not stats), but it also felt like faculty and staff made it as hard as possible to meet graduation and general progress requirements. There was also a lot of ego going around Hope the stats department is different! From what I understand, requirements were eased over the last few years or so, but I am not sure how much to be honest after we left, I basically never looked back.\n\nOn a different note The Claremont area is lovely, and gives off small town, safe vibes, and the downtown has lots of charm. Location is great on the outskirts of the greater LA area so you can work your way around heavily trafficked areas at times, but you will definitely need a vehicle to make the most of your location. Still, plenty of great dashes to make for day trips - Palm Springs, Orange County, LA itself, Glen Ivy Hotsprings, Big Bear, etc.\n\nGood luck whichever way you choose!', 'UWs stats department is ranked #7 on US News.', 'Depends what is important to you? I live 10 mins away from the Claremont colleges and I know McKenna is a great engineering school but apart from that all I know is they are all expensive private schools and there isnt much to do in the area but its only 30 miles to LA and the beach and mountains are an hr away.', 'Ill bet Claremont is unfunded.', ""What's this Claremont Uni? Never heard of it in my entire life!"", 'Claremont is a cool little town. If extracurricular activities are important to you, youre an hour from numerous great beaches, 30 min to LA etc. mountains even closer.', 'Go to whatever is cheaper. Generally no one cares where you go to school.', ""I didn't expect this threat to blow up so much.\n\nAll of you have said so many good things about UW that I'm in a little bit of a dilemma here.\n\nUW comes off as the more high powered option by far but they didn't fund me. I'm also not a a state resident of Washington. Meanwhile Claremont funded me so its actually cheaper to go to Claremont.  \n\n\nDo you UW folks think its worth it  to take on some debt to go for UW?"", 'Go Huskies', 'UW', 'Hi, Im actually an incoming MS student at UW Stats program (deferred entry to this yr), and based on what Ive learned from current students I think the curriculum at UW has a good mix of rigorous theory and applied stuffs. Around half tend to go to phd and the other half enter industry, and while I havent experienced the school yet I think the UW name value in stats will be a plus if u are considering phd afterwards as well.', 'Go to UW. Amazing connections to tech and reputable stats department ', 'If writing to faculty at The UW, Please note, it is absolutely not UoW. It is UW, or The UW.', 'Isnt UW biostatistics also equally as highly ranked?', 'I hope MS arent this open-ended. Phd, sure. But masters has no structure?', 'Thank you for posting this --- I think UW statistics is willfully blind about this: It has been brought to their attention multiple times, but it is much easier to work to not see it, than to work to rectify it (and frankly I think it would be tremendously hard to rectify without decreasing the program size).\n\nI would argue that biostat at UW engages differently with their students (though I know that was not in the original question!).', 'While OP didn\'t reply, I\'d venture to say, ""absolutely not"". If they were a PhD student there would be many routes, including paying for their first year while expecting to get a Research Assistant or Teaching Assistant position around year 2 (which, more often than not, include a modest pay, health benefits, and full tuition).', 'You raise a great point on private Based on the most detailed responses of both schools, the global optimum answer might be apply to somewhere else?', 'what an inspiring story, I can only imagine what you had to go thru with PhD studies, loans and others. But i am genuinely happy for you, to land jobs, pay off debts and most importantly met the love of your life. You are a winner. Congrats.', 'Solid bet.', ""it's a graduate school affiliated with the better known claremont colleges. I think it's most reputable program is theological. I went to one of the claremont colleges and am a stats professional myself: I didn't even know CGU had a stats program."", 'It is in Claremont and a member of the Claremont colleges', '[deleted]', 'Came here to say this.  Just make sure and build your social connections in which ever school you go to, and your outcomes can be good at either.', 'What is the difference in cost between the two schools with the offer situations? If it is a small amount, I would still vote for UW based on name recognition, strength of program, and likely cohort environment. That said, if it is a huge difference, Claremont is fine, and you will get some name recognition from its attachment to the more known undergrad schools which will help with your first job or internships.\n\nI can vouch for my classmates as well, as a CGU alumni When I look at LinkedIn, the majority of my classmates ended up at well-known, top of industry locations, successfully started their own business, or landed teaching jobs. Please note - I am coming out of the psychology and evaluation department, so YMMV for stats.\n\nI enjoyed my time at CGU, and it worked out for me, but for the most part these days, I still recommend folks look at other places first.', 'Oh that sounds like UW provides a good peer environment for both applied and theoretical students.', ""Not equally (both different N and different rank in the N). UW's biostatistics program is ranked higher nationally (3 by USNews) than their statistics program (7 by USNews).\n\nThe programs are also rather different and in different Colleges/Schools (the largest academic units in The UW). The Department of Biostatistics is homed in the School of Public Health. the Department of Statistics is homed in the College of Arts and Sciences"", ""All degrees have structure. So, it is unclear why and what you are wanting to know. But you will note that the full time masters is about 1/3 electives. You can find the details for the department's 2 master's programs here:\n\nhttps://stat.uw.edu/academics/graduate/programs/masters\n\nhttps://stat.uw.edu/academics/graduate/programs/fulltimemasters\n\nBecause these degrees do not have a thesis component, the full time degree is expected to end a quarter early."", 'Yeah quite frankly i find it quite hard to understand what kind of people are able to take part in these MS programs. Like you gotta have been a working professional and are going back to school cause theres no other way you could pay for it.', ""Employers over degrees. Maybe it mattered for your first or even second job, but after that i think it's like bringing up your high school gpa or SAT score"", 'Maybe but I stand by my advice to not go into much debt for education. \n\nI went to shit state schools for free and have made over $100,000 for years.', 'In terms of just course tuition, UW is about $29,000 more expensive than CGU over the course of 2 years. So about $14,500 more expensive per school year.', 'bullcrow98, would you be ok if I dm you?', 'Gotcha. Yeah I did figure they were both in the top 10 or something like that', ""I'm unclear if you are asking to better understand how people afford college, or merely noting that college is expensive. But your assertion that these the UW's MS statistics programs are for working professionals going back to school is inaccurate and does not reflect who enrolls in them."", '[deleted]', 'In the broader picture of all this, is that a non-negligible amount to you (and also compared to the target position salary you hope to achieve)?\n\nIn all honesty, either school is probably fine for your end goal, but the journey may just look different between what a public and private school will offer.\n\nPerhaps one more thing to keep in mind with your plan is that I assume when you return for your PhD, UW will be more likely to fully fund you and provide a stipend. CGU may not offer you much more coverage which may even out the costs toward the end.', 'No prob, go ahead!', 'Im saying how expensive graduate school is. Like who would he one to pay that much for an MS', ""I definitely agree academia values the name of the school you went to, as an MS holder working in academia\n\nBut obv the skills you get in your program are much more important than the name. I would look at recent graduates of a program and see where they landed - that's probably the best indicator for a master's program imo"", ""Presumably because they want to learn the core material but don't intend to become a practicing academic, and so wouldn't need the PhD.""]"
[Q] Two confidence intervals. One is narrower than the other implies it follows a certain distribution ?,"Okay so, I have two 95% confidence intervals. One is obtained from the bootstrap distribution using quantile function in R with a confidence interval of ( 81000, 100000) and another is based on the asymptotic theory where I used the approximation of the estimator to be normally distributed since the sample size was large and the resulting confidence interval was much narrower like (89905,90094). Does this imply the bootstrap distribution is a normal distribution because the 95% asymptotic CI is much narrower than the 95% CI of the bootstrap distribution?",11b0frj,Masterkuze,1677268660.0,2,0.75,"['No. Parametric models, because of assumptions, have less uncertainty. Whether the assumptions hold or not is another issue.', 'I dont see why that would follow.']"
[E] Gradient Boosting with Regression Trees,"Hi guys,

I have made a video on YouTube [here](https://youtu.be/lOwsMpdjxog) where I explain what gradient boosting is and how it works.

I hope it may be of use to some of you out there. As always, feedback is more than welcomed! :)",11ayopm,Personal-Trainer-541,1677264267.0,3,1.0,[]
[Q] How does one directly convert risk difference to risk ratio (also relative risk),I'm analysing a study in the medical field and struggling hard with this.,11avehv,usernameflavor,1677256034.0,1,1.0,"[""You can't, unless you know some additional information.  If you know both individual risks, then that is easy.  If you know one measure of association (RR, RD, OR) and one of the individual risks, then you can convert.  But they are nonlinear functionals of two variables, so unless you know something else, you cannot do it.\n\nTo see this, note that if p0=.2 and p1=.4, then the RD = .2 and the RR is 2.  If p0=.4 and p1=.8, then RD = .4 and RR = 2.  So the same RR gives you different RD."", ""There's no magic formula and you can't do it directly unless you want to make it more complicated than necessary. To calculate RR you need the proportions on each arm, which might be reported directly, or indirectly, or not at all.""]"
[Q] Bayesian Networks,"Hey,


I am currently working on a master thesis in
which I want to estimate heightmap values of soil dumped from an excavator
into the dumping region inside of a truck. I would like to predict the
height map value Xij(t) of the soil in a given 1cmx1cm cell ij inside this
dumping region, i.e. estimate the heightmap of soil inside the dumping
region. I decided to use a bayesian Network approach.



To do this, we measured 5 variables: Y1-Y5, which resemble variables such
as the dumping point, volume in bucket, soil distribution in bucket and
the velocity with which the bucket is opened. All these variables Y are
continuous and also the heightmap values Xij are continuous.



Moreover, we have observed values Oij which are values estimated from a
sensor on the excavator and we also measured the groundtruth data after
each dumping, let us call them Gij.



So, I handcrafted a probabilistical graphical model in which I have an
edge between Y1,,Y5 to Xij, an edge from Xij to Oij, and also a time
dependency, i.e. the estimated value of soil height in a cell ij at time
point Xij(t-1) has an edge to Xij(t).



I have attached a visualization of the graphical model I handcrafted.

Now, I want to learn the conditional probability distributions between all
these variables and my heightmap values X. Since all my variables are
continuous, I would pick a continous distribution, e.g. Gaussian, and
learn the parameters of this cpd via mle. In order to do so, I would use
the groundtruth data G I recorded.

Then, since the next step would be to learn a posterior distribution and
P(X^t+1 given Y^t+1, O^t+1, X^t) is hard to estimate cause in the
denominator we have to build the integral over X^t+1, I thought about
using approximate methods such as MCMC or VI to generate predictions and
then measure goodness of fit via rmse.



Does this approach make sense from your perspective?



Best regards,
Ne",11aucxr,Upbeat-Illustrator44,1677253359.0,1,1.0,"['Are regression tasks usually performed via Bayesian networks?', ""I dunno -- this seems overly complex, to the extent that it's going to be very hard to get to any results. Also, I don't see anything at all about the physics of the problem -- building in as much as you can about the actual mechanics of the situation is going to greatly constrain the possible results, and therefore is going to be a huge win computationally speaking.\n\nMy advice is to start out with a simplified model of dumping a scoop of incompressible granules on a flat surface. What is the shape of that pile? There has probably been work on that before. Now assume that the location is not fixed but distributed according to some distribution. How do the granules pile up after a number of scoops have been dumped? How does it change the picture to assume the scoops are not the same size but, again, distributed according to a distribution? \n\nAdditional physical considerations. What if the material is compressible? What if the granules are sticky? Etc.\n\nBy the way, if you work from a physical model, you will find there are free parameters which correspond to things in your world. E.g. the angle of repose of loose material. This is a very good situation to be in, because it means, above all, that other people will understand what's going on. Good luck and have fun.""]"
Intuition about standard errors and serial autocorrelation [Q],"I was reading a paper recently that said something along the lines of: ""if a panel data set demonstrates serial positive autocorrelation, unadjusted standard errors will be too small. This is intuitive because the model believes there is more information than there really is.""

This was in the context of DID regressions. Could you help explain this intuition for me?",11arzvu,onemandave1,1677247053.0,1,1.0,"['Autocorrelation creates strong positive (negative) effect sizes for a series of 30+ observations. At some point the direction flips to negative (or positive). And then it eventually flips again. The flips never cancel each other out, and the resulting short term and long term trends have nothing to do with x influencing y in any useful manner. \n\nSo you end up with spurious effects. They happen to have very small standard errors, because the artificial trend vastly outweighs the random fluctuations in the data.', 'Thanks so much, this is a good answer. So would the fact that negative serial autocorrelation leading to too high standard errprs be that any trends between X and y and obscured by the negative relationship of the errors?']"
[Q] How to check randomness of a binary pattern?,"I have a simple problem, I have a strong of 1/0 that's 100 bits long. I expect the pattern to be random, and NOT 10101010etc (or 100100100 etc). Basically shouldn't have any noticeable random.  Expecting something random like 1001100001001101110010.


My question is , how can ""calculate"" the randomness of such string, with a known length of string, and known # of transitions (ie. How many transitions from 1>0 or 0>1). Just looking for a quick and simple idea and threshold.",11arw5e,LibertyState,1677246748.0,22,0.9,"['There is an amazing thread on how to do this here https://stats.stackexchange.com/questions/574333/how-can-i-determine-which-of-two-sequences-of-coin-flips-is-real-and-which-is-fa/574359#574359', ""Under pure randomness there's a 0.5 chance of transitioning in each spot and all subsequent spots are independent. There are 99 spots where a transition can occur so the number of transitions follows a binomial distribution with n=99, p=0.5. So for example random sequences will have between 40-60 transitions 95% of the time."", 'Look into Kolmogorov complexity and algorithm probability. As others have pointed out any binary string is as likely as any other when viewed  as a random variable, but when viewed as an object its algorithmic probability can differ', '>\tMy question is , how can calculate the randomness of such string, with a known length of string, and known # of transitions (ie. How many transitions from 1>0 or 0>1).\n\nIs the following string random:  0101010101010101010101 ? Likely not. To measure randomness youd have to check distribution of 0 vs 1 also 00 vs 01 vs 10 vs 11 and so on for longer lengths. \n\n>\tJust looking for a quick and simple idea and threshold.\n\nIf you are programming a simple approximation is to apply the best  string compression function you have available and measure compression ratio. For **most** strings the compression ratio will be your best measure of randomness. There are exceptions but they likely do not matter for your use case.\n\nTo figure out a threshold generate a large sample of random strings and see how well they compress and pick your threshold empiracly based on how many truly random strings you want to reject. This will depend on the exact compression algorithm - compression algorithms are designed and optimized to efficiently spot patterns so why not use them for that purpose?', '**Individual strings cannot be random**\n\nThe *process* that generates strings can be random, but given a single string, it is impossible to know whether it was generated by a random or deterministic process. \n\nThink of a shuffled deck of playing cards. It\'s a common fun fact that, after shuffling, the particular combinations of cards produced by your shuffle has ever existed. The meaningless permutation you just produced by shuffling is no more or less random than new deck order (A,2,3,...). We just ascribe significance to some permutations and lump all the others into the ""random"" category.\n\nSimilarly, if you have a process that randomly generates 100 bit strings by pulling from the space of 2^100 possibilities with maximum entropy, then seeing 0101010...10 is no more or less surprising than seeing 110010111..1. It\'s just that the first string looks more structured than the second one. \n\nNow, maybe you\'re not interested in whether a given string was randomly chosen from the space of strings, but instead, the distribution of 0s and 1s in your string is random or not. If by ""random"" you mean ""maximum entropy"", then it may be enough to test whether the entropy of the string is 1 bit or not. Compute H(string), and then generate an ensemble of null strings where you know that P(1) = P(0) = 1/2 and test your empirical entropy against the null distribution. If you don\'t hit your pre-determined significance level, then your string is indistinguishable from the strings produced by a memoryless, maximum entropy process. \n\nBut what if the process that produced your string is not memoryless? Maybe the probability of seeing a 1 at the ith place changes depending on whether there is a 1 or a 0 in the i-1th place. In that case, you want to compute the [conditional entropy rate](https://en.wikipedia.org/wiki/Entropy_rate) of your string and compare *that* to a maximum-entropy null model. Conditional entropy rate is pretty simple, but it does require fixing a time horizon, which can be a tricky thing to optimize. \n\nYou can also approximate the entropy rate by using the Lempel-Ziv compressibility. Again, compute an empirical value, and compare to a distribution of nulls (in this case, a distribution of shuffled nulls).', 'You probably want to look at using a (Wald-Wolfowitz) runs test.', '1. Pattern is ill-defined. There\'s a very large space of potential ""patterns"". What counts as a pattern depends on what you might choose to regard as a pattern.\n\n   What sort of patterns do you care about?\n\n   What about one that starts off 1010101 ... for a while, then breaks that pattern for a few digits, then goes back to ...101010? Where do you draw the line? What if the pattern at the end was different from the one at the start? How long does it take for a pattern to exist before you care about the pattern being a pattern?\n\n   What do you do about something that doesn\'t look like a pattern at first glance but ... then someone notices there *is* a subtle pattern, does that count? e.g. what if the message was 8-bit ASCII for ""HIMOMSEND$""? with some leading 0\'s? What if it encoded digits of pi in some way? What if arranging the 0\'s and 1\'s into a 10x10 grid made an image?  (i.e. is it that the pattern is obvious that\'s the problem? Or is any kind of pattern, even if it\'s subtle, to be ruled out?)\n\n2. In a purely random sequence, patterns will certainly occur. What fraction of the 2^(100) possible sequences are you prepared to remove as \'patterned""?', '[deleted]', 'Maybe something simple like kalman filtering. \n\nIn your specific case you can even use a bernoulli process with a beta prior (think conjugate priors). So you can measure the randomness with the moments of your beta after each step', 'That was a great read, I wonder if OP will realize that their expectations of randomness is the opposite of actual randomness.', 'Written by you of course, so it must be amazing', ""This wouldn't work for nonrandom sequences like 001100110011, though. An ACF plot might be a good idea that follows the same line of thinking but can account for longer patterns"", 'The compression is great!\n\n&#x200B;\n\nAlso, you can do a runs test, and loo at the autocorrelation function.', 'Thanks, I was looking for something like this.. but I am still little confused (I am not a stats/math guy). For a string of say 100 1\'s or 0\'s, how do I calculate the entropy? Does this mean we have 100 events, and probability is 0.5 for each, and use the formula from the site to calculate entropy?\n\nAnd after that, how do I know what\'s a ""good"" entropy range?', 'Yea oops, the link is to my answer.  But all the other answers are equally if not more cool than mine.', ""True, but at the end OP specifies needing a method that takes the length of the string and the number of transitions. For autocorrelation (and some of the other proposed methods) we'd need to know the entire string."", '>\tAlso, you can do a runs test, and loo at the autocorrelation function.\n\nWould runs test or autocorrelation give different results than a compression ratio test? A reasonable compression algorithm I think would detect runs and autocorrelations and all manner of patterns that are possible. \n\nA tunable parameter for compression is how large a window or look back buffer is used for pattern detection. With larger windows larger patterns can be spotted (if any such patterns exist) but the algorithms take longer to process the data.', 'I think that the compression test is great (I had never thought of that).  The runs and ACF would give you some more specific feedback.']"
[Q] Using marignal means to better understand three-way interactions and test specific hypotheses,"My question relates to the use of the emmeans package in R to better understand a three-way interaction in my linear mixed model.

 I have structured the dataset in such a way that each patient has four repeated measurements (t=0, t=0.5, t=1, and t=2 years) for each of the six domains of the BASDAI, resulting in a total of 24 observations per patient. 

 The variables in my model include:

* ""basdai"": the outcome variable, which represents a treatment response.
* ""gender"": the independent variable of interest
* ""time"": a covariate indicating the time of the measurement (t=0, t=0.5, t=1, or t=2 years).
* ""domain"": a variable indicating the domain of the BASDAI (1-6).

Research question: Does the magnitude of the sex differences over time vary in the components of the BASDAI after treatment?

To answer this question, I built the following model in R:

    library(lme4)
    library(lmerTest)
    mixed_final_model = lmer(basdai ~ 1 + gender + time + domain + gender*time + gender*domain + time*domain + gender*time*domain + (1 | ID) + (1| country), data = dat, REML = F, control=lmerControl(optimizer=""bobyqa""))

Now I am trying to better understand the results of the three-way interaction (gender\*time\*domain) by using marginal means of the emmeans package. Thus far, I have this code:

    emm_model5  <- emmeans(mixed_final_model, ~gender*time*domain, ref_level = c(domain = 1))
    comparison_sex_differences_domain <- contrast(emm_model5, ""pairwise"" , by = c(""domain"", ""time""))
    comparison_sex_differences_domain

This gives me the following [output](https://ibb.co/fxPpv9Z) for the first 3 domains (the other 3 not shown).  

I think the null hypothesis being tested in the contrast function is whether the mean difference between males and females at different time points within each domain is significantly different from zero. However, what I am interested in is whether the magnitude of the sex differences in domain 2 is different from that in domain 1 at each time point. For instance, I want to know if the mean difference between males and females at t=0.5 in domain 2 is significantly different from that at t=0.5 in domain 1. Similarly, I want to compare the mean difference at each time point in all other domains with that in domain 1. 

I hope I was able to clarify myself and that someone can explain to me how to test these hypothesis.",11arou1,pashtun92,1677246163.0,5,1.0,"['To get at the three-way interaction youll need to run an additional contrast.  Youd do this the same way as your two-way contrast, but by time alone and using your two-way contrast as the input.', 'Okay thanks so I use emm_model5 as parameter of the contrast and by = time?\n\nWill try it out on monday. Could you explain why this would work? Pairwise comparison? Or would trt vs. Ctrl make more sense (to compare with domain 1?). Also, I thought mine was a three-way-interaction contrast because I included that in the first emmeans function as a parameter, but you are saying it is a two way contrast. Please explain.']"
"[Question] I see some people calling the analysis of confidence intervals (as opposed to p-values) ""Bayesian Statistics"": why?",What is the relation between confidence intervals and Bayes' Law?,11a85us,1b992b,1677184949.0,4,0.58,"['Who do you see calling the analysis of confidence intervals Bayesian statistics?', ""Are you sure they're use *confidence* intervals and not *credible* intervals?"", 'You might be seeing credible intervals (Bayesian interval for parameters) or even prediction intervals (which might arise in either paradigm) rather than confidence  intervals. Or the person might be confused.\n\nCan you point to an example?', ""I would say that the defining characteristic of Bayesian statistics is that the result is a joint posterior distribution of the parameters. From that, you can generate point estimates for the parameters, credible intervals for the parameters, and predictive distributions of new data from the same generating process, among other things.\n\nBayes's Theorem and Bayes's Rule are two ways to write a mathematical fact. Using either of them does not imply that you are doing Bayesian statistics."", 'Sometimes credible intervals are called Bayesian confidence intervals. One of the unfortunate elements of probability and statistics is that the same word or phrase can be used in both Bayesian and Frequentist methods and have radically different meanings. There has been a conscious attempt to use different naming conventions to prevent exactly this confusion. \n\nYou can use Bayesian methods to analyze Frequentist confidence intervals. For example, Frequentist confidence intervals can sit in locations known to be impossible under the likelihood. As long as that happens less than the chosen percentage of time it is considered irrelevant. Likewise, credible intervals can be analyzed for coverage frequency using Frequentist tools. They can have poor coverage despite having a high posterior probability. \n\nExcept in certain elementary cases, they are not linked. Credible intervals can be disjoint sets and asymmetrical. Any interval that sums to the chosen value is a alpha percent credible interval. It is true that only a handful are used in practice because they have known optimality properties. \n\nA confidence interval is *any* function of that data, that upon infinite repetition, covers the parameter at *least* alpha percent of the time, except exact intervals. There are an infinite number of functions that meets that criterion in many general cases. Only a handful get used because of their known optimality properties. \n\nCredible intervals and confidence intervals perform opposite functions. A confidence interval expands the amount of information from a data set when compared to a point estimate. A credible interval reduces the amount of information compared to the full posterior. \n\nBecause a confidence interval is *any* function of the data that meets coverage requirements, there is generally no relation to Bayes law. And since Frequentist intervals do not conform to the Likelihood principle, except by inheritance when they match each other, they are grounded in radically differing paradigms. \n\nConsider a confidence interval of birth weight of [-1,8] on a small sample and a credible interval of [6,9] with a strongly informative prior from prior years births. The Bayesian prior would have zero mass from negative infinity to zero. It would discount unusual weights as being akin to statistical runs because the prior has little mass there. The Frequentist interval will minimize the maximum risk created by getting a non-representative sample but can produce results impossible under some prior information.', ""Confidence intervals and Bayes' Law are both used to quantify uncertainty in statistical inference, but they are different concepts. Confidence intervals provide a range of plausible values for a population parameter based on a sample statistic, while Bayes' Law updates beliefs about the probability of a hypothesis given data. In Bayesian statistics, the posterior distribution obtained using Bayes' Law can be used to construct a credible interval, similar to a confidence interval."", 'You said ""confidence intervals"" but did you mean ""Bayes factors""?', 'Its crdible interval in Bayesian stat \nIts a plus to add to your bayes factor (10) \nYour credible interval can be based on different calcul as the posterior distribution on Highest Density Interval (HDI) or the Equal-tailed Interval (ETI) \n\nAnd difference with confidence interval is the % too. In Bayesian we take 89% of Credible interval instead of 95% confidence interval in classical inference stat', '(Frequentist) confidence intervals and (Bayesian) confidence intervals coincide under certain model configurations, most notably [for Gaussian linear regression with flat priors](https://people.sc.fsu.edu/~mye/pdf/paper31.pdf). Confidence intervals are often interpreted *as if* they were Bayesian credible intervals, which is generally wrong, but sometimes accidentally correct due to the aforementioned numerical equivalence in special cases.', 'Asking the real questions here. I have never seen someone refer to analysis of confidence intervals as Bayesian...', '[deleted]', 'That\'s probably what he meant. But most credible intervals are just the equivalent of confidence intervals, which are in fact a very frequentist concept. So calling the analysis of those credible intervals ""Bayesian statistics"" would still be weird to me as it involves using Bayesian estimation for a frequentist purpose.', '[deleted]', 'The default value in arviz is 94% for the posterior and I every time I make charts I know someone reads them and thinks I did a typo and meant 95%.', ""I've seen that where it's called a confidence interval in the main text and a credible interval in the footnote. Often this is because then you don't need to explain what a credible interval is. And there's often so many confidence intervals, with many of them being approximate, that it kinda makes sense to call all interval estimates for parameter values confidence intervals since the supposed properties of confidence intervals may not even hold for them exactly anyway, and one can often show that Bayesian intervals work well as frequentist intervals too.a"", 'That strikes me as a strange perspective. If you\'re representing the parameters as having a distribution, combining a prior and a likelihood and basing inference off a posterior, it\'s Bayesian. Sure, a credible interval is a Bayesian analogue to a confidence interval in the sense that it\'s an interval estimate of some quantity. But I wouldn\'t consider ""interval estimation"" to be a particularly frequentist concept. It\'s a statistical concept that both the frequentist and Bayesian approaches employ.\n\nMaybe I\'m not fully understanding how your perspective. What are things that you think are *not* weird to call Bayesian?', ""Indeed outside McElreath it's *nowhere near* conventional."", 'The way I see it the interval is a threshold on the distribution of a parameter. We use a confidence level to basically ""mark"" the region of outliers in that distribution. Setting a 95% confidence level in order to set intervals or apply hypothesis tests is not philosophically a Bayesian practice as far as I know.', 'Sorry if I\'m being dense, but that doesn\'t really help me to understand your perspective. Why is summarizing the posterior distribution in terms of, say, a point estimate and an interval not philosophically Bayesian? \n\nYou\'ve said one or two things that you think are weird to call Bayesian, what are things that you think are *not* weird to call Bayesian? What constitutes ""Bayesian statistics"" to you? Suppose you perform a Bayesian analysis for a customer. What types of results would you be providing? What are you doing with the posterior distribution such that intervals or point estimates are weird?\n\nTo me, obtaining interval or point estimates from a posterior distribution is not at all inconsistent with a Bayesian philosophy. I just flipped through the Bayesian books on my shelf (BDA3 by Gelman et al, the Multilevel Modeling book by Gelman and Hill, Hoff\'s book, McElreath\'s Statistical Rethinking, Banerjee et al\'s Spatial book), and they all obtain point and interval summaries of the posterior for summarizing results.', ""I didn't say anything about point estimates. All I'm saying is that using a confidence level to get a credibility region beats the objective of Bayesian analysis since confidence is a very frequentist concept. Gelman is a proponent of that view as well afaik. \n\nThere are ways to compute uncertainty/credibility regions which incorporate prior knowledge without making any assumption on the acceptable uncertainty levels. So I would say it all depends on how you're getting the intervals."", 'Yes, I know that you are saying that credible intervals are somehow counter to Bayesian philosophy (now it\'s apparently certain kinds?). Your comments are very vague, however, so it\'s not clear why you think this, so I\'m trying to get you to explain why.\n\nAs I see it, a credible interval is just one way of summarizing a posterior. So to me saying credible intervals run counter to Bayesian philosophy would imply the same about point estimates derived from the posterior.\n\n\nI don\'t know if you mean something particular by ""*using a confidence level to get a credibility region*"". The only way that makes sense to me is to mean selecting some coverage level, e.g. 95%. But that wouldn\'t be using a frequentist concept in any meaningful sense.\n\nI looked in two different Gelman books and saw Bayesian posterior summaries including credible intervals in both.', '>Yes, I know that you are saying that credible intervals are somehow counter to Bayesian philosophy (now it\'s apparently certain kinds?). \n\nYep, only certain kinds. Maybe I wasn\'t clear enough in my first comment but I\'ve elaborated since.\n\n>Your comments are very vague, however, so it\'s not clear why you think this, so I\'m trying to get you to explain why.\n\nThey\'re not vague, they\'re brief. What I\'m suggesting is crystal clear but I don\'t want to get into a huge heated discussion that\'s why I\'m using a language which implies that ""I may be wrong"" though I guess some egos are too fragile anyway. \n\n>As I see it, a credible interval is just one way of summarizing a posterior. So to me saying credible intervals run counter to Bayesian philosophy would imply the same about point estimates derived from the posterior.\n\nThe way I see it, an interval using confidence levels is no more a summary but a decision. As I made it specific already, methods of finding intervals (I prefer to use the word ""region"") that incorporate prior knowledge are much closer to the Bayesian mindset for obvious reasons. None of this has nothing to do with point estimates.\n\n>I don\'t know if you mean something particular by ""using a confidence level to get a credibility region"". The only way that makes sense to me is to mean selecting some coverage level, e.g. 95%. But that wouldn\'t be using a frequentist concept in any meaningful sense.\n\nA 95% confidence level or a credibility threshold on a posterior distribution is against the Bayesian mindset for at least two reasons that come to mind: 1. the 5% threshold is historically a practice started in classical/frequentist analysis, 2. Setting a threshold of any kind is against the probabilistic/Bayesian mindset.\n\n>I looked in two different Gelman books and saw Bayesian posterior summaries including credible intervals in both.\n\nhttps://www.bmj.com/content/366/bmj.l5381', ""> for at least two reasons that come to mind: 1. the 5% threshold is historically a practice started in classical/frequentist analysis, 2. Setting a threshold of any kind is against the probabilistic/Bayesian mindset.\n\nFor (1), are you saying that if I reported a 95% credible region, that's taking too much from frequentist philosophy? But some other probability that is not typically associated with a frequentist confidence interval, say 80%, would not carry the same baggage?\n\nThen (2) I'm not sure how that works in practice. How would you report a credible interval without selecting some probability value?\n\nFWIW, I'm not seeing this has a heated debate or battle of egos. You're just expressing a perspective that I'm either not understanding, have never encountered, or both. And whichever it is, I'd like to understand it. Apologies if my comments have come off otherwise.""]"
[Q] Techniques for spotting patterns in a sequence of events,"Not too familiar with statistics beyond a few requisite undergrad courses from years back, so I'm trying to figure out what techniques/terms I should be googling (if there are any) in order to achieve what I'm trying to do. If there are any statistics/programming libraries available to help me tackle this issue, I'd love links as well (mostly familiar with Python and to a lesser extent MATLAB, not too familiar with R).

I have a pseudorandom number generator that spits out integers from 1 to 5, one at a time. I'm free to generate as many numbers from it as I want to get as large of a sample size as needed. Numbers are recorded in the order I fetch them from the generator, that forms my sequence of numbers. I have no access to the code behind the number generation.

The suspicion is that this number generator is not truly random.

I'm trying to analyze this sequence of numbers I get from the generator and possibly identify any patterns that may exist in the sequence. Things like the following:

1. If there is a maximum number of times a number disappears from the sequence before it appears again. E.g. there is never a case where a sub-sequence (of say 20 numbers) does not contain a 3 in it at some point.
1. If there is consistently some number of events in non-overlapping consecutive sub-sequences (of the same length, if that makes it easier) where the number of occurrences of each number is equal. E.g. for non-overlapping consecutive sub-sequences (of say length 45) each number is always guaranteed to occur exactly 9 times.
1. If there is an abnormally high chance that the same number occurs consecutively. E.g. the same number appears three times in a row at abnormally high rates.

I'd like advice on how to determine whether results I get from determining the above are statistically significant, and how many samples I would need to take to determine that with a reasonable degree of confidence.

---

My initial thoughts for how to approach this were to just dump the sequence into a list, and repeatedly iterate through the list with a bunch of counters and keep track of all of the things I mentioned above. But this seems pretty brute-forcey, so I'm wondering if there's a better way to approach this.",11a8025,helpimanengundergrad,1677184551.0,1,1.0,"['Statistically, there is no real way to test a sequence for ""true randomness"", as all patterns are uniformly equiprobable in an RNG. You can test specific kinds of randomness (e.g. frequency of each digit, or of all subsequences, etc) but any individual test would always have specific kinds of non-randomness that it can\'t detect.\n\nI would suggest the most powerful method would be to fight black-box with black-box, train a model to predict the next digit based on previous digits. If the model can perform better than random, then you have good evidence of some non-random pattern (although we couldn\'t say *what kind* of non-random).', ""All patterns are possible, some are more likely to occur by chance than others.  What's the reason for checking the randomness of the random number generator?  Would you learn more if you went to the source of the algorithm and code for that generator?  Whoever wrote it has had to withstand critique of its quality.  And, if you're writing your own, you'll get to learn about what those critics are looking for."", 'Gotcha, I\'ll definitely consider that approach! Though when evaluating the trained model, what would be the right way to measure ""better performance"" in this case?', ""The generator is a black box. The goal is to test if the generator is truly random or not.\n\nI know all patterns are possible with a true random number generator. I'm trying to verify if the one I'm working with is one or not. If it isn't a TRNG, I know it's likely to follow one or some of the patterns I listed."", 'Accuracy > 1/N, for N possible characters (i.e. 10% if the RNG is giving digits 0-9).\n\nIf you are really able to generate a large number of examples for testing, you should have a good idea of the true accuracy and whether or not it converges to 1/N.', ""That just tests for an equal distribution among all the numbers being generated right? If the model somehow ends up approximating something like the second case I posted, that would technically end up having the same accuracy as a true random number generator wouldn't it?"", '> That just tests for an equal distribution among all the numbers being generated right? \n\nNo, it tests whether a model is able to learn any meaningful insights about the sequence. Theoretically, if such a pattern exists, a well-specified model could identify it and learn it, yielding better-than-random accuracy.\n\nOf course, ""well-specified"" is doing the heavy lifting here, as the model is only as strong as the limitations you bake into it (e.g. max_sequence_length or other parameters).']"
[Q] Likelihood of new data after Bayesian regression,"I have a variable Y, that is determined by some predictors X\_i. After establishing the priors, posteriors, and the parameters that determine the effect of the predictors on the parameters of Y, I did a bayesian regression with historical data to get a distribution for all the parameters. Now, I want to know the likelihood of a new observation, given my model. This new observation is of both the predictors and the Y. 

I have done likelihoods for deterministic parameters, but never on a bayesian regression where the parameters are random variables. I am unsure if I can actually even compute that likelihood. My idea was to sample from the parameters, and for each sampling, along with the new measure, obtain a sample of Y based on the model and the new measurement. Then with a big enough sample, estimate the distribution of this Y|model,new X and see how my new measurement of Y fits in there. It makes sensd to me that this must work, but probably people here have already dealt with something similar and can give me insght. Any comments about the apporach, how you would do it or resources I can look into that deal with this are appreaciated :)",11a570b,Chus717,1677177697.0,1,0.67,"['The key concept you need is posterior predictive distribution.  Most books on bayesian inference will describe it.  How you actually compute it depends on the model and the approach to inference.  It sounds from your post that you can sample from the posterior so you can also use that to generate samples from the posterior predictive distribution with your new predictors.  Give that sample you can estimate a density if you want to, there are lots of ways to do this.', "">  this Y|model,new X and see how my new measurement of Y fits in there.\n\nIsn't this just the predictive posterior distribution? This is easy to calculate, especially if your priors are gaussian."", 'I am unfamiliar with that term sadly. Well how do I calculate it? I am specially confused on conditioning on distributions of parameters instead of actual parameters', 'See this book, specifically the section on Bayesian linear regression:\n\nhttps://mml-book.github.io/book/mml-book.pdf', 'Will do, thanks!']"
"Statistical Learning, Bayesian Statistics, or Time Series Forecasting [Q]","Hello, I was looking at the curriculum for one of my grad programs and I noticed there was a choice of 1 extra elective, and it was a choice between statistical learning, which uses ESL, Bayesian statistics, which uses BDA3, and then time series forecasting, which uses brockwell and Davis. 

Which of these do you think is the best course that would be the most useful?",11a487y,AdFew4357,1677175380.0,2,1.0,"['I would take Bayesian statistics or time series. There are limitless good machine learning resources derived from ESL out there for self teaching but learning Bayesian statistics will change the way you look at data while time series is a valuable specialty that is difficulty to self teach (at least imo)', ""Are you in a master's or Ph.D. program?"", 'Took all three and the Bayesian one was by far my fave. That being said, the other two were more immediately useful.', 'Definitely Bayesian statistics', ""Can't say much about the first two, but I had a time series course that followed that book and it was really nice and useful, so that one will always probably be a solid choice."", 'The program is an MS program', 'Statistical learning here isbasically machine learning.  I would go for that.']"
[Question] What information should I stay current for Data Science/Machine Learning?,"Ive studied a decent amount of Statistics alongside Computer Science. Namely, the basic intro stats class, Regression and Correlated data w/ R, and two Theory of Statistics classes based on Mathematical Statistics with Application by Wackerly.   
 

I always read that Computer Scientists should learn more statistics, especially if they are interested in data science/machine learning. So, as someone with some background, I was wondering what topics should I stay current on so that I am not an embarrassment to my people (jokingly)? I plan on doing a review of statistics and then diving into Machine Learning.",11a3m5f,rogerwavecap,1677173943.0,1,0.6,"['The main thing is to get at least a 400-level 2 semester, calc-based sequence.  It sounds like you have done that. Are you in a program, or online?', 'An easy starting point would be Introduction to Statistical Learning and Elements of Statistical Learning. Both are free and Im sure you can find lectures based them floating around.', ""For the stats I've already done, I was in a program. I am going to attempt using online material to learn more about Machine learning and other things. I don't remember many of the proofs from the 400-level theory courses, how important are those?"", 'You want to be facile at using the concepts.', ""Start with 'Elements of Statistical Learning'."", 'Is elements a good book to get entry into reading ML papers? How much are the concepts in think up to data with modern ML trends?', 'Think of it as entry level knowledge, the ante for getting into the game.', 'Gotcha thanks', 'A lot of  your work in the beginning is to get the background to learn more.', 'I agree. But one issue I have is after getting background, I struggle to know where to go next. Or what to learn next. \n\nThe issue right now for me is the field itself (statistics & ML) is so vast that I:\n\na) find a lot of things which are tangentially related interesting, resulting in a lot of random topics which I like and bounce between \n\nb) reach a certain point where I have read a lot of background, but then dont know where to go next.\n\nIm going to sit down and read ESLR. I really want to. I have gone through ISLR twice and the concepts are great, and I want to learn more thru ESLR.\n\nBut then after ESLR, I say, well what next? Then I have to go through the task of trying to find the exact niche I like. This is what happened to me after going through a chunk of the Gelman BDA3 book. I realize I like Bayesian statistics, I think heirarchical models are awesome, applied it to a dataset of my own, and then said, well, what next? Maybe I should also talk to more people too.', 'Talk with lots of people.']"
[Question] What percent of statistics are actually made up?," I know this is the go-to joke response, but does anyone know the actual percentage of statistics that are made up? Is there an accurate measure for this?

If you reply with a made-up statistic I will cry.",11a0ky2,lilboithiccysmalls,1677166586.0,0,0.38,"['This sub is about the field of statistics, not the factoids called statistics.\n\nIf you want a serious answer, you will need to define your population:\n\nfactoids cited in the media (which media?) \n\n+/- factoids cited by establishment figures (which kinds of establishment figures?) \n\n+/- ordinary people spouting off (on social media or also at work/in the pub?) \n\n+/- researchers being very bad/dishonest at their jobs (which researchers, and what jobs?) \n\n... etc etc.\n\nThen you will need to define what you mean by ""made up"". The vast majority of claims made are false but they\'re often based on a bad analysis of good data or a good analysis of incomplete data, not just pulled straight out of someone\'s arse.\n\nOnce you\'ve defined your terms, you will reaiise that neither the numerator nor the denominator are countable. And that it doesn\'t really matter. What matters is being able to distinguish between made up nonsense, unreliable analyses, and claims that might have a decent shot at getting us closer to the truth.', 'The number is not significant.', 'garbage in garbage out. Statistics are not made up, data are.', ""Who knows, its kinda a vague question.  What's worse is statistical models and measurements that are accurate, but are interpreted by people incorrectly.  A problem far more ubiquitous imo."", '69% \n\nI made that number up tho', '40%', 'About three fiddy', 'There are statistics on the amount of clinical trials with irreproducible results which is pretty close\n\nA minority of critical care practices with research published in high-profile journals were evaluated for reproducibility; less than half had reproducible effects. Of the 92 studies with a reproduction attempt, 34% replicated the effects (CI 2444%).\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5820784/', ""What's the statistical probability OP will cry?"", 'Haha very funny OP', ""I think Guideline 4 probably counts here:\n\n> *4\\. Just because it has a statistic in it doesn't make it statistics.*\n\nI expect there's no fruitful way to pursue this question, certainly not without narrowing the scope. Are you going to sample and factcheck everyone's conversations?"", '65% of all statistics are made up right there on the spot.', '80%', '100%, but please dont tell anyone we will all get fired', 'Yeah! Stats, bitch!', '@pharma', 'Billy Madison liked this comment', 'If the result is not reproduced, it does not necessarily mean the stats were made up. If I run 100 clinical trials, about 5 will be statistically significant even if there is no effect, with no data manipulation on my part.']"
[Q] Chi-square test ( Equal probability or equal length ),"Hi, I'm currently trying to figure out whether I should carry out an equal probability chi-square test or an equal length. I have a set of data that I want to test to check if it fits a lognormal distribution.   
I was told to minimize arbitrary choices when carrying out the test. After carrying out both tests, I found that I needed to combine cells when using the equal-length method as there were expected frequencies below the value of 5. Is this what the question meant by arbitrary? Secondly, my p-values were different for both methods but still reached the same conclusion (accepting the null hypothesis).",11a07vr,Masterkuze,1677165664.0,1,1.0,"[""> I'm currently trying to figure out whether I should carry out an equal probability chi-square test or an equal length. \n\nI think you're omitting some context here that you're assuming we'll be aware of.\n\nDo you mean that you're choosing to bin a continuous variable? Why would you do that?"", ""Because the data is continuous. It's not discrete. We are testing goodness of fit"", 'Okay, thanks, I see, the thing that was missing was \'I am doing a chi-squared *goodness of fit test* of a lognormal distribution.\'\n\nIt\'s conventional to call the size of bins by the term \'width\', rather than \'length\'. Between those two things I was a little confused.\n\nI\'ll give two different responses, depending on whether this is just ""I\'m doing this for a class exercise and I don\'t really care if it\'s a bad idea"" (go straight to answer 2, do not read answer 1), or whether it\'s ""I\'m trying to learn to do something sensible"" (go to answer 1 first, which explains a little of why this isn\'t in the class of more or less sensible things.)\n\n1. Treating it as real piece of statistical work:\n\n  *Yikes*. This is really not a great idea. \n\n  (a) How did you estimate the parameters of the lognormal so you can compute the bin probabilities? I presume you took logs and calculated sample mean and variance on the unbinned data, right? The problem is if you do it that way, *you no longer have a chi-squared test*. You can at best find upper and lower bounds on the p-value that way.\n\n  (That you don\'t have a chi-squared test has been understood for about 70 years, possibly considerably longer, but I\'d have to do some research to figure out exactly when. Certainly by the early 50s Chernoff and Lehmann were writing about the particular consequences on the test of estimation from unbinned data so at least that long)\n\n    (b) Even if you have a fully specified distribution, this has pretty *terrible* power for continuous distributions (and even for discrete distributions). \n\n     However, it\'s actually pretty powerful for the multinomial case it was designed for, if the sample size is decent; it can suffer from bias problems at small to medium sample sizes (or sometimes even surprisingly large sample sizes if the bin-probabilities are sufficiently unequal).\n\n   (c) If you really want to test lognormality, take logs and use a general test of normality that doesn\'t discretize the distribution (Chen-Shapiro is decent enough for an omnibus default, more specific advice depends on what sort of alternatives you most want power against. A Shapiro-Wilk is okay as a slightly weaker alternative if you can\'t do Chen-Shapiro).\n\n   (d) Final piece of actual statistical advice: even the more powerful goodness of fit tests are not particularly good ways to answer the sorts of questions people typically use goodness of fit tests for. In general there\'s something better to do.\n\n2. Alternatively, if this is a course where you just want to learn the mechanics needed to pass the subject ... then:\n\n   Equal probabilities is better than equal width (Mann and Wald, 1942), though naturally that advice assumes you\'re not doing parameter estimation. (They also gave advice on the number of equiprobable cells to use; I presume you chose alpha at 5% in which case you want roughly 1.88n^(2/5) bins. So roughly about 12 bins at n=100, vs about 20 bins at n=400. n=100 is on the low side though, power will be poor; I\'d hesitate to do anything less than 20 observations per bin and even that\'s not great. The power is low so you need a lot of data to have a good chance to pick much up.)', 'Hey sorry for the late reply but thank you for giving me a detailed response. Not sure If could reference a reddit post on my assignments haha', 'Indeed you should not use that comment\\* as a reference; but if you wanted a reference, why ask that way at all (which gave no hint of requiring a reference)?\n\nIf you required a reference, I gave an explicit reference in my comment; Mann and Wald 1942^([1]); an alternative would be the book by D\'Agostino and Stephens \n\n---\n\n[1] for which a quick google should get you to   \nMann, H.B. and Wald, A. (1942),  \n""On the choice of the number of class intervals in the application of the chisquare test"",   \n*Ann. Math. Statist.*, **13**, 306317\n\n---\n\n\\* (BTW did you read the subreddit guidelines, specifically the one about homework?)']"
[Q] Thoughts on pursuing statistics in the future?,"I (18m) am in my last year of college in the UK, about to sit my A-levels.

Going into college in 2021 i had absolutely no idea what I wanted to do for the future and almost arbitrarily chose my A-levels based off of subjects that seemed interesting. I ended up choosing Psychology, Economics, and Statistics.

Id always thoroughly enjoyed maths for most of primary/secondary school, even getting a grade 9 in GCSE maths (the highest grade possible for those unaware of the UK exam system). However, through a mix of an unlikeable maths teacher from ages 14-16, not knowing what I wanted to do, and perhaps pure laziness, I decided against choosing A-level maths, which I do now regret but theres no changing the past.

This was fine until I started my Statistics course and it has been far and away the most enjoyable course I take. I frequently get the top marks in my class on practice exams and I feel I have a good understanding of some statistical concepts ( https://imgur.com/a/l0Y2zfi for a picture of what the course involves).

Now, a few months away from finishing my A-levels, I decided I wanted to continue down the path of statistics and applied for 5 Universitys mathematics courses in the hopes of eventually developing into statistics along the way. This does mean that no matter which Uni I go to Ill have to complete a foundation year first, as a result of not having A-level maths, which I am honestly excited for! Ive received offers from all 5 to study there so its only a matter of choosing what would be the best for me.

I hope to continue to study and learn about statistics, it has been without a doubt the most fun Ive had learning in a long time. Potentially one day I could develop a career in a statistics field.

That is all I have to say, I just wanted to know if this sub had any thoughts about it and whether this could be a good route for me?

Thank you :)",119zhrk,Wantedandotr,1677163825.0,0,0.5,"['I study data science. Fancy, modern name for statistics with elements of programming. Its 80% math, statistics, data analysis etc. Now Im mid 30, based in UK, mid management lvl in private sector company working with broadly speaking data that can be sliced and diced to show myriad different KPIs (key performance indicators). Studying on-line. My view on statistics and data analysis is that private sector managers/directors even auditors are morons - they have other skills on broad how to run a business + I wanted to sound harsh. Its such a shame that so few ppl can wrap their heads round advanced statistical concepts that could help improve so many things in non high-tech, still big companies. It hurts me when ppl cant read % properly, and its a shame that with my adhd and allergy to very hierarchical and formal academic environment, I couldnt find place for myself and got stuck here. \n\nBut its not about me. Its about enormous gap between academics and real world making money places doing all sorts of activities to keep everyone going, but going against the research and scientific findings. In very many places, your degree is basically a proof that you are eloquent enough, and smart enough to learn your role in work environment from the bottom using very little of your learned core skills and its even when your role matches your degree. Similar with psychology when it comes to management/leadership positions. Good manager is not the one who can do one kind of job best, its someone who can piece together team that can do things beyond capacity and capability of a single individual no matter how smart that individual is. \n\nIn my opinion, usage of statistics in real world compared to its fantastic and powerful range of tools is like I dont know having a jet but travelling on a bike. On a bike not on foot cause bike is like % everyone is glued to % and average and median/frequency measures on big data sets causes blank stares and inability to act on it. Its always we need to keep it simple regardless time and resources wasted.\n\nIts my first post here so Id be glad to hear statisticians that are 1. not working for gov 2. were working for gov now are teaching and 3. other teachers, 4.  Are in sector by definition being created from statistics like I dont know commercial surveys/research. Are there any statisticians that managed to add value as analyst, actually using advanced statistics in business decisions making process? Im asking out of curiosity and to learn about other people views and experiences, for op, it could be good way of learning future career options and opportunities. Thanks if you managed to read it all:)']"
[Q] Kmeans VS Kmodes,"Hello everyone!

Im was wondering about the relationship between kmeans and kmodes. 

I have binary data where both 1 and 0 contain the same value. I can use simple matching coefficient (SMC) to obtain similarity. This could then be clustered using Kmeans.

Is this exactly what Kmodes does as well?

My question came to be since I need to get the similarity matrix anyway to calculate silhouette score. Since I already have it, running Kmeans directly seems appropriate rather than using an implementation of Kmodes. But this is only if my understanding of the algorithms is correct.

Thanks for the help in advance!",119v4rt,AnkanTV,1677150537.0,3,1.0,"[""From my understanding, k modes functions like k means but with the L1 distance, which makes the minizer of the criteria function modes instead of centroids. K modes is more robust against outliers and I think it is more used on categorical data, but both can be used.\n\nIn clustering there's rarely a definitive best way to proceed because it is used for exploration purposes and usually there is little information about our data. I would probably use K means by default and if you are worried of extreme values try k modes."", 'K-means is a clustering algorithm that partitions data points into K clusters based on their similarity to the centroid of each cluster. K-means assumes that the data points are continuous and normally distributed, which makes it suitable for numerical data. However, K-means can be less effective when dealing with categorical data, as there is no meaningful way to calculate the distance between categorical variables.\r  \n\r  \nK-modes is a clustering algorithm that is specifically designed to handle categorical data. K-modes replaces the mean of each cluster in K-means with the mode, which is the most frequent value of each categorical variable. K-modes calculates the distance between data points using a matching dissimilarity measure that counts the number of mismatches between categorical variables. K-modes can be more effective than K-means when dealing with categorical data.\r  \n\r  \nIn summary, K-means is a clustering algorithm that works well with numerical data, while K-modes is a clustering algorithm that is specifically designed for handling categorical data.']"
[Q]Any relation between a Jefferys uninformative prior and the Cramer-Rao lower bound?,"Im a fresh masters student and Ive noticed that the Cramer-Rao lower bound is the inverse of the Fisher Information, while the uninformative Jeffrys prior is proportional to the square root of the Fisher Information. These seem like completely separate topics to me. Is there some kind of intuitive link between these two? If there is please explain at the level of an advanced undergraduate. I find much of the posts of stack-exchange to be slightly too advanced for me.",119q81m,see_2_see,1677132218.0,42,0.98,"[""Other that what you already noticed, no.\n\nThe Jeffreys (note spelling) prior is derived from the change-of-variable theorem for distributions (the one with Jacobian determinants).   That is why multivariate  Jeffreys priors involve the square root of the determinant of the Fisher information matrix.  Since Fisher information transforms like a tensor under change of parameter, using this definition gives the Jeffreys property: the same recipe gives the same prior (transformed by change of variable) in all parameterizations.\n\nThe Cramer-Rao lower bound is about asymptotics of maximum likelihood and ultimately arises from differentiability under the integral sign argument that says the variance matrix of the first derivative vector of the log likelihood is minus the expectation of the second derivative matrix of the log likelihood, so the CLT applied to the first derivative of the log likelihood (score vector) has the Fisher information matrix as its asymptotic variance.  The inverse in the asymptotic variance for the MLE comes from solving (inverting) the likelihood equations to obtain MLE.  The Cramer-Rao lower bound (or the Hajek convolution theorem) says the MLE has lowest possible variance (under some assumptions), so that doesn't add anything to why inverse Fisher information in the first place.\n\nSo I don't see it.  Of course the reason why Fisher information transforms like a tensor and the differentiation under the integral sign equality do have something to do with each other.  So there is some connection, but I wouldn't call it intuitive."", 'Following this, would love to read answers.', ""They both make use of the Fisher information because both are related to the underlying geometry of a particular statistical model. The Jeffry's prior is related to the concept of a Haar measure on a statistical manifold, which is roughly like a uniform distribution over the parameter space, taking into account that the geometry of the parameter space (encoded by the Fisher information metric) may be different from that of ordinary Euclidean space. In the case of the Cramer-Rao bound, bounds for estimation accuracy are often related to the local geometry (e.g. the curvature) of a statistical model, and so they'll naturally relate to the Fisher information in some way.\n\nTLDR: The Fisher information determines the geometry of a particular statistical model, which means it appears in many different contexts related to the estimation of the parameters of the model."", 'Well no not really, but actually yes\n\nJeffreys\' prior is a type of uninformative prior that is invariant under reparameterization of the model. In other words, it is a prior that does not depend on the specific parameterization of the model and is only based on the geometry of the parameter space. Jeffreys\' prior is defined as the square root of the determinant of the Fisher information matrix, which is a measure of the curvature of the likelihood function.\n\nThe Cramr-Rao lower bound (CRLB) is a fundamental limit on the variance of any unbiased estimator of a parameter in a statistical model. The CRLB is based on the Fisher information matrix, which measures the amount of information that the data provide about the parameter of interest. The CRLB is the inverse of the Fisher information matrix and provides a lower bound on the variance of any unbiased estimator.\n\nThe relationship between Jeffreys\' prior and the CRLB is that the CRLB is equal to the inverse of the Fisher information under Jeffreys\' prior. This means that Jeffreys\' prior achieves the minimum variance bound for any unbiased estimator of the parameter of interest. In other words, Jeffreys\' prior is the ""best"" uninformative prior in terms of achieving the smallest possible variance for any unbiased estimator.\n\n&#x200B;\n\nSo sure it has a relatinship, but it might be a bit of a stretch', 'No.\n\nIf you look at the formulas it becomes clear as to why they can expressed as functions of Fisher Information.', 'Same', 'Is there anywhere you can point me to read more about this statistical geometry? Or a nice YouTube video perhaps?', ""Google Information Geometry. I have been out of the field for two decades so I don't have to-date links, but this topic is called that. It uses of differential geometry to study statistical models."", ""The field is called information geometry. I don't know of any accessible introductions which don't assume some background in differential geometry, measure theory, and a pretty advanced background in general statistics. Honestly, the educational literature on the subject is extremely poor all around, even if you do have a strong background.""]"
